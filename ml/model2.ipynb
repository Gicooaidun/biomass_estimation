{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S2_bands': {'B01': {'mean': 0.13021514, 'std': 0.017152175, 'min': 1e-04, 'max': 1.1213, 'p1': 0.1273, 'p99': 0.1074}, 'B02': {'mean': 0.1363337, 'std': 0.018509913, 'min': 1e-04, 'max': 1.8768, 'p1': 0.1366, 'p99': 0.1128}, 'B03': {'mean': 0.16427371, 'std': 0.02087248, 'min': 0.0411, 'max': 1.7888, 'p1': 0.1692, 'p99': 0.1364}, 'B04': {'mean': 0.13865142, 'std': 0.025569845, 'min': 0.0121, 'max': 1.7232, 'p1': 0.1445, 'p99': 0.1184}, 'B05': {'mean': 0.20296873, 'std': 0.028621713, 'min': 0.0672, 'max': 1.6344, 'p1': 0.2157, 'p99': 0.1591}, 'B06': {'mean': 0.38582557, 'std': 0.070499, 'min': 0.0758, 'max': 1.6699, 'p1': 0.3286, 'p99': 0.2766}, 'B07': {'mean': 0.4361872, 'std': 0.086211845, 'min': 0.0573, 'max': 1.6645, 'p1': 0.3621, 'p99': 0.2278}, 'B08': {'mean': 0.4448093, 'std': 0.08623231, 'min': 0.0737, 'max': 1.6976, 'p1': 0.3588, 'p99': 0.2122}, 'B8A': {'mean': 0.4580875, 'std': 0.08798952, 'min': 0.0772, 'max': 1.6709, 'p1': 0.3775, 'p99': 0.26}, 'B09': {'mean': 0.45806482, 'std': 0.08441155, 'min': 0.0066, 'max': 1.7178, 'p1': 0.3818, 'p99': 0.2655}, 'B11': {'mean': 0.2941879, 'std': 0.04741236, 'min': 0.0994, 'max': 1.5738, 'p1': 0.3279, 'p99': 0.5299}, 'B12': {'mean': 0.19771282, 'std': 0.0438055, 'min': 0.0974, 'max': 1.6086, 'p1': 0.22, 'p99': 0.1688}}, 'BM': {'bm': {'mean': 50.714764, 'std': 35.78637, 'min': 0.0, 'max': 312.03958, 'p1': 0.0, 'p99': 127.57817}, 'std': {'mean': 7.036375, 'std': 4.400113, 'min': 0.0, 'max': 145.97275, 'p1': 0.0, 'p99': 2.180725}}, 'Sentinel_metadata': {'S2_vegetation_score': {'mean': 96.60825, 'std': 11.659279, 'min': 20.0, 'max': 100.0, 'p1': 100.0, 'p99': 48.0}, 'S2_date': {'mean': 470.45947, 'std': 169.251, 'min': 1.0, 'max': 620.0, 'p1': 212.0, 'p99': 122.0}}, 'GEDI': {'agbd': {'mean': 77.14966, 'std': 79.39693, 'min': 0.66613775, 'max': 499.96765, 'p1': 1.0619253, 'p99': 75.21779}, 'agbd_se': {'mean': 7.2501493, 'std': 2.1052322, 'min': 2.981795, 'max': 12.131867, 'p1': 3.004118, 'p99': 11.066575}, 'rh98': {'mean': 12.344467, 'std': 9.239699, 'min': 0.030004844, 'max': 96.50001, 'p1': 2.9900095, 'p99': 33.88001}, 'date': {'mean': 394.3948, 'std': 149.13177, 'min': 46.0, 'max': 531.0, 'p1': 158.0, 'p99': 503.0}}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the file in binary mode for reading\n",
    "with open('data/normalization_values.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Now you can analyze the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training tiles:  45\n",
      "validation tiles:  10\n",
      "testing tiles:  15\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "data = {'train': [], 'val': [], 'test': []} \n",
    "path_h5 = '/scratch2/biomass_estimation/code/ml/data'\n",
    "\n",
    "# Iterate over all the h5 files\n",
    "for fname in os.listdir(path_h5):\n",
    "    if fname.endswith('.h5'):\n",
    "        with h5py.File(os.path.join(path_h5, fname), 'r') as f:\n",
    "            # Get the list of all tiles in the file\n",
    "            all_tiles = list(f.keys())\n",
    "            \n",
    "            # Select one tile for validation, one for testing, and the rest for training\n",
    "            val_tile = all_tiles[0:2]\n",
    "            test_tile = all_tiles[2:5]\n",
    "            train_tiles = all_tiles[5:]\n",
    "            \n",
    "            # Add the selected tiles to the dictionary\n",
    "            data['val'].extend(val_tile)\n",
    "            data['test'].extend(test_tile)\n",
    "            data['train'].extend(train_tiles)\n",
    "\n",
    "print(\"training tiles: \", len(data['train']))\n",
    "print(\"validation tiles: \", len(data['val']))\n",
    "print(\"testing tiles: \", len(data['test']))\n",
    "# Pickle the DataFrame and save it to a file\n",
    "with open('/scratch2/biomass_estimation/code/ml/data/mapping2.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n"
     ]
    }
   ],
   "source": [
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=18,\n",
    "                 channel_dims = (16, 32, 64, 128),\n",
    "                 num_outputs=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=1):\n",
    "        \"\"\"\n",
    "        A simple fully convolutional neural network.\n",
    "        \"\"\"\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        layers = list()\n",
    "        for i in range(len(channel_dims)):\n",
    "            in_channels = in_features if i == 0 else channel_dims[i-1]\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=channel_dims[i], \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(num_features=channel_dims[i]))\n",
    "            layers.append(self.relu)\n",
    "        print(layers)\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(in_channels=channel_dims[-1], out_channels=num_outputs, kernel_size=1,\n",
    "                                     stride=1, padding=0, bias=True)\n",
    "        # self.fc = nn.Linear(15*15*num_outputs, 1)  # Fully connected layer to get a single output value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_output(x)\n",
    "        # x = x.flatten(start_dim=1)\n",
    "        # predictions = self.fc(x)\n",
    "        # return predictions.squeeze()  # Remove the extra dimension\n",
    "        return x\n",
    "    \n",
    "model = SimpleFCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n",
      "Epoch 1 \t Batch 20 \t Training Loss: 109.57520904541016\n",
      "Epoch 1 \t Batch 40 \t Training Loss: 98.65093517303467\n",
      "Epoch 1 \t Batch 60 \t Training Loss: 87.56623617808025\n",
      "Epoch 1 \t Batch 80 \t Training Loss: 79.36099495887757\n",
      "Epoch 1 \t Batch 100 \t Training Loss: 74.23469905853271\n",
      "Epoch 1 \t Batch 120 \t Training Loss: 70.86601209640503\n",
      "Epoch 1 \t Batch 140 \t Training Loss: 68.3813807623727\n",
      "Epoch 1 \t Batch 160 \t Training Loss: 66.60697207450866\n",
      "Epoch 1 \t Batch 180 \t Training Loss: 65.04681466420492\n",
      "Epoch 1 \t Batch 200 \t Training Loss: 63.97086687088013\n",
      "Epoch 1 \t Batch 220 \t Training Loss: 63.11709398789839\n",
      "Epoch 1 \t Batch 240 \t Training Loss: 62.223071082433066\n",
      "Epoch 1 \t Batch 260 \t Training Loss: 61.63299022087684\n",
      "Epoch 1 \t Batch 280 \t Training Loss: 60.87944984436035\n",
      "Epoch 1 \t Batch 300 \t Training Loss: 60.303937962849936\n",
      "Epoch 1 \t Batch 320 \t Training Loss: 59.74504283666611\n",
      "Epoch 1 \t Batch 340 \t Training Loss: 59.41683048921473\n",
      "Epoch 1 \t Batch 360 \t Training Loss: 59.0484724468655\n",
      "Epoch 1 \t Batch 380 \t Training Loss: 58.61995270377711\n",
      "Epoch 1 \t Batch 400 \t Training Loss: 58.247486667633055\n",
      "Epoch 1 \t Batch 420 \t Training Loss: 57.84358367011661\n",
      "Epoch 1 \t Batch 440 \t Training Loss: 57.64971233714711\n",
      "Epoch 1 \t Batch 460 \t Training Loss: 57.42951106195864\n",
      "Epoch 1 \t Batch 480 \t Training Loss: 57.20460251967112\n",
      "Epoch 1 \t Batch 500 \t Training Loss: 57.003876945495605\n",
      "Epoch 1 \t Batch 520 \t Training Loss: 56.801849937438966\n",
      "Epoch 1 \t Batch 540 \t Training Loss: 56.598744540744356\n",
      "Epoch 1 \t Batch 560 \t Training Loss: 56.407788794381275\n",
      "Epoch 1 \t Batch 580 \t Training Loss: 56.19825177685968\n",
      "Epoch 1 \t Batch 600 \t Training Loss: 56.029460741678875\n",
      "Epoch 1 \t Batch 620 \t Training Loss: 55.834162416765764\n",
      "Epoch 1 \t Batch 640 \t Training Loss: 55.771957302093504\n",
      "Epoch 1 \t Batch 660 \t Training Loss: 55.6474941369259\n",
      "Epoch 1 \t Batch 680 \t Training Loss: 55.51822517058429\n",
      "Epoch 1 \t Batch 700 \t Training Loss: 55.43650466373989\n",
      "Epoch 1 \t Batch 720 \t Training Loss: 55.36187665197584\n",
      "Epoch 1 \t Batch 740 \t Training Loss: 55.19933681488037\n",
      "Epoch 1 \t Batch 760 \t Training Loss: 55.12880717829654\n",
      "Epoch 1 \t Batch 780 \t Training Loss: 55.04160992059952\n",
      "Epoch 1 \t Batch 800 \t Training Loss: 54.9794099521637\n",
      "Epoch 1 \t Batch 820 \t Training Loss: 54.92809210056212\n",
      "Epoch 1 \t Batch 840 \t Training Loss: 54.85022682916551\n",
      "Epoch 1 \t Batch 860 \t Training Loss: 54.75754444654598\n",
      "Epoch 1 \t Batch 880 \t Training Loss: 54.691535065390845\n",
      "Epoch 1 \t Batch 900 \t Training Loss: 54.61909531487359\n",
      "Epoch 1 \t Batch 20 \t Validation Loss: 18.061865234375\n",
      "Epoch 1 \t Batch 40 \t Validation Loss: 21.796929121017456\n",
      "Epoch 1 \t Batch 60 \t Validation Loss: 20.911891078948976\n",
      "Epoch 1 \t Batch 80 \t Validation Loss: 21.685929977893828\n",
      "Epoch 1 \t Batch 100 \t Validation Loss: 23.084184885025024\n",
      "Epoch 1 \t Batch 120 \t Validation Loss: 24.280981826782227\n",
      "Epoch 1 \t Batch 140 \t Validation Loss: 24.97482376098633\n",
      "Epoch 1 \t Batch 160 \t Validation Loss: 27.32661578655243\n",
      "Epoch 1 \t Batch 180 \t Validation Loss: 31.002242845959135\n",
      "Epoch 1 \t Batch 200 \t Validation Loss: 32.682817063331605\n",
      "Epoch 1 \t Batch 220 \t Validation Loss: 34.17231857559898\n",
      "Epoch 1 \t Batch 240 \t Validation Loss: 34.735348220666246\n",
      "Epoch 1 \t Batch 260 \t Validation Loss: 36.873254104760974\n",
      "Epoch 1 \t Batch 280 \t Validation Loss: 38.101703139713834\n",
      "Epoch 1 \t Batch 300 \t Validation Loss: 39.25011612574259\n",
      "Epoch 1 \t Batch 320 \t Validation Loss: 39.78404904007912\n",
      "Epoch 1 \t Batch 340 \t Validation Loss: 39.82742430743049\n",
      "Epoch 1 \t Batch 360 \t Validation Loss: 39.868631760279335\n",
      "Epoch 1 \t Batch 380 \t Validation Loss: 40.238424376437536\n",
      "Epoch 1 \t Batch 400 \t Validation Loss: 40.14512995243072\n",
      "Epoch 1 \t Batch 420 \t Validation Loss: 40.26741263979957\n",
      "Epoch 1 \t Batch 440 \t Validation Loss: 40.23571091565219\n",
      "Epoch 1 \t Batch 460 \t Validation Loss: 40.578593320431914\n",
      "Epoch 1 \t Batch 480 \t Validation Loss: 41.116674002011614\n",
      "Epoch 1 \t Batch 500 \t Validation Loss: 40.873489501953124\n",
      "Epoch 1 \t Batch 520 \t Validation Loss: 40.91490913537832\n",
      "Epoch 1 \t Batch 540 \t Validation Loss: 40.719004249572755\n",
      "Epoch 1 \t Batch 560 \t Validation Loss: 40.53223176343101\n",
      "Epoch 1 \t Batch 580 \t Validation Loss: 40.41516174119094\n",
      "Epoch 1 \t Batch 600 \t Validation Loss: 40.588112179438276\n",
      "Epoch 1 Training Loss: 54.51791303212406 Validation Loss: 41.26226294195497\n",
      "Validation Loss Decreased(inf--->25417.553972244263) Saving The Model\n",
      "Epoch 1 completed\n",
      "Epoch 2 \t Batch 20 \t Training Loss: 51.01539897918701\n",
      "Epoch 2 \t Batch 40 \t Training Loss: 50.55330514907837\n",
      "Epoch 2 \t Batch 60 \t Training Loss: 50.57772185007731\n",
      "Epoch 2 \t Batch 80 \t Training Loss: 50.76990280151367\n",
      "Epoch 2 \t Batch 100 \t Training Loss: 50.986930809021\n",
      "Epoch 2 \t Batch 120 \t Training Loss: 51.00486167271932\n",
      "Epoch 2 \t Batch 140 \t Training Loss: 51.06539944240025\n",
      "Epoch 2 \t Batch 160 \t Training Loss: 51.0060877084732\n",
      "Epoch 2 \t Batch 180 \t Training Loss: 50.94574794769287\n",
      "Epoch 2 \t Batch 200 \t Training Loss: 50.88050706863403\n",
      "Epoch 2 \t Batch 220 \t Training Loss: 50.901433459195225\n",
      "Epoch 2 \t Batch 240 \t Training Loss: 50.88896112442016\n",
      "Epoch 2 \t Batch 260 \t Training Loss: 50.98375354179969\n",
      "Epoch 2 \t Batch 280 \t Training Loss: 50.917529351370675\n",
      "Epoch 2 \t Batch 300 \t Training Loss: 50.9890798441569\n",
      "Epoch 2 \t Batch 320 \t Training Loss: 50.96009111404419\n",
      "Epoch 2 \t Batch 340 \t Training Loss: 50.879039472692156\n",
      "Epoch 2 \t Batch 360 \t Training Loss: 50.85832824707031\n",
      "Epoch 2 \t Batch 380 \t Training Loss: 50.834742977744654\n",
      "Epoch 2 \t Batch 400 \t Training Loss: 50.84794155120849\n",
      "Epoch 2 \t Batch 420 \t Training Loss: 50.83037015824091\n",
      "Epoch 2 \t Batch 440 \t Training Loss: 50.82929789803245\n",
      "Epoch 2 \t Batch 460 \t Training Loss: 50.82639649432638\n",
      "Epoch 2 \t Batch 480 \t Training Loss: 50.75446339448293\n",
      "Epoch 2 \t Batch 500 \t Training Loss: 50.827914215087894\n",
      "Epoch 2 \t Batch 520 \t Training Loss: 50.81452476794903\n",
      "Epoch 2 \t Batch 540 \t Training Loss: 50.85632203420003\n",
      "Epoch 2 \t Batch 560 \t Training Loss: 50.84048295702253\n",
      "Epoch 2 \t Batch 580 \t Training Loss: 50.81743258772225\n",
      "Epoch 2 \t Batch 600 \t Training Loss: 50.81923540751139\n",
      "Epoch 2 \t Batch 620 \t Training Loss: 50.81405492598011\n",
      "Epoch 2 \t Batch 640 \t Training Loss: 50.773215126991275\n",
      "Epoch 2 \t Batch 660 \t Training Loss: 50.736773346409656\n",
      "Epoch 2 \t Batch 680 \t Training Loss: 50.70466481938082\n",
      "Epoch 2 \t Batch 700 \t Training Loss: 50.71245365687779\n",
      "Epoch 2 \t Batch 720 \t Training Loss: 50.682223632600575\n",
      "Epoch 2 \t Batch 740 \t Training Loss: 50.71193779610299\n",
      "Epoch 2 \t Batch 760 \t Training Loss: 50.68893636904265\n",
      "Epoch 2 \t Batch 780 \t Training Loss: 50.66335409115522\n",
      "Epoch 2 \t Batch 800 \t Training Loss: 50.63344349861145\n",
      "Epoch 2 \t Batch 820 \t Training Loss: 50.57880994750232\n",
      "Epoch 2 \t Batch 840 \t Training Loss: 50.58429362887428\n",
      "Epoch 2 \t Batch 860 \t Training Loss: 50.570483300852224\n",
      "Epoch 2 \t Batch 880 \t Training Loss: 50.562782408974385\n",
      "Epoch 2 \t Batch 900 \t Training Loss: 50.54102403004964\n",
      "Epoch 2 \t Batch 20 \t Validation Loss: 15.705199003219604\n",
      "Epoch 2 \t Batch 40 \t Validation Loss: 19.030930519104004\n",
      "Epoch 2 \t Batch 60 \t Validation Loss: 18.75674165089925\n",
      "Epoch 2 \t Batch 80 \t Validation Loss: 20.00480818748474\n",
      "Epoch 2 \t Batch 100 \t Validation Loss: 21.294930877685548\n",
      "Epoch 2 \t Batch 120 \t Validation Loss: 22.693019088109335\n",
      "Epoch 2 \t Batch 140 \t Validation Loss: 23.322762959344047\n",
      "Epoch 2 \t Batch 160 \t Validation Loss: 25.515137082338335\n",
      "Epoch 2 \t Batch 180 \t Validation Loss: 28.755476114485\n",
      "Epoch 2 \t Batch 200 \t Validation Loss: 30.017433958053587\n",
      "Epoch 2 \t Batch 220 \t Validation Loss: 31.322794463417747\n",
      "Epoch 2 \t Batch 240 \t Validation Loss: 31.815972407658894\n",
      "Epoch 2 \t Batch 260 \t Validation Loss: 33.78443249188937\n",
      "Epoch 2 \t Batch 280 \t Validation Loss: 34.83988759177072\n",
      "Epoch 2 \t Batch 300 \t Validation Loss: 35.92829226175944\n",
      "Epoch 2 \t Batch 320 \t Validation Loss: 36.465224117040634\n",
      "Epoch 2 \t Batch 340 \t Validation Loss: 36.51470769433414\n",
      "Epoch 2 \t Batch 360 \t Validation Loss: 36.500353044933746\n",
      "Epoch 2 \t Batch 380 \t Validation Loss: 36.8592807067068\n",
      "Epoch 2 \t Batch 400 \t Validation Loss: 36.60003620147705\n",
      "Epoch 2 \t Batch 420 \t Validation Loss: 36.755443332308815\n",
      "Epoch 2 \t Batch 440 \t Validation Loss: 36.593007278442386\n",
      "Epoch 2 \t Batch 460 \t Validation Loss: 36.875042591924256\n",
      "Epoch 2 \t Batch 480 \t Validation Loss: 37.439397331078844\n",
      "Epoch 2 \t Batch 500 \t Validation Loss: 37.221586555480954\n",
      "Epoch 2 \t Batch 520 \t Validation Loss: 37.06378234899961\n",
      "Epoch 2 \t Batch 540 \t Validation Loss: 36.79990945922004\n",
      "Epoch 2 \t Batch 560 \t Validation Loss: 36.590107306412285\n",
      "Epoch 2 \t Batch 580 \t Validation Loss: 36.33903666693589\n",
      "Epoch 2 \t Batch 600 \t Validation Loss: 36.555276087125144\n",
      "Epoch 2 Training Loss: 50.54404423255047 Validation Loss: 37.18388895400159\n",
      "Validation Loss Decreased(25417.553972244263--->22905.275595664978) Saving The Model\n",
      "Epoch 2 completed\n",
      "Epoch 3 \t Batch 20 \t Training Loss: 49.53908901214599\n",
      "Epoch 3 \t Batch 40 \t Training Loss: 49.17517004013062\n",
      "Epoch 3 \t Batch 60 \t Training Loss: 49.323702557881674\n",
      "Epoch 3 \t Batch 80 \t Training Loss: 49.20522723197937\n",
      "Epoch 3 \t Batch 100 \t Training Loss: 49.39608798980713\n",
      "Epoch 3 \t Batch 120 \t Training Loss: 49.61047910054525\n",
      "Epoch 3 \t Batch 140 \t Training Loss: 49.561306953430176\n",
      "Epoch 3 \t Batch 160 \t Training Loss: 49.70105996131897\n",
      "Epoch 3 \t Batch 180 \t Training Loss: 49.89092239803738\n",
      "Epoch 3 \t Batch 200 \t Training Loss: 49.92436405181885\n",
      "Epoch 3 \t Batch 220 \t Training Loss: 49.83977588306774\n",
      "Epoch 3 \t Batch 240 \t Training Loss: 49.82702059745789\n",
      "Epoch 3 \t Batch 260 \t Training Loss: 49.69244229243352\n",
      "Epoch 3 \t Batch 280 \t Training Loss: 49.792218003954204\n",
      "Epoch 3 \t Batch 300 \t Training Loss: 49.76724798838298\n",
      "Epoch 3 \t Batch 320 \t Training Loss: 49.79983098506928\n",
      "Epoch 3 \t Batch 340 \t Training Loss: 49.855209036434395\n",
      "Epoch 3 \t Batch 360 \t Training Loss: 49.80837917327881\n",
      "Epoch 3 \t Batch 380 \t Training Loss: 49.88291897020842\n",
      "Epoch 3 \t Batch 400 \t Training Loss: 49.83047795295715\n",
      "Epoch 3 \t Batch 420 \t Training Loss: 49.91988308316186\n",
      "Epoch 3 \t Batch 440 \t Training Loss: 49.904021349820226\n",
      "Epoch 3 \t Batch 460 \t Training Loss: 49.92546534330948\n",
      "Epoch 3 \t Batch 480 \t Training Loss: 49.94215605258942\n",
      "Epoch 3 \t Batch 500 \t Training Loss: 49.899780548095706\n",
      "Epoch 3 \t Batch 520 \t Training Loss: 49.95398464936476\n",
      "Epoch 3 \t Batch 540 \t Training Loss: 49.94558610562925\n",
      "Epoch 3 \t Batch 560 \t Training Loss: 50.02690275056022\n",
      "Epoch 3 \t Batch 580 \t Training Loss: 50.02589939380514\n",
      "Epoch 3 \t Batch 600 \t Training Loss: 49.94630326588948\n",
      "Epoch 3 \t Batch 620 \t Training Loss: 49.94017179550663\n",
      "Epoch 3 \t Batch 640 \t Training Loss: 49.93672273755074\n",
      "Epoch 3 \t Batch 660 \t Training Loss: 49.95885029417096\n",
      "Epoch 3 \t Batch 680 \t Training Loss: 49.90917382520788\n",
      "Epoch 3 \t Batch 700 \t Training Loss: 49.93955093383789\n",
      "Epoch 3 \t Batch 720 \t Training Loss: 49.94915910826789\n",
      "Epoch 3 \t Batch 740 \t Training Loss: 49.97873486183785\n",
      "Epoch 3 \t Batch 760 \t Training Loss: 49.985204531017104\n",
      "Epoch 3 \t Batch 780 \t Training Loss: 49.98682809487367\n",
      "Epoch 3 \t Batch 800 \t Training Loss: 49.99403676509857\n",
      "Epoch 3 \t Batch 820 \t Training Loss: 49.976605178088676\n",
      "Epoch 3 \t Batch 840 \t Training Loss: 49.96802978969755\n",
      "Epoch 3 \t Batch 860 \t Training Loss: 49.92831863580748\n",
      "Epoch 3 \t Batch 880 \t Training Loss: 49.94380499666387\n",
      "Epoch 3 \t Batch 900 \t Training Loss: 49.895801845126684\n",
      "Epoch 3 \t Batch 20 \t Validation Loss: 13.861097860336304\n",
      "Epoch 3 \t Batch 40 \t Validation Loss: 18.169169306755066\n",
      "Epoch 3 \t Batch 60 \t Validation Loss: 17.489597098032633\n",
      "Epoch 3 \t Batch 80 \t Validation Loss: 18.40579310655594\n",
      "Epoch 3 \t Batch 100 \t Validation Loss: 20.296888751983644\n",
      "Epoch 3 \t Batch 120 \t Validation Loss: 21.93411139647166\n",
      "Epoch 3 \t Batch 140 \t Validation Loss: 22.856242758887156\n",
      "Epoch 3 \t Batch 160 \t Validation Loss: 25.450980800390244\n",
      "Epoch 3 \t Batch 180 \t Validation Loss: 29.608741394678752\n",
      "Epoch 3 \t Batch 200 \t Validation Loss: 31.500421233177185\n",
      "Epoch 3 \t Batch 220 \t Validation Loss: 33.335279659791425\n",
      "Epoch 3 \t Batch 240 \t Validation Loss: 34.09942732254664\n",
      "Epoch 3 \t Batch 260 \t Validation Loss: 36.49892021325918\n",
      "Epoch 3 \t Batch 280 \t Validation Loss: 37.8164224760873\n",
      "Epoch 3 \t Batch 300 \t Validation Loss: 39.11903874715169\n",
      "Epoch 3 \t Batch 320 \t Validation Loss: 39.72321600317955\n",
      "Epoch 3 \t Batch 340 \t Validation Loss: 39.72685361189001\n",
      "Epoch 3 \t Batch 360 \t Validation Loss: 39.69570353031158\n",
      "Epoch 3 \t Batch 380 \t Validation Loss: 40.09769151336268\n",
      "Epoch 3 \t Batch 400 \t Validation Loss: 39.81334233522415\n",
      "Epoch 3 \t Batch 420 \t Validation Loss: 39.83459920201983\n",
      "Epoch 3 \t Batch 440 \t Validation Loss: 39.64624058766798\n",
      "Epoch 3 \t Batch 460 \t Validation Loss: 39.86032697221507\n",
      "Epoch 3 \t Batch 480 \t Validation Loss: 40.37778056263924\n",
      "Epoch 3 \t Batch 500 \t Validation Loss: 40.07775405311585\n",
      "Epoch 3 \t Batch 520 \t Validation Loss: 39.99650965287135\n",
      "Epoch 3 \t Batch 540 \t Validation Loss: 39.69515063497755\n",
      "Epoch 3 \t Batch 560 \t Validation Loss: 39.423458138534\n",
      "Epoch 3 \t Batch 580 \t Validation Loss: 39.188966469929134\n",
      "Epoch 3 \t Batch 600 \t Validation Loss: 39.329656624794005\n",
      "Epoch 3 Training Loss: 49.901587366667805 Validation Loss: 39.92194351437804\n",
      "Epoch 3 completed\n",
      "Epoch 4 \t Batch 20 \t Training Loss: 48.925818252563474\n",
      "Epoch 4 \t Batch 40 \t Training Loss: 49.47777452468872\n",
      "Epoch 4 \t Batch 60 \t Training Loss: 49.45300013224284\n",
      "Epoch 4 \t Batch 80 \t Training Loss: 49.604903650283816\n",
      "Epoch 4 \t Batch 100 \t Training Loss: 49.2898261642456\n",
      "Epoch 4 \t Batch 120 \t Training Loss: 49.29211813608806\n",
      "Epoch 4 \t Batch 140 \t Training Loss: 49.14243937901088\n",
      "Epoch 4 \t Batch 160 \t Training Loss: 49.21960232257843\n",
      "Epoch 4 \t Batch 180 \t Training Loss: 49.056498273213705\n",
      "Epoch 4 \t Batch 200 \t Training Loss: 49.103015995025636\n",
      "Epoch 4 \t Batch 220 \t Training Loss: 49.304539819197224\n",
      "Epoch 4 \t Batch 240 \t Training Loss: 49.337588993708295\n",
      "Epoch 4 \t Batch 260 \t Training Loss: 49.40361407353328\n",
      "Epoch 4 \t Batch 280 \t Training Loss: 49.36529688153948\n",
      "Epoch 4 \t Batch 300 \t Training Loss: 49.475777537027994\n",
      "Epoch 4 \t Batch 320 \t Training Loss: 49.47120704650879\n",
      "Epoch 4 \t Batch 340 \t Training Loss: 49.50997593823601\n",
      "Epoch 4 \t Batch 360 \t Training Loss: 49.46904645495945\n",
      "Epoch 4 \t Batch 380 \t Training Loss: 49.523624831751775\n",
      "Epoch 4 \t Batch 400 \t Training Loss: 49.52764269828796\n",
      "Epoch 4 \t Batch 420 \t Training Loss: 49.49912828717913\n",
      "Epoch 4 \t Batch 440 \t Training Loss: 49.493258120796895\n",
      "Epoch 4 \t Batch 460 \t Training Loss: 49.447873820429265\n",
      "Epoch 4 \t Batch 480 \t Training Loss: 49.49132806460063\n",
      "Epoch 4 \t Batch 500 \t Training Loss: 49.48434564971924\n",
      "Epoch 4 \t Batch 520 \t Training Loss: 49.558372813004716\n",
      "Epoch 4 \t Batch 540 \t Training Loss: 49.54342591321027\n",
      "Epoch 4 \t Batch 560 \t Training Loss: 49.53159704208374\n",
      "Epoch 4 \t Batch 580 \t Training Loss: 49.52744078142889\n",
      "Epoch 4 \t Batch 600 \t Training Loss: 49.53114475250244\n",
      "Epoch 4 \t Batch 620 \t Training Loss: 49.503403983577606\n",
      "Epoch 4 \t Batch 640 \t Training Loss: 49.50575121045112\n",
      "Epoch 4 \t Batch 660 \t Training Loss: 49.499673987879895\n",
      "Epoch 4 \t Batch 680 \t Training Loss: 49.516217972250544\n",
      "Epoch 4 \t Batch 700 \t Training Loss: 49.54354869842529\n",
      "Epoch 4 \t Batch 720 \t Training Loss: 49.54281838734945\n",
      "Epoch 4 \t Batch 740 \t Training Loss: 49.55809758160565\n",
      "Epoch 4 \t Batch 760 \t Training Loss: 49.532116518522564\n",
      "Epoch 4 \t Batch 780 \t Training Loss: 49.57847435780061\n",
      "Epoch 4 \t Batch 800 \t Training Loss: 49.60441335678101\n",
      "Epoch 4 \t Batch 820 \t Training Loss: 49.59086968491717\n",
      "Epoch 4 \t Batch 840 \t Training Loss: 49.57989611398606\n",
      "Epoch 4 \t Batch 860 \t Training Loss: 49.585584449768064\n",
      "Epoch 4 \t Batch 880 \t Training Loss: 49.58325923139399\n",
      "Epoch 4 \t Batch 900 \t Training Loss: 49.5613343556722\n",
      "Epoch 4 \t Batch 20 \t Validation Loss: 15.704887390136719\n",
      "Epoch 4 \t Batch 40 \t Validation Loss: 20.320668601989745\n",
      "Epoch 4 \t Batch 60 \t Validation Loss: 19.308760945002238\n",
      "Epoch 4 \t Batch 80 \t Validation Loss: 20.163615143299104\n",
      "Epoch 4 \t Batch 100 \t Validation Loss: 21.69123243331909\n",
      "Epoch 4 \t Batch 120 \t Validation Loss: 23.07687493165334\n",
      "Epoch 4 \t Batch 140 \t Validation Loss: 23.93024127823966\n",
      "Epoch 4 \t Batch 160 \t Validation Loss: 26.486195546388625\n",
      "Epoch 4 \t Batch 180 \t Validation Loss: 30.14807178179423\n",
      "Epoch 4 \t Batch 200 \t Validation Loss: 31.7096279668808\n",
      "Epoch 4 \t Batch 220 \t Validation Loss: 33.208305397900666\n",
      "Epoch 4 \t Batch 240 \t Validation Loss: 33.867570372422534\n",
      "Epoch 4 \t Batch 260 \t Validation Loss: 35.93611192703247\n",
      "Epoch 4 \t Batch 280 \t Validation Loss: 37.00533266408103\n",
      "Epoch 4 \t Batch 300 \t Validation Loss: 38.30635038057963\n",
      "Epoch 4 \t Batch 320 \t Validation Loss: 38.95565157234669\n",
      "Epoch 4 \t Batch 340 \t Validation Loss: 39.06110355994281\n",
      "Epoch 4 \t Batch 360 \t Validation Loss: 39.16668006314172\n",
      "Epoch 4 \t Batch 380 \t Validation Loss: 39.566942423268365\n",
      "Epoch 4 \t Batch 400 \t Validation Loss: 39.49365170240402\n",
      "Epoch 4 \t Batch 420 \t Validation Loss: 39.645090182622276\n",
      "Epoch 4 \t Batch 440 \t Validation Loss: 39.658147159489715\n",
      "Epoch 4 \t Batch 460 \t Validation Loss: 39.988163141582326\n",
      "Epoch 4 \t Batch 480 \t Validation Loss: 40.57060652772586\n",
      "Epoch 4 \t Batch 500 \t Validation Loss: 40.36090088462829\n",
      "Epoch 4 \t Batch 520 \t Validation Loss: 40.45447924137115\n",
      "Epoch 4 \t Batch 540 \t Validation Loss: 40.42767750245554\n",
      "Epoch 4 \t Batch 560 \t Validation Loss: 40.47760288204466\n",
      "Epoch 4 \t Batch 580 \t Validation Loss: 40.62922694272009\n",
      "Epoch 4 \t Batch 600 \t Validation Loss: 40.94123093763987\n",
      "Epoch 4 Training Loss: 49.542282728488466 Validation Loss: 41.672774239019915\n",
      "Epoch 4 completed\n",
      "Epoch 5 \t Batch 20 \t Training Loss: 49.90452766418457\n",
      "Epoch 5 \t Batch 40 \t Training Loss: 49.596871089935306\n",
      "Epoch 5 \t Batch 60 \t Training Loss: 49.4450039545695\n",
      "Epoch 5 \t Batch 80 \t Training Loss: 49.380281162261966\n",
      "Epoch 5 \t Batch 100 \t Training Loss: 49.48950065612793\n",
      "Epoch 5 \t Batch 120 \t Training Loss: 49.488877550760904\n",
      "Epoch 5 \t Batch 140 \t Training Loss: 49.39611535753522\n",
      "Epoch 5 \t Batch 160 \t Training Loss: 49.46680474281311\n",
      "Epoch 5 \t Batch 180 \t Training Loss: 49.35483057234023\n",
      "Epoch 5 \t Batch 200 \t Training Loss: 49.4165353012085\n",
      "Epoch 5 \t Batch 220 \t Training Loss: 49.292882000316276\n",
      "Epoch 5 \t Batch 240 \t Training Loss: 49.39653353691101\n",
      "Epoch 5 \t Batch 260 \t Training Loss: 49.381885763315054\n",
      "Epoch 5 \t Batch 280 \t Training Loss: 49.39257149015154\n",
      "Epoch 5 \t Batch 300 \t Training Loss: 49.502496045430505\n",
      "Epoch 5 \t Batch 320 \t Training Loss: 49.52272762060166\n",
      "Epoch 5 \t Batch 340 \t Training Loss: 49.59076540329877\n",
      "Epoch 5 \t Batch 360 \t Training Loss: 49.45669322543674\n",
      "Epoch 5 \t Batch 380 \t Training Loss: 49.53951545514558\n",
      "Epoch 5 \t Batch 400 \t Training Loss: 49.60310910224914\n",
      "Epoch 5 \t Batch 420 \t Training Loss: 49.54513014838809\n",
      "Epoch 5 \t Batch 440 \t Training Loss: 49.53071250915527\n",
      "Epoch 5 \t Batch 460 \t Training Loss: 49.53422657510509\n",
      "Epoch 5 \t Batch 480 \t Training Loss: 49.4904435078303\n",
      "Epoch 5 \t Batch 500 \t Training Loss: 49.41361767578125\n",
      "Epoch 5 \t Batch 520 \t Training Loss: 49.34139841519869\n",
      "Epoch 5 \t Batch 540 \t Training Loss: 49.36887659849944\n",
      "Epoch 5 \t Batch 560 \t Training Loss: 49.333782938548495\n",
      "Epoch 5 \t Batch 580 \t Training Loss: 49.34170185615277\n",
      "Epoch 5 \t Batch 600 \t Training Loss: 49.359134171803795\n",
      "Epoch 5 \t Batch 620 \t Training Loss: 49.33813093862226\n",
      "Epoch 5 \t Batch 640 \t Training Loss: 49.34217373132706\n",
      "Epoch 5 \t Batch 660 \t Training Loss: 49.2990426381429\n",
      "Epoch 5 \t Batch 680 \t Training Loss: 49.30766560610603\n",
      "Epoch 5 \t Batch 700 \t Training Loss: 49.26926769256592\n",
      "Epoch 5 \t Batch 720 \t Training Loss: 49.23254255188836\n",
      "Epoch 5 \t Batch 740 \t Training Loss: 49.28707155279211\n",
      "Epoch 5 \t Batch 760 \t Training Loss: 49.25562010313335\n",
      "Epoch 5 \t Batch 780 \t Training Loss: 49.235752081259704\n",
      "Epoch 5 \t Batch 800 \t Training Loss: 49.21732879638672\n",
      "Epoch 5 \t Batch 820 \t Training Loss: 49.237783511092026\n",
      "Epoch 5 \t Batch 840 \t Training Loss: 49.250676681881856\n",
      "Epoch 5 \t Batch 860 \t Training Loss: 49.25773709319358\n",
      "Epoch 5 \t Batch 880 \t Training Loss: 49.26310435641896\n",
      "Epoch 5 \t Batch 900 \t Training Loss: 49.29437833150228\n",
      "Epoch 5 \t Batch 20 \t Validation Loss: 14.586094903945924\n",
      "Epoch 5 \t Batch 40 \t Validation Loss: 17.46895213127136\n",
      "Epoch 5 \t Batch 60 \t Validation Loss: 17.16124890645345\n",
      "Epoch 5 \t Batch 80 \t Validation Loss: 17.91831679344177\n",
      "Epoch 5 \t Batch 100 \t Validation Loss: 19.775057277679444\n",
      "Epoch 5 \t Batch 120 \t Validation Loss: 21.496180764834087\n",
      "Epoch 5 \t Batch 140 \t Validation Loss: 22.343637377875194\n",
      "Epoch 5 \t Batch 160 \t Validation Loss: 24.85813825726509\n",
      "Epoch 5 \t Batch 180 \t Validation Loss: 29.074422619077893\n",
      "Epoch 5 \t Batch 200 \t Validation Loss: 30.953202929496765\n",
      "Epoch 5 \t Batch 220 \t Validation Loss: 32.798772391405976\n",
      "Epoch 5 \t Batch 240 \t Validation Loss: 33.598759444554645\n",
      "Epoch 5 \t Batch 260 \t Validation Loss: 35.95363763662485\n",
      "Epoch 5 \t Batch 280 \t Validation Loss: 37.29270780767713\n",
      "Epoch 5 \t Batch 300 \t Validation Loss: 38.62198145230611\n",
      "Epoch 5 \t Batch 320 \t Validation Loss: 39.275498488545416\n",
      "Epoch 5 \t Batch 340 \t Validation Loss: 39.30756136669832\n",
      "Epoch 5 \t Batch 360 \t Validation Loss: 39.26833926041921\n",
      "Epoch 5 \t Batch 380 \t Validation Loss: 39.62374147364968\n",
      "Epoch 5 \t Batch 400 \t Validation Loss: 39.21674352407455\n",
      "Epoch 5 \t Batch 420 \t Validation Loss: 39.287240807215376\n",
      "Epoch 5 \t Batch 440 \t Validation Loss: 39.01756594181061\n",
      "Epoch 5 \t Batch 460 \t Validation Loss: 39.22288552367169\n",
      "Epoch 5 \t Batch 480 \t Validation Loss: 39.71552379727363\n",
      "Epoch 5 \t Batch 500 \t Validation Loss: 39.436513521194456\n",
      "Epoch 5 \t Batch 520 \t Validation Loss: 39.14302210991199\n",
      "Epoch 5 \t Batch 540 \t Validation Loss: 38.97446320675038\n",
      "Epoch 5 \t Batch 560 \t Validation Loss: 38.83305846282414\n",
      "Epoch 5 \t Batch 580 \t Validation Loss: 38.55117160205183\n",
      "Epoch 5 \t Batch 600 \t Validation Loss: 38.89987193743388\n",
      "Epoch 5 Training Loss: 49.302888514561374 Validation Loss: 39.49777384547444\n",
      "Epoch 5 completed\n",
      "Epoch 6 \t Batch 20 \t Training Loss: 48.76146335601807\n",
      "Epoch 6 \t Batch 40 \t Training Loss: 48.82686700820923\n",
      "Epoch 6 \t Batch 60 \t Training Loss: 49.37419001261393\n",
      "Epoch 6 \t Batch 80 \t Training Loss: 49.111399269104005\n",
      "Epoch 6 \t Batch 100 \t Training Loss: 49.45539768218994\n",
      "Epoch 6 \t Batch 120 \t Training Loss: 49.426237201690675\n",
      "Epoch 6 \t Batch 140 \t Training Loss: 49.494939940316335\n",
      "Epoch 6 \t Batch 160 \t Training Loss: 49.340368318557736\n",
      "Epoch 6 \t Batch 180 \t Training Loss: 49.36675900353326\n",
      "Epoch 6 \t Batch 200 \t Training Loss: 49.3008731842041\n",
      "Epoch 6 \t Batch 220 \t Training Loss: 49.25841721621427\n",
      "Epoch 6 \t Batch 240 \t Training Loss: 49.185163434346514\n",
      "Epoch 6 \t Batch 260 \t Training Loss: 49.255251943148096\n",
      "Epoch 6 \t Batch 280 \t Training Loss: 49.26088173730033\n",
      "Epoch 6 \t Batch 300 \t Training Loss: 49.31789309183757\n",
      "Epoch 6 \t Batch 320 \t Training Loss: 49.274262118339536\n",
      "Epoch 6 \t Batch 340 \t Training Loss: 49.28061768026913\n",
      "Epoch 6 \t Batch 360 \t Training Loss: 49.25078558391995\n",
      "Epoch 6 \t Batch 380 \t Training Loss: 49.27087614159835\n",
      "Epoch 6 \t Batch 400 \t Training Loss: 49.317627029418944\n",
      "Epoch 6 \t Batch 420 \t Training Loss: 49.26797462645031\n",
      "Epoch 6 \t Batch 440 \t Training Loss: 49.25343490947377\n",
      "Epoch 6 \t Batch 460 \t Training Loss: 49.16442586650019\n",
      "Epoch 6 \t Batch 480 \t Training Loss: 49.14815098444621\n",
      "Epoch 6 \t Batch 500 \t Training Loss: 49.17050912475586\n",
      "Epoch 6 \t Batch 520 \t Training Loss: 49.17573334620549\n",
      "Epoch 6 \t Batch 540 \t Training Loss: 49.22298309184887\n",
      "Epoch 6 \t Batch 560 \t Training Loss: 49.2024485043117\n",
      "Epoch 6 \t Batch 580 \t Training Loss: 49.23839503978861\n",
      "Epoch 6 \t Batch 600 \t Training Loss: 49.23953296025594\n",
      "Epoch 6 \t Batch 620 \t Training Loss: 49.19601618243802\n",
      "Epoch 6 \t Batch 640 \t Training Loss: 49.174783545732495\n",
      "Epoch 6 \t Batch 660 \t Training Loss: 49.14816688190807\n",
      "Epoch 6 \t Batch 680 \t Training Loss: 49.14919648450964\n",
      "Epoch 6 \t Batch 700 \t Training Loss: 49.189629358564105\n",
      "Epoch 6 \t Batch 720 \t Training Loss: 49.16769585079617\n",
      "Epoch 6 \t Batch 740 \t Training Loss: 49.15956441518423\n",
      "Epoch 6 \t Batch 760 \t Training Loss: 49.12256291540046\n",
      "Epoch 6 \t Batch 780 \t Training Loss: 49.08751634940123\n",
      "Epoch 6 \t Batch 800 \t Training Loss: 49.12517897605896\n",
      "Epoch 6 \t Batch 820 \t Training Loss: 49.13551391508521\n",
      "Epoch 6 \t Batch 840 \t Training Loss: 49.12920604887463\n",
      "Epoch 6 \t Batch 860 \t Training Loss: 49.124497985839845\n",
      "Epoch 6 \t Batch 880 \t Training Loss: 49.09877439412204\n",
      "Epoch 6 \t Batch 900 \t Training Loss: 49.08388478597005\n",
      "Epoch 6 \t Batch 20 \t Validation Loss: 22.695227575302123\n",
      "Epoch 6 \t Batch 40 \t Validation Loss: 25.249354314804076\n",
      "Epoch 6 \t Batch 60 \t Validation Loss: 25.22849504152934\n",
      "Epoch 6 \t Batch 80 \t Validation Loss: 25.98984067440033\n",
      "Epoch 6 \t Batch 100 \t Validation Loss: 26.128678169250488\n",
      "Epoch 6 \t Batch 120 \t Validation Loss: 26.619071801503498\n",
      "Epoch 6 \t Batch 140 \t Validation Loss: 26.730911663600377\n",
      "Epoch 6 \t Batch 160 \t Validation Loss: 28.419649040699007\n",
      "Epoch 6 \t Batch 180 \t Validation Loss: 31.26150411499871\n",
      "Epoch 6 \t Batch 200 \t Validation Loss: 32.15484133243561\n",
      "Epoch 6 \t Batch 220 \t Validation Loss: 33.110147107731215\n",
      "Epoch 6 \t Batch 240 \t Validation Loss: 33.398700972398125\n",
      "Epoch 6 \t Batch 260 \t Validation Loss: 35.0145607141348\n",
      "Epoch 6 \t Batch 280 \t Validation Loss: 35.79139699935913\n",
      "Epoch 6 \t Batch 300 \t Validation Loss: 36.85507397969564\n",
      "Epoch 6 \t Batch 320 \t Validation Loss: 37.29882305264473\n",
      "Epoch 6 \t Batch 340 \t Validation Loss: 37.28873676973231\n",
      "Epoch 6 \t Batch 360 \t Validation Loss: 37.26436971028646\n",
      "Epoch 6 \t Batch 380 \t Validation Loss: 37.482162109174226\n",
      "Epoch 6 \t Batch 400 \t Validation Loss: 37.19732346057892\n",
      "Epoch 6 \t Batch 420 \t Validation Loss: 37.290330905006044\n",
      "Epoch 6 \t Batch 440 \t Validation Loss: 37.07150190960277\n",
      "Epoch 6 \t Batch 460 \t Validation Loss: 37.33361455253933\n",
      "Epoch 6 \t Batch 480 \t Validation Loss: 37.863893671830496\n",
      "Epoch 6 \t Batch 500 \t Validation Loss: 37.615181392669676\n",
      "Epoch 6 \t Batch 520 \t Validation Loss: 37.43364114027757\n",
      "Epoch 6 \t Batch 540 \t Validation Loss: 37.21788001943518\n",
      "Epoch 6 \t Batch 560 \t Validation Loss: 37.03721811090197\n",
      "Epoch 6 \t Batch 580 \t Validation Loss: 36.87076251917872\n",
      "Epoch 6 \t Batch 600 \t Validation Loss: 37.0961323928833\n",
      "Epoch 6 Training Loss: 49.09301550645735 Validation Loss: 37.72261577147942\n",
      "Epoch 6 completed\n",
      "Epoch 7 \t Batch 20 \t Training Loss: 49.161536026000974\n",
      "Epoch 7 \t Batch 40 \t Training Loss: 49.288998985290526\n",
      "Epoch 7 \t Batch 60 \t Training Loss: 49.366676266988115\n",
      "Epoch 7 \t Batch 80 \t Training Loss: 49.932912302017215\n",
      "Epoch 7 \t Batch 100 \t Training Loss: 49.523484497070314\n",
      "Epoch 7 \t Batch 120 \t Training Loss: 49.55913966496785\n",
      "Epoch 7 \t Batch 140 \t Training Loss: 49.32124737330845\n",
      "Epoch 7 \t Batch 160 \t Training Loss: 49.35235652923584\n",
      "Epoch 7 \t Batch 180 \t Training Loss: 49.2224689271715\n",
      "Epoch 7 \t Batch 200 \t Training Loss: 49.20715148925781\n",
      "Epoch 7 \t Batch 220 \t Training Loss: 49.15994769009677\n",
      "Epoch 7 \t Batch 240 \t Training Loss: 48.97844864527384\n",
      "Epoch 7 \t Batch 260 \t Training Loss: 48.9173487003033\n",
      "Epoch 7 \t Batch 280 \t Training Loss: 48.960337557111465\n",
      "Epoch 7 \t Batch 300 \t Training Loss: 48.93817192077637\n",
      "Epoch 7 \t Batch 320 \t Training Loss: 48.86074075698853\n",
      "Epoch 7 \t Batch 340 \t Training Loss: 48.859727949254655\n",
      "Epoch 7 \t Batch 360 \t Training Loss: 48.80995648701985\n",
      "Epoch 7 \t Batch 380 \t Training Loss: 48.822011104382966\n",
      "Epoch 7 \t Batch 400 \t Training Loss: 48.80209981918335\n",
      "Epoch 7 \t Batch 420 \t Training Loss: 48.83975911821638\n",
      "Epoch 7 \t Batch 440 \t Training Loss: 48.86751646562056\n",
      "Epoch 7 \t Batch 460 \t Training Loss: 48.863082280366314\n",
      "Epoch 7 \t Batch 480 \t Training Loss: 48.87015868028005\n",
      "Epoch 7 \t Batch 500 \t Training Loss: 48.87934065246582\n",
      "Epoch 7 \t Batch 520 \t Training Loss: 48.86888055067796\n",
      "Epoch 7 \t Batch 540 \t Training Loss: 48.89156465883608\n",
      "Epoch 7 \t Batch 560 \t Training Loss: 48.8613183089665\n",
      "Epoch 7 \t Batch 580 \t Training Loss: 48.82724714608028\n",
      "Epoch 7 \t Batch 600 \t Training Loss: 48.85106461207072\n",
      "Epoch 7 \t Batch 620 \t Training Loss: 48.88509344900808\n",
      "Epoch 7 \t Batch 640 \t Training Loss: 48.908313220739366\n",
      "Epoch 7 \t Batch 660 \t Training Loss: 48.93205260652484\n",
      "Epoch 7 \t Batch 680 \t Training Loss: 48.91527483323041\n",
      "Epoch 7 \t Batch 700 \t Training Loss: 48.93062970297677\n",
      "Epoch 7 \t Batch 720 \t Training Loss: 48.93842027982076\n",
      "Epoch 7 \t Batch 740 \t Training Loss: 48.94473597036826\n",
      "Epoch 7 \t Batch 760 \t Training Loss: 48.92689567867078\n",
      "Epoch 7 \t Batch 780 \t Training Loss: 48.90745380108173\n",
      "Epoch 7 \t Batch 800 \t Training Loss: 48.957515816688534\n",
      "Epoch 7 \t Batch 820 \t Training Loss: 48.944136042711214\n",
      "Epoch 7 \t Batch 840 \t Training Loss: 48.961834557851155\n",
      "Epoch 7 \t Batch 860 \t Training Loss: 48.97283122484074\n",
      "Epoch 7 \t Batch 880 \t Training Loss: 48.95194037177346\n",
      "Epoch 7 \t Batch 900 \t Training Loss: 48.932063289218476\n",
      "Epoch 7 \t Batch 20 \t Validation Loss: 15.579801106452942\n",
      "Epoch 7 \t Batch 40 \t Validation Loss: 19.052806866168975\n",
      "Epoch 7 \t Batch 60 \t Validation Loss: 18.59974755446116\n",
      "Epoch 7 \t Batch 80 \t Validation Loss: 19.053006702661513\n",
      "Epoch 7 \t Batch 100 \t Validation Loss: 20.630166811943056\n",
      "Epoch 7 \t Batch 120 \t Validation Loss: 21.993036448955536\n",
      "Epoch 7 \t Batch 140 \t Validation Loss: 22.773661712237768\n",
      "Epoch 7 \t Batch 160 \t Validation Loss: 25.117404249310493\n",
      "Epoch 7 \t Batch 180 \t Validation Loss: 28.892702603340148\n",
      "Epoch 7 \t Batch 200 \t Validation Loss: 30.414699375629425\n",
      "Epoch 7 \t Batch 220 \t Validation Loss: 31.937065126679162\n",
      "Epoch 7 \t Batch 240 \t Validation Loss: 32.57401188413302\n",
      "Epoch 7 \t Batch 260 \t Validation Loss: 34.72129577673399\n",
      "Epoch 7 \t Batch 280 \t Validation Loss: 35.86434312377657\n",
      "Epoch 7 \t Batch 300 \t Validation Loss: 37.17722511132558\n",
      "Epoch 7 \t Batch 320 \t Validation Loss: 37.77092852443457\n",
      "Epoch 7 \t Batch 340 \t Validation Loss: 37.78178736602559\n",
      "Epoch 7 \t Batch 360 \t Validation Loss: 37.77016561428706\n",
      "Epoch 7 \t Batch 380 \t Validation Loss: 38.08443971056687\n",
      "Epoch 7 \t Batch 400 \t Validation Loss: 37.705637928247455\n",
      "Epoch 7 \t Batch 420 \t Validation Loss: 37.772001880691164\n",
      "Epoch 7 \t Batch 440 \t Validation Loss: 37.48903232162649\n",
      "Epoch 7 \t Batch 460 \t Validation Loss: 37.72498705179795\n",
      "Epoch 7 \t Batch 480 \t Validation Loss: 38.23210531175137\n",
      "Epoch 7 \t Batch 500 \t Validation Loss: 37.96367360401153\n",
      "Epoch 7 \t Batch 520 \t Validation Loss: 37.74640104312163\n",
      "Epoch 7 \t Batch 540 \t Validation Loss: 37.4897179612407\n",
      "Epoch 7 \t Batch 560 \t Validation Loss: 37.26762389540672\n",
      "Epoch 7 \t Batch 580 \t Validation Loss: 37.031356259872176\n",
      "Epoch 7 \t Batch 600 \t Validation Loss: 37.23752606153488\n",
      "Epoch 7 Training Loss: 48.942068272614556 Validation Loss: 37.85310911900037\n",
      "Epoch 7 completed\n",
      "Epoch 8 \t Batch 20 \t Training Loss: 50.065420150756836\n",
      "Epoch 8 \t Batch 40 \t Training Loss: 49.356886291503905\n",
      "Epoch 8 \t Batch 60 \t Training Loss: 49.33021462758382\n",
      "Epoch 8 \t Batch 80 \t Training Loss: 49.278537082672116\n",
      "Epoch 8 \t Batch 100 \t Training Loss: 49.29903102874756\n",
      "Epoch 8 \t Batch 120 \t Training Loss: 49.21819836298625\n",
      "Epoch 8 \t Batch 140 \t Training Loss: 49.090554755074635\n",
      "Epoch 8 \t Batch 160 \t Training Loss: 48.98767623901367\n",
      "Epoch 8 \t Batch 180 \t Training Loss: 49.115991062588165\n",
      "Epoch 8 \t Batch 200 \t Training Loss: 48.941779193878176\n",
      "Epoch 8 \t Batch 220 \t Training Loss: 48.940199089050296\n",
      "Epoch 8 \t Batch 240 \t Training Loss: 48.97524240811666\n",
      "Epoch 8 \t Batch 260 \t Training Loss: 49.02541279425988\n",
      "Epoch 8 \t Batch 280 \t Training Loss: 49.05478042875018\n",
      "Epoch 8 \t Batch 300 \t Training Loss: 49.058746096293135\n",
      "Epoch 8 \t Batch 320 \t Training Loss: 49.05262768268585\n",
      "Epoch 8 \t Batch 340 \t Training Loss: 49.064222358254824\n",
      "Epoch 8 \t Batch 360 \t Training Loss: 48.99827869203356\n",
      "Epoch 8 \t Batch 380 \t Training Loss: 48.9825688412315\n",
      "Epoch 8 \t Batch 400 \t Training Loss: 49.00193043708801\n",
      "Epoch 8 \t Batch 420 \t Training Loss: 49.01290613810222\n",
      "Epoch 8 \t Batch 440 \t Training Loss: 48.971333200281315\n",
      "Epoch 8 \t Batch 460 \t Training Loss: 48.99004364013672\n",
      "Epoch 8 \t Batch 480 \t Training Loss: 48.981905261675514\n",
      "Epoch 8 \t Batch 500 \t Training Loss: 49.002041038513184\n",
      "Epoch 8 \t Batch 520 \t Training Loss: 48.923997226128215\n",
      "Epoch 8 \t Batch 540 \t Training Loss: 48.94897742094817\n",
      "Epoch 8 \t Batch 560 \t Training Loss: 48.9555775301797\n",
      "Epoch 8 \t Batch 580 \t Training Loss: 48.88884305625126\n",
      "Epoch 8 \t Batch 600 \t Training Loss: 48.88326271692912\n",
      "Epoch 8 \t Batch 620 \t Training Loss: 48.86037990200904\n",
      "Epoch 8 \t Batch 640 \t Training Loss: 48.86561394929886\n",
      "Epoch 8 \t Batch 660 \t Training Loss: 48.898911695769335\n",
      "Epoch 8 \t Batch 680 \t Training Loss: 48.874327973758476\n",
      "Epoch 8 \t Batch 700 \t Training Loss: 48.87011615208217\n",
      "Epoch 8 \t Batch 720 \t Training Loss: 48.87694239086575\n",
      "Epoch 8 \t Batch 740 \t Training Loss: 48.867571990554396\n",
      "Epoch 8 \t Batch 760 \t Training Loss: 48.8863705534684\n",
      "Epoch 8 \t Batch 780 \t Training Loss: 48.85210371261988\n",
      "Epoch 8 \t Batch 800 \t Training Loss: 48.863873596191404\n",
      "Epoch 8 \t Batch 820 \t Training Loss: 48.822249575359066\n",
      "Epoch 8 \t Batch 840 \t Training Loss: 48.80016173408145\n",
      "Epoch 8 \t Batch 860 \t Training Loss: 48.78945442465849\n",
      "Epoch 8 \t Batch 880 \t Training Loss: 48.74564586119218\n",
      "Epoch 8 \t Batch 900 \t Training Loss: 48.77989159901937\n",
      "Epoch 8 \t Batch 20 \t Validation Loss: 17.84438419342041\n",
      "Epoch 8 \t Batch 40 \t Validation Loss: 20.680405378341675\n",
      "Epoch 8 \t Batch 60 \t Validation Loss: 20.597529141108193\n",
      "Epoch 8 \t Batch 80 \t Validation Loss: 21.170524501800536\n",
      "Epoch 8 \t Batch 100 \t Validation Loss: 22.18671977996826\n",
      "Epoch 8 \t Batch 120 \t Validation Loss: 23.30583377679189\n",
      "Epoch 8 \t Batch 140 \t Validation Loss: 23.815303209849766\n",
      "Epoch 8 \t Batch 160 \t Validation Loss: 25.97275783419609\n",
      "Epoch 8 \t Batch 180 \t Validation Loss: 29.770546139611138\n",
      "Epoch 8 \t Batch 200 \t Validation Loss: 31.333292331695556\n",
      "Epoch 8 \t Batch 220 \t Validation Loss: 32.82137920206243\n",
      "Epoch 8 \t Batch 240 \t Validation Loss: 33.47806007464727\n",
      "Epoch 8 \t Batch 260 \t Validation Loss: 35.578431859383215\n",
      "Epoch 8 \t Batch 280 \t Validation Loss: 36.64128984042576\n",
      "Epoch 8 \t Batch 300 \t Validation Loss: 37.95654697418213\n",
      "Epoch 8 \t Batch 320 \t Validation Loss: 38.55963243842125\n",
      "Epoch 8 \t Batch 340 \t Validation Loss: 38.55954995435827\n",
      "Epoch 8 \t Batch 360 \t Validation Loss: 38.48329751756456\n",
      "Epoch 8 \t Batch 380 \t Validation Loss: 38.76039618441933\n",
      "Epoch 8 \t Batch 400 \t Validation Loss: 38.38342029094696\n",
      "Epoch 8 \t Batch 420 \t Validation Loss: 38.4548684324537\n",
      "Epoch 8 \t Batch 440 \t Validation Loss: 38.17632327296517\n",
      "Epoch 8 \t Batch 460 \t Validation Loss: 38.36882642870364\n",
      "Epoch 8 \t Batch 480 \t Validation Loss: 38.8619376162688\n",
      "Epoch 8 \t Batch 500 \t Validation Loss: 38.57967847251892\n",
      "Epoch 8 \t Batch 520 \t Validation Loss: 38.328848459170416\n",
      "Epoch 8 \t Batch 540 \t Validation Loss: 38.032887541806254\n",
      "Epoch 8 \t Batch 560 \t Validation Loss: 37.79481000559671\n",
      "Epoch 8 \t Batch 580 \t Validation Loss: 37.56449499952382\n",
      "Epoch 8 \t Batch 600 \t Validation Loss: 37.74214030742645\n",
      "Epoch 8 Training Loss: 48.78811101799053 Validation Loss: 38.364017927801456\n",
      "Epoch 8 completed\n",
      "Epoch 9 \t Batch 20 \t Training Loss: 47.764369583129884\n",
      "Epoch 9 \t Batch 40 \t Training Loss: 47.75233869552612\n",
      "Epoch 9 \t Batch 60 \t Training Loss: 48.25230032602946\n",
      "Epoch 9 \t Batch 80 \t Training Loss: 48.17262191772461\n",
      "Epoch 9 \t Batch 100 \t Training Loss: 48.12176349639893\n",
      "Epoch 9 \t Batch 120 \t Training Loss: 47.953072738647464\n",
      "Epoch 9 \t Batch 140 \t Training Loss: 48.15922388349261\n",
      "Epoch 9 \t Batch 160 \t Training Loss: 48.33203887939453\n",
      "Epoch 9 \t Batch 180 \t Training Loss: 48.317018021477594\n",
      "Epoch 9 \t Batch 200 \t Training Loss: 48.32910106658935\n",
      "Epoch 9 \t Batch 220 \t Training Loss: 48.339602730490945\n",
      "Epoch 9 \t Batch 240 \t Training Loss: 48.43939723968506\n",
      "Epoch 9 \t Batch 260 \t Training Loss: 48.49260459313026\n",
      "Epoch 9 \t Batch 280 \t Training Loss: 48.432280608585906\n",
      "Epoch 9 \t Batch 300 \t Training Loss: 48.39651269276937\n",
      "Epoch 9 \t Batch 320 \t Training Loss: 48.40891119241714\n",
      "Epoch 9 \t Batch 340 \t Training Loss: 48.45255852867575\n",
      "Epoch 9 \t Batch 360 \t Training Loss: 48.41057307985094\n",
      "Epoch 9 \t Batch 380 \t Training Loss: 48.53013387981214\n",
      "Epoch 9 \t Batch 400 \t Training Loss: 48.53266719818115\n",
      "Epoch 9 \t Batch 420 \t Training Loss: 48.55680670057024\n",
      "Epoch 9 \t Batch 440 \t Training Loss: 48.540873778950086\n",
      "Epoch 9 \t Batch 460 \t Training Loss: 48.56949640356976\n",
      "Epoch 9 \t Batch 480 \t Training Loss: 48.6052837451299\n",
      "Epoch 9 \t Batch 500 \t Training Loss: 48.64329531860351\n",
      "Epoch 9 \t Batch 520 \t Training Loss: 48.634156740628754\n",
      "Epoch 9 \t Batch 540 \t Training Loss: 48.64337037404378\n",
      "Epoch 9 \t Batch 560 \t Training Loss: 48.67013795716422\n",
      "Epoch 9 \t Batch 580 \t Training Loss: 48.70662425468708\n",
      "Epoch 9 \t Batch 600 \t Training Loss: 48.69942358652751\n",
      "Epoch 9 \t Batch 620 \t Training Loss: 48.71708340798655\n",
      "Epoch 9 \t Batch 640 \t Training Loss: 48.7179091155529\n",
      "Epoch 9 \t Batch 660 \t Training Loss: 48.691744370894\n",
      "Epoch 9 \t Batch 680 \t Training Loss: 48.63907449946684\n",
      "Epoch 9 \t Batch 700 \t Training Loss: 48.66277475629534\n",
      "Epoch 9 \t Batch 720 \t Training Loss: 48.657667271296184\n",
      "Epoch 9 \t Batch 740 \t Training Loss: 48.704943770331305\n",
      "Epoch 9 \t Batch 760 \t Training Loss: 48.701725809197676\n",
      "Epoch 9 \t Batch 780 \t Training Loss: 48.65567653362567\n",
      "Epoch 9 \t Batch 800 \t Training Loss: 48.649324893951416\n",
      "Epoch 9 \t Batch 820 \t Training Loss: 48.66798078955674\n",
      "Epoch 9 \t Batch 840 \t Training Loss: 48.634003711882094\n",
      "Epoch 9 \t Batch 860 \t Training Loss: 48.661086605870445\n",
      "Epoch 9 \t Batch 880 \t Training Loss: 48.67267275723544\n",
      "Epoch 9 \t Batch 900 \t Training Loss: 48.644245876736115\n",
      "Epoch 9 \t Batch 20 \t Validation Loss: 14.596839570999146\n",
      "Epoch 9 \t Batch 40 \t Validation Loss: 19.147961354255678\n",
      "Epoch 9 \t Batch 60 \t Validation Loss: 18.300120226542155\n",
      "Epoch 9 \t Batch 80 \t Validation Loss: 18.725975382328034\n",
      "Epoch 9 \t Batch 100 \t Validation Loss: 20.518569660186767\n",
      "Epoch 9 \t Batch 120 \t Validation Loss: 22.086801632245383\n",
      "Epoch 9 \t Batch 140 \t Validation Loss: 23.02394050189427\n",
      "Epoch 9 \t Batch 160 \t Validation Loss: 25.51690519452095\n",
      "Epoch 9 \t Batch 180 \t Validation Loss: 29.88329986996121\n",
      "Epoch 9 \t Batch 200 \t Validation Loss: 31.76412362098694\n",
      "Epoch 9 \t Batch 220 \t Validation Loss: 33.558197324926205\n",
      "Epoch 9 \t Batch 240 \t Validation Loss: 34.37631827195485\n",
      "Epoch 9 \t Batch 260 \t Validation Loss: 36.71825259648836\n",
      "Epoch 9 \t Batch 280 \t Validation Loss: 37.98463338102613\n",
      "Epoch 9 \t Batch 300 \t Validation Loss: 39.361714111963906\n",
      "Epoch 9 \t Batch 320 \t Validation Loss: 39.993740662932396\n",
      "Epoch 9 \t Batch 340 \t Validation Loss: 39.980376134199254\n",
      "Epoch 9 \t Batch 360 \t Validation Loss: 39.93698827160729\n",
      "Epoch 9 \t Batch 380 \t Validation Loss: 40.20986132119831\n",
      "Epoch 9 \t Batch 400 \t Validation Loss: 39.7707618021965\n",
      "Epoch 9 \t Batch 420 \t Validation Loss: 39.76300285657247\n",
      "Epoch 9 \t Batch 440 \t Validation Loss: 39.39748424833471\n",
      "Epoch 9 \t Batch 460 \t Validation Loss: 39.567608163667764\n",
      "Epoch 9 \t Batch 480 \t Validation Loss: 40.02099735935529\n",
      "Epoch 9 \t Batch 500 \t Validation Loss: 39.73037079429626\n",
      "Epoch 9 \t Batch 520 \t Validation Loss: 39.45500683601086\n",
      "Epoch 9 \t Batch 540 \t Validation Loss: 39.16602896054586\n",
      "Epoch 9 \t Batch 560 \t Validation Loss: 38.911302850927626\n",
      "Epoch 9 \t Batch 580 \t Validation Loss: 38.62272771473589\n",
      "Epoch 9 \t Batch 600 \t Validation Loss: 38.805753156344096\n",
      "Epoch 9 Training Loss: 48.67040352940949 Validation Loss: 39.42196473053524\n",
      "Epoch 9 completed\n",
      "Epoch 10 \t Batch 20 \t Training Loss: 48.30882015228271\n",
      "Epoch 10 \t Batch 40 \t Training Loss: 48.61640558242798\n",
      "Epoch 10 \t Batch 60 \t Training Loss: 48.683493550618486\n",
      "Epoch 10 \t Batch 80 \t Training Loss: 48.47433967590332\n",
      "Epoch 10 \t Batch 100 \t Training Loss: 48.4025675201416\n",
      "Epoch 10 \t Batch 120 \t Training Loss: 48.42679951985677\n",
      "Epoch 10 \t Batch 140 \t Training Loss: 48.46324225834438\n",
      "Epoch 10 \t Batch 160 \t Training Loss: 48.28634521961212\n",
      "Epoch 10 \t Batch 180 \t Training Loss: 48.31888190375434\n",
      "Epoch 10 \t Batch 200 \t Training Loss: 48.31044208526611\n",
      "Epoch 10 \t Batch 220 \t Training Loss: 48.37834354747425\n",
      "Epoch 10 \t Batch 240 \t Training Loss: 48.30144707361857\n",
      "Epoch 10 \t Batch 260 \t Training Loss: 48.398018704927885\n",
      "Epoch 10 \t Batch 280 \t Training Loss: 48.37213550295149\n",
      "Epoch 10 \t Batch 300 \t Training Loss: 48.38202213287354\n",
      "Epoch 10 \t Batch 320 \t Training Loss: 48.49079122543335\n",
      "Epoch 10 \t Batch 340 \t Training Loss: 48.404741713579966\n",
      "Epoch 10 \t Batch 360 \t Training Loss: 48.53496363957723\n",
      "Epoch 10 \t Batch 380 \t Training Loss: 48.497083202161285\n",
      "Epoch 10 \t Batch 400 \t Training Loss: 48.52330192565918\n",
      "Epoch 10 \t Batch 420 \t Training Loss: 48.50473296755836\n",
      "Epoch 10 \t Batch 440 \t Training Loss: 48.56471787366\n",
      "Epoch 10 \t Batch 460 \t Training Loss: 48.53675419351329\n",
      "Epoch 10 \t Batch 480 \t Training Loss: 48.57460219860077\n",
      "Epoch 10 \t Batch 500 \t Training Loss: 48.538893005371094\n",
      "Epoch 10 \t Batch 520 \t Training Loss: 48.51945210970365\n",
      "Epoch 10 \t Batch 540 \t Training Loss: 48.4965152811121\n",
      "Epoch 10 \t Batch 560 \t Training Loss: 48.50513366290501\n",
      "Epoch 10 \t Batch 580 \t Training Loss: 48.47502156619368\n",
      "Epoch 10 \t Batch 600 \t Training Loss: 48.43185180028279\n",
      "Epoch 10 \t Batch 620 \t Training Loss: 48.43603086163921\n",
      "Epoch 10 \t Batch 640 \t Training Loss: 48.452337723970416\n",
      "Epoch 10 \t Batch 660 \t Training Loss: 48.479894973292495\n",
      "Epoch 10 \t Batch 680 \t Training Loss: 48.50708778044757\n",
      "Epoch 10 \t Batch 700 \t Training Loss: 48.454796033586774\n",
      "Epoch 10 \t Batch 720 \t Training Loss: 48.442910242080686\n",
      "Epoch 10 \t Batch 740 \t Training Loss: 48.47300947550181\n",
      "Epoch 10 \t Batch 760 \t Training Loss: 48.49107556092112\n",
      "Epoch 10 \t Batch 780 \t Training Loss: 48.50264388353397\n",
      "Epoch 10 \t Batch 800 \t Training Loss: 48.562747616767886\n",
      "Epoch 10 \t Batch 820 \t Training Loss: 48.56747878470072\n",
      "Epoch 10 \t Batch 840 \t Training Loss: 48.5775400797526\n",
      "Epoch 10 \t Batch 860 \t Training Loss: 48.578855403634\n",
      "Epoch 10 \t Batch 880 \t Training Loss: 48.54872582175515\n",
      "Epoch 10 \t Batch 900 \t Training Loss: 48.57457455529107\n",
      "Epoch 10 \t Batch 20 \t Validation Loss: 18.152754497528075\n",
      "Epoch 10 \t Batch 40 \t Validation Loss: 20.832267928123475\n",
      "Epoch 10 \t Batch 60 \t Validation Loss: 20.741328636805218\n",
      "Epoch 10 \t Batch 80 \t Validation Loss: 21.401031720638276\n",
      "Epoch 10 \t Batch 100 \t Validation Loss: 22.498493185043333\n",
      "Epoch 10 \t Batch 120 \t Validation Loss: 23.59250821272532\n",
      "Epoch 10 \t Batch 140 \t Validation Loss: 24.14548215866089\n",
      "Epoch 10 \t Batch 160 \t Validation Loss: 26.2964009642601\n",
      "Epoch 10 \t Batch 180 \t Validation Loss: 30.08036578496297\n",
      "Epoch 10 \t Batch 200 \t Validation Loss: 31.537931060791017\n",
      "Epoch 10 \t Batch 220 \t Validation Loss: 32.99890304045244\n",
      "Epoch 10 \t Batch 240 \t Validation Loss: 33.63890740076701\n",
      "Epoch 10 \t Batch 260 \t Validation Loss: 35.749970568143404\n",
      "Epoch 10 \t Batch 280 \t Validation Loss: 36.854065145765034\n",
      "Epoch 10 \t Batch 300 \t Validation Loss: 38.15191388448079\n",
      "Epoch 10 \t Batch 320 \t Validation Loss: 38.72836492657662\n",
      "Epoch 10 \t Batch 340 \t Validation Loss: 38.69958147722132\n",
      "Epoch 10 \t Batch 360 \t Validation Loss: 38.64451006783379\n",
      "Epoch 10 \t Batch 380 \t Validation Loss: 38.89519308491757\n",
      "Epoch 10 \t Batch 400 \t Validation Loss: 38.46529179096222\n",
      "Epoch 10 \t Batch 420 \t Validation Loss: 38.5038121405102\n",
      "Epoch 10 \t Batch 440 \t Validation Loss: 38.18954861380837\n",
      "Epoch 10 \t Batch 460 \t Validation Loss: 38.371928866013235\n",
      "Epoch 10 \t Batch 480 \t Validation Loss: 38.863831742604575\n",
      "Epoch 10 \t Batch 500 \t Validation Loss: 38.58248992919922\n",
      "Epoch 10 \t Batch 520 \t Validation Loss: 38.31460904341478\n",
      "Epoch 10 \t Batch 540 \t Validation Loss: 38.03835202323066\n",
      "Epoch 10 \t Batch 560 \t Validation Loss: 37.804528968674795\n",
      "Epoch 10 \t Batch 580 \t Validation Loss: 37.50802920111295\n",
      "Epoch 10 \t Batch 600 \t Validation Loss: 37.72058523178101\n",
      "Epoch 10 Training Loss: 48.5696130500885 Validation Loss: 38.354424761487294\n",
      "Epoch 10 completed\n",
      "Epoch 11 \t Batch 20 \t Training Loss: 47.47440128326416\n",
      "Epoch 11 \t Batch 40 \t Training Loss: 47.638087558746335\n",
      "Epoch 11 \t Batch 60 \t Training Loss: 47.36649233500163\n",
      "Epoch 11 \t Batch 80 \t Training Loss: 47.857253837585446\n",
      "Epoch 11 \t Batch 100 \t Training Loss: 48.13323783874512\n",
      "Epoch 11 \t Batch 120 \t Training Loss: 48.07066059112549\n",
      "Epoch 11 \t Batch 140 \t Training Loss: 48.40173906598773\n",
      "Epoch 11 \t Batch 160 \t Training Loss: 48.38626339435577\n",
      "Epoch 11 \t Batch 180 \t Training Loss: 48.25694461398655\n",
      "Epoch 11 \t Batch 200 \t Training Loss: 48.40096004486084\n",
      "Epoch 11 \t Batch 220 \t Training Loss: 48.32693587216464\n",
      "Epoch 11 \t Batch 240 \t Training Loss: 48.411868778864545\n",
      "Epoch 11 \t Batch 260 \t Training Loss: 48.41983853853666\n",
      "Epoch 11 \t Batch 280 \t Training Loss: 48.56055338723319\n",
      "Epoch 11 \t Batch 300 \t Training Loss: 48.59168025970459\n",
      "Epoch 11 \t Batch 320 \t Training Loss: 48.564004349708554\n",
      "Epoch 11 \t Batch 340 \t Training Loss: 48.465482162026795\n",
      "Epoch 11 \t Batch 360 \t Training Loss: 48.479427888658314\n",
      "Epoch 11 \t Batch 380 \t Training Loss: 48.54225338383725\n",
      "Epoch 11 \t Batch 400 \t Training Loss: 48.57764352798462\n",
      "Epoch 11 \t Batch 420 \t Training Loss: 48.598180480230425\n",
      "Epoch 11 \t Batch 440 \t Training Loss: 48.54133860848167\n",
      "Epoch 11 \t Batch 460 \t Training Loss: 48.56192363241445\n",
      "Epoch 11 \t Batch 480 \t Training Loss: 48.591564957300825\n",
      "Epoch 11 \t Batch 500 \t Training Loss: 48.58730495452881\n",
      "Epoch 11 \t Batch 520 \t Training Loss: 48.60412932175856\n",
      "Epoch 11 \t Batch 540 \t Training Loss: 48.567984333744754\n",
      "Epoch 11 \t Batch 560 \t Training Loss: 48.575960227421355\n",
      "Epoch 11 \t Batch 580 \t Training Loss: 48.51087993095661\n",
      "Epoch 11 \t Batch 600 \t Training Loss: 48.473631318410234\n",
      "Epoch 11 \t Batch 620 \t Training Loss: 48.44464717988045\n",
      "Epoch 11 \t Batch 640 \t Training Loss: 48.457147645950315\n",
      "Epoch 11 \t Batch 660 \t Training Loss: 48.43121566772461\n",
      "Epoch 11 \t Batch 680 \t Training Loss: 48.409752318438365\n",
      "Epoch 11 \t Batch 700 \t Training Loss: 48.39318673815046\n",
      "Epoch 11 \t Batch 720 \t Training Loss: 48.39046474562751\n",
      "Epoch 11 \t Batch 740 \t Training Loss: 48.42211343404409\n",
      "Epoch 11 \t Batch 760 \t Training Loss: 48.37233064551103\n",
      "Epoch 11 \t Batch 780 \t Training Loss: 48.38128307049091\n",
      "Epoch 11 \t Batch 800 \t Training Loss: 48.39652740001679\n",
      "Epoch 11 \t Batch 820 \t Training Loss: 48.3792676181328\n",
      "Epoch 11 \t Batch 840 \t Training Loss: 48.38437827428182\n",
      "Epoch 11 \t Batch 860 \t Training Loss: 48.40430144376533\n",
      "Epoch 11 \t Batch 880 \t Training Loss: 48.42946907823736\n",
      "Epoch 11 \t Batch 900 \t Training Loss: 48.43416119045681\n",
      "Epoch 11 \t Batch 20 \t Validation Loss: 22.79461030960083\n",
      "Epoch 11 \t Batch 40 \t Validation Loss: 24.29023320674896\n",
      "Epoch 11 \t Batch 60 \t Validation Loss: 24.426650190353392\n",
      "Epoch 11 \t Batch 80 \t Validation Loss: 24.291016280651093\n",
      "Epoch 11 \t Batch 100 \t Validation Loss: 24.83406765937805\n",
      "Epoch 11 \t Batch 120 \t Validation Loss: 25.576480929056803\n",
      "Epoch 11 \t Batch 140 \t Validation Loss: 25.79362497329712\n",
      "Epoch 11 \t Batch 160 \t Validation Loss: 27.696506643295287\n",
      "Epoch 11 \t Batch 180 \t Validation Loss: 31.443185726801556\n",
      "Epoch 11 \t Batch 200 \t Validation Loss: 32.971648297309876\n",
      "Epoch 11 \t Batch 220 \t Validation Loss: 34.42454693967646\n",
      "Epoch 11 \t Batch 240 \t Validation Loss: 34.94856951236725\n",
      "Epoch 11 \t Batch 260 \t Validation Loss: 37.11900235689603\n",
      "Epoch 11 \t Batch 280 \t Validation Loss: 38.288214036396575\n",
      "Epoch 11 \t Batch 300 \t Validation Loss: 39.46176928838094\n",
      "Epoch 11 \t Batch 320 \t Validation Loss: 39.94037102460861\n",
      "Epoch 11 \t Batch 340 \t Validation Loss: 39.853639883153576\n",
      "Epoch 11 \t Batch 360 \t Validation Loss: 39.684134931034514\n",
      "Epoch 11 \t Batch 380 \t Validation Loss: 39.911893460625095\n",
      "Epoch 11 \t Batch 400 \t Validation Loss: 39.43833158254623\n",
      "Epoch 11 \t Batch 420 \t Validation Loss: 39.41484484445481\n",
      "Epoch 11 \t Batch 440 \t Validation Loss: 39.05135629827326\n",
      "Epoch 11 \t Batch 460 \t Validation Loss: 39.21050779508508\n",
      "Epoch 11 \t Batch 480 \t Validation Loss: 39.65510424375534\n",
      "Epoch 11 \t Batch 500 \t Validation Loss: 39.33179188156128\n",
      "Epoch 11 \t Batch 520 \t Validation Loss: 39.041164917212264\n",
      "Epoch 11 \t Batch 540 \t Validation Loss: 38.7548390088258\n",
      "Epoch 11 \t Batch 560 \t Validation Loss: 38.49476927178247\n",
      "Epoch 11 \t Batch 580 \t Validation Loss: 38.15717975188946\n",
      "Epoch 11 \t Batch 600 \t Validation Loss: 38.35924312750498\n",
      "Epoch 11 Training Loss: 48.44781179230502 Validation Loss: 38.95219550968765\n",
      "Epoch 11 completed\n",
      "Epoch 12 \t Batch 20 \t Training Loss: 48.480526542663576\n",
      "Epoch 12 \t Batch 40 \t Training Loss: 48.431771469116214\n",
      "Epoch 12 \t Batch 60 \t Training Loss: 48.07377001444499\n",
      "Epoch 12 \t Batch 80 \t Training Loss: 48.715998315811156\n",
      "Epoch 12 \t Batch 100 \t Training Loss: 48.32647689819336\n",
      "Epoch 12 \t Batch 120 \t Training Loss: 48.04005661010742\n",
      "Epoch 12 \t Batch 140 \t Training Loss: 48.246248490469796\n",
      "Epoch 12 \t Batch 160 \t Training Loss: 48.4617681980133\n",
      "Epoch 12 \t Batch 180 \t Training Loss: 48.27389411926269\n",
      "Epoch 12 \t Batch 200 \t Training Loss: 48.1845682144165\n",
      "Epoch 12 \t Batch 220 \t Training Loss: 48.19290344931863\n",
      "Epoch 12 \t Batch 240 \t Training Loss: 48.27087348302205\n",
      "Epoch 12 \t Batch 260 \t Training Loss: 48.236113592294544\n",
      "Epoch 12 \t Batch 280 \t Training Loss: 48.200787339891704\n",
      "Epoch 12 \t Batch 300 \t Training Loss: 48.39390771230062\n",
      "Epoch 12 \t Batch 320 \t Training Loss: 48.49650603532791\n",
      "Epoch 12 \t Batch 340 \t Training Loss: 48.43019480985754\n",
      "Epoch 12 \t Batch 360 \t Training Loss: 48.409310266706676\n",
      "Epoch 12 \t Batch 380 \t Training Loss: 48.43639945983887\n",
      "Epoch 12 \t Batch 400 \t Training Loss: 48.35164384841919\n",
      "Epoch 12 \t Batch 420 \t Training Loss: 48.312092935471306\n",
      "Epoch 12 \t Batch 440 \t Training Loss: 48.27553925947709\n",
      "Epoch 12 \t Batch 460 \t Training Loss: 48.275819786735205\n",
      "Epoch 12 \t Batch 480 \t Training Loss: 48.31769696076711\n",
      "Epoch 12 \t Batch 500 \t Training Loss: 48.34066660308838\n",
      "Epoch 12 \t Batch 520 \t Training Loss: 48.31074441029475\n",
      "Epoch 12 \t Batch 540 \t Training Loss: 48.384713632089124\n",
      "Epoch 12 \t Batch 560 \t Training Loss: 48.35384914534433\n",
      "Epoch 12 \t Batch 580 \t Training Loss: 48.40781472962478\n",
      "Epoch 12 \t Batch 600 \t Training Loss: 48.41251328150431\n",
      "Epoch 12 \t Batch 620 \t Training Loss: 48.390402320123485\n",
      "Epoch 12 \t Batch 640 \t Training Loss: 48.38182485103607\n",
      "Epoch 12 \t Batch 660 \t Training Loss: 48.34962227561257\n",
      "Epoch 12 \t Batch 680 \t Training Loss: 48.33763013727525\n",
      "Epoch 12 \t Batch 700 \t Training Loss: 48.328969704764226\n",
      "Epoch 12 \t Batch 720 \t Training Loss: 48.32590803040399\n",
      "Epoch 12 \t Batch 740 \t Training Loss: 48.31371224377607\n",
      "Epoch 12 \t Batch 760 \t Training Loss: 48.335560211382415\n",
      "Epoch 12 \t Batch 780 \t Training Loss: 48.367232063489084\n",
      "Epoch 12 \t Batch 800 \t Training Loss: 48.34157493591309\n",
      "Epoch 12 \t Batch 820 \t Training Loss: 48.361731719970706\n",
      "Epoch 12 \t Batch 840 \t Training Loss: 48.37480047316778\n",
      "Epoch 12 \t Batch 860 \t Training Loss: 48.36805576501891\n",
      "Epoch 12 \t Batch 880 \t Training Loss: 48.37245734821666\n",
      "Epoch 12 \t Batch 900 \t Training Loss: 48.38437159220378\n",
      "Epoch 12 \t Batch 20 \t Validation Loss: 13.221112918853759\n",
      "Epoch 12 \t Batch 40 \t Validation Loss: 16.56211508512497\n",
      "Epoch 12 \t Batch 60 \t Validation Loss: 16.172555756568908\n",
      "Epoch 12 \t Batch 80 \t Validation Loss: 16.774841076135637\n",
      "Epoch 12 \t Batch 100 \t Validation Loss: 18.726605820655823\n",
      "Epoch 12 \t Batch 120 \t Validation Loss: 20.551767488320667\n",
      "Epoch 12 \t Batch 140 \t Validation Loss: 21.51711665221623\n",
      "Epoch 12 \t Batch 160 \t Validation Loss: 24.016322907805442\n",
      "Epoch 12 \t Batch 180 \t Validation Loss: 28.07934993373023\n",
      "Epoch 12 \t Batch 200 \t Validation Loss: 29.901447298526765\n",
      "Epoch 12 \t Batch 220 \t Validation Loss: 31.561909530379555\n",
      "Epoch 12 \t Batch 240 \t Validation Loss: 32.31778606375058\n",
      "Epoch 12 \t Batch 260 \t Validation Loss: 34.64444160644825\n",
      "Epoch 12 \t Batch 280 \t Validation Loss: 35.904536357947755\n",
      "Epoch 12 \t Batch 300 \t Validation Loss: 37.211726768811545\n",
      "Epoch 12 \t Batch 320 \t Validation Loss: 37.838729770481585\n",
      "Epoch 12 \t Batch 340 \t Validation Loss: 37.867718613848965\n",
      "Epoch 12 \t Batch 360 \t Validation Loss: 37.82226031223933\n",
      "Epoch 12 \t Batch 380 \t Validation Loss: 38.12853991483387\n",
      "Epoch 12 \t Batch 400 \t Validation Loss: 37.74571119427681\n",
      "Epoch 12 \t Batch 420 \t Validation Loss: 37.81829065708887\n",
      "Epoch 12 \t Batch 440 \t Validation Loss: 37.53481873707338\n",
      "Epoch 12 \t Batch 460 \t Validation Loss: 37.74978573944258\n",
      "Epoch 12 \t Batch 480 \t Validation Loss: 38.26814967095852\n",
      "Epoch 12 \t Batch 500 \t Validation Loss: 38.01726345539093\n",
      "Epoch 12 \t Batch 520 \t Validation Loss: 37.767338540920846\n",
      "Epoch 12 \t Batch 540 \t Validation Loss: 37.466818496033\n",
      "Epoch 12 \t Batch 560 \t Validation Loss: 37.24746808750289\n",
      "Epoch 12 \t Batch 580 \t Validation Loss: 36.98567991010074\n",
      "Epoch 12 \t Batch 600 \t Validation Loss: 37.16864361524582\n",
      "Epoch 12 Training Loss: 48.368161591474724 Validation Loss: 37.80517639510043\n",
      "Epoch 12 completed\n",
      "Epoch 13 \t Batch 20 \t Training Loss: 47.572980880737305\n",
      "Epoch 13 \t Batch 40 \t Training Loss: 48.016459941864014\n",
      "Epoch 13 \t Batch 60 \t Training Loss: 48.10441888173421\n",
      "Epoch 13 \t Batch 80 \t Training Loss: 48.454751586914064\n",
      "Epoch 13 \t Batch 100 \t Training Loss: 48.3298987197876\n",
      "Epoch 13 \t Batch 120 \t Training Loss: 48.31472953160604\n",
      "Epoch 13 \t Batch 140 \t Training Loss: 48.405169296264646\n",
      "Epoch 13 \t Batch 160 \t Training Loss: 48.494148468971254\n",
      "Epoch 13 \t Batch 180 \t Training Loss: 48.41140028635661\n",
      "Epoch 13 \t Batch 200 \t Training Loss: 48.428675193786624\n",
      "Epoch 13 \t Batch 220 \t Training Loss: 48.435221689397636\n",
      "Epoch 13 \t Batch 240 \t Training Loss: 48.27892295519511\n",
      "Epoch 13 \t Batch 260 \t Training Loss: 48.230817339970514\n",
      "Epoch 13 \t Batch 280 \t Training Loss: 48.27022909436907\n",
      "Epoch 13 \t Batch 300 \t Training Loss: 48.28884113311768\n",
      "Epoch 13 \t Batch 320 \t Training Loss: 48.321486341953275\n",
      "Epoch 13 \t Batch 340 \t Training Loss: 48.28060915329877\n",
      "Epoch 13 \t Batch 360 \t Training Loss: 48.19650100072225\n",
      "Epoch 13 \t Batch 380 \t Training Loss: 48.1925460213109\n",
      "Epoch 13 \t Batch 400 \t Training Loss: 48.127865352630614\n",
      "Epoch 13 \t Batch 420 \t Training Loss: 48.17852577936082\n",
      "Epoch 13 \t Batch 440 \t Training Loss: 48.09355333501642\n",
      "Epoch 13 \t Batch 460 \t Training Loss: 48.1585007874862\n",
      "Epoch 13 \t Batch 480 \t Training Loss: 48.22553356488546\n",
      "Epoch 13 \t Batch 500 \t Training Loss: 48.2732280960083\n",
      "Epoch 13 \t Batch 520 \t Training Loss: 48.31286733333881\n",
      "Epoch 13 \t Batch 540 \t Training Loss: 48.29711391307689\n",
      "Epoch 13 \t Batch 560 \t Training Loss: 48.29158124923706\n",
      "Epoch 13 \t Batch 580 \t Training Loss: 48.275810603437755\n",
      "Epoch 13 \t Batch 600 \t Training Loss: 48.23236841201782\n",
      "Epoch 13 \t Batch 620 \t Training Loss: 48.18407240836851\n",
      "Epoch 13 \t Batch 640 \t Training Loss: 48.26936515569687\n",
      "Epoch 13 \t Batch 660 \t Training Loss: 48.23710500543768\n",
      "Epoch 13 \t Batch 680 \t Training Loss: 48.23424237756168\n",
      "Epoch 13 \t Batch 700 \t Training Loss: 48.252816783360075\n",
      "Epoch 13 \t Batch 720 \t Training Loss: 48.234740278455945\n",
      "Epoch 13 \t Batch 740 \t Training Loss: 48.250001458863956\n",
      "Epoch 13 \t Batch 760 \t Training Loss: 48.27170238494873\n",
      "Epoch 13 \t Batch 780 \t Training Loss: 48.288776519971016\n",
      "Epoch 13 \t Batch 800 \t Training Loss: 48.312895808219906\n",
      "Epoch 13 \t Batch 820 \t Training Loss: 48.29741969224883\n",
      "Epoch 13 \t Batch 840 \t Training Loss: 48.31528344835554\n",
      "Epoch 13 \t Batch 860 \t Training Loss: 48.27296297383863\n",
      "Epoch 13 \t Batch 880 \t Training Loss: 48.27231474356218\n",
      "Epoch 13 \t Batch 900 \t Training Loss: 48.26239386664496\n",
      "Epoch 13 \t Batch 20 \t Validation Loss: 16.213587045669556\n",
      "Epoch 13 \t Batch 40 \t Validation Loss: 19.656278908252716\n",
      "Epoch 13 \t Batch 60 \t Validation Loss: 19.09059062798818\n",
      "Epoch 13 \t Batch 80 \t Validation Loss: 19.495848840475084\n",
      "Epoch 13 \t Batch 100 \t Validation Loss: 21.081193175315857\n",
      "Epoch 13 \t Batch 120 \t Validation Loss: 22.44139111439387\n",
      "Epoch 13 \t Batch 140 \t Validation Loss: 23.130024640900746\n",
      "Epoch 13 \t Batch 160 \t Validation Loss: 25.365171805024147\n",
      "Epoch 13 \t Batch 180 \t Validation Loss: 29.53791361120012\n",
      "Epoch 13 \t Batch 200 \t Validation Loss: 31.30188731431961\n",
      "Epoch 13 \t Batch 220 \t Validation Loss: 32.93108582279899\n",
      "Epoch 13 \t Batch 240 \t Validation Loss: 33.64036870598793\n",
      "Epoch 13 \t Batch 260 \t Validation Loss: 35.90212302758143\n",
      "Epoch 13 \t Batch 280 \t Validation Loss: 37.12715233904975\n",
      "Epoch 13 \t Batch 300 \t Validation Loss: 38.47230322360993\n",
      "Epoch 13 \t Batch 320 \t Validation Loss: 39.0717338129878\n",
      "Epoch 13 \t Batch 340 \t Validation Loss: 39.03487495394314\n",
      "Epoch 13 \t Batch 360 \t Validation Loss: 38.93892432186339\n",
      "Epoch 13 \t Batch 380 \t Validation Loss: 39.1821609886069\n",
      "Epoch 13 \t Batch 400 \t Validation Loss: 38.719859470129016\n",
      "Epoch 13 \t Batch 420 \t Validation Loss: 38.72209150564103\n",
      "Epoch 13 \t Batch 440 \t Validation Loss: 38.377778724106875\n",
      "Epoch 13 \t Batch 460 \t Validation Loss: 38.554344217673595\n",
      "Epoch 13 \t Batch 480 \t Validation Loss: 39.0199336240689\n",
      "Epoch 13 \t Batch 500 \t Validation Loss: 38.71496536922455\n",
      "Epoch 13 \t Batch 520 \t Validation Loss: 38.420076717780184\n",
      "Epoch 13 \t Batch 540 \t Validation Loss: 38.130228260711384\n",
      "Epoch 13 \t Batch 560 \t Validation Loss: 37.88357247199331\n",
      "Epoch 13 \t Batch 580 \t Validation Loss: 37.55553939506925\n",
      "Epoch 13 \t Batch 600 \t Validation Loss: 37.75557850599289\n",
      "Epoch 13 Training Loss: 48.27809341913221 Validation Loss: 38.342426759082\n",
      "Epoch 13 completed\n",
      "Epoch 14 \t Batch 20 \t Training Loss: 46.900823974609374\n",
      "Epoch 14 \t Batch 40 \t Training Loss: 47.855240154266355\n",
      "Epoch 14 \t Batch 60 \t Training Loss: 48.229880777994794\n",
      "Epoch 14 \t Batch 80 \t Training Loss: 47.91172204017639\n",
      "Epoch 14 \t Batch 100 \t Training Loss: 48.32074073791504\n",
      "Epoch 14 \t Batch 120 \t Training Loss: 48.26704800923665\n",
      "Epoch 14 \t Batch 140 \t Training Loss: 48.29084655216762\n",
      "Epoch 14 \t Batch 160 \t Training Loss: 48.332127809524536\n",
      "Epoch 14 \t Batch 180 \t Training Loss: 48.38979969024658\n",
      "Epoch 14 \t Batch 200 \t Training Loss: 48.40496723175049\n",
      "Epoch 14 \t Batch 220 \t Training Loss: 48.45414435646751\n",
      "Epoch 14 \t Batch 240 \t Training Loss: 48.35749991734823\n",
      "Epoch 14 \t Batch 260 \t Training Loss: 48.52559261322021\n",
      "Epoch 14 \t Batch 280 \t Training Loss: 48.60374134608677\n",
      "Epoch 14 \t Batch 300 \t Training Loss: 48.463969434102374\n",
      "Epoch 14 \t Batch 320 \t Training Loss: 48.45756837129593\n",
      "Epoch 14 \t Batch 340 \t Training Loss: 48.503622694576485\n",
      "Epoch 14 \t Batch 360 \t Training Loss: 48.446839565700955\n",
      "Epoch 14 \t Batch 380 \t Training Loss: 48.36856132306551\n",
      "Epoch 14 \t Batch 400 \t Training Loss: 48.37752935409546\n",
      "Epoch 14 \t Batch 420 \t Training Loss: 48.36926703680129\n",
      "Epoch 14 \t Batch 440 \t Training Loss: 48.303857352516864\n",
      "Epoch 14 \t Batch 460 \t Training Loss: 48.334513000819996\n",
      "Epoch 14 \t Batch 480 \t Training Loss: 48.345529961585996\n",
      "Epoch 14 \t Batch 500 \t Training Loss: 48.301285690307616\n",
      "Epoch 14 \t Batch 520 \t Training Loss: 48.22693848976722\n",
      "Epoch 14 \t Batch 540 \t Training Loss: 48.18666613543475\n",
      "Epoch 14 \t Batch 560 \t Training Loss: 48.17999397005354\n",
      "Epoch 14 \t Batch 580 \t Training Loss: 48.181900333536085\n",
      "Epoch 14 \t Batch 600 \t Training Loss: 48.226604563395185\n",
      "Epoch 14 \t Batch 620 \t Training Loss: 48.20560222748787\n",
      "Epoch 14 \t Batch 640 \t Training Loss: 48.21664627194404\n",
      "Epoch 14 \t Batch 660 \t Training Loss: 48.22646528301817\n",
      "Epoch 14 \t Batch 680 \t Training Loss: 48.25888013839722\n",
      "Epoch 14 \t Batch 700 \t Training Loss: 48.26667895725795\n",
      "Epoch 14 \t Batch 720 \t Training Loss: 48.29133491516113\n",
      "Epoch 14 \t Batch 740 \t Training Loss: 48.28009317243421\n",
      "Epoch 14 \t Batch 760 \t Training Loss: 48.256549820147065\n",
      "Epoch 14 \t Batch 780 \t Training Loss: 48.251155867943396\n",
      "Epoch 14 \t Batch 800 \t Training Loss: 48.268447723388675\n",
      "Epoch 14 \t Batch 820 \t Training Loss: 48.281884109683155\n",
      "Epoch 14 \t Batch 840 \t Training Loss: 48.25547393617176\n",
      "Epoch 14 \t Batch 860 \t Training Loss: 48.221370022795924\n",
      "Epoch 14 \t Batch 880 \t Training Loss: 48.212687223607844\n",
      "Epoch 14 \t Batch 900 \t Training Loss: 48.206861686706546\n",
      "Epoch 14 \t Batch 20 \t Validation Loss: 19.912670707702638\n",
      "Epoch 14 \t Batch 40 \t Validation Loss: 22.99764738082886\n",
      "Epoch 14 \t Batch 60 \t Validation Loss: 22.523303683598836\n",
      "Epoch 14 \t Batch 80 \t Validation Loss: 22.918306159973145\n",
      "Epoch 14 \t Batch 100 \t Validation Loss: 23.77160343170166\n",
      "Epoch 14 \t Batch 120 \t Validation Loss: 24.646891315778095\n",
      "Epoch 14 \t Batch 140 \t Validation Loss: 25.043420287540982\n",
      "Epoch 14 \t Batch 160 \t Validation Loss: 27.27812329530716\n",
      "Epoch 14 \t Batch 180 \t Validation Loss: 31.523674085405137\n",
      "Epoch 14 \t Batch 200 \t Validation Loss: 33.3018919467926\n",
      "Epoch 14 \t Batch 220 \t Validation Loss: 34.95584831237793\n",
      "Epoch 14 \t Batch 240 \t Validation Loss: 35.6640761256218\n",
      "Epoch 14 \t Batch 260 \t Validation Loss: 37.9395470729241\n",
      "Epoch 14 \t Batch 280 \t Validation Loss: 39.054615415845596\n",
      "Epoch 14 \t Batch 300 \t Validation Loss: 40.4699207051595\n",
      "Epoch 14 \t Batch 320 \t Validation Loss: 41.08511999845505\n",
      "Epoch 14 \t Batch 340 \t Validation Loss: 41.00483149921193\n",
      "Epoch 14 \t Batch 360 \t Validation Loss: 40.874482811821835\n",
      "Epoch 14 \t Batch 380 \t Validation Loss: 41.08471646559866\n",
      "Epoch 14 \t Batch 400 \t Validation Loss: 40.597012333869934\n",
      "Epoch 14 \t Batch 420 \t Validation Loss: 40.549804321924846\n",
      "Epoch 14 \t Batch 440 \t Validation Loss: 40.187400663982736\n",
      "Epoch 14 \t Batch 460 \t Validation Loss: 40.339270436245464\n",
      "Epoch 14 \t Batch 480 \t Validation Loss: 40.78242705066999\n",
      "Epoch 14 \t Batch 500 \t Validation Loss: 40.45909951972961\n",
      "Epoch 14 \t Batch 520 \t Validation Loss: 40.17904535256899\n",
      "Epoch 14 \t Batch 540 \t Validation Loss: 39.86137532658047\n",
      "Epoch 14 \t Batch 560 \t Validation Loss: 39.59425142662866\n",
      "Epoch 14 \t Batch 580 \t Validation Loss: 39.32010201585704\n",
      "Epoch 14 \t Batch 600 \t Validation Loss: 39.46188175360362\n",
      "Epoch 14 Training Loss: 48.181140550220285 Validation Loss: 40.089784406996394\n",
      "Epoch 14 completed\n",
      "Epoch 15 \t Batch 20 \t Training Loss: 47.58076400756836\n",
      "Epoch 15 \t Batch 40 \t Training Loss: 46.78455924987793\n",
      "Epoch 15 \t Batch 60 \t Training Loss: 47.25778071085612\n",
      "Epoch 15 \t Batch 80 \t Training Loss: 47.58682403564453\n",
      "Epoch 15 \t Batch 100 \t Training Loss: 47.25767524719238\n",
      "Epoch 15 \t Batch 120 \t Training Loss: 47.461699676513675\n",
      "Epoch 15 \t Batch 140 \t Training Loss: 47.378027370997835\n",
      "Epoch 15 \t Batch 160 \t Training Loss: 47.52762670516968\n",
      "Epoch 15 \t Batch 180 \t Training Loss: 47.573487514919705\n",
      "Epoch 15 \t Batch 200 \t Training Loss: 47.74308656692505\n",
      "Epoch 15 \t Batch 220 \t Training Loss: 47.72546827142889\n",
      "Epoch 15 \t Batch 240 \t Training Loss: 47.79904217720032\n",
      "Epoch 15 \t Batch 260 \t Training Loss: 47.95354204911452\n",
      "Epoch 15 \t Batch 280 \t Training Loss: 47.98184811728341\n",
      "Epoch 15 \t Batch 300 \t Training Loss: 47.910819155375165\n",
      "Epoch 15 \t Batch 320 \t Training Loss: 47.95305480957031\n",
      "Epoch 15 \t Batch 340 \t Training Loss: 47.975261564815746\n",
      "Epoch 15 \t Batch 360 \t Training Loss: 48.02646928363376\n",
      "Epoch 15 \t Batch 380 \t Training Loss: 47.9236857464439\n",
      "Epoch 15 \t Batch 400 \t Training Loss: 47.87086453437805\n",
      "Epoch 15 \t Batch 420 \t Training Loss: 47.85598864782424\n",
      "Epoch 15 \t Batch 440 \t Training Loss: 47.86957562186501\n",
      "Epoch 15 \t Batch 460 \t Training Loss: 47.83411171954611\n",
      "Epoch 15 \t Batch 480 \t Training Loss: 47.82569092114766\n",
      "Epoch 15 \t Batch 500 \t Training Loss: 47.874219345092776\n",
      "Epoch 15 \t Batch 520 \t Training Loss: 47.919139289855956\n",
      "Epoch 15 \t Batch 540 \t Training Loss: 47.91058445683232\n",
      "Epoch 15 \t Batch 560 \t Training Loss: 47.96899996485029\n",
      "Epoch 15 \t Batch 580 \t Training Loss: 47.96093815770642\n",
      "Epoch 15 \t Batch 600 \t Training Loss: 47.9804368464152\n",
      "Epoch 15 \t Batch 620 \t Training Loss: 47.9815220309842\n",
      "Epoch 15 \t Batch 640 \t Training Loss: 47.991655844449994\n",
      "Epoch 15 \t Batch 660 \t Training Loss: 47.97834514271129\n",
      "Epoch 15 \t Batch 680 \t Training Loss: 48.036405125786274\n",
      "Epoch 15 \t Batch 700 \t Training Loss: 47.98833255767822\n",
      "Epoch 15 \t Batch 720 \t Training Loss: 48.013697693083024\n",
      "Epoch 15 \t Batch 740 \t Training Loss: 48.034572632248334\n",
      "Epoch 15 \t Batch 760 \t Training Loss: 48.03639326095581\n",
      "Epoch 15 \t Batch 780 \t Training Loss: 48.062004250746504\n",
      "Epoch 15 \t Batch 800 \t Training Loss: 48.06255609035492\n",
      "Epoch 15 \t Batch 820 \t Training Loss: 48.09917843051073\n",
      "Epoch 15 \t Batch 840 \t Training Loss: 48.10085314796085\n",
      "Epoch 15 \t Batch 860 \t Training Loss: 48.11882858719937\n",
      "Epoch 15 \t Batch 880 \t Training Loss: 48.1320736321536\n",
      "Epoch 15 \t Batch 900 \t Training Loss: 48.10434150271946\n",
      "Epoch 15 \t Batch 20 \t Validation Loss: 15.55880880355835\n",
      "Epoch 15 \t Batch 40 \t Validation Loss: 19.77278107404709\n",
      "Epoch 15 \t Batch 60 \t Validation Loss: 19.030643232663472\n",
      "Epoch 15 \t Batch 80 \t Validation Loss: 19.27765459418297\n",
      "Epoch 15 \t Batch 100 \t Validation Loss: 20.900170969963074\n",
      "Epoch 15 \t Batch 120 \t Validation Loss: 22.307677718003593\n",
      "Epoch 15 \t Batch 140 \t Validation Loss: 23.095477298327854\n",
      "Epoch 15 \t Batch 160 \t Validation Loss: 25.370184674859047\n",
      "Epoch 15 \t Batch 180 \t Validation Loss: 29.461600677172342\n",
      "Epoch 15 \t Batch 200 \t Validation Loss: 31.216462767124177\n",
      "Epoch 15 \t Batch 220 \t Validation Loss: 32.78525326685472\n",
      "Epoch 15 \t Batch 240 \t Validation Loss: 33.48898692329725\n",
      "Epoch 15 \t Batch 260 \t Validation Loss: 35.736121832407434\n",
      "Epoch 15 \t Batch 280 \t Validation Loss: 36.95086574384144\n",
      "Epoch 15 \t Batch 300 \t Validation Loss: 38.262680630683896\n",
      "Epoch 15 \t Batch 320 \t Validation Loss: 38.867034529149535\n",
      "Epoch 15 \t Batch 340 \t Validation Loss: 38.847138094902036\n",
      "Epoch 15 \t Batch 360 \t Validation Loss: 38.76666951047049\n",
      "Epoch 15 \t Batch 380 \t Validation Loss: 39.032743442685984\n",
      "Epoch 15 \t Batch 400 \t Validation Loss: 38.63904105067253\n",
      "Epoch 15 \t Batch 420 \t Validation Loss: 38.65817182064056\n",
      "Epoch 15 \t Batch 440 \t Validation Loss: 38.36751946427605\n",
      "Epoch 15 \t Batch 460 \t Validation Loss: 38.586042063132574\n",
      "Epoch 15 \t Batch 480 \t Validation Loss: 39.06081426044305\n",
      "Epoch 15 \t Batch 500 \t Validation Loss: 38.77207853984833\n",
      "Epoch 15 \t Batch 520 \t Validation Loss: 38.545766306840456\n",
      "Epoch 15 \t Batch 540 \t Validation Loss: 38.30428333370774\n",
      "Epoch 15 \t Batch 560 \t Validation Loss: 38.08844841974122\n",
      "Epoch 15 \t Batch 580 \t Validation Loss: 37.871062960295845\n",
      "Epoch 15 \t Batch 600 \t Validation Loss: 38.07084855794906\n",
      "Epoch 15 Training Loss: 48.10052028477127 Validation Loss: 38.71677372440115\n",
      "Epoch 15 completed\n",
      "Epoch 16 \t Batch 20 \t Training Loss: 47.288803291320804\n",
      "Epoch 16 \t Batch 40 \t Training Loss: 46.96865386962891\n",
      "Epoch 16 \t Batch 60 \t Training Loss: 46.98094730377197\n",
      "Epoch 16 \t Batch 80 \t Training Loss: 47.04463782310486\n",
      "Epoch 16 \t Batch 100 \t Training Loss: 47.2679923248291\n",
      "Epoch 16 \t Batch 120 \t Training Loss: 47.40980342229207\n",
      "Epoch 16 \t Batch 140 \t Training Loss: 47.61103030613491\n",
      "Epoch 16 \t Batch 160 \t Training Loss: 47.657113790512085\n",
      "Epoch 16 \t Batch 180 \t Training Loss: 47.744700029161244\n",
      "Epoch 16 \t Batch 200 \t Training Loss: 47.800326633453366\n",
      "Epoch 16 \t Batch 220 \t Training Loss: 47.88762533014471\n",
      "Epoch 16 \t Batch 240 \t Training Loss: 47.88103391329447\n",
      "Epoch 16 \t Batch 260 \t Training Loss: 47.992567135737495\n",
      "Epoch 16 \t Batch 280 \t Training Loss: 48.04436276299613\n",
      "Epoch 16 \t Batch 300 \t Training Loss: 48.05772872924805\n",
      "Epoch 16 \t Batch 320 \t Training Loss: 48.05171662569046\n",
      "Epoch 16 \t Batch 340 \t Training Loss: 48.023594497231876\n",
      "Epoch 16 \t Batch 360 \t Training Loss: 48.08019193013509\n",
      "Epoch 16 \t Batch 380 \t Training Loss: 48.03038857108668\n",
      "Epoch 16 \t Batch 400 \t Training Loss: 48.071308336257935\n",
      "Epoch 16 \t Batch 420 \t Training Loss: 48.13816876184373\n",
      "Epoch 16 \t Batch 440 \t Training Loss: 48.180840917067094\n",
      "Epoch 16 \t Batch 460 \t Training Loss: 48.1514728711999\n",
      "Epoch 16 \t Batch 480 \t Training Loss: 48.136790760358174\n",
      "Epoch 16 \t Batch 500 \t Training Loss: 48.12161904144287\n",
      "Epoch 16 \t Batch 520 \t Training Loss: 48.138822804964505\n",
      "Epoch 16 \t Batch 540 \t Training Loss: 48.110821900544344\n",
      "Epoch 16 \t Batch 560 \t Training Loss: 48.119272913251606\n",
      "Epoch 16 \t Batch 580 \t Training Loss: 48.10623560280636\n",
      "Epoch 16 \t Batch 600 \t Training Loss: 48.07488384246826\n",
      "Epoch 16 \t Batch 620 \t Training Loss: 48.09721691377701\n",
      "Epoch 16 \t Batch 640 \t Training Loss: 48.069110721349716\n",
      "Epoch 16 \t Batch 660 \t Training Loss: 48.07597190394546\n",
      "Epoch 16 \t Batch 680 \t Training Loss: 48.06108221166274\n",
      "Epoch 16 \t Batch 700 \t Training Loss: 48.12722194671631\n",
      "Epoch 16 \t Batch 720 \t Training Loss: 48.124139383104115\n",
      "Epoch 16 \t Batch 740 \t Training Loss: 48.0824150446299\n",
      "Epoch 16 \t Batch 760 \t Training Loss: 48.04926145453202\n",
      "Epoch 16 \t Batch 780 \t Training Loss: 48.04527920453976\n",
      "Epoch 16 \t Batch 800 \t Training Loss: 48.0690568113327\n",
      "Epoch 16 \t Batch 820 \t Training Loss: 48.09961758125119\n",
      "Epoch 16 \t Batch 840 \t Training Loss: 48.05726786113921\n",
      "Epoch 16 \t Batch 860 \t Training Loss: 48.06353871101557\n",
      "Epoch 16 \t Batch 880 \t Training Loss: 48.03367957201871\n",
      "Epoch 16 \t Batch 900 \t Training Loss: 48.00615042368571\n",
      "Epoch 16 \t Batch 20 \t Validation Loss: 16.165450239181517\n",
      "Epoch 16 \t Batch 40 \t Validation Loss: 20.26899070739746\n",
      "Epoch 16 \t Batch 60 \t Validation Loss: 19.43787077267965\n",
      "Epoch 16 \t Batch 80 \t Validation Loss: 20.15525723695755\n",
      "Epoch 16 \t Batch 100 \t Validation Loss: 21.54570580482483\n",
      "Epoch 16 \t Batch 120 \t Validation Loss: 22.751726341247558\n",
      "Epoch 16 \t Batch 140 \t Validation Loss: 23.450089318411692\n",
      "Epoch 16 \t Batch 160 \t Validation Loss: 25.787584793567657\n",
      "Epoch 16 \t Batch 180 \t Validation Loss: 29.972438102298312\n",
      "Epoch 16 \t Batch 200 \t Validation Loss: 31.770213594436644\n",
      "Epoch 16 \t Batch 220 \t Validation Loss: 33.40135224082253\n",
      "Epoch 16 \t Batch 240 \t Validation Loss: 34.14459119240443\n",
      "Epoch 16 \t Batch 260 \t Validation Loss: 36.47281829393827\n",
      "Epoch 16 \t Batch 280 \t Validation Loss: 37.68933852059501\n",
      "Epoch 16 \t Batch 300 \t Validation Loss: 39.03092912038167\n",
      "Epoch 16 \t Batch 320 \t Validation Loss: 39.63415230512619\n",
      "Epoch 16 \t Batch 340 \t Validation Loss: 39.58849334716797\n",
      "Epoch 16 \t Batch 360 \t Validation Loss: 39.51378985775842\n",
      "Epoch 16 \t Batch 380 \t Validation Loss: 39.78061796238548\n",
      "Epoch 16 \t Batch 400 \t Validation Loss: 39.3567887377739\n",
      "Epoch 16 \t Batch 420 \t Validation Loss: 39.31881350562686\n",
      "Epoch 16 \t Batch 440 \t Validation Loss: 38.98774555813183\n",
      "Epoch 16 \t Batch 460 \t Validation Loss: 39.15539380778437\n",
      "Epoch 16 \t Batch 480 \t Validation Loss: 39.63201415538788\n",
      "Epoch 16 \t Batch 500 \t Validation Loss: 39.320781414031984\n",
      "Epoch 16 \t Batch 520 \t Validation Loss: 39.094214430222145\n",
      "Epoch 16 \t Batch 540 \t Validation Loss: 38.868943205586184\n",
      "Epoch 16 \t Batch 560 \t Validation Loss: 38.699585436071665\n",
      "Epoch 16 \t Batch 580 \t Validation Loss: 38.58693707071502\n",
      "Epoch 16 \t Batch 600 \t Validation Loss: 38.79892077922821\n",
      "Epoch 16 Training Loss: 48.038308970410796 Validation Loss: 39.468822694444036\n",
      "Epoch 16 completed\n",
      "Epoch 17 \t Batch 20 \t Training Loss: 47.849956321716306\n",
      "Epoch 17 \t Batch 40 \t Training Loss: 47.766967964172366\n",
      "Epoch 17 \t Batch 60 \t Training Loss: 47.74569396972656\n",
      "Epoch 17 \t Batch 80 \t Training Loss: 47.889486122131345\n",
      "Epoch 17 \t Batch 100 \t Training Loss: 47.97831687927246\n",
      "Epoch 17 \t Batch 120 \t Training Loss: 47.89075921376546\n",
      "Epoch 17 \t Batch 140 \t Training Loss: 48.05707152230399\n",
      "Epoch 17 \t Batch 160 \t Training Loss: 47.99459636211395\n",
      "Epoch 17 \t Batch 180 \t Training Loss: 48.05770123799642\n",
      "Epoch 17 \t Batch 200 \t Training Loss: 48.15685241699219\n",
      "Epoch 17 \t Batch 220 \t Training Loss: 48.147471219843084\n",
      "Epoch 17 \t Batch 240 \t Training Loss: 48.22072679201762\n",
      "Epoch 17 \t Batch 260 \t Training Loss: 48.12403180049016\n",
      "Epoch 17 \t Batch 280 \t Training Loss: 48.11291555677141\n",
      "Epoch 17 \t Batch 300 \t Training Loss: 48.00219332377116\n",
      "Epoch 17 \t Batch 320 \t Training Loss: 47.96229919195175\n",
      "Epoch 17 \t Batch 340 \t Training Loss: 47.8958854563096\n",
      "Epoch 17 \t Batch 360 \t Training Loss: 47.86551107830471\n",
      "Epoch 17 \t Batch 380 \t Training Loss: 47.922862765663545\n",
      "Epoch 17 \t Batch 400 \t Training Loss: 47.87724104881286\n",
      "Epoch 17 \t Batch 420 \t Training Loss: 47.87478086380732\n",
      "Epoch 17 \t Batch 440 \t Training Loss: 47.88159411170266\n",
      "Epoch 17 \t Batch 460 \t Training Loss: 47.81078271451204\n",
      "Epoch 17 \t Batch 480 \t Training Loss: 47.81426083246867\n",
      "Epoch 17 \t Batch 500 \t Training Loss: 47.77481881713867\n",
      "Epoch 17 \t Batch 520 \t Training Loss: 47.803985984508806\n",
      "Epoch 17 \t Batch 540 \t Training Loss: 47.80118842654758\n",
      "Epoch 17 \t Batch 560 \t Training Loss: 47.823377050672256\n",
      "Epoch 17 \t Batch 580 \t Training Loss: 47.86586832506903\n",
      "Epoch 17 \t Batch 600 \t Training Loss: 47.85801700592041\n",
      "Epoch 17 \t Batch 620 \t Training Loss: 47.89255248654273\n",
      "Epoch 17 \t Batch 640 \t Training Loss: 47.960515832901\n",
      "Epoch 17 \t Batch 660 \t Training Loss: 47.90990776293206\n",
      "Epoch 17 \t Batch 680 \t Training Loss: 47.94940725214341\n",
      "Epoch 17 \t Batch 700 \t Training Loss: 47.948700784955705\n",
      "Epoch 17 \t Batch 720 \t Training Loss: 47.9546173148685\n",
      "Epoch 17 \t Batch 740 \t Training Loss: 47.958713485099175\n",
      "Epoch 17 \t Batch 760 \t Training Loss: 47.907651514756054\n",
      "Epoch 17 \t Batch 780 \t Training Loss: 47.89384558751033\n",
      "Epoch 17 \t Batch 800 \t Training Loss: 47.880963468551634\n",
      "Epoch 17 \t Batch 820 \t Training Loss: 47.892530124943434\n",
      "Epoch 17 \t Batch 840 \t Training Loss: 47.89253535951887\n",
      "Epoch 17 \t Batch 860 \t Training Loss: 47.93694078312364\n",
      "Epoch 17 \t Batch 880 \t Training Loss: 47.97765606966885\n",
      "Epoch 17 \t Batch 900 \t Training Loss: 47.96183132595486\n",
      "Epoch 17 \t Batch 20 \t Validation Loss: 18.115143823623658\n",
      "Epoch 17 \t Batch 40 \t Validation Loss: 21.297126626968385\n",
      "Epoch 17 \t Batch 60 \t Validation Loss: 20.866172393163044\n",
      "Epoch 17 \t Batch 80 \t Validation Loss: 21.14830837249756\n",
      "Epoch 17 \t Batch 100 \t Validation Loss: 22.471067848205568\n",
      "Epoch 17 \t Batch 120 \t Validation Loss: 23.63141485850016\n",
      "Epoch 17 \t Batch 140 \t Validation Loss: 24.20583144596645\n",
      "Epoch 17 \t Batch 160 \t Validation Loss: 26.434824550151824\n",
      "Epoch 17 \t Batch 180 \t Validation Loss: 30.438973675833807\n",
      "Epoch 17 \t Batch 200 \t Validation Loss: 32.1478218793869\n",
      "Epoch 17 \t Batch 220 \t Validation Loss: 33.794677660681984\n",
      "Epoch 17 \t Batch 240 \t Validation Loss: 34.491066034634905\n",
      "Epoch 17 \t Batch 260 \t Validation Loss: 36.82457248981182\n",
      "Epoch 17 \t Batch 280 \t Validation Loss: 38.10275875840868\n",
      "Epoch 17 \t Batch 300 \t Validation Loss: 39.2730273660024\n",
      "Epoch 17 \t Batch 320 \t Validation Loss: 39.81785407364369\n",
      "Epoch 17 \t Batch 340 \t Validation Loss: 39.75991187095642\n",
      "Epoch 17 \t Batch 360 \t Validation Loss: 39.62587885326809\n",
      "Epoch 17 \t Batch 380 \t Validation Loss: 39.8928357827036\n",
      "Epoch 17 \t Batch 400 \t Validation Loss: 39.42555709838867\n",
      "Epoch 17 \t Batch 420 \t Validation Loss: 39.40871740522839\n",
      "Epoch 17 \t Batch 440 \t Validation Loss: 39.08199625882236\n",
      "Epoch 17 \t Batch 460 \t Validation Loss: 39.26612519388613\n",
      "Epoch 17 \t Batch 480 \t Validation Loss: 39.711999678611754\n",
      "Epoch 17 \t Batch 500 \t Validation Loss: 39.40597204208374\n",
      "Epoch 17 \t Batch 520 \t Validation Loss: 39.11900151692904\n",
      "Epoch 17 \t Batch 540 \t Validation Loss: 38.846157232920326\n",
      "Epoch 17 \t Batch 560 \t Validation Loss: 38.61698387690953\n",
      "Epoch 17 \t Batch 580 \t Validation Loss: 38.32410883081371\n",
      "Epoch 17 \t Batch 600 \t Validation Loss: 38.509164079030356\n",
      "Epoch 17 Training Loss: 47.977622565682346 Validation Loss: 39.10677278815926\n",
      "Epoch 17 completed\n",
      "Epoch 18 \t Batch 20 \t Training Loss: 49.155300331115725\n",
      "Epoch 18 \t Batch 40 \t Training Loss: 48.281379318237306\n",
      "Epoch 18 \t Batch 60 \t Training Loss: 48.476268196105956\n",
      "Epoch 18 \t Batch 80 \t Training Loss: 48.35196666717529\n",
      "Epoch 18 \t Batch 100 \t Training Loss: 48.1548747253418\n",
      "Epoch 18 \t Batch 120 \t Training Loss: 48.27903385162354\n",
      "Epoch 18 \t Batch 140 \t Training Loss: 48.391453688485285\n",
      "Epoch 18 \t Batch 160 \t Training Loss: 48.3727703332901\n",
      "Epoch 18 \t Batch 180 \t Training Loss: 48.16569324069553\n",
      "Epoch 18 \t Batch 200 \t Training Loss: 48.040006275177\n",
      "Epoch 18 \t Batch 220 \t Training Loss: 48.06321783932773\n",
      "Epoch 18 \t Batch 240 \t Training Loss: 48.10960442225139\n",
      "Epoch 18 \t Batch 260 \t Training Loss: 47.95189701960637\n",
      "Epoch 18 \t Batch 280 \t Training Loss: 47.98761555807931\n",
      "Epoch 18 \t Batch 300 \t Training Loss: 48.185715738932295\n",
      "Epoch 18 \t Batch 320 \t Training Loss: 48.23754036426544\n",
      "Epoch 18 \t Batch 340 \t Training Loss: 48.205750644908235\n",
      "Epoch 18 \t Batch 360 \t Training Loss: 48.16494160758124\n",
      "Epoch 18 \t Batch 380 \t Training Loss: 48.07183699356882\n",
      "Epoch 18 \t Batch 400 \t Training Loss: 48.03387315750122\n",
      "Epoch 18 \t Batch 420 \t Training Loss: 47.990194656735376\n",
      "Epoch 18 \t Batch 440 \t Training Loss: 47.948415184020995\n",
      "Epoch 18 \t Batch 460 \t Training Loss: 47.87585332289986\n",
      "Epoch 18 \t Batch 480 \t Training Loss: 47.822549724578856\n",
      "Epoch 18 \t Batch 500 \t Training Loss: 47.76590166473389\n",
      "Epoch 18 \t Batch 520 \t Training Loss: 47.754519015092114\n",
      "Epoch 18 \t Batch 540 \t Training Loss: 47.79663402416088\n",
      "Epoch 18 \t Batch 560 \t Training Loss: 47.83571934700012\n",
      "Epoch 18 \t Batch 580 \t Training Loss: 47.86187754663928\n",
      "Epoch 18 \t Batch 600 \t Training Loss: 47.83470948537191\n",
      "Epoch 18 \t Batch 620 \t Training Loss: 47.84707794804727\n",
      "Epoch 18 \t Batch 640 \t Training Loss: 47.84011477828026\n",
      "Epoch 18 \t Batch 660 \t Training Loss: 47.857784594911514\n",
      "Epoch 18 \t Batch 680 \t Training Loss: 47.85577632118674\n",
      "Epoch 18 \t Batch 700 \t Training Loss: 47.795471856253485\n",
      "Epoch 18 \t Batch 720 \t Training Loss: 47.76903377109104\n",
      "Epoch 18 \t Batch 740 \t Training Loss: 47.767535926200246\n",
      "Epoch 18 \t Batch 760 \t Training Loss: 47.767551141036186\n",
      "Epoch 18 \t Batch 780 \t Training Loss: 47.78228852687738\n",
      "Epoch 18 \t Batch 800 \t Training Loss: 47.82110888004303\n",
      "Epoch 18 \t Batch 820 \t Training Loss: 47.82434084822492\n",
      "Epoch 18 \t Batch 840 \t Training Loss: 47.827316838219055\n",
      "Epoch 18 \t Batch 860 \t Training Loss: 47.83783766280773\n",
      "Epoch 18 \t Batch 880 \t Training Loss: 47.86151478073814\n",
      "Epoch 18 \t Batch 900 \t Training Loss: 47.877930590311685\n",
      "Epoch 18 \t Batch 20 \t Validation Loss: 26.984204149246217\n",
      "Epoch 18 \t Batch 40 \t Validation Loss: 29.36853082180023\n",
      "Epoch 18 \t Batch 60 \t Validation Loss: 29.22356530825297\n",
      "Epoch 18 \t Batch 80 \t Validation Loss: 29.754844033718108\n",
      "Epoch 18 \t Batch 100 \t Validation Loss: 29.37006127357483\n",
      "Epoch 18 \t Batch 120 \t Validation Loss: 29.329754082361855\n",
      "Epoch 18 \t Batch 140 \t Validation Loss: 29.119047233036586\n",
      "Epoch 18 \t Batch 160 \t Validation Loss: 30.61798859834671\n",
      "Epoch 18 \t Batch 180 \t Validation Loss: 33.471193393071495\n",
      "Epoch 18 \t Batch 200 \t Validation Loss: 34.392878613471986\n",
      "Epoch 18 \t Batch 220 \t Validation Loss: 35.263281445069744\n",
      "Epoch 18 \t Batch 240 \t Validation Loss: 35.41430083910624\n",
      "Epoch 18 \t Batch 260 \t Validation Loss: 37.071073950254004\n",
      "Epoch 18 \t Batch 280 \t Validation Loss: 37.767569582802906\n",
      "Epoch 18 \t Batch 300 \t Validation Loss: 38.824154179890954\n",
      "Epoch 18 \t Batch 320 \t Validation Loss: 39.15808790326118\n",
      "Epoch 18 \t Batch 340 \t Validation Loss: 39.0476651023416\n",
      "Epoch 18 \t Batch 360 \t Validation Loss: 38.944684982299805\n",
      "Epoch 18 \t Batch 380 \t Validation Loss: 39.0819077843114\n",
      "Epoch 18 \t Batch 400 \t Validation Loss: 38.68335029602051\n",
      "Epoch 18 \t Batch 420 \t Validation Loss: 38.72674443835304\n",
      "Epoch 18 \t Batch 440 \t Validation Loss: 38.457486967606975\n",
      "Epoch 18 \t Batch 460 \t Validation Loss: 38.697351447395654\n",
      "Epoch 18 \t Batch 480 \t Validation Loss: 39.17248279651006\n",
      "Epoch 18 \t Batch 500 \t Validation Loss: 38.89905326461792\n",
      "Epoch 18 \t Batch 520 \t Validation Loss: 38.66831383705139\n",
      "Epoch 18 \t Batch 540 \t Validation Loss: 38.38857781798751\n",
      "Epoch 18 \t Batch 560 \t Validation Loss: 38.220429754257204\n",
      "Epoch 18 \t Batch 580 \t Validation Loss: 38.070463183830526\n",
      "Epoch 18 \t Batch 600 \t Validation Loss: 38.25152988751729\n",
      "Epoch 18 Training Loss: 47.89580015059776 Validation Loss: 38.893465035921565\n",
      "Epoch 18 completed\n",
      "Epoch 19 \t Batch 20 \t Training Loss: 48.99064903259277\n",
      "Epoch 19 \t Batch 40 \t Training Loss: 48.83125820159912\n",
      "Epoch 19 \t Batch 60 \t Training Loss: 48.58120905558268\n",
      "Epoch 19 \t Batch 80 \t Training Loss: 48.54236574172974\n",
      "Epoch 19 \t Batch 100 \t Training Loss: 48.18381004333496\n",
      "Epoch 19 \t Batch 120 \t Training Loss: 48.26423848470052\n",
      "Epoch 19 \t Batch 140 \t Training Loss: 48.30466703687395\n",
      "Epoch 19 \t Batch 160 \t Training Loss: 48.21001362800598\n",
      "Epoch 19 \t Batch 180 \t Training Loss: 48.03895899454753\n",
      "Epoch 19 \t Batch 200 \t Training Loss: 47.87509696960449\n",
      "Epoch 19 \t Batch 220 \t Training Loss: 47.901951841874556\n",
      "Epoch 19 \t Batch 240 \t Training Loss: 47.8283626238505\n",
      "Epoch 19 \t Batch 260 \t Training Loss: 47.80448873960055\n",
      "Epoch 19 \t Batch 280 \t Training Loss: 47.721480996268134\n",
      "Epoch 19 \t Batch 300 \t Training Loss: 47.722210273742675\n",
      "Epoch 19 \t Batch 320 \t Training Loss: 47.72119717597961\n",
      "Epoch 19 \t Batch 340 \t Training Loss: 47.75848112667308\n",
      "Epoch 19 \t Batch 360 \t Training Loss: 47.7486944410536\n",
      "Epoch 19 \t Batch 380 \t Training Loss: 47.69554411235609\n",
      "Epoch 19 \t Batch 400 \t Training Loss: 47.750476360321045\n",
      "Epoch 19 \t Batch 420 \t Training Loss: 47.753258432660786\n",
      "Epoch 19 \t Batch 440 \t Training Loss: 47.775447472659025\n",
      "Epoch 19 \t Batch 460 \t Training Loss: 47.709032390428625\n",
      "Epoch 19 \t Batch 480 \t Training Loss: 47.745332622528075\n",
      "Epoch 19 \t Batch 500 \t Training Loss: 47.67095570373535\n",
      "Epoch 19 \t Batch 520 \t Training Loss: 47.71766609778771\n",
      "Epoch 19 \t Batch 540 \t Training Loss: 47.72338942068595\n",
      "Epoch 19 \t Batch 560 \t Training Loss: 47.72592590195792\n",
      "Epoch 19 \t Batch 580 \t Training Loss: 47.67351496466275\n",
      "Epoch 19 \t Batch 600 \t Training Loss: 47.72806500116984\n",
      "Epoch 19 \t Batch 620 \t Training Loss: 47.702016092115834\n",
      "Epoch 19 \t Batch 640 \t Training Loss: 47.699760645627975\n",
      "Epoch 19 \t Batch 660 \t Training Loss: 47.66269111055316\n",
      "Epoch 19 \t Batch 680 \t Training Loss: 47.69601926242604\n",
      "Epoch 19 \t Batch 700 \t Training Loss: 47.67141514914376\n",
      "Epoch 19 \t Batch 720 \t Training Loss: 47.73298271497091\n",
      "Epoch 19 \t Batch 740 \t Training Loss: 47.73570067431476\n",
      "Epoch 19 \t Batch 760 \t Training Loss: 47.72969859775744\n",
      "Epoch 19 \t Batch 780 \t Training Loss: 47.70231416653364\n",
      "Epoch 19 \t Batch 800 \t Training Loss: 47.735779695510864\n",
      "Epoch 19 \t Batch 820 \t Training Loss: 47.73985616172232\n",
      "Epoch 19 \t Batch 840 \t Training Loss: 47.7749233563741\n",
      "Epoch 19 \t Batch 860 \t Training Loss: 47.8022366634635\n",
      "Epoch 19 \t Batch 880 \t Training Loss: 47.828705003044824\n",
      "Epoch 19 \t Batch 900 \t Training Loss: 47.82768388960096\n",
      "Epoch 19 \t Batch 20 \t Validation Loss: 14.153640699386596\n",
      "Epoch 19 \t Batch 40 \t Validation Loss: 17.911617708206176\n",
      "Epoch 19 \t Batch 60 \t Validation Loss: 17.327198266983032\n",
      "Epoch 19 \t Batch 80 \t Validation Loss: 18.063108205795288\n",
      "Epoch 19 \t Batch 100 \t Validation Loss: 19.93517333984375\n",
      "Epoch 19 \t Batch 120 \t Validation Loss: 21.57645632425944\n",
      "Epoch 19 \t Batch 140 \t Validation Loss: 22.408874654769896\n",
      "Epoch 19 \t Batch 160 \t Validation Loss: 24.716533464193343\n",
      "Epoch 19 \t Batch 180 \t Validation Loss: 28.947240326139664\n",
      "Epoch 19 \t Batch 200 \t Validation Loss: 30.792016711235046\n",
      "Epoch 19 \t Batch 220 \t Validation Loss: 32.423503359881316\n",
      "Epoch 19 \t Batch 240 \t Validation Loss: 33.193323985735574\n",
      "Epoch 19 \t Batch 260 \t Validation Loss: 35.548003779924834\n",
      "Epoch 19 \t Batch 280 \t Validation Loss: 36.83612564291273\n",
      "Epoch 19 \t Batch 300 \t Validation Loss: 38.172019001642866\n",
      "Epoch 19 \t Batch 320 \t Validation Loss: 38.80086461007595\n",
      "Epoch 19 \t Batch 340 \t Validation Loss: 38.788324151319614\n",
      "Epoch 19 \t Batch 360 \t Validation Loss: 38.66494272814857\n",
      "Epoch 19 \t Batch 380 \t Validation Loss: 38.95066223897432\n",
      "Epoch 19 \t Batch 400 \t Validation Loss: 38.52928439855575\n",
      "Epoch 19 \t Batch 420 \t Validation Loss: 38.55806298710051\n",
      "Epoch 19 \t Batch 440 \t Validation Loss: 38.26133199821819\n",
      "Epoch 19 \t Batch 460 \t Validation Loss: 38.477951176270196\n",
      "Epoch 19 \t Batch 480 \t Validation Loss: 38.96308019359906\n",
      "Epoch 19 \t Batch 500 \t Validation Loss: 38.66909961128235\n",
      "Epoch 19 \t Batch 520 \t Validation Loss: 38.40175624260536\n",
      "Epoch 19 \t Batch 540 \t Validation Loss: 38.15850882706819\n",
      "Epoch 19 \t Batch 560 \t Validation Loss: 37.94720400401524\n",
      "Epoch 19 \t Batch 580 \t Validation Loss: 37.635200727397\n",
      "Epoch 19 \t Batch 600 \t Validation Loss: 37.87767186482748\n",
      "Epoch 19 Training Loss: 47.84989673776741 Validation Loss: 38.50794947921456\n",
      "Epoch 19 completed\n",
      "Epoch 20 \t Batch 20 \t Training Loss: 46.59144039154053\n",
      "Epoch 20 \t Batch 40 \t Training Loss: 47.80842685699463\n",
      "Epoch 20 \t Batch 60 \t Training Loss: 47.69592157999674\n",
      "Epoch 20 \t Batch 80 \t Training Loss: 47.68462138175964\n",
      "Epoch 20 \t Batch 100 \t Training Loss: 47.46545970916748\n",
      "Epoch 20 \t Batch 120 \t Training Loss: 47.48736941019694\n",
      "Epoch 20 \t Batch 140 \t Training Loss: 47.52802132197789\n",
      "Epoch 20 \t Batch 160 \t Training Loss: 47.555831384658816\n",
      "Epoch 20 \t Batch 180 \t Training Loss: 47.5599616156684\n",
      "Epoch 20 \t Batch 200 \t Training Loss: 47.67568260192871\n",
      "Epoch 20 \t Batch 220 \t Training Loss: 47.712155636874115\n",
      "Epoch 20 \t Batch 240 \t Training Loss: 47.81822009086609\n",
      "Epoch 20 \t Batch 260 \t Training Loss: 47.883126229506274\n",
      "Epoch 20 \t Batch 280 \t Training Loss: 47.91300548825945\n",
      "Epoch 20 \t Batch 300 \t Training Loss: 47.81264663696289\n",
      "Epoch 20 \t Batch 320 \t Training Loss: 47.802318561077115\n",
      "Epoch 20 \t Batch 340 \t Training Loss: 47.7661111943862\n",
      "Epoch 20 \t Batch 360 \t Training Loss: 47.815794987148706\n",
      "Epoch 20 \t Batch 380 \t Training Loss: 47.79917572423032\n",
      "Epoch 20 \t Batch 400 \t Training Loss: 47.740238666534424\n",
      "Epoch 20 \t Batch 420 \t Training Loss: 47.733368537539526\n",
      "Epoch 20 \t Batch 440 \t Training Loss: 47.728797695853494\n",
      "Epoch 20 \t Batch 460 \t Training Loss: 47.72287891222083\n",
      "Epoch 20 \t Batch 480 \t Training Loss: 47.71076072851817\n",
      "Epoch 20 \t Batch 500 \t Training Loss: 47.71382219696045\n",
      "Epoch 20 \t Batch 520 \t Training Loss: 47.729065440251276\n",
      "Epoch 20 \t Batch 540 \t Training Loss: 47.65988897394251\n",
      "Epoch 20 \t Batch 560 \t Training Loss: 47.69142724445888\n",
      "Epoch 20 \t Batch 580 \t Training Loss: 47.71525386284138\n",
      "Epoch 20 \t Batch 600 \t Training Loss: 47.70086429595947\n",
      "Epoch 20 \t Batch 620 \t Training Loss: 47.66737584760112\n",
      "Epoch 20 \t Batch 640 \t Training Loss: 47.67179357409477\n",
      "Epoch 20 \t Batch 660 \t Training Loss: 47.68487747076786\n",
      "Epoch 20 \t Batch 680 \t Training Loss: 47.69866336373722\n",
      "Epoch 20 \t Batch 700 \t Training Loss: 47.704060467311315\n",
      "Epoch 20 \t Batch 720 \t Training Loss: 47.71008617613051\n",
      "Epoch 20 \t Batch 740 \t Training Loss: 47.7121474652677\n",
      "Epoch 20 \t Batch 760 \t Training Loss: 47.73672428633037\n",
      "Epoch 20 \t Batch 780 \t Training Loss: 47.73959042964837\n",
      "Epoch 20 \t Batch 800 \t Training Loss: 47.72979060173034\n",
      "Epoch 20 \t Batch 820 \t Training Loss: 47.739565430617915\n",
      "Epoch 20 \t Batch 840 \t Training Loss: 47.76048887343634\n",
      "Epoch 20 \t Batch 860 \t Training Loss: 47.75861943710682\n",
      "Epoch 20 \t Batch 880 \t Training Loss: 47.75875287055969\n",
      "Epoch 20 \t Batch 900 \t Training Loss: 47.76807829538981\n",
      "Epoch 20 \t Batch 20 \t Validation Loss: 22.22731533050537\n",
      "Epoch 20 \t Batch 40 \t Validation Loss: 24.911827182769777\n",
      "Epoch 20 \t Batch 60 \t Validation Loss: 24.52631834348043\n",
      "Epoch 20 \t Batch 80 \t Validation Loss: 24.61825387477875\n",
      "Epoch 20 \t Batch 100 \t Validation Loss: 25.28828329086304\n",
      "Epoch 20 \t Batch 120 \t Validation Loss: 26.072120269139607\n",
      "Epoch 20 \t Batch 140 \t Validation Loss: 26.357171685355052\n",
      "Epoch 20 \t Batch 160 \t Validation Loss: 28.165859615802766\n",
      "Epoch 20 \t Batch 180 \t Validation Loss: 31.6977462026808\n",
      "Epoch 20 \t Batch 200 \t Validation Loss: 33.16390379905701\n",
      "Epoch 20 \t Batch 220 \t Validation Loss: 34.3792537429116\n",
      "Epoch 20 \t Batch 240 \t Validation Loss: 34.825672030448914\n",
      "Epoch 20 \t Batch 260 \t Validation Loss: 36.84248878405644\n",
      "Epoch 20 \t Batch 280 \t Validation Loss: 37.87110179833004\n",
      "Epoch 20 \t Batch 300 \t Validation Loss: 38.98920838991801\n",
      "Epoch 20 \t Batch 320 \t Validation Loss: 39.468298360705376\n",
      "Epoch 20 \t Batch 340 \t Validation Loss: 39.38525494126713\n",
      "Epoch 20 \t Batch 360 \t Validation Loss: 39.23083079655965\n",
      "Epoch 20 \t Batch 380 \t Validation Loss: 39.42635486502397\n",
      "Epoch 20 \t Batch 400 \t Validation Loss: 39.000687742233275\n",
      "Epoch 20 \t Batch 420 \t Validation Loss: 39.006363001323884\n",
      "Epoch 20 \t Batch 440 \t Validation Loss: 38.68902121890675\n",
      "Epoch 20 \t Batch 460 \t Validation Loss: 38.87166974855506\n",
      "Epoch 20 \t Batch 480 \t Validation Loss: 39.3357130686442\n",
      "Epoch 20 \t Batch 500 \t Validation Loss: 39.03992413330078\n",
      "Epoch 20 \t Batch 520 \t Validation Loss: 38.80133217481466\n",
      "Epoch 20 \t Batch 540 \t Validation Loss: 38.57107013596429\n",
      "Epoch 20 \t Batch 560 \t Validation Loss: 38.382107136930735\n",
      "Epoch 20 \t Batch 580 \t Validation Loss: 38.218892151733925\n",
      "Epoch 20 \t Batch 600 \t Validation Loss: 38.42847563902537\n",
      "Epoch 20 Training Loss: 47.78893746778515 Validation Loss: 39.059608656090575\n",
      "Epoch 20 completed\n",
      "Epoch 21 \t Batch 20 \t Training Loss: 47.446671867370604\n",
      "Epoch 21 \t Batch 40 \t Training Loss: 48.17950086593628\n",
      "Epoch 21 \t Batch 60 \t Training Loss: 48.3891404469808\n",
      "Epoch 21 \t Batch 80 \t Training Loss: 47.97493290901184\n",
      "Epoch 21 \t Batch 100 \t Training Loss: 47.96140087127686\n",
      "Epoch 21 \t Batch 120 \t Training Loss: 47.68042672475179\n",
      "Epoch 21 \t Batch 140 \t Training Loss: 47.76640123639788\n",
      "Epoch 21 \t Batch 160 \t Training Loss: 47.68128898143768\n",
      "Epoch 21 \t Batch 180 \t Training Loss: 47.69754761589898\n",
      "Epoch 21 \t Batch 200 \t Training Loss: 47.614683837890624\n",
      "Epoch 21 \t Batch 220 \t Training Loss: 47.72370742451061\n",
      "Epoch 21 \t Batch 240 \t Training Loss: 47.77726240158081\n",
      "Epoch 21 \t Batch 260 \t Training Loss: 47.65901724008413\n",
      "Epoch 21 \t Batch 280 \t Training Loss: 47.67856258664813\n",
      "Epoch 21 \t Batch 300 \t Training Loss: 47.722971229553224\n",
      "Epoch 21 \t Batch 320 \t Training Loss: 47.70079439878464\n",
      "Epoch 21 \t Batch 340 \t Training Loss: 47.78711471557617\n",
      "Epoch 21 \t Batch 360 \t Training Loss: 47.771882979075116\n",
      "Epoch 21 \t Batch 380 \t Training Loss: 47.762520157663445\n",
      "Epoch 21 \t Batch 400 \t Training Loss: 47.80652710914612\n",
      "Epoch 21 \t Batch 420 \t Training Loss: 47.79517280941918\n",
      "Epoch 21 \t Batch 440 \t Training Loss: 47.83856133547696\n",
      "Epoch 21 \t Batch 460 \t Training Loss: 47.81369570856509\n",
      "Epoch 21 \t Batch 480 \t Training Loss: 47.804406785964964\n",
      "Epoch 21 \t Batch 500 \t Training Loss: 47.78136853027344\n",
      "Epoch 21 \t Batch 520 \t Training Loss: 47.76580998347356\n",
      "Epoch 21 \t Batch 540 \t Training Loss: 47.83861526913113\n",
      "Epoch 21 \t Batch 560 \t Training Loss: 47.8001960005079\n",
      "Epoch 21 \t Batch 580 \t Training Loss: 47.797964964241814\n",
      "Epoch 21 \t Batch 600 \t Training Loss: 47.81494857152303\n",
      "Epoch 21 \t Batch 620 \t Training Loss: 47.761952880121044\n",
      "Epoch 21 \t Batch 640 \t Training Loss: 47.729143005609515\n",
      "Epoch 21 \t Batch 660 \t Training Loss: 47.74075145143451\n",
      "Epoch 21 \t Batch 680 \t Training Loss: 47.72381085788503\n",
      "Epoch 21 \t Batch 700 \t Training Loss: 47.720220794677736\n",
      "Epoch 21 \t Batch 720 \t Training Loss: 47.72981399430169\n",
      "Epoch 21 \t Batch 740 \t Training Loss: 47.74246574092556\n",
      "Epoch 21 \t Batch 760 \t Training Loss: 47.75666954140914\n",
      "Epoch 21 \t Batch 780 \t Training Loss: 47.800124222193006\n",
      "Epoch 21 \t Batch 800 \t Training Loss: 47.842162199020386\n",
      "Epoch 21 \t Batch 820 \t Training Loss: 47.813351817247344\n",
      "Epoch 21 \t Batch 840 \t Training Loss: 47.80110663459415\n",
      "Epoch 21 \t Batch 860 \t Training Loss: 47.79183940443882\n",
      "Epoch 21 \t Batch 880 \t Training Loss: 47.77669522545555\n",
      "Epoch 21 \t Batch 900 \t Training Loss: 47.77138768513998\n",
      "Epoch 21 \t Batch 20 \t Validation Loss: 23.649476051330566\n",
      "Epoch 21 \t Batch 40 \t Validation Loss: 26.171230363845826\n",
      "Epoch 21 \t Batch 60 \t Validation Loss: 25.549786218007405\n",
      "Epoch 21 \t Batch 80 \t Validation Loss: 25.52621877193451\n",
      "Epoch 21 \t Batch 100 \t Validation Loss: 25.968993263244627\n",
      "Epoch 21 \t Batch 120 \t Validation Loss: 26.544833135604858\n",
      "Epoch 21 \t Batch 140 \t Validation Loss: 26.797589193071637\n",
      "Epoch 21 \t Batch 160 \t Validation Loss: 28.633026909828185\n",
      "Epoch 21 \t Batch 180 \t Validation Loss: 32.26859039200677\n",
      "Epoch 21 \t Batch 200 \t Validation Loss: 33.71617558956146\n",
      "Epoch 21 \t Batch 220 \t Validation Loss: 34.94031941240484\n",
      "Epoch 21 \t Batch 240 \t Validation Loss: 35.41469722191493\n",
      "Epoch 21 \t Batch 260 \t Validation Loss: 37.46069996173565\n",
      "Epoch 21 \t Batch 280 \t Validation Loss: 38.55181945051466\n",
      "Epoch 21 \t Batch 300 \t Validation Loss: 39.71432503064474\n",
      "Epoch 21 \t Batch 320 \t Validation Loss: 40.195993623137475\n",
      "Epoch 21 \t Batch 340 \t Validation Loss: 40.12699229016024\n",
      "Epoch 21 \t Batch 360 \t Validation Loss: 39.98874523639679\n",
      "Epoch 21 \t Batch 380 \t Validation Loss: 40.193534291417976\n",
      "Epoch 21 \t Batch 400 \t Validation Loss: 39.79631604909897\n",
      "Epoch 21 \t Batch 420 \t Validation Loss: 39.75826943261283\n",
      "Epoch 21 \t Batch 440 \t Validation Loss: 39.446093394539574\n",
      "Epoch 21 \t Batch 460 \t Validation Loss: 39.65824313785719\n",
      "Epoch 21 \t Batch 480 \t Validation Loss: 40.10726484060287\n",
      "Epoch 21 \t Batch 500 \t Validation Loss: 39.78953660583496\n",
      "Epoch 21 \t Batch 520 \t Validation Loss: 39.577170001543486\n",
      "Epoch 21 \t Batch 540 \t Validation Loss: 39.36637017285382\n",
      "Epoch 21 \t Batch 560 \t Validation Loss: 39.185144543647766\n",
      "Epoch 21 \t Batch 580 \t Validation Loss: 39.042323099333665\n",
      "Epoch 21 \t Batch 600 \t Validation Loss: 39.22764331181844\n",
      "Epoch 21 Training Loss: 47.739145601328744 Validation Loss: 39.878668518809526\n",
      "Epoch 21 completed\n",
      "Epoch 22 \t Batch 20 \t Training Loss: 48.517020797729494\n",
      "Epoch 22 \t Batch 40 \t Training Loss: 48.069917869567874\n",
      "Epoch 22 \t Batch 60 \t Training Loss: 47.48287080128988\n",
      "Epoch 22 \t Batch 80 \t Training Loss: 47.69648809432984\n",
      "Epoch 22 \t Batch 100 \t Training Loss: 47.49796558380127\n",
      "Epoch 22 \t Batch 120 \t Training Loss: 47.61937615076701\n",
      "Epoch 22 \t Batch 140 \t Training Loss: 47.90432423182896\n",
      "Epoch 22 \t Batch 160 \t Training Loss: 47.98770318031311\n",
      "Epoch 22 \t Batch 180 \t Training Loss: 48.056121253967284\n",
      "Epoch 22 \t Batch 200 \t Training Loss: 47.89589595794678\n",
      "Epoch 22 \t Batch 220 \t Training Loss: 47.84565322182395\n",
      "Epoch 22 \t Batch 240 \t Training Loss: 47.81872485478719\n",
      "Epoch 22 \t Batch 260 \t Training Loss: 47.80681910881629\n",
      "Epoch 22 \t Batch 280 \t Training Loss: 47.80747737884521\n",
      "Epoch 22 \t Batch 300 \t Training Loss: 47.875066655476886\n",
      "Epoch 22 \t Batch 320 \t Training Loss: 47.841331934928895\n",
      "Epoch 22 \t Batch 340 \t Training Loss: 47.84827314264634\n",
      "Epoch 22 \t Batch 360 \t Training Loss: 47.87541624704997\n",
      "Epoch 22 \t Batch 380 \t Training Loss: 47.918917615790114\n",
      "Epoch 22 \t Batch 400 \t Training Loss: 47.858721532821654\n",
      "Epoch 22 \t Batch 420 \t Training Loss: 47.86501632872082\n",
      "Epoch 22 \t Batch 440 \t Training Loss: 47.864936585859816\n",
      "Epoch 22 \t Batch 460 \t Training Loss: 47.93629557568094\n",
      "Epoch 22 \t Batch 480 \t Training Loss: 47.88694563706716\n",
      "Epoch 22 \t Batch 500 \t Training Loss: 47.91035494995117\n",
      "Epoch 22 \t Batch 520 \t Training Loss: 47.91529362751887\n",
      "Epoch 22 \t Batch 540 \t Training Loss: 47.89155665503608\n",
      "Epoch 22 \t Batch 560 \t Training Loss: 47.85171822820391\n",
      "Epoch 22 \t Batch 580 \t Training Loss: 47.842433646629594\n",
      "Epoch 22 \t Batch 600 \t Training Loss: 47.81534636179606\n",
      "Epoch 22 \t Batch 620 \t Training Loss: 47.82369577346309\n",
      "Epoch 22 \t Batch 640 \t Training Loss: 47.81110615730286\n",
      "Epoch 22 \t Batch 660 \t Training Loss: 47.76926639441288\n",
      "Epoch 22 \t Batch 680 \t Training Loss: 47.76069383060231\n",
      "Epoch 22 \t Batch 700 \t Training Loss: 47.745784334455216\n",
      "Epoch 22 \t Batch 720 \t Training Loss: 47.75938870112101\n",
      "Epoch 22 \t Batch 740 \t Training Loss: 47.736519566097776\n",
      "Epoch 22 \t Batch 760 \t Training Loss: 47.76365031694111\n",
      "Epoch 22 \t Batch 780 \t Training Loss: 47.73839040902945\n",
      "Epoch 22 \t Batch 800 \t Training Loss: 47.721045727729795\n",
      "Epoch 22 \t Batch 820 \t Training Loss: 47.752323178547186\n",
      "Epoch 22 \t Batch 840 \t Training Loss: 47.74433068774995\n",
      "Epoch 22 \t Batch 860 \t Training Loss: 47.74370919604634\n",
      "Epoch 22 \t Batch 880 \t Training Loss: 47.709914125095715\n",
      "Epoch 22 \t Batch 900 \t Training Loss: 47.688993788825144\n",
      "Epoch 22 \t Batch 20 \t Validation Loss: 13.661030507087707\n",
      "Epoch 22 \t Batch 40 \t Validation Loss: 17.07075390815735\n",
      "Epoch 22 \t Batch 60 \t Validation Loss: 16.88136795361837\n",
      "Epoch 22 \t Batch 80 \t Validation Loss: 17.430274665355682\n",
      "Epoch 22 \t Batch 100 \t Validation Loss: 19.388716220855713\n",
      "Epoch 22 \t Batch 120 \t Validation Loss: 21.019935115178427\n",
      "Epoch 22 \t Batch 140 \t Validation Loss: 21.885412134443012\n",
      "Epoch 22 \t Batch 160 \t Validation Loss: 24.000355195999145\n",
      "Epoch 22 \t Batch 180 \t Validation Loss: 27.727517721388075\n",
      "Epoch 22 \t Batch 200 \t Validation Loss: 29.38841570854187\n",
      "Epoch 22 \t Batch 220 \t Validation Loss: 30.748811184276235\n",
      "Epoch 22 \t Batch 240 \t Validation Loss: 31.35142582654953\n",
      "Epoch 22 \t Batch 260 \t Validation Loss: 33.49288725119371\n",
      "Epoch 22 \t Batch 280 \t Validation Loss: 34.70733416761671\n",
      "Epoch 22 \t Batch 300 \t Validation Loss: 35.86772053082784\n",
      "Epoch 22 \t Batch 320 \t Validation Loss: 36.41981941461563\n",
      "Epoch 22 \t Batch 340 \t Validation Loss: 36.459757737552415\n",
      "Epoch 22 \t Batch 360 \t Validation Loss: 36.364439347055225\n",
      "Epoch 22 \t Batch 380 \t Validation Loss: 36.64268649000871\n",
      "Epoch 22 \t Batch 400 \t Validation Loss: 36.31479208230972\n",
      "Epoch 22 \t Batch 420 \t Validation Loss: 36.42878252211071\n",
      "Epoch 22 \t Batch 440 \t Validation Loss: 36.20581470836293\n",
      "Epoch 22 \t Batch 460 \t Validation Loss: 36.479549296005914\n",
      "Epoch 22 \t Batch 480 \t Validation Loss: 37.0257670879364\n",
      "Epoch 22 \t Batch 500 \t Validation Loss: 36.801587284088136\n",
      "Epoch 22 \t Batch 520 \t Validation Loss: 36.58875587903536\n",
      "Epoch 22 \t Batch 540 \t Validation Loss: 36.36544057704784\n",
      "Epoch 22 \t Batch 560 \t Validation Loss: 36.189972509656634\n",
      "Epoch 22 \t Batch 580 \t Validation Loss: 35.96750133777487\n",
      "Epoch 22 \t Batch 600 \t Validation Loss: 36.212396246592206\n",
      "Epoch 22 Training Loss: 47.67691115750611 Validation Loss: 36.85930092613418\n",
      "Validation Loss Decreased(22905.275595664978--->22705.329370498657) Saving The Model\n",
      "Epoch 22 completed\n",
      "Epoch 23 \t Batch 20 \t Training Loss: 47.75817470550537\n",
      "Epoch 23 \t Batch 40 \t Training Loss: 48.31409816741943\n",
      "Epoch 23 \t Batch 60 \t Training Loss: 47.75846951802571\n",
      "Epoch 23 \t Batch 80 \t Training Loss: 47.62005114555359\n",
      "Epoch 23 \t Batch 100 \t Training Loss: 47.66894290924072\n",
      "Epoch 23 \t Batch 120 \t Training Loss: 47.87786750793457\n",
      "Epoch 23 \t Batch 140 \t Training Loss: 47.802562141418456\n",
      "Epoch 23 \t Batch 160 \t Training Loss: 47.808177256584166\n",
      "Epoch 23 \t Batch 180 \t Training Loss: 47.77980766296387\n",
      "Epoch 23 \t Batch 200 \t Training Loss: 47.70706451416016\n",
      "Epoch 23 \t Batch 220 \t Training Loss: 47.881320675936614\n",
      "Epoch 23 \t Batch 240 \t Training Loss: 47.818378671010336\n",
      "Epoch 23 \t Batch 260 \t Training Loss: 47.678390605633076\n",
      "Epoch 23 \t Batch 280 \t Training Loss: 47.714544895717076\n",
      "Epoch 23 \t Batch 300 \t Training Loss: 47.68487617492676\n",
      "Epoch 23 \t Batch 320 \t Training Loss: 47.761674070358275\n",
      "Epoch 23 \t Batch 340 \t Training Loss: 47.73772661545697\n",
      "Epoch 23 \t Batch 360 \t Training Loss: 47.72102837032742\n",
      "Epoch 23 \t Batch 380 \t Training Loss: 47.74978818391499\n",
      "Epoch 23 \t Batch 400 \t Training Loss: 47.699610223770144\n",
      "Epoch 23 \t Batch 420 \t Training Loss: 47.686604454403835\n",
      "Epoch 23 \t Batch 440 \t Training Loss: 47.7264480937611\n",
      "Epoch 23 \t Batch 460 \t Training Loss: 47.72030261495839\n",
      "Epoch 23 \t Batch 480 \t Training Loss: 47.74461270173391\n",
      "Epoch 23 \t Batch 500 \t Training Loss: 47.71589443206787\n",
      "Epoch 23 \t Batch 520 \t Training Loss: 47.67453413743239\n",
      "Epoch 23 \t Batch 540 \t Training Loss: 47.59685427347819\n",
      "Epoch 23 \t Batch 560 \t Training Loss: 47.56164039884295\n",
      "Epoch 23 \t Batch 580 \t Training Loss: 47.58374402934107\n",
      "Epoch 23 \t Batch 600 \t Training Loss: 47.61339172999064\n",
      "Epoch 23 \t Batch 620 \t Training Loss: 47.70248982214159\n",
      "Epoch 23 \t Batch 640 \t Training Loss: 47.72147373557091\n",
      "Epoch 23 \t Batch 660 \t Training Loss: 47.72091024572199\n",
      "Epoch 23 \t Batch 680 \t Training Loss: 47.73879815830904\n",
      "Epoch 23 \t Batch 700 \t Training Loss: 47.72024010249547\n",
      "Epoch 23 \t Batch 720 \t Training Loss: 47.70609458817376\n",
      "Epoch 23 \t Batch 740 \t Training Loss: 47.69014436360952\n",
      "Epoch 23 \t Batch 760 \t Training Loss: 47.701888365494575\n",
      "Epoch 23 \t Batch 780 \t Training Loss: 47.67507162338648\n",
      "Epoch 23 \t Batch 800 \t Training Loss: 47.66062221050262\n",
      "Epoch 23 \t Batch 820 \t Training Loss: 47.66360365704792\n",
      "Epoch 23 \t Batch 840 \t Training Loss: 47.63888095674061\n",
      "Epoch 23 \t Batch 860 \t Training Loss: 47.63433863617653\n",
      "Epoch 23 \t Batch 880 \t Training Loss: 47.65718163576993\n",
      "Epoch 23 \t Batch 900 \t Training Loss: 47.66675077650282\n",
      "Epoch 23 \t Batch 20 \t Validation Loss: 21.053193664550783\n",
      "Epoch 23 \t Batch 40 \t Validation Loss: 24.224856066703797\n",
      "Epoch 23 \t Batch 60 \t Validation Loss: 23.775803661346437\n",
      "Epoch 23 \t Batch 80 \t Validation Loss: 24.39670512676239\n",
      "Epoch 23 \t Batch 100 \t Validation Loss: 24.974646549224854\n",
      "Epoch 23 \t Batch 120 \t Validation Loss: 25.74636579354604\n",
      "Epoch 23 \t Batch 140 \t Validation Loss: 26.024541875294275\n",
      "Epoch 23 \t Batch 160 \t Validation Loss: 27.784412294626236\n",
      "Epoch 23 \t Batch 180 \t Validation Loss: 31.293349255455865\n",
      "Epoch 23 \t Batch 200 \t Validation Loss: 32.707517261505124\n",
      "Epoch 23 \t Batch 220 \t Validation Loss: 33.87491147301414\n",
      "Epoch 23 \t Batch 240 \t Validation Loss: 34.33819699684779\n",
      "Epoch 23 \t Batch 260 \t Validation Loss: 36.318406339792105\n",
      "Epoch 23 \t Batch 280 \t Validation Loss: 37.32504358291626\n",
      "Epoch 23 \t Batch 300 \t Validation Loss: 38.46886681874593\n",
      "Epoch 23 \t Batch 320 \t Validation Loss: 38.97122743427754\n",
      "Epoch 23 \t Batch 340 \t Validation Loss: 38.93408663413104\n",
      "Epoch 23 \t Batch 360 \t Validation Loss: 38.79094785319434\n",
      "Epoch 23 \t Batch 380 \t Validation Loss: 38.98217375152989\n",
      "Epoch 23 \t Batch 400 \t Validation Loss: 38.59299649477005\n",
      "Epoch 23 \t Batch 420 \t Validation Loss: 38.671043489092874\n",
      "Epoch 23 \t Batch 440 \t Validation Loss: 38.38644257242029\n",
      "Epoch 23 \t Batch 460 \t Validation Loss: 38.61878214919049\n",
      "Epoch 23 \t Batch 480 \t Validation Loss: 39.13963837424914\n",
      "Epoch 23 \t Batch 500 \t Validation Loss: 38.88500345802307\n",
      "Epoch 23 \t Batch 520 \t Validation Loss: 38.61869047054878\n",
      "Epoch 23 \t Batch 540 \t Validation Loss: 38.35435441511649\n",
      "Epoch 23 \t Batch 560 \t Validation Loss: 38.136329567432405\n",
      "Epoch 23 \t Batch 580 \t Validation Loss: 37.870211990948384\n",
      "Epoch 23 \t Batch 600 \t Validation Loss: 38.09177553017934\n",
      "Epoch 23 Training Loss: 47.63317160569984 Validation Loss: 38.72394898495117\n",
      "Epoch 23 completed\n",
      "Epoch 24 \t Batch 20 \t Training Loss: 47.532568740844724\n",
      "Epoch 24 \t Batch 40 \t Training Loss: 47.6506160736084\n",
      "Epoch 24 \t Batch 60 \t Training Loss: 47.19285360972086\n",
      "Epoch 24 \t Batch 80 \t Training Loss: 47.5747549533844\n",
      "Epoch 24 \t Batch 100 \t Training Loss: 47.48728351593017\n",
      "Epoch 24 \t Batch 120 \t Training Loss: 47.46979360580444\n",
      "Epoch 24 \t Batch 140 \t Training Loss: 47.282670211791995\n",
      "Epoch 24 \t Batch 160 \t Training Loss: 47.41561181545258\n",
      "Epoch 24 \t Batch 180 \t Training Loss: 47.48052043914795\n",
      "Epoch 24 \t Batch 200 \t Training Loss: 47.49147560119629\n",
      "Epoch 24 \t Batch 220 \t Training Loss: 47.39031162261963\n",
      "Epoch 24 \t Batch 240 \t Training Loss: 47.42084182103475\n",
      "Epoch 24 \t Batch 260 \t Training Loss: 47.595156713632434\n",
      "Epoch 24 \t Batch 280 \t Training Loss: 47.55697892052787\n",
      "Epoch 24 \t Batch 300 \t Training Loss: 47.51204989115397\n",
      "Epoch 24 \t Batch 320 \t Training Loss: 47.497552597522734\n",
      "Epoch 24 \t Batch 340 \t Training Loss: 47.459631067163805\n",
      "Epoch 24 \t Batch 360 \t Training Loss: 47.42275235917833\n",
      "Epoch 24 \t Batch 380 \t Training Loss: 47.42590669330798\n",
      "Epoch 24 \t Batch 400 \t Training Loss: 47.442619199752805\n",
      "Epoch 24 \t Batch 420 \t Training Loss: 47.44048180353074\n",
      "Epoch 24 \t Batch 440 \t Training Loss: 47.460979947176845\n",
      "Epoch 24 \t Batch 460 \t Training Loss: 47.43923045448635\n",
      "Epoch 24 \t Batch 480 \t Training Loss: 47.50289901097616\n",
      "Epoch 24 \t Batch 500 \t Training Loss: 47.593587020874025\n",
      "Epoch 24 \t Batch 520 \t Training Loss: 47.58093043840849\n",
      "Epoch 24 \t Batch 540 \t Training Loss: 47.529850239223904\n",
      "Epoch 24 \t Batch 560 \t Training Loss: 47.528112983703615\n",
      "Epoch 24 \t Batch 580 \t Training Loss: 47.58316455051817\n",
      "Epoch 24 \t Batch 600 \t Training Loss: 47.58218108495077\n",
      "Epoch 24 \t Batch 620 \t Training Loss: 47.5522211874685\n",
      "Epoch 24 \t Batch 640 \t Training Loss: 47.59662547111511\n",
      "Epoch 24 \t Batch 660 \t Training Loss: 47.633654513503565\n",
      "Epoch 24 \t Batch 680 \t Training Loss: 47.62947140300975\n",
      "Epoch 24 \t Batch 700 \t Training Loss: 47.61890320369176\n",
      "Epoch 24 \t Batch 720 \t Training Loss: 47.61675729221768\n",
      "Epoch 24 \t Batch 740 \t Training Loss: 47.6391602954349\n",
      "Epoch 24 \t Batch 760 \t Training Loss: 47.66190658368562\n",
      "Epoch 24 \t Batch 780 \t Training Loss: 47.658652956057814\n",
      "Epoch 24 \t Batch 800 \t Training Loss: 47.61759732723236\n",
      "Epoch 24 \t Batch 820 \t Training Loss: 47.58371472242402\n",
      "Epoch 24 \t Batch 840 \t Training Loss: 47.6151022320702\n",
      "Epoch 24 \t Batch 860 \t Training Loss: 47.61466288455697\n",
      "Epoch 24 \t Batch 880 \t Training Loss: 47.594286818937825\n",
      "Epoch 24 \t Batch 900 \t Training Loss: 47.597337574428984\n",
      "Epoch 24 \t Batch 20 \t Validation Loss: 26.73742208480835\n",
      "Epoch 24 \t Batch 40 \t Validation Loss: 28.239254212379457\n",
      "Epoch 24 \t Batch 60 \t Validation Loss: 28.231270265579223\n",
      "Epoch 24 \t Batch 80 \t Validation Loss: 27.96171499490738\n",
      "Epoch 24 \t Batch 100 \t Validation Loss: 28.03613450050354\n",
      "Epoch 24 \t Batch 120 \t Validation Loss: 28.53004841009776\n",
      "Epoch 24 \t Batch 140 \t Validation Loss: 28.53954870360238\n",
      "Epoch 24 \t Batch 160 \t Validation Loss: 29.884820598363877\n",
      "Epoch 24 \t Batch 180 \t Validation Loss: 32.86529619428847\n",
      "Epoch 24 \t Batch 200 \t Validation Loss: 33.96515542030335\n",
      "Epoch 24 \t Batch 220 \t Validation Loss: 34.86610695232044\n",
      "Epoch 24 \t Batch 240 \t Validation Loss: 35.11750147740046\n",
      "Epoch 24 \t Batch 260 \t Validation Loss: 36.92839011412401\n",
      "Epoch 24 \t Batch 280 \t Validation Loss: 37.869942736625674\n",
      "Epoch 24 \t Batch 300 \t Validation Loss: 38.747589133580526\n",
      "Epoch 24 \t Batch 320 \t Validation Loss: 39.1208616733551\n",
      "Epoch 24 \t Batch 340 \t Validation Loss: 39.00480268141803\n",
      "Epoch 24 \t Batch 360 \t Validation Loss: 38.81513311068217\n",
      "Epoch 24 \t Batch 380 \t Validation Loss: 38.959421770196215\n",
      "Epoch 24 \t Batch 400 \t Validation Loss: 38.548567390441896\n",
      "Epoch 24 \t Batch 420 \t Validation Loss: 38.55368938446045\n",
      "Epoch 24 \t Batch 440 \t Validation Loss: 38.24403562979265\n",
      "Epoch 24 \t Batch 460 \t Validation Loss: 38.445966820094895\n",
      "Epoch 24 \t Batch 480 \t Validation Loss: 38.91741053263346\n",
      "Epoch 24 \t Batch 500 \t Validation Loss: 38.62388151168823\n",
      "Epoch 24 \t Batch 520 \t Validation Loss: 38.37491816373972\n",
      "Epoch 24 \t Batch 540 \t Validation Loss: 38.14737660090129\n",
      "Epoch 24 \t Batch 560 \t Validation Loss: 37.97920960358211\n",
      "Epoch 24 \t Batch 580 \t Validation Loss: 37.78113810769443\n",
      "Epoch 24 \t Batch 600 \t Validation Loss: 38.00651048342387\n",
      "Epoch 24 Training Loss: 47.57652675000506 Validation Loss: 38.71218321540139\n",
      "Epoch 24 completed\n",
      "Epoch 25 \t Batch 20 \t Training Loss: 46.92275562286377\n",
      "Epoch 25 \t Batch 40 \t Training Loss: 47.29990768432617\n",
      "Epoch 25 \t Batch 60 \t Training Loss: 47.64889430999756\n",
      "Epoch 25 \t Batch 80 \t Training Loss: 47.68826513290405\n",
      "Epoch 25 \t Batch 100 \t Training Loss: 48.013503875732425\n",
      "Epoch 25 \t Batch 120 \t Training Loss: 47.68508011500041\n",
      "Epoch 25 \t Batch 140 \t Training Loss: 47.709793962751114\n",
      "Epoch 25 \t Batch 160 \t Training Loss: 47.595240759849545\n",
      "Epoch 25 \t Batch 180 \t Training Loss: 47.565546332465274\n",
      "Epoch 25 \t Batch 200 \t Training Loss: 47.583278770446775\n",
      "Epoch 25 \t Batch 220 \t Training Loss: 47.55730984427712\n",
      "Epoch 25 \t Batch 240 \t Training Loss: 47.48829992612203\n",
      "Epoch 25 \t Batch 260 \t Training Loss: 47.575968214181756\n",
      "Epoch 25 \t Batch 280 \t Training Loss: 47.60064462934221\n",
      "Epoch 25 \t Batch 300 \t Training Loss: 47.61793956756592\n",
      "Epoch 25 \t Batch 320 \t Training Loss: 47.68659118413925\n",
      "Epoch 25 \t Batch 340 \t Training Loss: 47.62181813857135\n",
      "Epoch 25 \t Batch 360 \t Training Loss: 47.698225593566896\n",
      "Epoch 25 \t Batch 380 \t Training Loss: 47.717855734574165\n",
      "Epoch 25 \t Batch 400 \t Training Loss: 47.65088541030884\n",
      "Epoch 25 \t Batch 420 \t Training Loss: 47.63417623610724\n",
      "Epoch 25 \t Batch 440 \t Training Loss: 47.595321759310636\n",
      "Epoch 25 \t Batch 460 \t Training Loss: 47.510646040543264\n",
      "Epoch 25 \t Batch 480 \t Training Loss: 47.47317320505778\n",
      "Epoch 25 \t Batch 500 \t Training Loss: 47.48884886932373\n",
      "Epoch 25 \t Batch 520 \t Training Loss: 47.47509133999164\n",
      "Epoch 25 \t Batch 540 \t Training Loss: 47.43789691925049\n",
      "Epoch 25 \t Batch 560 \t Training Loss: 47.43778581619263\n",
      "Epoch 25 \t Batch 580 \t Training Loss: 47.417853092325146\n",
      "Epoch 25 \t Batch 600 \t Training Loss: 47.4202286974589\n",
      "Epoch 25 \t Batch 620 \t Training Loss: 47.392008873724166\n",
      "Epoch 25 \t Batch 640 \t Training Loss: 47.40657439827919\n",
      "Epoch 25 \t Batch 660 \t Training Loss: 47.38568587447658\n",
      "Epoch 25 \t Batch 680 \t Training Loss: 47.40865226633409\n",
      "Epoch 25 \t Batch 700 \t Training Loss: 47.395561888558525\n",
      "Epoch 25 \t Batch 720 \t Training Loss: 47.40618504948086\n",
      "Epoch 25 \t Batch 740 \t Training Loss: 47.40300350704709\n",
      "Epoch 25 \t Batch 760 \t Training Loss: 47.393755907761424\n",
      "Epoch 25 \t Batch 780 \t Training Loss: 47.420560797666894\n",
      "Epoch 25 \t Batch 800 \t Training Loss: 47.4304622888565\n",
      "Epoch 25 \t Batch 820 \t Training Loss: 47.46898214758896\n",
      "Epoch 25 \t Batch 840 \t Training Loss: 47.469462626320976\n",
      "Epoch 25 \t Batch 860 \t Training Loss: 47.484669663185294\n",
      "Epoch 25 \t Batch 880 \t Training Loss: 47.49298774112355\n",
      "Epoch 25 \t Batch 900 \t Training Loss: 47.5047764884101\n",
      "Epoch 25 \t Batch 20 \t Validation Loss: 22.228020858764648\n",
      "Epoch 25 \t Batch 40 \t Validation Loss: 24.42227931022644\n",
      "Epoch 25 \t Batch 60 \t Validation Loss: 24.249752378463747\n",
      "Epoch 25 \t Batch 80 \t Validation Loss: 24.2882248044014\n",
      "Epoch 25 \t Batch 100 \t Validation Loss: 25.81845088005066\n",
      "Epoch 25 \t Batch 120 \t Validation Loss: 27.106355404853822\n",
      "Epoch 25 \t Batch 140 \t Validation Loss: 27.558096483775547\n",
      "Epoch 25 \t Batch 160 \t Validation Loss: 29.147388249635696\n",
      "Epoch 25 \t Batch 180 \t Validation Loss: 32.472679646809894\n",
      "Epoch 25 \t Batch 200 \t Validation Loss: 33.70279120445252\n",
      "Epoch 25 \t Batch 220 \t Validation Loss: 34.809032526883215\n",
      "Epoch 25 \t Batch 240 \t Validation Loss: 35.19138280948003\n",
      "Epoch 25 \t Batch 260 \t Validation Loss: 37.142812574826756\n",
      "Epoch 25 \t Batch 280 \t Validation Loss: 38.18747347763607\n",
      "Epoch 25 \t Batch 300 \t Validation Loss: 39.15953273455302\n",
      "Epoch 25 \t Batch 320 \t Validation Loss: 39.579233440756795\n",
      "Epoch 25 \t Batch 340 \t Validation Loss: 39.47319418682772\n",
      "Epoch 25 \t Batch 360 \t Validation Loss: 39.2716551038954\n",
      "Epoch 25 \t Batch 380 \t Validation Loss: 39.43597183729473\n",
      "Epoch 25 \t Batch 400 \t Validation Loss: 38.99329246520996\n",
      "Epoch 25 \t Batch 420 \t Validation Loss: 38.967519928160165\n",
      "Epoch 25 \t Batch 440 \t Validation Loss: 38.64792467030612\n",
      "Epoch 25 \t Batch 460 \t Validation Loss: 38.84385088630344\n",
      "Epoch 25 \t Batch 480 \t Validation Loss: 39.28597745895386\n",
      "Epoch 25 \t Batch 500 \t Validation Loss: 38.95772748565674\n",
      "Epoch 25 \t Batch 520 \t Validation Loss: 38.674145779242885\n",
      "Epoch 25 \t Batch 540 \t Validation Loss: 38.467800532446965\n",
      "Epoch 25 \t Batch 560 \t Validation Loss: 38.29988742555891\n",
      "Epoch 25 \t Batch 580 \t Validation Loss: 38.033101361373376\n",
      "Epoch 25 \t Batch 600 \t Validation Loss: 38.308630253473915\n",
      "Epoch 25 Training Loss: 47.52217746430941 Validation Loss: 38.9394107329381\n",
      "Epoch 25 completed\n",
      "Epoch 26 \t Batch 20 \t Training Loss: 47.060173416137694\n",
      "Epoch 26 \t Batch 40 \t Training Loss: 47.168496227264406\n",
      "Epoch 26 \t Batch 60 \t Training Loss: 47.33676662445068\n",
      "Epoch 26 \t Batch 80 \t Training Loss: 47.307852029800415\n",
      "Epoch 26 \t Batch 100 \t Training Loss: 47.106409530639645\n",
      "Epoch 26 \t Batch 120 \t Training Loss: 47.11404848098755\n",
      "Epoch 26 \t Batch 140 \t Training Loss: 47.0480007989066\n",
      "Epoch 26 \t Batch 160 \t Training Loss: 47.16905145645141\n",
      "Epoch 26 \t Batch 180 \t Training Loss: 47.30334765116374\n",
      "Epoch 26 \t Batch 200 \t Training Loss: 47.22558902740479\n",
      "Epoch 26 \t Batch 220 \t Training Loss: 47.263259679620916\n",
      "Epoch 26 \t Batch 240 \t Training Loss: 47.4283127784729\n",
      "Epoch 26 \t Batch 260 \t Training Loss: 47.544756463857794\n",
      "Epoch 26 \t Batch 280 \t Training Loss: 47.50406773430961\n",
      "Epoch 26 \t Batch 300 \t Training Loss: 47.48612631479899\n",
      "Epoch 26 \t Batch 320 \t Training Loss: 47.50595138072968\n",
      "Epoch 26 \t Batch 340 \t Training Loss: 47.56476365257712\n",
      "Epoch 26 \t Batch 360 \t Training Loss: 47.550885020362\n",
      "Epoch 26 \t Batch 380 \t Training Loss: 47.615947592885874\n",
      "Epoch 26 \t Batch 400 \t Training Loss: 47.57213970184326\n",
      "Epoch 26 \t Batch 420 \t Training Loss: 47.59739082881382\n",
      "Epoch 26 \t Batch 440 \t Training Loss: 47.591628161343664\n",
      "Epoch 26 \t Batch 460 \t Training Loss: 47.54623705822488\n",
      "Epoch 26 \t Batch 480 \t Training Loss: 47.483012890815736\n",
      "Epoch 26 \t Batch 500 \t Training Loss: 47.46315056610108\n",
      "Epoch 26 \t Batch 520 \t Training Loss: 47.434613132476805\n",
      "Epoch 26 \t Batch 540 \t Training Loss: 47.414779592443395\n",
      "Epoch 26 \t Batch 560 \t Training Loss: 47.438061271395\n",
      "Epoch 26 \t Batch 580 \t Training Loss: 47.43401325488912\n",
      "Epoch 26 \t Batch 600 \t Training Loss: 47.45229278564453\n",
      "Epoch 26 \t Batch 620 \t Training Loss: 47.4421858449136\n",
      "Epoch 26 \t Batch 640 \t Training Loss: 47.46437497138977\n",
      "Epoch 26 \t Batch 660 \t Training Loss: 47.421508089701334\n",
      "Epoch 26 \t Batch 680 \t Training Loss: 47.47019034553976\n",
      "Epoch 26 \t Batch 700 \t Training Loss: 47.506935457502095\n",
      "Epoch 26 \t Batch 720 \t Training Loss: 47.52626119189792\n",
      "Epoch 26 \t Batch 740 \t Training Loss: 47.4748874148807\n",
      "Epoch 26 \t Batch 760 \t Training Loss: 47.47318515777588\n",
      "Epoch 26 \t Batch 780 \t Training Loss: 47.453267743037294\n",
      "Epoch 26 \t Batch 800 \t Training Loss: 47.406973261833194\n",
      "Epoch 26 \t Batch 820 \t Training Loss: 47.41681149180342\n",
      "Epoch 26 \t Batch 840 \t Training Loss: 47.44028108233497\n",
      "Epoch 26 \t Batch 860 \t Training Loss: 47.46825615195341\n",
      "Epoch 26 \t Batch 880 \t Training Loss: 47.431355255300346\n",
      "Epoch 26 \t Batch 900 \t Training Loss: 47.443864080641006\n",
      "Epoch 26 \t Batch 20 \t Validation Loss: 15.511997318267822\n",
      "Epoch 26 \t Batch 40 \t Validation Loss: 19.847421860694887\n",
      "Epoch 26 \t Batch 60 \t Validation Loss: 19.2873898824056\n",
      "Epoch 26 \t Batch 80 \t Validation Loss: 20.08547019958496\n",
      "Epoch 26 \t Batch 100 \t Validation Loss: 21.45187795639038\n",
      "Epoch 26 \t Batch 120 \t Validation Loss: 22.747796519597372\n",
      "Epoch 26 \t Batch 140 \t Validation Loss: 23.383727850232805\n",
      "Epoch 26 \t Batch 160 \t Validation Loss: 25.40057256221771\n",
      "Epoch 26 \t Batch 180 \t Validation Loss: 28.857395712534586\n",
      "Epoch 26 \t Batch 200 \t Validation Loss: 30.350283718109132\n",
      "Epoch 26 \t Batch 220 \t Validation Loss: 31.62458404194225\n",
      "Epoch 26 \t Batch 240 \t Validation Loss: 32.19210763374964\n",
      "Epoch 26 \t Batch 260 \t Validation Loss: 34.24215018932636\n",
      "Epoch 26 \t Batch 280 \t Validation Loss: 35.366523783547535\n",
      "Epoch 26 \t Batch 300 \t Validation Loss: 36.398377176920576\n",
      "Epoch 26 \t Batch 320 \t Validation Loss: 36.945718365907666\n",
      "Epoch 26 \t Batch 340 \t Validation Loss: 36.987842481276566\n",
      "Epoch 26 \t Batch 360 \t Validation Loss: 36.88915982776218\n",
      "Epoch 26 \t Batch 380 \t Validation Loss: 37.16113002174779\n",
      "Epoch 26 \t Batch 400 \t Validation Loss: 36.872494940757754\n",
      "Epoch 26 \t Batch 420 \t Validation Loss: 37.00974188305083\n",
      "Epoch 26 \t Batch 440 \t Validation Loss: 36.8054183179682\n",
      "Epoch 26 \t Batch 460 \t Validation Loss: 37.078821285911225\n",
      "Epoch 26 \t Batch 480 \t Validation Loss: 37.61084028482437\n",
      "Epoch 26 \t Batch 500 \t Validation Loss: 37.39091614913941\n",
      "Epoch 26 \t Batch 520 \t Validation Loss: 37.18449840178857\n",
      "Epoch 26 \t Batch 540 \t Validation Loss: 36.984247832828096\n",
      "Epoch 26 \t Batch 560 \t Validation Loss: 36.82780984469822\n",
      "Epoch 26 \t Batch 580 \t Validation Loss: 36.580222734911686\n",
      "Epoch 26 \t Batch 600 \t Validation Loss: 36.837889496485396\n",
      "Epoch 26 Training Loss: 47.45419559041962 Validation Loss: 37.48983633363402\n",
      "Epoch 26 completed\n",
      "Epoch 27 \t Batch 20 \t Training Loss: 47.015487098693846\n",
      "Epoch 27 \t Batch 40 \t Training Loss: 46.96387166976929\n",
      "Epoch 27 \t Batch 60 \t Training Loss: 47.573686536153154\n",
      "Epoch 27 \t Batch 80 \t Training Loss: 47.70586247444153\n",
      "Epoch 27 \t Batch 100 \t Training Loss: 47.7549649810791\n",
      "Epoch 27 \t Batch 120 \t Training Loss: 47.71310749053955\n",
      "Epoch 27 \t Batch 140 \t Training Loss: 47.38582314082554\n",
      "Epoch 27 \t Batch 160 \t Training Loss: 47.55048768520355\n",
      "Epoch 27 \t Batch 180 \t Training Loss: 47.745363299051924\n",
      "Epoch 27 \t Batch 200 \t Training Loss: 47.75368547439575\n",
      "Epoch 27 \t Batch 220 \t Training Loss: 47.74627474004572\n",
      "Epoch 27 \t Batch 240 \t Training Loss: 47.78505851427714\n",
      "Epoch 27 \t Batch 260 \t Training Loss: 47.76005514585055\n",
      "Epoch 27 \t Batch 280 \t Training Loss: 47.819566113608225\n",
      "Epoch 27 \t Batch 300 \t Training Loss: 47.68491095225016\n",
      "Epoch 27 \t Batch 320 \t Training Loss: 47.6318475484848\n",
      "Epoch 27 \t Batch 340 \t Training Loss: 47.61663955239688\n",
      "Epoch 27 \t Batch 360 \t Training Loss: 47.66910830603705\n",
      "Epoch 27 \t Batch 380 \t Training Loss: 47.594048871492085\n",
      "Epoch 27 \t Batch 400 \t Training Loss: 47.59768904685974\n",
      "Epoch 27 \t Batch 420 \t Training Loss: 47.530593063717795\n",
      "Epoch 27 \t Batch 440 \t Training Loss: 47.46493532874367\n",
      "Epoch 27 \t Batch 460 \t Training Loss: 47.39499998507292\n",
      "Epoch 27 \t Batch 480 \t Training Loss: 47.37457753022512\n",
      "Epoch 27 \t Batch 500 \t Training Loss: 47.332123214721676\n",
      "Epoch 27 \t Batch 520 \t Training Loss: 47.350608789003815\n",
      "Epoch 27 \t Batch 540 \t Training Loss: 47.29755694777877\n",
      "Epoch 27 \t Batch 560 \t Training Loss: 47.293253782817295\n",
      "Epoch 27 \t Batch 580 \t Training Loss: 47.2599125697695\n",
      "Epoch 27 \t Batch 600 \t Training Loss: 47.24825310389201\n",
      "Epoch 27 \t Batch 620 \t Training Loss: 47.31631448191981\n",
      "Epoch 27 \t Batch 640 \t Training Loss: 47.34604551196098\n",
      "Epoch 27 \t Batch 660 \t Training Loss: 47.37449427517978\n",
      "Epoch 27 \t Batch 680 \t Training Loss: 47.372820523205924\n",
      "Epoch 27 \t Batch 700 \t Training Loss: 47.37484011513846\n",
      "Epoch 27 \t Batch 720 \t Training Loss: 47.41013257768419\n",
      "Epoch 27 \t Batch 740 \t Training Loss: 47.45890215280894\n",
      "Epoch 27 \t Batch 760 \t Training Loss: 47.42817013389186\n",
      "Epoch 27 \t Batch 780 \t Training Loss: 47.40154110835149\n",
      "Epoch 27 \t Batch 800 \t Training Loss: 47.408260016441346\n",
      "Epoch 27 \t Batch 820 \t Training Loss: 47.36562893565108\n",
      "Epoch 27 \t Batch 840 \t Training Loss: 47.37126687367757\n",
      "Epoch 27 \t Batch 860 \t Training Loss: 47.35519363048465\n",
      "Epoch 27 \t Batch 880 \t Training Loss: 47.36793898235668\n",
      "Epoch 27 \t Batch 900 \t Training Loss: 47.392591116163466\n",
      "Epoch 27 \t Batch 20 \t Validation Loss: 18.325043153762817\n",
      "Epoch 27 \t Batch 40 \t Validation Loss: 22.0988849401474\n",
      "Epoch 27 \t Batch 60 \t Validation Loss: 21.233305803934734\n",
      "Epoch 27 \t Batch 80 \t Validation Loss: 21.584884691238404\n",
      "Epoch 27 \t Batch 100 \t Validation Loss: 22.80306499481201\n",
      "Epoch 27 \t Batch 120 \t Validation Loss: 23.998536912600198\n",
      "Epoch 27 \t Batch 140 \t Validation Loss: 24.614312723704746\n",
      "Epoch 27 \t Batch 160 \t Validation Loss: 26.652032965421675\n",
      "Epoch 27 \t Batch 180 \t Validation Loss: 30.367888270484077\n",
      "Epoch 27 \t Batch 200 \t Validation Loss: 31.910821828842163\n",
      "Epoch 27 \t Batch 220 \t Validation Loss: 33.13899552605369\n",
      "Epoch 27 \t Batch 240 \t Validation Loss: 33.65241667032242\n",
      "Epoch 27 \t Batch 260 \t Validation Loss: 35.74125276345473\n",
      "Epoch 27 \t Batch 280 \t Validation Loss: 36.86156962939671\n",
      "Epoch 27 \t Batch 300 \t Validation Loss: 38.05275661468506\n",
      "Epoch 27 \t Batch 320 \t Validation Loss: 38.603567057847975\n",
      "Epoch 27 \t Batch 340 \t Validation Loss: 38.57329060610603\n",
      "Epoch 27 \t Batch 360 \t Validation Loss: 38.494663657082455\n",
      "Epoch 27 \t Batch 380 \t Validation Loss: 38.72231452339574\n",
      "Epoch 27 \t Batch 400 \t Validation Loss: 38.34396478176117\n",
      "Epoch 27 \t Batch 420 \t Validation Loss: 38.361975983210975\n",
      "Epoch 27 \t Batch 440 \t Validation Loss: 38.088360604372895\n",
      "Epoch 27 \t Batch 460 \t Validation Loss: 38.38558702883513\n",
      "Epoch 27 \t Batch 480 \t Validation Loss: 38.90088904301326\n",
      "Epoch 27 \t Batch 500 \t Validation Loss: 38.647276958465575\n",
      "Epoch 27 \t Batch 520 \t Validation Loss: 38.47680435180664\n",
      "Epoch 27 \t Batch 540 \t Validation Loss: 38.26150072592276\n",
      "Epoch 27 \t Batch 560 \t Validation Loss: 38.07904611655644\n",
      "Epoch 27 \t Batch 580 \t Validation Loss: 37.9138849685932\n",
      "Epoch 27 \t Batch 600 \t Validation Loss: 38.13108302434286\n",
      "Epoch 27 Training Loss: 47.41741033572797 Validation Loss: 38.79704301388233\n",
      "Epoch 27 completed\n",
      "Epoch 28 \t Batch 20 \t Training Loss: 48.37343215942383\n",
      "Epoch 28 \t Batch 40 \t Training Loss: 47.56594648361206\n",
      "Epoch 28 \t Batch 60 \t Training Loss: 47.106685129801434\n",
      "Epoch 28 \t Batch 80 \t Training Loss: 47.3130078792572\n",
      "Epoch 28 \t Batch 100 \t Training Loss: 46.98946720123291\n",
      "Epoch 28 \t Batch 120 \t Training Loss: 47.11891066233317\n",
      "Epoch 28 \t Batch 140 \t Training Loss: 47.03010035923549\n",
      "Epoch 28 \t Batch 160 \t Training Loss: 47.173100757598874\n",
      "Epoch 28 \t Batch 180 \t Training Loss: 47.31934356689453\n",
      "Epoch 28 \t Batch 200 \t Training Loss: 47.33663198471069\n",
      "Epoch 28 \t Batch 220 \t Training Loss: 47.335466488924894\n",
      "Epoch 28 \t Batch 240 \t Training Loss: 47.4822364171346\n",
      "Epoch 28 \t Batch 260 \t Training Loss: 47.479549642709586\n",
      "Epoch 28 \t Batch 280 \t Training Loss: 47.56316303525652\n",
      "Epoch 28 \t Batch 300 \t Training Loss: 47.561043930053714\n",
      "Epoch 28 \t Batch 320 \t Training Loss: 47.56983579397202\n",
      "Epoch 28 \t Batch 340 \t Training Loss: 47.624665282754336\n",
      "Epoch 28 \t Batch 360 \t Training Loss: 47.51621444490221\n",
      "Epoch 28 \t Batch 380 \t Training Loss: 47.5774344193308\n",
      "Epoch 28 \t Batch 400 \t Training Loss: 47.58626546859741\n",
      "Epoch 28 \t Batch 420 \t Training Loss: 47.55880059741792\n",
      "Epoch 28 \t Batch 440 \t Training Loss: 47.56799995248968\n",
      "Epoch 28 \t Batch 460 \t Training Loss: 47.64366348100745\n",
      "Epoch 28 \t Batch 480 \t Training Loss: 47.608104689915976\n",
      "Epoch 28 \t Batch 500 \t Training Loss: 47.627830871582034\n",
      "Epoch 28 \t Batch 520 \t Training Loss: 47.56400741430429\n",
      "Epoch 28 \t Batch 540 \t Training Loss: 47.581713379753964\n",
      "Epoch 28 \t Batch 560 \t Training Loss: 47.535091284343174\n",
      "Epoch 28 \t Batch 580 \t Training Loss: 47.54519692124992\n",
      "Epoch 28 \t Batch 600 \t Training Loss: 47.538085301717125\n",
      "Epoch 28 \t Batch 620 \t Training Loss: 47.466344230405745\n",
      "Epoch 28 \t Batch 640 \t Training Loss: 47.45231067538261\n",
      "Epoch 28 \t Batch 660 \t Training Loss: 47.46678830927068\n",
      "Epoch 28 \t Batch 680 \t Training Loss: 47.47284792170805\n",
      "Epoch 28 \t Batch 700 \t Training Loss: 47.472421182904924\n",
      "Epoch 28 \t Batch 720 \t Training Loss: 47.45170573658413\n",
      "Epoch 28 \t Batch 740 \t Training Loss: 47.388834886293154\n",
      "Epoch 28 \t Batch 760 \t Training Loss: 47.36138791034096\n",
      "Epoch 28 \t Batch 780 \t Training Loss: 47.36410332215138\n",
      "Epoch 28 \t Batch 800 \t Training Loss: 47.33738691329956\n",
      "Epoch 28 \t Batch 820 \t Training Loss: 47.35456228488829\n",
      "Epoch 28 \t Batch 840 \t Training Loss: 47.352046380724225\n",
      "Epoch 28 \t Batch 860 \t Training Loss: 47.36679330426593\n",
      "Epoch 28 \t Batch 880 \t Training Loss: 47.41403361667286\n",
      "Epoch 28 \t Batch 900 \t Training Loss: 47.39204995473226\n",
      "Epoch 28 \t Batch 20 \t Validation Loss: 16.122427701950073\n",
      "Epoch 28 \t Batch 40 \t Validation Loss: 21.079489278793336\n",
      "Epoch 28 \t Batch 60 \t Validation Loss: 20.49715929031372\n",
      "Epoch 28 \t Batch 80 \t Validation Loss: 21.19933182001114\n",
      "Epoch 28 \t Batch 100 \t Validation Loss: 22.589463605880738\n",
      "Epoch 28 \t Batch 120 \t Validation Loss: 23.791390363375346\n",
      "Epoch 28 \t Batch 140 \t Validation Loss: 24.392457696369718\n",
      "Epoch 28 \t Batch 160 \t Validation Loss: 26.510843116045\n",
      "Epoch 28 \t Batch 180 \t Validation Loss: 30.224751785066392\n",
      "Epoch 28 \t Batch 200 \t Validation Loss: 31.78591185092926\n",
      "Epoch 28 \t Batch 220 \t Validation Loss: 33.21355380578475\n",
      "Epoch 28 \t Batch 240 \t Validation Loss: 33.7881929119428\n",
      "Epoch 28 \t Batch 260 \t Validation Loss: 35.966589109714214\n",
      "Epoch 28 \t Batch 280 \t Validation Loss: 37.17023060321808\n",
      "Epoch 28 \t Batch 300 \t Validation Loss: 38.30705189069112\n",
      "Epoch 28 \t Batch 320 \t Validation Loss: 38.82467792928219\n",
      "Epoch 28 \t Batch 340 \t Validation Loss: 38.77904932639178\n",
      "Epoch 28 \t Batch 360 \t Validation Loss: 38.69354809919993\n",
      "Epoch 28 \t Batch 380 \t Validation Loss: 38.955020194304616\n",
      "Epoch 28 \t Batch 400 \t Validation Loss: 38.550771491527556\n",
      "Epoch 28 \t Batch 420 \t Validation Loss: 38.5602625642504\n",
      "Epoch 28 \t Batch 440 \t Validation Loss: 38.26806728839874\n",
      "Epoch 28 \t Batch 460 \t Validation Loss: 38.47202474552652\n",
      "Epoch 28 \t Batch 480 \t Validation Loss: 38.94525455037753\n",
      "Epoch 28 \t Batch 500 \t Validation Loss: 38.67328032875061\n",
      "Epoch 28 \t Batch 520 \t Validation Loss: 38.476542054689844\n",
      "Epoch 28 \t Batch 540 \t Validation Loss: 38.27194229761759\n",
      "Epoch 28 \t Batch 560 \t Validation Loss: 38.10948866435459\n",
      "Epoch 28 \t Batch 580 \t Validation Loss: 37.94424520361012\n",
      "Epoch 28 \t Batch 600 \t Validation Loss: 38.17194364547729\n",
      "Epoch 28 Training Loss: 47.37475869231812 Validation Loss: 38.81005219050816\n",
      "Epoch 28 completed\n",
      "Epoch 29 \t Batch 20 \t Training Loss: 47.357429122924806\n",
      "Epoch 29 \t Batch 40 \t Training Loss: 47.86747360229492\n",
      "Epoch 29 \t Batch 60 \t Training Loss: 47.87647482554118\n",
      "Epoch 29 \t Batch 80 \t Training Loss: 47.77244319915771\n",
      "Epoch 29 \t Batch 100 \t Training Loss: 47.85155132293701\n",
      "Epoch 29 \t Batch 120 \t Training Loss: 47.718217913309736\n",
      "Epoch 29 \t Batch 140 \t Training Loss: 47.519850676400324\n",
      "Epoch 29 \t Batch 160 \t Training Loss: 47.506527757644655\n",
      "Epoch 29 \t Batch 180 \t Training Loss: 47.59278269873725\n",
      "Epoch 29 \t Batch 200 \t Training Loss: 47.377705116271976\n",
      "Epoch 29 \t Batch 220 \t Training Loss: 47.32417167316783\n",
      "Epoch 29 \t Batch 240 \t Training Loss: 47.42245906194051\n",
      "Epoch 29 \t Batch 260 \t Training Loss: 47.22993111243615\n",
      "Epoch 29 \t Batch 280 \t Training Loss: 47.31514746802194\n",
      "Epoch 29 \t Batch 300 \t Training Loss: 47.3268498357137\n",
      "Epoch 29 \t Batch 320 \t Training Loss: 47.307831740379335\n",
      "Epoch 29 \t Batch 340 \t Training Loss: 47.276002401464126\n",
      "Epoch 29 \t Batch 360 \t Training Loss: 47.26074196497599\n",
      "Epoch 29 \t Batch 380 \t Training Loss: 47.335165756627134\n",
      "Epoch 29 \t Batch 400 \t Training Loss: 47.397516918182376\n",
      "Epoch 29 \t Batch 420 \t Training Loss: 47.31279480343773\n",
      "Epoch 29 \t Batch 440 \t Training Loss: 47.30913972854614\n",
      "Epoch 29 \t Batch 460 \t Training Loss: 47.28720545561417\n",
      "Epoch 29 \t Batch 480 \t Training Loss: 47.29080085754394\n",
      "Epoch 29 \t Batch 500 \t Training Loss: 47.299223251342774\n",
      "Epoch 29 \t Batch 520 \t Training Loss: 47.35118719981267\n",
      "Epoch 29 \t Batch 540 \t Training Loss: 47.36442364586724\n",
      "Epoch 29 \t Batch 560 \t Training Loss: 47.462265505109514\n",
      "Epoch 29 \t Batch 580 \t Training Loss: 47.44169967256743\n",
      "Epoch 29 \t Batch 600 \t Training Loss: 47.39746128082275\n",
      "Epoch 29 \t Batch 620 \t Training Loss: 47.37661882215931\n",
      "Epoch 29 \t Batch 640 \t Training Loss: 47.32282964587212\n",
      "Epoch 29 \t Batch 660 \t Training Loss: 47.32260279799953\n",
      "Epoch 29 \t Batch 680 \t Training Loss: 47.30942865820492\n",
      "Epoch 29 \t Batch 700 \t Training Loss: 47.305631501334055\n",
      "Epoch 29 \t Batch 720 \t Training Loss: 47.32982521586948\n",
      "Epoch 29 \t Batch 740 \t Training Loss: 47.341251512475914\n",
      "Epoch 29 \t Batch 760 \t Training Loss: 47.35456990693745\n",
      "Epoch 29 \t Batch 780 \t Training Loss: 47.35611029893924\n",
      "Epoch 29 \t Batch 800 \t Training Loss: 47.33661256790161\n",
      "Epoch 29 \t Batch 820 \t Training Loss: 47.33731346130371\n",
      "Epoch 29 \t Batch 840 \t Training Loss: 47.33679770969209\n",
      "Epoch 29 \t Batch 860 \t Training Loss: 47.32287969367449\n",
      "Epoch 29 \t Batch 880 \t Training Loss: 47.337790034034036\n",
      "Epoch 29 \t Batch 900 \t Training Loss: 47.333758705986874\n",
      "Epoch 29 \t Batch 20 \t Validation Loss: 23.478685522079466\n",
      "Epoch 29 \t Batch 40 \t Validation Loss: 25.607304430007936\n",
      "Epoch 29 \t Batch 60 \t Validation Loss: 25.39790770212809\n",
      "Epoch 29 \t Batch 80 \t Validation Loss: 25.860645401477814\n",
      "Epoch 29 \t Batch 100 \t Validation Loss: 26.29352255821228\n",
      "Epoch 29 \t Batch 120 \t Validation Loss: 26.999254250526427\n",
      "Epoch 29 \t Batch 140 \t Validation Loss: 27.164373786108836\n",
      "Epoch 29 \t Batch 160 \t Validation Loss: 29.072920745611192\n",
      "Epoch 29 \t Batch 180 \t Validation Loss: 32.60616948339674\n",
      "Epoch 29 \t Batch 200 \t Validation Loss: 34.05638565540314\n",
      "Epoch 29 \t Batch 220 \t Validation Loss: 35.44343023733659\n",
      "Epoch 29 \t Batch 240 \t Validation Loss: 35.94012772639592\n",
      "Epoch 29 \t Batch 260 \t Validation Loss: 38.056434279221754\n",
      "Epoch 29 \t Batch 280 \t Validation Loss: 39.19923026561737\n",
      "Epoch 29 \t Batch 300 \t Validation Loss: 40.247574179967245\n",
      "Epoch 29 \t Batch 320 \t Validation Loss: 40.72784282267094\n",
      "Epoch 29 \t Batch 340 \t Validation Loss: 40.6353608047261\n",
      "Epoch 29 \t Batch 360 \t Validation Loss: 40.483605927891205\n",
      "Epoch 29 \t Batch 380 \t Validation Loss: 40.72730714647393\n",
      "Epoch 29 \t Batch 400 \t Validation Loss: 40.303971478939054\n",
      "Epoch 29 \t Batch 420 \t Validation Loss: 40.3281449613117\n",
      "Epoch 29 \t Batch 440 \t Validation Loss: 40.022973747686905\n",
      "Epoch 29 \t Batch 460 \t Validation Loss: 40.2492087882498\n",
      "Epoch 29 \t Batch 480 \t Validation Loss: 40.715478005011875\n",
      "Epoch 29 \t Batch 500 \t Validation Loss: 40.44442758750915\n",
      "Epoch 29 \t Batch 520 \t Validation Loss: 40.18224179011125\n",
      "Epoch 29 \t Batch 540 \t Validation Loss: 39.88697777677466\n",
      "Epoch 29 \t Batch 560 \t Validation Loss: 39.6272735783032\n",
      "Epoch 29 \t Batch 580 \t Validation Loss: 39.29908793054778\n",
      "Epoch 29 \t Batch 600 \t Validation Loss: 39.451526303291324\n",
      "Epoch 29 Training Loss: 47.33251026698521 Validation Loss: 40.03190755379664\n",
      "Epoch 29 completed\n",
      "Epoch 30 \t Batch 20 \t Training Loss: 46.37544555664063\n",
      "Epoch 30 \t Batch 40 \t Training Loss: 46.384036254882815\n",
      "Epoch 30 \t Batch 60 \t Training Loss: 46.77581418355306\n",
      "Epoch 30 \t Batch 80 \t Training Loss: 46.570323848724364\n",
      "Epoch 30 \t Batch 100 \t Training Loss: 46.75962657928467\n",
      "Epoch 30 \t Batch 120 \t Training Loss: 46.94067977269491\n",
      "Epoch 30 \t Batch 140 \t Training Loss: 47.1037094116211\n",
      "Epoch 30 \t Batch 160 \t Training Loss: 47.22743818759918\n",
      "Epoch 30 \t Batch 180 \t Training Loss: 47.273545286390515\n",
      "Epoch 30 \t Batch 200 \t Training Loss: 47.284412155151365\n",
      "Epoch 30 \t Batch 220 \t Training Loss: 47.34421788995916\n",
      "Epoch 30 \t Batch 240 \t Training Loss: 47.25460569063822\n",
      "Epoch 30 \t Batch 260 \t Training Loss: 47.1530098254864\n",
      "Epoch 30 \t Batch 280 \t Training Loss: 47.21865753446306\n",
      "Epoch 30 \t Batch 300 \t Training Loss: 47.29381586710612\n",
      "Epoch 30 \t Batch 320 \t Training Loss: 47.29462380409241\n",
      "Epoch 30 \t Batch 340 \t Training Loss: 47.28512796514175\n",
      "Epoch 30 \t Batch 360 \t Training Loss: 47.30899556477865\n",
      "Epoch 30 \t Batch 380 \t Training Loss: 47.27249348289088\n",
      "Epoch 30 \t Batch 400 \t Training Loss: 47.29333294868469\n",
      "Epoch 30 \t Batch 420 \t Training Loss: 47.32062940143403\n",
      "Epoch 30 \t Batch 440 \t Training Loss: 47.344204954667525\n",
      "Epoch 30 \t Batch 460 \t Training Loss: 47.3798514407614\n",
      "Epoch 30 \t Batch 480 \t Training Loss: 47.39221143722534\n",
      "Epoch 30 \t Batch 500 \t Training Loss: 47.367405029296876\n",
      "Epoch 30 \t Batch 520 \t Training Loss: 47.32504011300894\n",
      "Epoch 30 \t Batch 540 \t Training Loss: 47.351312269987886\n",
      "Epoch 30 \t Batch 560 \t Training Loss: 47.314319474356516\n",
      "Epoch 30 \t Batch 580 \t Training Loss: 47.35290370809621\n",
      "Epoch 30 \t Batch 600 \t Training Loss: 47.32604700724284\n",
      "Epoch 30 \t Batch 620 \t Training Loss: 47.277862548828125\n",
      "Epoch 30 \t Batch 640 \t Training Loss: 47.263829082250595\n",
      "Epoch 30 \t Batch 660 \t Training Loss: 47.273826344807944\n",
      "Epoch 30 \t Batch 680 \t Training Loss: 47.31952178057502\n",
      "Epoch 30 \t Batch 700 \t Training Loss: 47.31253235408238\n",
      "Epoch 30 \t Batch 720 \t Training Loss: 47.287049537234836\n",
      "Epoch 30 \t Batch 740 \t Training Loss: 47.30950605547106\n",
      "Epoch 30 \t Batch 760 \t Training Loss: 47.29443813625135\n",
      "Epoch 30 \t Batch 780 \t Training Loss: 47.27332118107722\n",
      "Epoch 30 \t Batch 800 \t Training Loss: 47.22285801887512\n",
      "Epoch 30 \t Batch 820 \t Training Loss: 47.20790530879323\n",
      "Epoch 30 \t Batch 840 \t Training Loss: 47.19788699831281\n",
      "Epoch 30 \t Batch 860 \t Training Loss: 47.23616079286087\n",
      "Epoch 30 \t Batch 880 \t Training Loss: 47.25274853272872\n",
      "Epoch 30 \t Batch 900 \t Training Loss: 47.26374117533366\n",
      "Epoch 30 \t Batch 20 \t Validation Loss: 21.16033320426941\n",
      "Epoch 30 \t Batch 40 \t Validation Loss: 23.858460998535158\n",
      "Epoch 30 \t Batch 60 \t Validation Loss: 23.44106763203939\n",
      "Epoch 30 \t Batch 80 \t Validation Loss: 23.940977108478545\n",
      "Epoch 30 \t Batch 100 \t Validation Loss: 24.79454300880432\n",
      "Epoch 30 \t Batch 120 \t Validation Loss: 25.72203696568807\n",
      "Epoch 30 \t Batch 140 \t Validation Loss: 26.03494088309152\n",
      "Epoch 30 \t Batch 160 \t Validation Loss: 27.791571617126465\n",
      "Epoch 30 \t Batch 180 \t Validation Loss: 31.10832297007243\n",
      "Epoch 30 \t Batch 200 \t Validation Loss: 32.40512485504151\n",
      "Epoch 30 \t Batch 220 \t Validation Loss: 33.59700995358554\n",
      "Epoch 30 \t Batch 240 \t Validation Loss: 34.04313395023346\n",
      "Epoch 30 \t Batch 260 \t Validation Loss: 36.0286145063547\n",
      "Epoch 30 \t Batch 280 \t Validation Loss: 37.081303926876615\n",
      "Epoch 30 \t Batch 300 \t Validation Loss: 38.03548973401387\n",
      "Epoch 30 \t Batch 320 \t Validation Loss: 38.49411675035954\n",
      "Epoch 30 \t Batch 340 \t Validation Loss: 38.41551565001993\n",
      "Epoch 30 \t Batch 360 \t Validation Loss: 38.23778660297394\n",
      "Epoch 30 \t Batch 380 \t Validation Loss: 38.474071409827786\n",
      "Epoch 30 \t Batch 400 \t Validation Loss: 38.083208515644074\n",
      "Epoch 30 \t Batch 420 \t Validation Loss: 38.12525619552249\n",
      "Epoch 30 \t Batch 440 \t Validation Loss: 37.868461311947215\n",
      "Epoch 30 \t Batch 460 \t Validation Loss: 38.08033072222834\n",
      "Epoch 30 \t Batch 480 \t Validation Loss: 38.55660238862038\n",
      "Epoch 30 \t Batch 500 \t Validation Loss: 38.248297082901004\n",
      "Epoch 30 \t Batch 520 \t Validation Loss: 37.994698900442856\n",
      "Epoch 30 \t Batch 540 \t Validation Loss: 37.78461454709371\n",
      "Epoch 30 \t Batch 560 \t Validation Loss: 37.64332550423486\n",
      "Epoch 30 \t Batch 580 \t Validation Loss: 37.402324997145556\n",
      "Epoch 30 \t Batch 600 \t Validation Loss: 37.68594180901845\n",
      "Epoch 30 Training Loss: 47.28292969814954 Validation Loss: 38.345248750277925\n",
      "Epoch 30 completed\n",
      "Epoch 31 \t Batch 20 \t Training Loss: 46.54959354400635\n",
      "Epoch 31 \t Batch 40 \t Training Loss: 47.37401275634765\n",
      "Epoch 31 \t Batch 60 \t Training Loss: 47.51202646891276\n",
      "Epoch 31 \t Batch 80 \t Training Loss: 47.57982802391052\n",
      "Epoch 31 \t Batch 100 \t Training Loss: 47.75454132080078\n",
      "Epoch 31 \t Batch 120 \t Training Loss: 47.62680559158325\n",
      "Epoch 31 \t Batch 140 \t Training Loss: 47.67515313284738\n",
      "Epoch 31 \t Batch 160 \t Training Loss: 47.61491184234619\n",
      "Epoch 31 \t Batch 180 \t Training Loss: 47.4760799407959\n",
      "Epoch 31 \t Batch 200 \t Training Loss: 47.51885368347168\n",
      "Epoch 31 \t Batch 220 \t Training Loss: 47.35250963731246\n",
      "Epoch 31 \t Batch 240 \t Training Loss: 47.36336714426677\n",
      "Epoch 31 \t Batch 260 \t Training Loss: 47.39156005565937\n",
      "Epoch 31 \t Batch 280 \t Training Loss: 47.4123816217695\n",
      "Epoch 31 \t Batch 300 \t Training Loss: 47.34103905995687\n",
      "Epoch 31 \t Batch 320 \t Training Loss: 47.383304476737976\n",
      "Epoch 31 \t Batch 340 \t Training Loss: 47.435628004635085\n",
      "Epoch 31 \t Batch 360 \t Training Loss: 47.45112547344632\n",
      "Epoch 31 \t Batch 380 \t Training Loss: 47.47575500889828\n",
      "Epoch 31 \t Batch 400 \t Training Loss: 47.46314569473267\n",
      "Epoch 31 \t Batch 420 \t Training Loss: 47.45838942754836\n",
      "Epoch 31 \t Batch 440 \t Training Loss: 47.47064254934138\n",
      "Epoch 31 \t Batch 460 \t Training Loss: 47.48638274151346\n",
      "Epoch 31 \t Batch 480 \t Training Loss: 47.46199812889099\n",
      "Epoch 31 \t Batch 500 \t Training Loss: 47.48115128326416\n",
      "Epoch 31 \t Batch 520 \t Training Loss: 47.50520798609807\n",
      "Epoch 31 \t Batch 540 \t Training Loss: 47.48859936043068\n",
      "Epoch 31 \t Batch 560 \t Training Loss: 47.50764950343541\n",
      "Epoch 31 \t Batch 580 \t Training Loss: 47.45907918338118\n",
      "Epoch 31 \t Batch 600 \t Training Loss: 47.47156050999959\n",
      "Epoch 31 \t Batch 620 \t Training Loss: 47.44051833614226\n",
      "Epoch 31 \t Batch 640 \t Training Loss: 47.44900558590889\n",
      "Epoch 31 \t Batch 660 \t Training Loss: 47.439773114522296\n",
      "Epoch 31 \t Batch 680 \t Training Loss: 47.45519382252413\n",
      "Epoch 31 \t Batch 700 \t Training Loss: 47.38053001403809\n",
      "Epoch 31 \t Batch 720 \t Training Loss: 47.35087543593512\n",
      "Epoch 31 \t Batch 740 \t Training Loss: 47.34473580025338\n",
      "Epoch 31 \t Batch 760 \t Training Loss: 47.32869315900301\n",
      "Epoch 31 \t Batch 780 \t Training Loss: 47.343662843948756\n",
      "Epoch 31 \t Batch 800 \t Training Loss: 47.32920025825501\n",
      "Epoch 31 \t Batch 820 \t Training Loss: 47.338945063149055\n",
      "Epoch 31 \t Batch 840 \t Training Loss: 47.34855195454189\n",
      "Epoch 31 \t Batch 860 \t Training Loss: 47.34301974274391\n",
      "Epoch 31 \t Batch 880 \t Training Loss: 47.28769109465859\n",
      "Epoch 31 \t Batch 900 \t Training Loss: 47.26463387383355\n",
      "Epoch 31 \t Batch 20 \t Validation Loss: 14.339942264556885\n",
      "Epoch 31 \t Batch 40 \t Validation Loss: 17.697092151641847\n",
      "Epoch 31 \t Batch 60 \t Validation Loss: 17.464017470677693\n",
      "Epoch 31 \t Batch 80 \t Validation Loss: 18.306121397018433\n",
      "Epoch 31 \t Batch 100 \t Validation Loss: 20.10952224731445\n",
      "Epoch 31 \t Batch 120 \t Validation Loss: 21.868007667859395\n",
      "Epoch 31 \t Batch 140 \t Validation Loss: 22.722402885981968\n",
      "Epoch 31 \t Batch 160 \t Validation Loss: 25.06372126340866\n",
      "Epoch 31 \t Batch 180 \t Validation Loss: 28.999579524993898\n",
      "Epoch 31 \t Batch 200 \t Validation Loss: 30.73972319602966\n",
      "Epoch 31 \t Batch 220 \t Validation Loss: 32.3462837826122\n",
      "Epoch 31 \t Batch 240 \t Validation Loss: 33.04273424943288\n",
      "Epoch 31 \t Batch 260 \t Validation Loss: 35.391577999408426\n",
      "Epoch 31 \t Batch 280 \t Validation Loss: 36.70021963800703\n",
      "Epoch 31 \t Batch 300 \t Validation Loss: 37.8299037361145\n",
      "Epoch 31 \t Batch 320 \t Validation Loss: 38.4066427052021\n",
      "Epoch 31 \t Batch 340 \t Validation Loss: 38.419906223521515\n",
      "Epoch 31 \t Batch 360 \t Validation Loss: 38.315654163890414\n",
      "Epoch 31 \t Batch 380 \t Validation Loss: 38.62473254203796\n",
      "Epoch 31 \t Batch 400 \t Validation Loss: 38.25529500246048\n",
      "Epoch 31 \t Batch 420 \t Validation Loss: 38.30193213281177\n",
      "Epoch 31 \t Batch 440 \t Validation Loss: 38.04310857382688\n",
      "Epoch 31 \t Batch 460 \t Validation Loss: 38.29449793981469\n",
      "Epoch 31 \t Batch 480 \t Validation Loss: 38.8302704513073\n",
      "Epoch 31 \t Batch 500 \t Validation Loss: 38.59092828178406\n",
      "Epoch 31 \t Batch 520 \t Validation Loss: 38.340204787254336\n",
      "Epoch 31 \t Batch 540 \t Validation Loss: 38.127227993364684\n",
      "Epoch 31 \t Batch 560 \t Validation Loss: 37.95990938629423\n",
      "Epoch 31 \t Batch 580 \t Validation Loss: 37.70136956346446\n",
      "Epoch 31 \t Batch 600 \t Validation Loss: 37.95213964303335\n",
      "Epoch 31 Training Loss: 47.258329239640396 Validation Loss: 38.59905379933196\n",
      "Epoch 31 completed\n",
      "Epoch 32 \t Batch 20 \t Training Loss: 47.16324710845947\n",
      "Epoch 32 \t Batch 40 \t Training Loss: 47.74479265213013\n",
      "Epoch 32 \t Batch 60 \t Training Loss: 47.38990421295166\n",
      "Epoch 32 \t Batch 80 \t Training Loss: 47.38033380508423\n",
      "Epoch 32 \t Batch 100 \t Training Loss: 47.189057693481445\n",
      "Epoch 32 \t Batch 120 \t Training Loss: 47.30619093577067\n",
      "Epoch 32 \t Batch 140 \t Training Loss: 47.14332823072161\n",
      "Epoch 32 \t Batch 160 \t Training Loss: 47.133036971092224\n",
      "Epoch 32 \t Batch 180 \t Training Loss: 47.270505078633626\n",
      "Epoch 32 \t Batch 200 \t Training Loss: 47.46377016067505\n",
      "Epoch 32 \t Batch 220 \t Training Loss: 47.424017281965774\n",
      "Epoch 32 \t Batch 240 \t Training Loss: 47.32327419916789\n",
      "Epoch 32 \t Batch 260 \t Training Loss: 47.29864336160513\n",
      "Epoch 32 \t Batch 280 \t Training Loss: 47.24636810847691\n",
      "Epoch 32 \t Batch 300 \t Training Loss: 47.205374565124515\n",
      "Epoch 32 \t Batch 320 \t Training Loss: 47.15041185617447\n",
      "Epoch 32 \t Batch 340 \t Training Loss: 47.21973461824305\n",
      "Epoch 32 \t Batch 360 \t Training Loss: 47.2812667104933\n",
      "Epoch 32 \t Batch 380 \t Training Loss: 47.19378752457468\n",
      "Epoch 32 \t Batch 400 \t Training Loss: 47.25401997566223\n",
      "Epoch 32 \t Batch 420 \t Training Loss: 47.273180443899975\n",
      "Epoch 32 \t Batch 440 \t Training Loss: 47.20481628071178\n",
      "Epoch 32 \t Batch 460 \t Training Loss: 47.15668443596881\n",
      "Epoch 32 \t Batch 480 \t Training Loss: 47.19168190161387\n",
      "Epoch 32 \t Batch 500 \t Training Loss: 47.16390514373779\n",
      "Epoch 32 \t Batch 520 \t Training Loss: 47.18246787144587\n",
      "Epoch 32 \t Batch 540 \t Training Loss: 47.158336342705624\n",
      "Epoch 32 \t Batch 560 \t Training Loss: 47.155129180635726\n",
      "Epoch 32 \t Batch 580 \t Training Loss: 47.183534194683205\n",
      "Epoch 32 \t Batch 600 \t Training Loss: 47.15859230677287\n",
      "Epoch 32 \t Batch 620 \t Training Loss: 47.19927133129489\n",
      "Epoch 32 \t Batch 640 \t Training Loss: 47.25656526684761\n",
      "Epoch 32 \t Batch 660 \t Training Loss: 47.257217806035825\n",
      "Epoch 32 \t Batch 680 \t Training Loss: 47.23146929460413\n",
      "Epoch 32 \t Batch 700 \t Training Loss: 47.27005638122559\n",
      "Epoch 32 \t Batch 720 \t Training Loss: 47.24180188708835\n",
      "Epoch 32 \t Batch 740 \t Training Loss: 47.24895611840326\n",
      "Epoch 32 \t Batch 760 \t Training Loss: 47.26614457180626\n",
      "Epoch 32 \t Batch 780 \t Training Loss: 47.2401296664507\n",
      "Epoch 32 \t Batch 800 \t Training Loss: 47.22617357730866\n",
      "Epoch 32 \t Batch 820 \t Training Loss: 47.239214445904985\n",
      "Epoch 32 \t Batch 840 \t Training Loss: 47.21966892878215\n",
      "Epoch 32 \t Batch 860 \t Training Loss: 47.22901735527571\n",
      "Epoch 32 \t Batch 880 \t Training Loss: 47.237127273732966\n",
      "Epoch 32 \t Batch 900 \t Training Loss: 47.23811230977376\n",
      "Epoch 32 \t Batch 20 \t Validation Loss: 20.87603669166565\n",
      "Epoch 32 \t Batch 40 \t Validation Loss: 22.698112344741823\n",
      "Epoch 32 \t Batch 60 \t Validation Loss: 22.476588106155397\n",
      "Epoch 32 \t Batch 80 \t Validation Loss: 22.86763949394226\n",
      "Epoch 32 \t Batch 100 \t Validation Loss: 23.8031329536438\n",
      "Epoch 32 \t Batch 120 \t Validation Loss: 24.71664691766103\n",
      "Epoch 32 \t Batch 140 \t Validation Loss: 25.178201280321392\n",
      "Epoch 32 \t Batch 160 \t Validation Loss: 27.055631864070893\n",
      "Epoch 32 \t Batch 180 \t Validation Loss: 30.597151221169366\n",
      "Epoch 32 \t Batch 200 \t Validation Loss: 32.05692341327667\n",
      "Epoch 32 \t Batch 220 \t Validation Loss: 33.272262473539875\n",
      "Epoch 32 \t Batch 240 \t Validation Loss: 33.74637823104858\n",
      "Epoch 32 \t Batch 260 \t Validation Loss: 35.79503926864037\n",
      "Epoch 32 \t Batch 280 \t Validation Loss: 36.87322452408927\n",
      "Epoch 32 \t Batch 300 \t Validation Loss: 37.965921554565426\n",
      "Epoch 32 \t Batch 320 \t Validation Loss: 38.44332764446735\n",
      "Epoch 32 \t Batch 340 \t Validation Loss: 38.37853031999924\n",
      "Epoch 32 \t Batch 360 \t Validation Loss: 38.24152534537845\n",
      "Epoch 32 \t Batch 380 \t Validation Loss: 38.452165018884756\n",
      "Epoch 32 \t Batch 400 \t Validation Loss: 38.06242818117142\n",
      "Epoch 32 \t Batch 420 \t Validation Loss: 38.07016686711992\n",
      "Epoch 32 \t Batch 440 \t Validation Loss: 37.78615623604168\n",
      "Epoch 32 \t Batch 460 \t Validation Loss: 38.007456400083456\n",
      "Epoch 32 \t Batch 480 \t Validation Loss: 38.49121497670809\n",
      "Epoch 32 \t Batch 500 \t Validation Loss: 38.196950803756714\n",
      "Epoch 32 \t Batch 520 \t Validation Loss: 38.02747287200047\n",
      "Epoch 32 \t Batch 540 \t Validation Loss: 37.825948819407714\n",
      "Epoch 32 \t Batch 560 \t Validation Loss: 37.65208686930793\n",
      "Epoch 32 \t Batch 580 \t Validation Loss: 37.48586776503201\n",
      "Epoch 32 \t Batch 600 \t Validation Loss: 37.70948403199514\n",
      "Epoch 32 Training Loss: 47.238438900580455 Validation Loss: 38.412046012940344\n",
      "Epoch 32 completed\n",
      "Epoch 33 \t Batch 20 \t Training Loss: 46.04521560668945\n",
      "Epoch 33 \t Batch 40 \t Training Loss: 46.301618576049805\n",
      "Epoch 33 \t Batch 60 \t Training Loss: 46.01997585296631\n",
      "Epoch 33 \t Batch 80 \t Training Loss: 46.29834661483765\n",
      "Epoch 33 \t Batch 100 \t Training Loss: 46.224685974121094\n",
      "Epoch 33 \t Batch 120 \t Training Loss: 46.28889195124308\n",
      "Epoch 33 \t Batch 140 \t Training Loss: 46.38254844120571\n",
      "Epoch 33 \t Batch 160 \t Training Loss: 46.30993821620941\n",
      "Epoch 33 \t Batch 180 \t Training Loss: 46.5153923034668\n",
      "Epoch 33 \t Batch 200 \t Training Loss: 46.48271995544434\n",
      "Epoch 33 \t Batch 220 \t Training Loss: 46.58805770874024\n",
      "Epoch 33 \t Batch 240 \t Training Loss: 46.70023409525553\n",
      "Epoch 33 \t Batch 260 \t Training Loss: 46.6244999225323\n",
      "Epoch 33 \t Batch 280 \t Training Loss: 46.78728358404977\n",
      "Epoch 33 \t Batch 300 \t Training Loss: 46.84622155507406\n",
      "Epoch 33 \t Batch 320 \t Training Loss: 46.97425276041031\n",
      "Epoch 33 \t Batch 340 \t Training Loss: 46.99075416116153\n",
      "Epoch 33 \t Batch 360 \t Training Loss: 47.049508825937906\n",
      "Epoch 33 \t Batch 380 \t Training Loss: 46.94906827023155\n",
      "Epoch 33 \t Batch 400 \t Training Loss: 47.04151019096375\n",
      "Epoch 33 \t Batch 420 \t Training Loss: 47.09483232044038\n",
      "Epoch 33 \t Batch 440 \t Training Loss: 47.16160443045876\n",
      "Epoch 33 \t Batch 460 \t Training Loss: 47.16068811831267\n",
      "Epoch 33 \t Batch 480 \t Training Loss: 47.19015996456146\n",
      "Epoch 33 \t Batch 500 \t Training Loss: 47.16695835113526\n",
      "Epoch 33 \t Batch 520 \t Training Loss: 47.160359162550705\n",
      "Epoch 33 \t Batch 540 \t Training Loss: 47.20018408740008\n",
      "Epoch 33 \t Batch 560 \t Training Loss: 47.181018822533744\n",
      "Epoch 33 \t Batch 580 \t Training Loss: 47.20207054532808\n",
      "Epoch 33 \t Batch 600 \t Training Loss: 47.2090474764506\n",
      "Epoch 33 \t Batch 620 \t Training Loss: 47.19449076498709\n",
      "Epoch 33 \t Batch 640 \t Training Loss: 47.18891511559487\n",
      "Epoch 33 \t Batch 660 \t Training Loss: 47.2267285491481\n",
      "Epoch 33 \t Batch 680 \t Training Loss: 47.22321528266458\n",
      "Epoch 33 \t Batch 700 \t Training Loss: 47.221285182407925\n",
      "Epoch 33 \t Batch 720 \t Training Loss: 47.21248271200392\n",
      "Epoch 33 \t Batch 740 \t Training Loss: 47.22333966332513\n",
      "Epoch 33 \t Batch 760 \t Training Loss: 47.24716732627467\n",
      "Epoch 33 \t Batch 780 \t Training Loss: 47.280359625205016\n",
      "Epoch 33 \t Batch 800 \t Training Loss: 47.26060406684876\n",
      "Epoch 33 \t Batch 820 \t Training Loss: 47.221123411597276\n",
      "Epoch 33 \t Batch 840 \t Training Loss: 47.22853073392596\n",
      "Epoch 33 \t Batch 860 \t Training Loss: 47.22472294208615\n",
      "Epoch 33 \t Batch 880 \t Training Loss: 47.18748545213179\n",
      "Epoch 33 \t Batch 900 \t Training Loss: 47.19283846537272\n",
      "Epoch 33 \t Batch 20 \t Validation Loss: 13.943245148658752\n",
      "Epoch 33 \t Batch 40 \t Validation Loss: 18.545339465141296\n",
      "Epoch 33 \t Batch 60 \t Validation Loss: 17.79760791460673\n",
      "Epoch 33 \t Batch 80 \t Validation Loss: 18.738871109485626\n",
      "Epoch 33 \t Batch 100 \t Validation Loss: 20.756176824569703\n",
      "Epoch 33 \t Batch 120 \t Validation Loss: 22.26631659666697\n",
      "Epoch 33 \t Batch 140 \t Validation Loss: 23.211716876711165\n",
      "Epoch 33 \t Batch 160 \t Validation Loss: 25.63221680521965\n",
      "Epoch 33 \t Batch 180 \t Validation Loss: 29.55268809000651\n",
      "Epoch 33 \t Batch 200 \t Validation Loss: 31.331706638336183\n",
      "Epoch 33 \t Batch 220 \t Validation Loss: 32.82574809681285\n",
      "Epoch 33 \t Batch 240 \t Validation Loss: 33.48637078603109\n",
      "Epoch 33 \t Batch 260 \t Validation Loss: 35.76954512229332\n",
      "Epoch 33 \t Batch 280 \t Validation Loss: 37.000975775718686\n",
      "Epoch 33 \t Batch 300 \t Validation Loss: 38.23424591382344\n",
      "Epoch 33 \t Batch 320 \t Validation Loss: 38.819731971621515\n",
      "Epoch 33 \t Batch 340 \t Validation Loss: 38.797296936371744\n",
      "Epoch 33 \t Batch 360 \t Validation Loss: 38.778999050458275\n",
      "Epoch 33 \t Batch 380 \t Validation Loss: 39.10903992150959\n",
      "Epoch 33 \t Batch 400 \t Validation Loss: 38.768344733715054\n",
      "Epoch 33 \t Batch 420 \t Validation Loss: 38.78070252055213\n",
      "Epoch 33 \t Batch 440 \t Validation Loss: 38.557271937890484\n",
      "Epoch 33 \t Batch 460 \t Validation Loss: 38.92405734476836\n",
      "Epoch 33 \t Batch 480 \t Validation Loss: 39.437407992283504\n",
      "Epoch 33 \t Batch 500 \t Validation Loss: 39.174736906051635\n",
      "Epoch 33 \t Batch 520 \t Validation Loss: 39.08961595755357\n",
      "Epoch 33 \t Batch 540 \t Validation Loss: 38.939760529553446\n",
      "Epoch 33 \t Batch 560 \t Validation Loss: 38.850285686765396\n",
      "Epoch 33 \t Batch 580 \t Validation Loss: 38.78131415926177\n",
      "Epoch 33 \t Batch 600 \t Validation Loss: 39.03928037643433\n",
      "Epoch 33 Training Loss: 47.17318691474141 Validation Loss: 39.756094861340216\n",
      "Epoch 33 completed\n",
      "Epoch 34 \t Batch 20 \t Training Loss: 48.62120304107666\n",
      "Epoch 34 \t Batch 40 \t Training Loss: 48.49942855834961\n",
      "Epoch 34 \t Batch 60 \t Training Loss: 48.680266571044925\n",
      "Epoch 34 \t Batch 80 \t Training Loss: 48.42004871368408\n",
      "Epoch 34 \t Batch 100 \t Training Loss: 48.02940277099609\n",
      "Epoch 34 \t Batch 120 \t Training Loss: 47.94292087554932\n",
      "Epoch 34 \t Batch 140 \t Training Loss: 47.72005378178188\n",
      "Epoch 34 \t Batch 160 \t Training Loss: 47.6104914188385\n",
      "Epoch 34 \t Batch 180 \t Training Loss: 47.724269994099934\n",
      "Epoch 34 \t Batch 200 \t Training Loss: 47.7250710105896\n",
      "Epoch 34 \t Batch 220 \t Training Loss: 47.66090100028298\n",
      "Epoch 34 \t Batch 240 \t Training Loss: 47.5868553797404\n",
      "Epoch 34 \t Batch 260 \t Training Loss: 47.51938976874718\n",
      "Epoch 34 \t Batch 280 \t Training Loss: 47.348048850468224\n",
      "Epoch 34 \t Batch 300 \t Training Loss: 47.30144865671794\n",
      "Epoch 34 \t Batch 320 \t Training Loss: 47.282901394367215\n",
      "Epoch 34 \t Batch 340 \t Training Loss: 47.28649546679328\n",
      "Epoch 34 \t Batch 360 \t Training Loss: 47.28364241917928\n",
      "Epoch 34 \t Batch 380 \t Training Loss: 47.25546200400905\n",
      "Epoch 34 \t Batch 400 \t Training Loss: 47.22221767425537\n",
      "Epoch 34 \t Batch 420 \t Training Loss: 47.17085699353899\n",
      "Epoch 34 \t Batch 440 \t Training Loss: 47.185941167311235\n",
      "Epoch 34 \t Batch 460 \t Training Loss: 47.205662743941595\n",
      "Epoch 34 \t Batch 480 \t Training Loss: 47.2188319683075\n",
      "Epoch 34 \t Batch 500 \t Training Loss: 47.198624618530275\n",
      "Epoch 34 \t Batch 520 \t Training Loss: 47.20882617510282\n",
      "Epoch 34 \t Batch 540 \t Training Loss: 47.18580098328767\n",
      "Epoch 34 \t Batch 560 \t Training Loss: 47.17580945151193\n",
      "Epoch 34 \t Batch 580 \t Training Loss: 47.200401615274366\n",
      "Epoch 34 \t Batch 600 \t Training Loss: 47.18386359532674\n",
      "Epoch 34 \t Batch 620 \t Training Loss: 47.215213861773094\n",
      "Epoch 34 \t Batch 640 \t Training Loss: 47.178140139579774\n",
      "Epoch 34 \t Batch 660 \t Training Loss: 47.1922942421653\n",
      "Epoch 34 \t Batch 680 \t Training Loss: 47.17876545962165\n",
      "Epoch 34 \t Batch 700 \t Training Loss: 47.196041477748324\n",
      "Epoch 34 \t Batch 720 \t Training Loss: 47.21809474627177\n",
      "Epoch 34 \t Batch 740 \t Training Loss: 47.21891691362536\n",
      "Epoch 34 \t Batch 760 \t Training Loss: 47.20743916160182\n",
      "Epoch 34 \t Batch 780 \t Training Loss: 47.152767161833935\n",
      "Epoch 34 \t Batch 800 \t Training Loss: 47.12075372695923\n",
      "Epoch 34 \t Batch 820 \t Training Loss: 47.11497050029475\n",
      "Epoch 34 \t Batch 840 \t Training Loss: 47.14701949982416\n",
      "Epoch 34 \t Batch 860 \t Training Loss: 47.15275142137394\n",
      "Epoch 34 \t Batch 880 \t Training Loss: 47.16938930858265\n",
      "Epoch 34 \t Batch 900 \t Training Loss: 47.18357042100695\n",
      "Epoch 34 \t Batch 20 \t Validation Loss: 18.805858182907105\n",
      "Epoch 34 \t Batch 40 \t Validation Loss: 21.03793046474457\n",
      "Epoch 34 \t Batch 60 \t Validation Loss: 21.024478594462078\n",
      "Epoch 34 \t Batch 80 \t Validation Loss: 21.591436088085175\n",
      "Epoch 34 \t Batch 100 \t Validation Loss: 22.79628664970398\n",
      "Epoch 34 \t Batch 120 \t Validation Loss: 23.8811953385671\n",
      "Epoch 34 \t Batch 140 \t Validation Loss: 24.33548413004194\n",
      "Epoch 34 \t Batch 160 \t Validation Loss: 26.003947669267653\n",
      "Epoch 34 \t Batch 180 \t Validation Loss: 29.081447511249117\n",
      "Epoch 34 \t Batch 200 \t Validation Loss: 30.298825240135194\n",
      "Epoch 34 \t Batch 220 \t Validation Loss: 31.36401677131653\n",
      "Epoch 34 \t Batch 240 \t Validation Loss: 31.73589752117793\n",
      "Epoch 34 \t Batch 260 \t Validation Loss: 33.64651773892916\n",
      "Epoch 34 \t Batch 280 \t Validation Loss: 34.689081362315584\n",
      "Epoch 34 \t Batch 300 \t Validation Loss: 35.59804056803385\n",
      "Epoch 34 \t Batch 320 \t Validation Loss: 36.07494803965092\n",
      "Epoch 34 \t Batch 340 \t Validation Loss: 36.10064365443061\n",
      "Epoch 34 \t Batch 360 \t Validation Loss: 35.94556966357761\n",
      "Epoch 34 \t Batch 380 \t Validation Loss: 36.207251001659195\n",
      "Epoch 34 \t Batch 400 \t Validation Loss: 35.88294605731964\n",
      "Epoch 34 \t Batch 420 \t Validation Loss: 36.003872701099944\n",
      "Epoch 34 \t Batch 440 \t Validation Loss: 35.791560972820626\n",
      "Epoch 34 \t Batch 460 \t Validation Loss: 36.089389006987865\n",
      "Epoch 34 \t Batch 480 \t Validation Loss: 36.64180897275607\n",
      "Epoch 34 \t Batch 500 \t Validation Loss: 36.40657820701599\n",
      "Epoch 34 \t Batch 520 \t Validation Loss: 36.19620384619786\n",
      "Epoch 34 \t Batch 540 \t Validation Loss: 36.031590686021026\n",
      "Epoch 34 \t Batch 560 \t Validation Loss: 35.90334272554943\n",
      "Epoch 34 \t Batch 580 \t Validation Loss: 35.649453404854086\n",
      "Epoch 34 \t Batch 600 \t Validation Loss: 35.96085482438405\n",
      "Epoch 34 Training Loss: 47.15323534916635 Validation Loss: 36.633244450990254\n",
      "Validation Loss Decreased(22705.329370498657--->22566.078581809998) Saving The Model\n",
      "Epoch 34 completed\n",
      "Epoch 35 \t Batch 20 \t Training Loss: 46.571598815917966\n",
      "Epoch 35 \t Batch 40 \t Training Loss: 47.2309811592102\n",
      "Epoch 35 \t Batch 60 \t Training Loss: 47.03533802032471\n",
      "Epoch 35 \t Batch 80 \t Training Loss: 47.389459466934206\n",
      "Epoch 35 \t Batch 100 \t Training Loss: 47.39729515075683\n",
      "Epoch 35 \t Batch 120 \t Training Loss: 47.049501991271974\n",
      "Epoch 35 \t Batch 140 \t Training Loss: 46.8555474962507\n",
      "Epoch 35 \t Batch 160 \t Training Loss: 46.95981268882751\n",
      "Epoch 35 \t Batch 180 \t Training Loss: 47.08042401207818\n",
      "Epoch 35 \t Batch 200 \t Training Loss: 47.07098934173584\n",
      "Epoch 35 \t Batch 220 \t Training Loss: 47.12258146459406\n",
      "Epoch 35 \t Batch 240 \t Training Loss: 47.113626607259114\n",
      "Epoch 35 \t Batch 260 \t Training Loss: 47.1134458395151\n",
      "Epoch 35 \t Batch 280 \t Training Loss: 47.12836164746966\n",
      "Epoch 35 \t Batch 300 \t Training Loss: 47.17012460072835\n",
      "Epoch 35 \t Batch 320 \t Training Loss: 47.135188376903535\n",
      "Epoch 35 \t Batch 340 \t Training Loss: 47.16189753588508\n",
      "Epoch 35 \t Batch 360 \t Training Loss: 47.13324237399631\n",
      "Epoch 35 \t Batch 380 \t Training Loss: 47.09639127630936\n",
      "Epoch 35 \t Batch 400 \t Training Loss: 47.02381521224976\n",
      "Epoch 35 \t Batch 420 \t Training Loss: 47.04329116457985\n",
      "Epoch 35 \t Batch 440 \t Training Loss: 47.165697175806216\n",
      "Epoch 35 \t Batch 460 \t Training Loss: 47.21781665138576\n",
      "Epoch 35 \t Batch 480 \t Training Loss: 47.2072732925415\n",
      "Epoch 35 \t Batch 500 \t Training Loss: 47.207830909729005\n",
      "Epoch 35 \t Batch 520 \t Training Loss: 47.23725129641019\n",
      "Epoch 35 \t Batch 540 \t Training Loss: 47.2679753197564\n",
      "Epoch 35 \t Batch 560 \t Training Loss: 47.252496889659334\n",
      "Epoch 35 \t Batch 580 \t Training Loss: 47.19794146439125\n",
      "Epoch 35 \t Batch 600 \t Training Loss: 47.23871894200643\n",
      "Epoch 35 \t Batch 620 \t Training Loss: 47.228636655499855\n",
      "Epoch 35 \t Batch 640 \t Training Loss: 47.247316187620164\n",
      "Epoch 35 \t Batch 660 \t Training Loss: 47.25497127301765\n",
      "Epoch 35 \t Batch 680 \t Training Loss: 47.26609609828276\n",
      "Epoch 35 \t Batch 700 \t Training Loss: 47.20943089076451\n",
      "Epoch 35 \t Batch 720 \t Training Loss: 47.2049377017551\n",
      "Epoch 35 \t Batch 740 \t Training Loss: 47.18037916647421\n",
      "Epoch 35 \t Batch 760 \t Training Loss: 47.18280729494597\n",
      "Epoch 35 \t Batch 780 \t Training Loss: 47.18622890863663\n",
      "Epoch 35 \t Batch 800 \t Training Loss: 47.19078777313232\n",
      "Epoch 35 \t Batch 820 \t Training Loss: 47.160159855354124\n",
      "Epoch 35 \t Batch 840 \t Training Loss: 47.18245204743885\n",
      "Epoch 35 \t Batch 860 \t Training Loss: 47.13990820064101\n",
      "Epoch 35 \t Batch 880 \t Training Loss: 47.1014424497431\n",
      "Epoch 35 \t Batch 900 \t Training Loss: 47.10889767540826\n",
      "Epoch 35 \t Batch 20 \t Validation Loss: 27.683884191513062\n",
      "Epoch 35 \t Batch 40 \t Validation Loss: 28.268236207962037\n",
      "Epoch 35 \t Batch 60 \t Validation Loss: 28.675656700134276\n",
      "Epoch 35 \t Batch 80 \t Validation Loss: 29.017143666744232\n",
      "Epoch 35 \t Batch 100 \t Validation Loss: 29.14410443305969\n",
      "Epoch 35 \t Batch 120 \t Validation Loss: 29.49901332060496\n",
      "Epoch 35 \t Batch 140 \t Validation Loss: 29.393360934938702\n",
      "Epoch 35 \t Batch 160 \t Validation Loss: 30.87650024294853\n",
      "Epoch 35 \t Batch 180 \t Validation Loss: 34.14453435473972\n",
      "Epoch 35 \t Batch 200 \t Validation Loss: 35.262995862960814\n",
      "Epoch 35 \t Batch 220 \t Validation Loss: 36.38522347536954\n",
      "Epoch 35 \t Batch 240 \t Validation Loss: 36.72592690785726\n",
      "Epoch 35 \t Batch 260 \t Validation Loss: 38.644411182403566\n",
      "Epoch 35 \t Batch 280 \t Validation Loss: 39.59690476485661\n",
      "Epoch 35 \t Batch 300 \t Validation Loss: 40.60447699546814\n",
      "Epoch 35 \t Batch 320 \t Validation Loss: 40.99821291267872\n",
      "Epoch 35 \t Batch 340 \t Validation Loss: 40.82859766623553\n",
      "Epoch 35 \t Batch 360 \t Validation Loss: 40.60803618166182\n",
      "Epoch 35 \t Batch 380 \t Validation Loss: 40.74080546780637\n",
      "Epoch 35 \t Batch 400 \t Validation Loss: 40.22103793859482\n",
      "Epoch 35 \t Batch 420 \t Validation Loss: 40.146542560486566\n",
      "Epoch 35 \t Batch 440 \t Validation Loss: 39.77490838440982\n",
      "Epoch 35 \t Batch 460 \t Validation Loss: 39.95286905247232\n",
      "Epoch 35 \t Batch 480 \t Validation Loss: 40.37113268574079\n",
      "Epoch 35 \t Batch 500 \t Validation Loss: 40.0491846408844\n",
      "Epoch 35 \t Batch 520 \t Validation Loss: 39.777000205333415\n",
      "Epoch 35 \t Batch 540 \t Validation Loss: 39.48155071293866\n",
      "Epoch 35 \t Batch 560 \t Validation Loss: 39.251005423069\n",
      "Epoch 35 \t Batch 580 \t Validation Loss: 38.97746606859668\n",
      "Epoch 35 \t Batch 600 \t Validation Loss: 39.15285545508067\n",
      "Epoch 35 Training Loss: 47.1005659196993 Validation Loss: 39.791679761626504\n",
      "Epoch 35 completed\n",
      "Epoch 36 \t Batch 20 \t Training Loss: 46.45032005310058\n",
      "Epoch 36 \t Batch 40 \t Training Loss: 46.51684379577637\n",
      "Epoch 36 \t Batch 60 \t Training Loss: 46.55026569366455\n",
      "Epoch 36 \t Batch 80 \t Training Loss: 46.571834325790405\n",
      "Epoch 36 \t Batch 100 \t Training Loss: 46.826362190246584\n",
      "Epoch 36 \t Batch 120 \t Training Loss: 46.95699116388957\n",
      "Epoch 36 \t Batch 140 \t Training Loss: 46.91908051627023\n",
      "Epoch 36 \t Batch 160 \t Training Loss: 46.83352482318878\n",
      "Epoch 36 \t Batch 180 \t Training Loss: 46.94492971632216\n",
      "Epoch 36 \t Batch 200 \t Training Loss: 46.986442985534666\n",
      "Epoch 36 \t Batch 220 \t Training Loss: 47.04015391956676\n",
      "Epoch 36 \t Batch 240 \t Training Loss: 47.01787357330322\n",
      "Epoch 36 \t Batch 260 \t Training Loss: 46.96819298083965\n",
      "Epoch 36 \t Batch 280 \t Training Loss: 46.91628243582589\n",
      "Epoch 36 \t Batch 300 \t Training Loss: 46.876563212076825\n",
      "Epoch 36 \t Batch 320 \t Training Loss: 46.84991809129715\n",
      "Epoch 36 \t Batch 340 \t Training Loss: 46.840166271434114\n",
      "Epoch 36 \t Batch 360 \t Training Loss: 46.78793687820435\n",
      "Epoch 36 \t Batch 380 \t Training Loss: 46.79406439128675\n",
      "Epoch 36 \t Batch 400 \t Training Loss: 46.771175937652586\n",
      "Epoch 36 \t Batch 420 \t Training Loss: 46.837282444181895\n",
      "Epoch 36 \t Batch 440 \t Training Loss: 46.834066625074904\n",
      "Epoch 36 \t Batch 460 \t Training Loss: 46.90944758705471\n",
      "Epoch 36 \t Batch 480 \t Training Loss: 46.918594733874\n",
      "Epoch 36 \t Batch 500 \t Training Loss: 46.9011528930664\n",
      "Epoch 36 \t Batch 520 \t Training Loss: 46.88298884171706\n",
      "Epoch 36 \t Batch 540 \t Training Loss: 46.882921586213286\n",
      "Epoch 36 \t Batch 560 \t Training Loss: 46.9236889907292\n",
      "Epoch 36 \t Batch 580 \t Training Loss: 46.93582743940682\n",
      "Epoch 36 \t Batch 600 \t Training Loss: 46.90816565831502\n",
      "Epoch 36 \t Batch 620 \t Training Loss: 46.88306744483209\n",
      "Epoch 36 \t Batch 640 \t Training Loss: 46.895346480607984\n",
      "Epoch 36 \t Batch 660 \t Training Loss: 46.915356595588456\n",
      "Epoch 36 \t Batch 680 \t Training Loss: 46.90243364782894\n",
      "Epoch 36 \t Batch 700 \t Training Loss: 46.877299003601074\n",
      "Epoch 36 \t Batch 720 \t Training Loss: 46.89703548219469\n",
      "Epoch 36 \t Batch 740 \t Training Loss: 46.92204083107613\n",
      "Epoch 36 \t Batch 760 \t Training Loss: 46.933494869031406\n",
      "Epoch 36 \t Batch 780 \t Training Loss: 46.89321961525159\n",
      "Epoch 36 \t Batch 800 \t Training Loss: 46.9110253572464\n",
      "Epoch 36 \t Batch 820 \t Training Loss: 46.947889723428865\n",
      "Epoch 36 \t Batch 840 \t Training Loss: 46.94446111406599\n",
      "Epoch 36 \t Batch 860 \t Training Loss: 46.99587216044581\n",
      "Epoch 36 \t Batch 880 \t Training Loss: 47.014488471638074\n",
      "Epoch 36 \t Batch 900 \t Training Loss: 47.03884642283122\n",
      "Epoch 36 \t Batch 20 \t Validation Loss: 24.366710329055785\n",
      "Epoch 36 \t Batch 40 \t Validation Loss: 26.150662970542907\n",
      "Epoch 36 \t Batch 60 \t Validation Loss: 25.98856890996297\n",
      "Epoch 36 \t Batch 80 \t Validation Loss: 26.484666669368742\n",
      "Epoch 36 \t Batch 100 \t Validation Loss: 26.910193128585817\n",
      "Epoch 36 \t Batch 120 \t Validation Loss: 27.381131235758463\n",
      "Epoch 36 \t Batch 140 \t Validation Loss: 27.547018909454344\n",
      "Epoch 36 \t Batch 160 \t Validation Loss: 29.35300507545471\n",
      "Epoch 36 \t Batch 180 \t Validation Loss: 33.16168154610528\n",
      "Epoch 36 \t Batch 200 \t Validation Loss: 34.628101139068605\n",
      "Epoch 36 \t Batch 220 \t Validation Loss: 35.87879424528642\n",
      "Epoch 36 \t Batch 240 \t Validation Loss: 36.36527219613393\n",
      "Epoch 36 \t Batch 260 \t Validation Loss: 38.44244029705341\n",
      "Epoch 36 \t Batch 280 \t Validation Loss: 39.476925826072694\n",
      "Epoch 36 \t Batch 300 \t Validation Loss: 40.70820081392924\n",
      "Epoch 36 \t Batch 320 \t Validation Loss: 41.192660990357396\n",
      "Epoch 36 \t Batch 340 \t Validation Loss: 41.060233304079844\n",
      "Epoch 36 \t Batch 360 \t Validation Loss: 40.88225574228499\n",
      "Epoch 36 \t Batch 380 \t Validation Loss: 41.03461478132951\n",
      "Epoch 36 \t Batch 400 \t Validation Loss: 40.56935560464859\n",
      "Epoch 36 \t Batch 420 \t Validation Loss: 40.494596274693805\n",
      "Epoch 36 \t Batch 440 \t Validation Loss: 40.135760773311965\n",
      "Epoch 36 \t Batch 460 \t Validation Loss: 40.32029991357223\n",
      "Epoch 36 \t Batch 480 \t Validation Loss: 40.740454159180324\n",
      "Epoch 36 \t Batch 500 \t Validation Loss: 40.41421922492981\n",
      "Epoch 36 \t Batch 520 \t Validation Loss: 40.18084691304427\n",
      "Epoch 36 \t Batch 540 \t Validation Loss: 39.93632977626942\n",
      "Epoch 36 \t Batch 560 \t Validation Loss: 39.72650334664753\n",
      "Epoch 36 \t Batch 580 \t Validation Loss: 39.521386174497934\n",
      "Epoch 36 \t Batch 600 \t Validation Loss: 39.686518702507016\n",
      "Epoch 36 Training Loss: 47.059126010507896 Validation Loss: 40.29698292465953\n",
      "Epoch 36 completed\n",
      "Epoch 37 \t Batch 20 \t Training Loss: 47.8786584854126\n",
      "Epoch 37 \t Batch 40 \t Training Loss: 46.82484617233276\n",
      "Epoch 37 \t Batch 60 \t Training Loss: 47.35740636189779\n",
      "Epoch 37 \t Batch 80 \t Training Loss: 47.13266448974609\n",
      "Epoch 37 \t Batch 100 \t Training Loss: 46.8549426651001\n",
      "Epoch 37 \t Batch 120 \t Training Loss: 47.01373157501221\n",
      "Epoch 37 \t Batch 140 \t Training Loss: 46.797604424612864\n",
      "Epoch 37 \t Batch 160 \t Training Loss: 46.80930242538452\n",
      "Epoch 37 \t Batch 180 \t Training Loss: 46.79851449330648\n",
      "Epoch 37 \t Batch 200 \t Training Loss: 47.06707151412964\n",
      "Epoch 37 \t Batch 220 \t Training Loss: 47.01810039173473\n",
      "Epoch 37 \t Batch 240 \t Training Loss: 47.09280142784119\n",
      "Epoch 37 \t Batch 260 \t Training Loss: 47.051087804941034\n",
      "Epoch 37 \t Batch 280 \t Training Loss: 46.942355673653736\n",
      "Epoch 37 \t Batch 300 \t Training Loss: 46.98376487731934\n",
      "Epoch 37 \t Batch 320 \t Training Loss: 46.8959415435791\n",
      "Epoch 37 \t Batch 340 \t Training Loss: 46.83013054342831\n",
      "Epoch 37 \t Batch 360 \t Training Loss: 46.78429882261488\n",
      "Epoch 37 \t Batch 380 \t Training Loss: 46.71781103234542\n",
      "Epoch 37 \t Batch 400 \t Training Loss: 46.73430292129517\n",
      "Epoch 37 \t Batch 420 \t Training Loss: 46.82903208051409\n",
      "Epoch 37 \t Batch 440 \t Training Loss: 46.93063331083818\n",
      "Epoch 37 \t Batch 460 \t Training Loss: 46.986905496016796\n",
      "Epoch 37 \t Batch 480 \t Training Loss: 47.0215743303299\n",
      "Epoch 37 \t Batch 500 \t Training Loss: 47.04758724975586\n",
      "Epoch 37 \t Batch 520 \t Training Loss: 47.026579541426436\n",
      "Epoch 37 \t Batch 540 \t Training Loss: 46.96145190486202\n",
      "Epoch 37 \t Batch 560 \t Training Loss: 46.95347818647112\n",
      "Epoch 37 \t Batch 580 \t Training Loss: 46.92243550399254\n",
      "Epoch 37 \t Batch 600 \t Training Loss: 46.88722270965576\n",
      "Epoch 37 \t Batch 620 \t Training Loss: 46.888312665877805\n",
      "Epoch 37 \t Batch 640 \t Training Loss: 46.90352384448052\n",
      "Epoch 37 \t Batch 660 \t Training Loss: 46.92190000360662\n",
      "Epoch 37 \t Batch 680 \t Training Loss: 46.88423925287583\n",
      "Epoch 37 \t Batch 700 \t Training Loss: 46.885401306152346\n",
      "Epoch 37 \t Batch 720 \t Training Loss: 46.9242449177636\n",
      "Epoch 37 \t Batch 740 \t Training Loss: 46.90925935796789\n",
      "Epoch 37 \t Batch 760 \t Training Loss: 46.909620997780245\n",
      "Epoch 37 \t Batch 780 \t Training Loss: 46.95316520104041\n",
      "Epoch 37 \t Batch 800 \t Training Loss: 46.98097856998444\n",
      "Epoch 37 \t Batch 820 \t Training Loss: 47.00486707454775\n",
      "Epoch 37 \t Batch 840 \t Training Loss: 47.0195316814241\n",
      "Epoch 37 \t Batch 860 \t Training Loss: 47.02265163244203\n",
      "Epoch 37 \t Batch 880 \t Training Loss: 47.02443700270219\n",
      "Epoch 37 \t Batch 900 \t Training Loss: 47.058250965542264\n",
      "Epoch 37 \t Batch 20 \t Validation Loss: 25.372597408294677\n",
      "Epoch 37 \t Batch 40 \t Validation Loss: 27.689544749259948\n",
      "Epoch 37 \t Batch 60 \t Validation Loss: 27.409863901138305\n",
      "Epoch 37 \t Batch 80 \t Validation Loss: 27.68526358604431\n",
      "Epoch 37 \t Batch 100 \t Validation Loss: 27.8742799949646\n",
      "Epoch 37 \t Batch 120 \t Validation Loss: 28.249009617169698\n",
      "Epoch 37 \t Batch 140 \t Validation Loss: 28.24577478000096\n",
      "Epoch 37 \t Batch 160 \t Validation Loss: 29.71091086268425\n",
      "Epoch 37 \t Batch 180 \t Validation Loss: 32.900831847720674\n",
      "Epoch 37 \t Batch 200 \t Validation Loss: 34.044194307327274\n",
      "Epoch 37 \t Batch 220 \t Validation Loss: 35.06548020622947\n",
      "Epoch 37 \t Batch 240 \t Validation Loss: 35.365712122122446\n",
      "Epoch 37 \t Batch 260 \t Validation Loss: 37.24760925219609\n",
      "Epoch 37 \t Batch 280 \t Validation Loss: 38.22076079504831\n",
      "Epoch 37 \t Batch 300 \t Validation Loss: 39.19652735392253\n",
      "Epoch 37 \t Batch 320 \t Validation Loss: 39.599648147821426\n",
      "Epoch 37 \t Batch 340 \t Validation Loss: 39.467653622346766\n",
      "Epoch 37 \t Batch 360 \t Validation Loss: 39.25819945865207\n",
      "Epoch 37 \t Batch 380 \t Validation Loss: 39.40839028107492\n",
      "Epoch 37 \t Batch 400 \t Validation Loss: 38.95077228069305\n",
      "Epoch 37 \t Batch 420 \t Validation Loss: 38.925804910205656\n",
      "Epoch 37 \t Batch 440 \t Validation Loss: 38.575101375579834\n",
      "Epoch 37 \t Batch 460 \t Validation Loss: 38.76148771203083\n",
      "Epoch 37 \t Batch 480 \t Validation Loss: 39.21526225010554\n",
      "Epoch 37 \t Batch 500 \t Validation Loss: 38.90612059020996\n",
      "Epoch 37 \t Batch 520 \t Validation Loss: 38.63024003689106\n",
      "Epoch 37 \t Batch 540 \t Validation Loss: 38.39454689379092\n",
      "Epoch 37 \t Batch 560 \t Validation Loss: 38.19970002174377\n",
      "Epoch 37 \t Batch 580 \t Validation Loss: 37.93316602378056\n",
      "Epoch 37 \t Batch 600 \t Validation Loss: 38.15967040379842\n",
      "Epoch 37 Training Loss: 47.02530678901964 Validation Loss: 38.79471961863629\n",
      "Epoch 37 completed\n",
      "Epoch 38 \t Batch 20 \t Training Loss: 47.23789901733399\n",
      "Epoch 38 \t Batch 40 \t Training Loss: 47.49524946212769\n",
      "Epoch 38 \t Batch 60 \t Training Loss: 47.10098648071289\n",
      "Epoch 38 \t Batch 80 \t Training Loss: 46.87623310089111\n",
      "Epoch 38 \t Batch 100 \t Training Loss: 46.690806503295896\n",
      "Epoch 38 \t Batch 120 \t Training Loss: 46.48903700510661\n",
      "Epoch 38 \t Batch 140 \t Training Loss: 46.5257774080549\n",
      "Epoch 38 \t Batch 160 \t Training Loss: 46.74410088062287\n",
      "Epoch 38 \t Batch 180 \t Training Loss: 46.817415640089244\n",
      "Epoch 38 \t Batch 200 \t Training Loss: 46.84988784790039\n",
      "Epoch 38 \t Batch 220 \t Training Loss: 46.87441602186723\n",
      "Epoch 38 \t Batch 240 \t Training Loss: 46.90060963630676\n",
      "Epoch 38 \t Batch 260 \t Training Loss: 46.97460121741662\n",
      "Epoch 38 \t Batch 280 \t Training Loss: 47.00076407023838\n",
      "Epoch 38 \t Batch 300 \t Training Loss: 47.010585708618166\n",
      "Epoch 38 \t Batch 320 \t Training Loss: 47.103527796268466\n",
      "Epoch 38 \t Batch 340 \t Training Loss: 47.09554387260886\n",
      "Epoch 38 \t Batch 360 \t Training Loss: 47.06207243601481\n",
      "Epoch 38 \t Batch 380 \t Training Loss: 47.08723378432425\n",
      "Epoch 38 \t Batch 400 \t Training Loss: 47.06097559928894\n",
      "Epoch 38 \t Batch 420 \t Training Loss: 47.011273574829104\n",
      "Epoch 38 \t Batch 440 \t Training Loss: 47.02773944681341\n",
      "Epoch 38 \t Batch 460 \t Training Loss: 46.9848715574845\n",
      "Epoch 38 \t Batch 480 \t Training Loss: 46.99571584860484\n",
      "Epoch 38 \t Batch 500 \t Training Loss: 46.92213436126709\n",
      "Epoch 38 \t Batch 520 \t Training Loss: 46.950264021066516\n",
      "Epoch 38 \t Batch 540 \t Training Loss: 46.90787850132695\n",
      "Epoch 38 \t Batch 560 \t Training Loss: 46.920959976741244\n",
      "Epoch 38 \t Batch 580 \t Training Loss: 46.93087001668996\n",
      "Epoch 38 \t Batch 600 \t Training Loss: 46.91825750986735\n",
      "Epoch 38 \t Batch 620 \t Training Loss: 46.90285469793504\n",
      "Epoch 38 \t Batch 640 \t Training Loss: 46.91557818651199\n",
      "Epoch 38 \t Batch 660 \t Training Loss: 46.94190373276219\n",
      "Epoch 38 \t Batch 680 \t Training Loss: 46.96079460031846\n",
      "Epoch 38 \t Batch 700 \t Training Loss: 46.95064796447754\n",
      "Epoch 38 \t Batch 720 \t Training Loss: 46.96888331307305\n",
      "Epoch 38 \t Batch 740 \t Training Loss: 46.95481163231102\n",
      "Epoch 38 \t Batch 760 \t Training Loss: 46.981344112597014\n",
      "Epoch 38 \t Batch 780 \t Training Loss: 46.98377417050875\n",
      "Epoch 38 \t Batch 800 \t Training Loss: 46.976708960533145\n",
      "Epoch 38 \t Batch 820 \t Training Loss: 47.02071493195324\n",
      "Epoch 38 \t Batch 840 \t Training Loss: 47.03880339577084\n",
      "Epoch 38 \t Batch 860 \t Training Loss: 47.028165622090185\n",
      "Epoch 38 \t Batch 880 \t Training Loss: 47.04010806083679\n",
      "Epoch 38 \t Batch 900 \t Training Loss: 47.04636761135525\n",
      "Epoch 38 \t Batch 20 \t Validation Loss: 22.63211283683777\n",
      "Epoch 38 \t Batch 40 \t Validation Loss: 25.733518958091736\n",
      "Epoch 38 \t Batch 60 \t Validation Loss: 24.968731451034547\n",
      "Epoch 38 \t Batch 80 \t Validation Loss: 25.2903701543808\n",
      "Epoch 38 \t Batch 100 \t Validation Loss: 26.235801639556886\n",
      "Epoch 38 \t Batch 120 \t Validation Loss: 27.02883348464966\n",
      "Epoch 38 \t Batch 140 \t Validation Loss: 27.289733151027136\n",
      "Epoch 38 \t Batch 160 \t Validation Loss: 29.081661641597748\n",
      "Epoch 38 \t Batch 180 \t Validation Loss: 32.69161523713006\n",
      "Epoch 38 \t Batch 200 \t Validation Loss: 34.06469032764435\n",
      "Epoch 38 \t Batch 220 \t Validation Loss: 35.37191322066567\n",
      "Epoch 38 \t Batch 240 \t Validation Loss: 35.83411133289337\n",
      "Epoch 38 \t Batch 260 \t Validation Loss: 37.91758516751803\n",
      "Epoch 38 \t Batch 280 \t Validation Loss: 39.01995891162327\n",
      "Epoch 38 \t Batch 300 \t Validation Loss: 40.10613286336263\n",
      "Epoch 38 \t Batch 320 \t Validation Loss: 40.58653470277786\n",
      "Epoch 38 \t Batch 340 \t Validation Loss: 40.45366061715519\n",
      "Epoch 38 \t Batch 360 \t Validation Loss: 40.267739624447294\n",
      "Epoch 38 \t Batch 380 \t Validation Loss: 40.454141195196854\n",
      "Epoch 38 \t Batch 400 \t Validation Loss: 39.9667714881897\n",
      "Epoch 38 \t Batch 420 \t Validation Loss: 39.907248047419955\n",
      "Epoch 38 \t Batch 440 \t Validation Loss: 39.5430815263228\n",
      "Epoch 38 \t Batch 460 \t Validation Loss: 39.70171982723733\n",
      "Epoch 38 \t Batch 480 \t Validation Loss: 40.121543697516124\n",
      "Epoch 38 \t Batch 500 \t Validation Loss: 39.8024919090271\n",
      "Epoch 38 \t Batch 520 \t Validation Loss: 39.55453292039724\n",
      "Epoch 38 \t Batch 540 \t Validation Loss: 39.248017671373155\n",
      "Epoch 38 \t Batch 560 \t Validation Loss: 39.01054402760097\n",
      "Epoch 38 \t Batch 580 \t Validation Loss: 38.73582301633111\n",
      "Epoch 38 \t Batch 600 \t Validation Loss: 38.899714403152466\n",
      "Epoch 38 Training Loss: 47.008962808024535 Validation Loss: 39.51871880308374\n",
      "Epoch 38 completed\n",
      "Epoch 39 \t Batch 20 \t Training Loss: 46.24899463653564\n",
      "Epoch 39 \t Batch 40 \t Training Loss: 46.63082618713379\n",
      "Epoch 39 \t Batch 60 \t Training Loss: 46.89318726857503\n",
      "Epoch 39 \t Batch 80 \t Training Loss: 47.18022994995117\n",
      "Epoch 39 \t Batch 100 \t Training Loss: 46.79823532104492\n",
      "Epoch 39 \t Batch 120 \t Training Loss: 46.711809698740645\n",
      "Epoch 39 \t Batch 140 \t Training Loss: 46.9078513281686\n",
      "Epoch 39 \t Batch 160 \t Training Loss: 46.78830437660217\n",
      "Epoch 39 \t Batch 180 \t Training Loss: 46.65391156938341\n",
      "Epoch 39 \t Batch 200 \t Training Loss: 46.69664041519165\n",
      "Epoch 39 \t Batch 220 \t Training Loss: 46.60246186689897\n",
      "Epoch 39 \t Batch 240 \t Training Loss: 46.64176007906596\n",
      "Epoch 39 \t Batch 260 \t Training Loss: 46.573649054307204\n",
      "Epoch 39 \t Batch 280 \t Training Loss: 46.5729100227356\n",
      "Epoch 39 \t Batch 300 \t Training Loss: 46.548051020304364\n",
      "Epoch 39 \t Batch 320 \t Training Loss: 46.66500461101532\n",
      "Epoch 39 \t Batch 340 \t Training Loss: 46.6600783179788\n",
      "Epoch 39 \t Batch 360 \t Training Loss: 46.698698700798886\n",
      "Epoch 39 \t Batch 380 \t Training Loss: 46.711672220732034\n",
      "Epoch 39 \t Batch 400 \t Training Loss: 46.81285984039307\n",
      "Epoch 39 \t Batch 420 \t Training Loss: 46.78038518088204\n",
      "Epoch 39 \t Batch 440 \t Training Loss: 46.77085345008157\n",
      "Epoch 39 \t Batch 460 \t Training Loss: 46.79786872034487\n",
      "Epoch 39 \t Batch 480 \t Training Loss: 46.81169752279917\n",
      "Epoch 39 \t Batch 500 \t Training Loss: 46.864017387390135\n",
      "Epoch 39 \t Batch 520 \t Training Loss: 46.900718688964844\n",
      "Epoch 39 \t Batch 540 \t Training Loss: 46.792744813142\n",
      "Epoch 39 \t Batch 560 \t Training Loss: 46.776018142700195\n",
      "Epoch 39 \t Batch 580 \t Training Loss: 46.75279637040763\n",
      "Epoch 39 \t Batch 600 \t Training Loss: 46.7231765238444\n",
      "Epoch 39 \t Batch 620 \t Training Loss: 46.74153065835276\n",
      "Epoch 39 \t Batch 640 \t Training Loss: 46.714896500110626\n",
      "Epoch 39 \t Batch 660 \t Training Loss: 46.76214723298044\n",
      "Epoch 39 \t Batch 680 \t Training Loss: 46.79450352612664\n",
      "Epoch 39 \t Batch 700 \t Training Loss: 46.77113535744803\n",
      "Epoch 39 \t Batch 720 \t Training Loss: 46.77864111264547\n",
      "Epoch 39 \t Batch 740 \t Training Loss: 46.78111854243923\n",
      "Epoch 39 \t Batch 760 \t Training Loss: 46.79973918011314\n",
      "Epoch 39 \t Batch 780 \t Training Loss: 46.822579251802885\n",
      "Epoch 39 \t Batch 800 \t Training Loss: 46.8371433877945\n",
      "Epoch 39 \t Batch 820 \t Training Loss: 46.87210262577708\n",
      "Epoch 39 \t Batch 840 \t Training Loss: 46.90306752522786\n",
      "Epoch 39 \t Batch 860 \t Training Loss: 46.94791459371877\n",
      "Epoch 39 \t Batch 880 \t Training Loss: 46.93032082644376\n",
      "Epoch 39 \t Batch 900 \t Training Loss: 46.95247030046251\n",
      "Epoch 39 \t Batch 20 \t Validation Loss: 26.030314254760743\n",
      "Epoch 39 \t Batch 40 \t Validation Loss: 27.02048318386078\n",
      "Epoch 39 \t Batch 60 \t Validation Loss: 26.964984941482545\n",
      "Epoch 39 \t Batch 80 \t Validation Loss: 27.264049661159515\n",
      "Epoch 39 \t Batch 100 \t Validation Loss: 28.040044260025024\n",
      "Epoch 39 \t Batch 120 \t Validation Loss: 28.978162328402203\n",
      "Epoch 39 \t Batch 140 \t Validation Loss: 29.222599049976893\n",
      "Epoch 39 \t Batch 160 \t Validation Loss: 30.765391474962236\n",
      "Epoch 39 \t Batch 180 \t Validation Loss: 33.9709543440077\n",
      "Epoch 39 \t Batch 200 \t Validation Loss: 35.196570320129396\n",
      "Epoch 39 \t Batch 220 \t Validation Loss: 36.25238236514005\n",
      "Epoch 39 \t Batch 240 \t Validation Loss: 36.55491162141164\n",
      "Epoch 39 \t Batch 260 \t Validation Loss: 38.48952933458182\n",
      "Epoch 39 \t Batch 280 \t Validation Loss: 39.48409747055599\n",
      "Epoch 39 \t Batch 300 \t Validation Loss: 40.434739713668826\n",
      "Epoch 39 \t Batch 320 \t Validation Loss: 40.84538556933403\n",
      "Epoch 39 \t Batch 340 \t Validation Loss: 40.69446997923009\n",
      "Epoch 39 \t Batch 360 \t Validation Loss: 40.493715355131364\n",
      "Epoch 39 \t Batch 380 \t Validation Loss: 40.66398276780781\n",
      "Epoch 39 \t Batch 400 \t Validation Loss: 40.192962398529055\n",
      "Epoch 39 \t Batch 420 \t Validation Loss: 40.11328013965062\n",
      "Epoch 39 \t Batch 440 \t Validation Loss: 39.768332249468024\n",
      "Epoch 39 \t Batch 460 \t Validation Loss: 40.016253444422844\n",
      "Epoch 39 \t Batch 480 \t Validation Loss: 40.41741345922152\n",
      "Epoch 39 \t Batch 500 \t Validation Loss: 40.1011932926178\n",
      "Epoch 39 \t Batch 520 \t Validation Loss: 39.92420904819782\n",
      "Epoch 39 \t Batch 540 \t Validation Loss: 39.65940608625059\n",
      "Epoch 39 \t Batch 560 \t Validation Loss: 39.433130151884896\n",
      "Epoch 39 \t Batch 580 \t Validation Loss: 39.16955826857994\n",
      "Epoch 39 \t Batch 600 \t Validation Loss: 39.34928064346313\n",
      "Epoch 39 Training Loss: 46.96701714152728 Validation Loss: 39.969919118014246\n",
      "Epoch 39 completed\n",
      "Epoch 40 \t Batch 20 \t Training Loss: 46.678564834594724\n",
      "Epoch 40 \t Batch 40 \t Training Loss: 46.90852270126343\n",
      "Epoch 40 \t Batch 60 \t Training Loss: 47.22443892161051\n",
      "Epoch 40 \t Batch 80 \t Training Loss: 47.061957120895386\n",
      "Epoch 40 \t Batch 100 \t Training Loss: 46.86796745300293\n",
      "Epoch 40 \t Batch 120 \t Training Loss: 46.65478642781576\n",
      "Epoch 40 \t Batch 140 \t Training Loss: 46.719380950927736\n",
      "Epoch 40 \t Batch 160 \t Training Loss: 46.52085208892822\n",
      "Epoch 40 \t Batch 180 \t Training Loss: 46.74094996982151\n",
      "Epoch 40 \t Batch 200 \t Training Loss: 46.74248529434204\n",
      "Epoch 40 \t Batch 220 \t Training Loss: 46.72466458407315\n",
      "Epoch 40 \t Batch 240 \t Training Loss: 46.70089880625407\n",
      "Epoch 40 \t Batch 260 \t Training Loss: 46.8109432220459\n",
      "Epoch 40 \t Batch 280 \t Training Loss: 46.78442715236119\n",
      "Epoch 40 \t Batch 300 \t Training Loss: 46.775792401631676\n",
      "Epoch 40 \t Batch 320 \t Training Loss: 46.78713548183441\n",
      "Epoch 40 \t Batch 340 \t Training Loss: 46.81930214377011\n",
      "Epoch 40 \t Batch 360 \t Training Loss: 46.85667787128025\n",
      "Epoch 40 \t Batch 380 \t Training Loss: 46.8946018720928\n",
      "Epoch 40 \t Batch 400 \t Training Loss: 46.878268575668336\n",
      "Epoch 40 \t Batch 420 \t Training Loss: 46.93176449366978\n",
      "Epoch 40 \t Batch 440 \t Training Loss: 46.9372974395752\n",
      "Epoch 40 \t Batch 460 \t Training Loss: 46.938822348221485\n",
      "Epoch 40 \t Batch 480 \t Training Loss: 46.85687844753265\n",
      "Epoch 40 \t Batch 500 \t Training Loss: 46.890872955322266\n",
      "Epoch 40 \t Batch 520 \t Training Loss: 46.91523346534142\n",
      "Epoch 40 \t Batch 540 \t Training Loss: 46.88370552769414\n",
      "Epoch 40 \t Batch 560 \t Training Loss: 46.90762905393328\n",
      "Epoch 40 \t Batch 580 \t Training Loss: 46.88884099105309\n",
      "Epoch 40 \t Batch 600 \t Training Loss: 46.87257807413737\n",
      "Epoch 40 \t Batch 620 \t Training Loss: 46.90661685697494\n",
      "Epoch 40 \t Batch 640 \t Training Loss: 46.903461277484894\n",
      "Epoch 40 \t Batch 660 \t Training Loss: 46.90291906992594\n",
      "Epoch 40 \t Batch 680 \t Training Loss: 46.86836832831888\n",
      "Epoch 40 \t Batch 700 \t Training Loss: 46.85073034014021\n",
      "Epoch 40 \t Batch 720 \t Training Loss: 46.85985619227092\n",
      "Epoch 40 \t Batch 740 \t Training Loss: 46.860567907384926\n",
      "Epoch 40 \t Batch 760 \t Training Loss: 46.900004778410256\n",
      "Epoch 40 \t Batch 780 \t Training Loss: 46.92697260929988\n",
      "Epoch 40 \t Batch 800 \t Training Loss: 46.906772084236145\n",
      "Epoch 40 \t Batch 820 \t Training Loss: 46.94557604906036\n",
      "Epoch 40 \t Batch 840 \t Training Loss: 46.948877148401166\n",
      "Epoch 40 \t Batch 860 \t Training Loss: 46.976421351765474\n",
      "Epoch 40 \t Batch 880 \t Training Loss: 46.96578923138705\n",
      "Epoch 40 \t Batch 900 \t Training Loss: 46.974699096679686\n",
      "Epoch 40 \t Batch 20 \t Validation Loss: 23.881889581680298\n",
      "Epoch 40 \t Batch 40 \t Validation Loss: 24.254200220108032\n",
      "Epoch 40 \t Batch 60 \t Validation Loss: 25.066539319356284\n",
      "Epoch 40 \t Batch 80 \t Validation Loss: 25.450368559360506\n",
      "Epoch 40 \t Batch 100 \t Validation Loss: 26.46483407020569\n",
      "Epoch 40 \t Batch 120 \t Validation Loss: 27.55200774669647\n",
      "Epoch 40 \t Batch 140 \t Validation Loss: 27.913816145488195\n",
      "Epoch 40 \t Batch 160 \t Validation Loss: 29.49984716773033\n",
      "Epoch 40 \t Batch 180 \t Validation Loss: 32.806509272257486\n",
      "Epoch 40 \t Batch 200 \t Validation Loss: 33.98346215248108\n",
      "Epoch 40 \t Batch 220 \t Validation Loss: 35.09096540971236\n",
      "Epoch 40 \t Batch 240 \t Validation Loss: 35.45005200703939\n",
      "Epoch 40 \t Batch 260 \t Validation Loss: 37.41507012660687\n",
      "Epoch 40 \t Batch 280 \t Validation Loss: 38.463469805036276\n",
      "Epoch 40 \t Batch 300 \t Validation Loss: 39.43732786178589\n",
      "Epoch 40 \t Batch 320 \t Validation Loss: 39.846142914891246\n",
      "Epoch 40 \t Batch 340 \t Validation Loss: 39.71137394344105\n",
      "Epoch 40 \t Batch 360 \t Validation Loss: 39.50555982854631\n",
      "Epoch 40 \t Batch 380 \t Validation Loss: 39.6670557900479\n",
      "Epoch 40 \t Batch 400 \t Validation Loss: 39.179357035160066\n",
      "Epoch 40 \t Batch 420 \t Validation Loss: 39.13637576557341\n",
      "Epoch 40 \t Batch 440 \t Validation Loss: 38.7643665638837\n",
      "Epoch 40 \t Batch 460 \t Validation Loss: 38.92515198873437\n",
      "Epoch 40 \t Batch 480 \t Validation Loss: 39.377626023689906\n",
      "Epoch 40 \t Batch 500 \t Validation Loss: 39.07226337242126\n",
      "Epoch 40 \t Batch 520 \t Validation Loss: 38.79303193275745\n",
      "Epoch 40 \t Batch 540 \t Validation Loss: 38.556526270619145\n",
      "Epoch 40 \t Batch 560 \t Validation Loss: 38.37107449769974\n",
      "Epoch 40 \t Batch 580 \t Validation Loss: 38.14076824681512\n",
      "Epoch 40 \t Batch 600 \t Validation Loss: 38.36341555754343\n",
      "Epoch 40 Training Loss: 46.93767741878264 Validation Loss: 39.03798318373693\n",
      "Epoch 40 completed\n",
      "Epoch 41 \t Batch 20 \t Training Loss: 48.270829391479495\n",
      "Epoch 41 \t Batch 40 \t Training Loss: 47.845941638946535\n",
      "Epoch 41 \t Batch 60 \t Training Loss: 47.58818302154541\n",
      "Epoch 41 \t Batch 80 \t Training Loss: 47.28030252456665\n",
      "Epoch 41 \t Batch 100 \t Training Loss: 47.48517135620117\n",
      "Epoch 41 \t Batch 120 \t Training Loss: 47.37020543416341\n",
      "Epoch 41 \t Batch 140 \t Training Loss: 47.22798385620117\n",
      "Epoch 41 \t Batch 160 \t Training Loss: 47.12476139068603\n",
      "Epoch 41 \t Batch 180 \t Training Loss: 47.1914022869534\n",
      "Epoch 41 \t Batch 200 \t Training Loss: 47.22177839279175\n",
      "Epoch 41 \t Batch 220 \t Training Loss: 47.3042762929743\n",
      "Epoch 41 \t Batch 240 \t Training Loss: 47.20706380208333\n",
      "Epoch 41 \t Batch 260 \t Training Loss: 47.201622126652644\n",
      "Epoch 41 \t Batch 280 \t Training Loss: 47.12902687617711\n",
      "Epoch 41 \t Batch 300 \t Training Loss: 47.13356266021729\n",
      "Epoch 41 \t Batch 320 \t Training Loss: 47.200369250774386\n",
      "Epoch 41 \t Batch 340 \t Training Loss: 47.14551529603846\n",
      "Epoch 41 \t Batch 360 \t Training Loss: 47.120554468366834\n",
      "Epoch 41 \t Batch 380 \t Training Loss: 47.1748107608996\n",
      "Epoch 41 \t Batch 400 \t Training Loss: 47.13989517211914\n",
      "Epoch 41 \t Batch 420 \t Training Loss: 47.072429012116935\n",
      "Epoch 41 \t Batch 440 \t Training Loss: 47.000430670651525\n",
      "Epoch 41 \t Batch 460 \t Training Loss: 46.93993055094843\n",
      "Epoch 41 \t Batch 480 \t Training Loss: 46.94136286576589\n",
      "Epoch 41 \t Batch 500 \t Training Loss: 46.89601531219483\n",
      "Epoch 41 \t Batch 520 \t Training Loss: 46.93948265222403\n",
      "Epoch 41 \t Batch 540 \t Training Loss: 46.92915025640417\n",
      "Epoch 41 \t Batch 560 \t Training Loss: 46.93786095210484\n",
      "Epoch 41 \t Batch 580 \t Training Loss: 46.92250527349012\n",
      "Epoch 41 \t Batch 600 \t Training Loss: 46.94705192565918\n",
      "Epoch 41 \t Batch 620 \t Training Loss: 46.9615266184653\n",
      "Epoch 41 \t Batch 640 \t Training Loss: 46.96594372391701\n",
      "Epoch 41 \t Batch 660 \t Training Loss: 46.976039331609556\n",
      "Epoch 41 \t Batch 680 \t Training Loss: 46.982202574786015\n",
      "Epoch 41 \t Batch 700 \t Training Loss: 46.96677637372698\n",
      "Epoch 41 \t Batch 720 \t Training Loss: 46.9252742184533\n",
      "Epoch 41 \t Batch 740 \t Training Loss: 46.920543773754225\n",
      "Epoch 41 \t Batch 760 \t Training Loss: 46.93333237296657\n",
      "Epoch 41 \t Batch 780 \t Training Loss: 46.91928847875351\n",
      "Epoch 41 \t Batch 800 \t Training Loss: 46.91089463710785\n",
      "Epoch 41 \t Batch 820 \t Training Loss: 46.93346064497785\n",
      "Epoch 41 \t Batch 840 \t Training Loss: 46.94467919213431\n",
      "Epoch 41 \t Batch 860 \t Training Loss: 46.934500840652824\n",
      "Epoch 41 \t Batch 880 \t Training Loss: 46.90730059363625\n",
      "Epoch 41 \t Batch 900 \t Training Loss: 46.89348843044705\n",
      "Epoch 41 \t Batch 20 \t Validation Loss: 24.35782747268677\n",
      "Epoch 41 \t Batch 40 \t Validation Loss: 26.05102319717407\n",
      "Epoch 41 \t Batch 60 \t Validation Loss: 25.75488986968994\n",
      "Epoch 41 \t Batch 80 \t Validation Loss: 26.352030479907988\n",
      "Epoch 41 \t Batch 100 \t Validation Loss: 27.090466451644897\n",
      "Epoch 41 \t Batch 120 \t Validation Loss: 27.724482973416645\n",
      "Epoch 41 \t Batch 140 \t Validation Loss: 27.832883855274744\n",
      "Epoch 41 \t Batch 160 \t Validation Loss: 29.557051008939744\n",
      "Epoch 41 \t Batch 180 \t Validation Loss: 33.06017631954617\n",
      "Epoch 41 \t Batch 200 \t Validation Loss: 34.36576539039612\n",
      "Epoch 41 \t Batch 220 \t Validation Loss: 35.619106622175735\n",
      "Epoch 41 \t Batch 240 \t Validation Loss: 36.033600405852\n",
      "Epoch 41 \t Batch 260 \t Validation Loss: 38.073085436454186\n",
      "Epoch 41 \t Batch 280 \t Validation Loss: 39.14964404446738\n",
      "Epoch 41 \t Batch 300 \t Validation Loss: 40.215002355575564\n",
      "Epoch 41 \t Batch 320 \t Validation Loss: 40.66352044045925\n",
      "Epoch 41 \t Batch 340 \t Validation Loss: 40.52630461244022\n",
      "Epoch 41 \t Batch 360 \t Validation Loss: 40.340634571181404\n",
      "Epoch 41 \t Batch 380 \t Validation Loss: 40.50404968512686\n",
      "Epoch 41 \t Batch 400 \t Validation Loss: 39.994535882472995\n",
      "Epoch 41 \t Batch 420 \t Validation Loss: 39.96084805216108\n",
      "Epoch 41 \t Batch 440 \t Validation Loss: 39.58722395246679\n",
      "Epoch 41 \t Batch 460 \t Validation Loss: 39.73941424203956\n",
      "Epoch 41 \t Batch 480 \t Validation Loss: 40.16336368918419\n",
      "Epoch 41 \t Batch 500 \t Validation Loss: 39.85179898262024\n",
      "Epoch 41 \t Batch 520 \t Validation Loss: 39.54101085112645\n",
      "Epoch 41 \t Batch 540 \t Validation Loss: 39.268023616296276\n",
      "Epoch 41 \t Batch 560 \t Validation Loss: 39.04671060528074\n",
      "Epoch 41 \t Batch 580 \t Validation Loss: 38.73545935565028\n",
      "Epoch 41 \t Batch 600 \t Validation Loss: 38.94062819004059\n",
      "Epoch 41 Training Loss: 46.910327848977325 Validation Loss: 39.56807157745609\n",
      "Epoch 41 completed\n",
      "Epoch 42 \t Batch 20 \t Training Loss: 45.08764915466308\n",
      "Epoch 42 \t Batch 40 \t Training Loss: 45.88587217330932\n",
      "Epoch 42 \t Batch 60 \t Training Loss: 46.15545069376628\n",
      "Epoch 42 \t Batch 80 \t Training Loss: 46.565794706344604\n",
      "Epoch 42 \t Batch 100 \t Training Loss: 46.60036979675293\n",
      "Epoch 42 \t Batch 120 \t Training Loss: 47.06710488001506\n",
      "Epoch 42 \t Batch 140 \t Training Loss: 46.9876059123448\n",
      "Epoch 42 \t Batch 160 \t Training Loss: 46.930363464355466\n",
      "Epoch 42 \t Batch 180 \t Training Loss: 47.047045368618434\n",
      "Epoch 42 \t Batch 200 \t Training Loss: 47.019784450531006\n",
      "Epoch 42 \t Batch 220 \t Training Loss: 47.11282707561146\n",
      "Epoch 42 \t Batch 240 \t Training Loss: 47.066729354858396\n",
      "Epoch 42 \t Batch 260 \t Training Loss: 46.99036772801326\n",
      "Epoch 42 \t Batch 280 \t Training Loss: 46.98894552503313\n",
      "Epoch 42 \t Batch 300 \t Training Loss: 46.8909423828125\n",
      "Epoch 42 \t Batch 320 \t Training Loss: 46.870474946498874\n",
      "Epoch 42 \t Batch 340 \t Training Loss: 46.82007587657255\n",
      "Epoch 42 \t Batch 360 \t Training Loss: 46.88116710450914\n",
      "Epoch 42 \t Batch 380 \t Training Loss: 46.889545450712504\n",
      "Epoch 42 \t Batch 400 \t Training Loss: 46.90687218666076\n",
      "Epoch 42 \t Batch 420 \t Training Loss: 46.86246098109654\n",
      "Epoch 42 \t Batch 440 \t Training Loss: 46.78314557508989\n",
      "Epoch 42 \t Batch 460 \t Training Loss: 46.80177478790283\n",
      "Epoch 42 \t Batch 480 \t Training Loss: 46.798472213745114\n",
      "Epoch 42 \t Batch 500 \t Training Loss: 46.78391381072998\n",
      "Epoch 42 \t Batch 520 \t Training Loss: 46.742044676267184\n",
      "Epoch 42 \t Batch 540 \t Training Loss: 46.74636775829174\n",
      "Epoch 42 \t Batch 560 \t Training Loss: 46.708276871272496\n",
      "Epoch 42 \t Batch 580 \t Training Loss: 46.7222179741695\n",
      "Epoch 42 \t Batch 600 \t Training Loss: 46.72083234786987\n",
      "Epoch 42 \t Batch 620 \t Training Loss: 46.76715241709063\n",
      "Epoch 42 \t Batch 640 \t Training Loss: 46.78178217411041\n",
      "Epoch 42 \t Batch 660 \t Training Loss: 46.738293237397166\n",
      "Epoch 42 \t Batch 680 \t Training Loss: 46.77032539143282\n",
      "Epoch 42 \t Batch 700 \t Training Loss: 46.785671457563126\n",
      "Epoch 42 \t Batch 720 \t Training Loss: 46.81202246877882\n",
      "Epoch 42 \t Batch 740 \t Training Loss: 46.855591877086745\n",
      "Epoch 42 \t Batch 760 \t Training Loss: 46.827010455884434\n",
      "Epoch 42 \t Batch 780 \t Training Loss: 46.82402383853228\n",
      "Epoch 42 \t Batch 800 \t Training Loss: 46.79497460365295\n",
      "Epoch 42 \t Batch 820 \t Training Loss: 46.82761030894954\n",
      "Epoch 42 \t Batch 840 \t Training Loss: 46.841483774639315\n",
      "Epoch 42 \t Batch 860 \t Training Loss: 46.854392907785815\n",
      "Epoch 42 \t Batch 880 \t Training Loss: 46.870949320359664\n",
      "Epoch 42 \t Batch 900 \t Training Loss: 46.859524455600315\n",
      "Epoch 42 \t Batch 20 \t Validation Loss: 22.65225167274475\n",
      "Epoch 42 \t Batch 40 \t Validation Loss: 23.601642656326295\n",
      "Epoch 42 \t Batch 60 \t Validation Loss: 23.713728125890096\n",
      "Epoch 42 \t Batch 80 \t Validation Loss: 24.067119550704955\n",
      "Epoch 42 \t Batch 100 \t Validation Loss: 24.90009504318237\n",
      "Epoch 42 \t Batch 120 \t Validation Loss: 25.775179227193195\n",
      "Epoch 42 \t Batch 140 \t Validation Loss: 26.08648338317871\n",
      "Epoch 42 \t Batch 160 \t Validation Loss: 27.94712747335434\n",
      "Epoch 42 \t Batch 180 \t Validation Loss: 31.517648855845135\n",
      "Epoch 42 \t Batch 200 \t Validation Loss: 32.88438342094421\n",
      "Epoch 42 \t Batch 220 \t Validation Loss: 34.104279058629814\n",
      "Epoch 42 \t Batch 240 \t Validation Loss: 34.55216287374496\n",
      "Epoch 42 \t Batch 260 \t Validation Loss: 36.59577247913067\n",
      "Epoch 42 \t Batch 280 \t Validation Loss: 37.68741858005524\n",
      "Epoch 42 \t Batch 300 \t Validation Loss: 38.82855767250061\n",
      "Epoch 42 \t Batch 320 \t Validation Loss: 39.355221810936925\n",
      "Epoch 42 \t Batch 340 \t Validation Loss: 39.281938000286324\n",
      "Epoch 42 \t Batch 360 \t Validation Loss: 39.13626799583435\n",
      "Epoch 42 \t Batch 380 \t Validation Loss: 39.32904090379414\n",
      "Epoch 42 \t Batch 400 \t Validation Loss: 38.877664165496824\n",
      "Epoch 42 \t Batch 420 \t Validation Loss: 38.8896692616599\n",
      "Epoch 42 \t Batch 440 \t Validation Loss: 38.559650614044884\n",
      "Epoch 42 \t Batch 460 \t Validation Loss: 38.78354247341985\n",
      "Epoch 42 \t Batch 480 \t Validation Loss: 39.27136221528053\n",
      "Epoch 42 \t Batch 500 \t Validation Loss: 39.00823691749573\n",
      "Epoch 42 \t Batch 520 \t Validation Loss: 38.76765497831198\n",
      "Epoch 42 \t Batch 540 \t Validation Loss: 38.507567153153595\n",
      "Epoch 42 \t Batch 560 \t Validation Loss: 38.2959062899862\n",
      "Epoch 42 \t Batch 580 \t Validation Loss: 38.0066651360742\n",
      "Epoch 42 \t Batch 600 \t Validation Loss: 38.22561720689138\n",
      "Epoch 42 Training Loss: 46.85894641439423 Validation Loss: 38.85542275224413\n",
      "Epoch 42 completed\n",
      "Epoch 43 \t Batch 20 \t Training Loss: 47.81547470092774\n",
      "Epoch 43 \t Batch 40 \t Training Loss: 47.46580114364624\n",
      "Epoch 43 \t Batch 60 \t Training Loss: 47.10278739929199\n",
      "Epoch 43 \t Batch 80 \t Training Loss: 47.324760389328006\n",
      "Epoch 43 \t Batch 100 \t Training Loss: 47.20976543426514\n",
      "Epoch 43 \t Batch 120 \t Training Loss: 47.01324927012126\n",
      "Epoch 43 \t Batch 140 \t Training Loss: 46.8056069782802\n",
      "Epoch 43 \t Batch 160 \t Training Loss: 46.79385068416595\n",
      "Epoch 43 \t Batch 180 \t Training Loss: 46.72275136311849\n",
      "Epoch 43 \t Batch 200 \t Training Loss: 46.710694522857665\n",
      "Epoch 43 \t Batch 220 \t Training Loss: 46.787849963795054\n",
      "Epoch 43 \t Batch 240 \t Training Loss: 46.7721931775411\n",
      "Epoch 43 \t Batch 260 \t Training Loss: 46.85460037818322\n",
      "Epoch 43 \t Batch 280 \t Training Loss: 46.79407250540597\n",
      "Epoch 43 \t Batch 300 \t Training Loss: 46.797200736999514\n",
      "Epoch 43 \t Batch 320 \t Training Loss: 46.786868917942044\n",
      "Epoch 43 \t Batch 340 \t Training Loss: 46.66054780623492\n",
      "Epoch 43 \t Batch 360 \t Training Loss: 46.65805912017822\n",
      "Epoch 43 \t Batch 380 \t Training Loss: 46.70360934608861\n",
      "Epoch 43 \t Batch 400 \t Training Loss: 46.69700103759766\n",
      "Epoch 43 \t Batch 420 \t Training Loss: 46.63423211233957\n",
      "Epoch 43 \t Batch 440 \t Training Loss: 46.60133007222956\n",
      "Epoch 43 \t Batch 460 \t Training Loss: 46.62609874891198\n",
      "Epoch 43 \t Batch 480 \t Training Loss: 46.63094782829285\n",
      "Epoch 43 \t Batch 500 \t Training Loss: 46.62246771240235\n",
      "Epoch 43 \t Batch 520 \t Training Loss: 46.62592740425697\n",
      "Epoch 43 \t Batch 540 \t Training Loss: 46.64642202589247\n",
      "Epoch 43 \t Batch 560 \t Training Loss: 46.6538745198931\n",
      "Epoch 43 \t Batch 580 \t Training Loss: 46.61544797173862\n",
      "Epoch 43 \t Batch 600 \t Training Loss: 46.67361310958862\n",
      "Epoch 43 \t Batch 620 \t Training Loss: 46.655092300907256\n",
      "Epoch 43 \t Batch 640 \t Training Loss: 46.646720266342165\n",
      "Epoch 43 \t Batch 660 \t Training Loss: 46.65862282839689\n",
      "Epoch 43 \t Batch 680 \t Training Loss: 46.66923009087058\n",
      "Epoch 43 \t Batch 700 \t Training Loss: 46.68049809047154\n",
      "Epoch 43 \t Batch 720 \t Training Loss: 46.6876568529341\n",
      "Epoch 43 \t Batch 740 \t Training Loss: 46.71707258997737\n",
      "Epoch 43 \t Batch 760 \t Training Loss: 46.72452799144544\n",
      "Epoch 43 \t Batch 780 \t Training Loss: 46.782119765648474\n",
      "Epoch 43 \t Batch 800 \t Training Loss: 46.76761264324188\n",
      "Epoch 43 \t Batch 820 \t Training Loss: 46.760817876676235\n",
      "Epoch 43 \t Batch 840 \t Training Loss: 46.783747023627875\n",
      "Epoch 43 \t Batch 860 \t Training Loss: 46.7810461487881\n",
      "Epoch 43 \t Batch 880 \t Training Loss: 46.78637867840854\n",
      "Epoch 43 \t Batch 900 \t Training Loss: 46.80725560506185\n",
      "Epoch 43 \t Batch 20 \t Validation Loss: 23.14600005149841\n",
      "Epoch 43 \t Batch 40 \t Validation Loss: 25.376705121994018\n",
      "Epoch 43 \t Batch 60 \t Validation Loss: 25.09662742614746\n",
      "Epoch 43 \t Batch 80 \t Validation Loss: 25.430525994300844\n",
      "Epoch 43 \t Batch 100 \t Validation Loss: 26.094969310760497\n",
      "Epoch 43 \t Batch 120 \t Validation Loss: 26.80409590403239\n",
      "Epoch 43 \t Batch 140 \t Validation Loss: 26.95862500326974\n",
      "Epoch 43 \t Batch 160 \t Validation Loss: 28.6670934677124\n",
      "Epoch 43 \t Batch 180 \t Validation Loss: 32.00031193627252\n",
      "Epoch 43 \t Batch 200 \t Validation Loss: 33.335650329589846\n",
      "Epoch 43 \t Batch 220 \t Validation Loss: 34.51398157639937\n",
      "Epoch 43 \t Batch 240 \t Validation Loss: 34.898955416679385\n",
      "Epoch 43 \t Batch 260 \t Validation Loss: 36.93877275540279\n",
      "Epoch 43 \t Batch 280 \t Validation Loss: 38.01447153091431\n",
      "Epoch 43 \t Batch 300 \t Validation Loss: 38.99620439529419\n",
      "Epoch 43 \t Batch 320 \t Validation Loss: 39.45741328597069\n",
      "Epoch 43 \t Batch 340 \t Validation Loss: 39.36015645195456\n",
      "Epoch 43 \t Batch 360 \t Validation Loss: 39.16896890004476\n",
      "Epoch 43 \t Batch 380 \t Validation Loss: 39.36844811188547\n",
      "Epoch 43 \t Batch 400 \t Validation Loss: 38.931888642311094\n",
      "Epoch 43 \t Batch 420 \t Validation Loss: 38.95716285478501\n",
      "Epoch 43 \t Batch 440 \t Validation Loss: 38.6295077085495\n",
      "Epoch 43 \t Batch 460 \t Validation Loss: 38.82869609542515\n",
      "Epoch 43 \t Batch 480 \t Validation Loss: 39.314078678687416\n",
      "Epoch 43 \t Batch 500 \t Validation Loss: 39.037700101852415\n",
      "Epoch 43 \t Batch 520 \t Validation Loss: 38.76364170404581\n",
      "Epoch 43 \t Batch 540 \t Validation Loss: 38.494057667696914\n",
      "Epoch 43 \t Batch 560 \t Validation Loss: 38.264472908633095\n",
      "Epoch 43 \t Batch 580 \t Validation Loss: 38.00157894759342\n",
      "Epoch 43 \t Batch 600 \t Validation Loss: 38.21118276119232\n",
      "Epoch 43 Training Loss: 46.81046508572744 Validation Loss: 38.85934177776436\n",
      "Epoch 43 completed\n",
      "Epoch 44 \t Batch 20 \t Training Loss: 49.18049945831299\n",
      "Epoch 44 \t Batch 40 \t Training Loss: 48.22367134094238\n",
      "Epoch 44 \t Batch 60 \t Training Loss: 47.3742592493693\n",
      "Epoch 44 \t Batch 80 \t Training Loss: 47.53636260032654\n",
      "Epoch 44 \t Batch 100 \t Training Loss: 47.394929542541504\n",
      "Epoch 44 \t Batch 120 \t Training Loss: 47.22705198923747\n",
      "Epoch 44 \t Batch 140 \t Training Loss: 47.28392500196185\n",
      "Epoch 44 \t Batch 160 \t Training Loss: 47.163571333885194\n",
      "Epoch 44 \t Batch 180 \t Training Loss: 47.11784265306261\n",
      "Epoch 44 \t Batch 200 \t Training Loss: 46.953175201416016\n",
      "Epoch 44 \t Batch 220 \t Training Loss: 46.90027479691939\n",
      "Epoch 44 \t Batch 240 \t Training Loss: 46.931736675898236\n",
      "Epoch 44 \t Batch 260 \t Training Loss: 46.94802103776198\n",
      "Epoch 44 \t Batch 280 \t Training Loss: 46.977518572126115\n",
      "Epoch 44 \t Batch 300 \t Training Loss: 47.0203891881307\n",
      "Epoch 44 \t Batch 320 \t Training Loss: 46.922361040115355\n",
      "Epoch 44 \t Batch 340 \t Training Loss: 46.88388250014361\n",
      "Epoch 44 \t Batch 360 \t Training Loss: 46.881540065341525\n",
      "Epoch 44 \t Batch 380 \t Training Loss: 46.95069130345395\n",
      "Epoch 44 \t Batch 400 \t Training Loss: 46.82637865066528\n",
      "Epoch 44 \t Batch 420 \t Training Loss: 46.7992799668085\n",
      "Epoch 44 \t Batch 440 \t Training Loss: 46.81672013889659\n",
      "Epoch 44 \t Batch 460 \t Training Loss: 46.90109173318614\n",
      "Epoch 44 \t Batch 480 \t Training Loss: 46.95407632191976\n",
      "Epoch 44 \t Batch 500 \t Training Loss: 47.020217460632324\n",
      "Epoch 44 \t Batch 520 \t Training Loss: 46.95958857169518\n",
      "Epoch 44 \t Batch 540 \t Training Loss: 46.91715848004377\n",
      "Epoch 44 \t Batch 560 \t Training Loss: 46.903296266283306\n",
      "Epoch 44 \t Batch 580 \t Training Loss: 46.89537936901224\n",
      "Epoch 44 \t Batch 600 \t Training Loss: 46.88597236633301\n",
      "Epoch 44 \t Batch 620 \t Training Loss: 46.88084946909258\n",
      "Epoch 44 \t Batch 640 \t Training Loss: 46.88030333518982\n",
      "Epoch 44 \t Batch 660 \t Training Loss: 46.89969029860063\n",
      "Epoch 44 \t Batch 680 \t Training Loss: 46.86196356380687\n",
      "Epoch 44 \t Batch 700 \t Training Loss: 46.833215408325195\n",
      "Epoch 44 \t Batch 720 \t Training Loss: 46.85174456702338\n",
      "Epoch 44 \t Batch 740 \t Training Loss: 46.874073420344175\n",
      "Epoch 44 \t Batch 760 \t Training Loss: 46.89571206946122\n",
      "Epoch 44 \t Batch 780 \t Training Loss: 46.89791148748153\n",
      "Epoch 44 \t Batch 800 \t Training Loss: 46.89730936527252\n",
      "Epoch 44 \t Batch 820 \t Training Loss: 46.893339366447634\n",
      "Epoch 44 \t Batch 840 \t Training Loss: 46.866989580790204\n",
      "Epoch 44 \t Batch 860 \t Training Loss: 46.84480355728504\n",
      "Epoch 44 \t Batch 880 \t Training Loss: 46.80503252202814\n",
      "Epoch 44 \t Batch 900 \t Training Loss: 46.83188740200467\n",
      "Epoch 44 \t Batch 20 \t Validation Loss: 20.589140367507934\n",
      "Epoch 44 \t Batch 40 \t Validation Loss: 23.632190418243407\n",
      "Epoch 44 \t Batch 60 \t Validation Loss: 23.20600035985311\n",
      "Epoch 44 \t Batch 80 \t Validation Loss: 23.883272433280943\n",
      "Epoch 44 \t Batch 100 \t Validation Loss: 24.76380910873413\n",
      "Epoch 44 \t Batch 120 \t Validation Loss: 25.693235182762145\n",
      "Epoch 44 \t Batch 140 \t Validation Loss: 26.044702659334455\n",
      "Epoch 44 \t Batch 160 \t Validation Loss: 27.890870863199233\n",
      "Epoch 44 \t Batch 180 \t Validation Loss: 31.574716499116686\n",
      "Epoch 44 \t Batch 200 \t Validation Loss: 32.99306771755219\n",
      "Epoch 44 \t Batch 220 \t Validation Loss: 34.230045184222135\n",
      "Epoch 44 \t Batch 240 \t Validation Loss: 34.7028214653333\n",
      "Epoch 44 \t Batch 260 \t Validation Loss: 36.77233566871056\n",
      "Epoch 44 \t Batch 280 \t Validation Loss: 37.88681382451739\n",
      "Epoch 44 \t Batch 300 \t Validation Loss: 39.04139778137207\n",
      "Epoch 44 \t Batch 320 \t Validation Loss: 39.56173260211945\n",
      "Epoch 44 \t Batch 340 \t Validation Loss: 39.46848690930535\n",
      "Epoch 44 \t Batch 360 \t Validation Loss: 39.30117327107324\n",
      "Epoch 44 \t Batch 380 \t Validation Loss: 39.50595964883503\n",
      "Epoch 44 \t Batch 400 \t Validation Loss: 39.06520475625992\n",
      "Epoch 44 \t Batch 420 \t Validation Loss: 39.05512663523356\n",
      "Epoch 44 \t Batch 440 \t Validation Loss: 38.742673995278096\n",
      "Epoch 44 \t Batch 460 \t Validation Loss: 38.98648390977279\n",
      "Epoch 44 \t Batch 480 \t Validation Loss: 39.44741268157959\n",
      "Epoch 44 \t Batch 500 \t Validation Loss: 39.14955175018311\n",
      "Epoch 44 \t Batch 520 \t Validation Loss: 38.92204559766329\n",
      "Epoch 44 \t Batch 540 \t Validation Loss: 38.653011586931015\n",
      "Epoch 44 \t Batch 560 \t Validation Loss: 38.43144089153835\n",
      "Epoch 44 \t Batch 580 \t Validation Loss: 38.148722306613266\n",
      "Epoch 44 \t Batch 600 \t Validation Loss: 38.346587324142455\n",
      "Epoch 44 Training Loss: 46.82561350232772 Validation Loss: 38.97959189600759\n",
      "Epoch 44 completed\n",
      "Epoch 45 \t Batch 20 \t Training Loss: 48.280795097351074\n",
      "Epoch 45 \t Batch 40 \t Training Loss: 46.59305582046509\n",
      "Epoch 45 \t Batch 60 \t Training Loss: 46.689389355977376\n",
      "Epoch 45 \t Batch 80 \t Training Loss: 46.73007154464722\n",
      "Epoch 45 \t Batch 100 \t Training Loss: 46.99252704620361\n",
      "Epoch 45 \t Batch 120 \t Training Loss: 46.93845911026001\n",
      "Epoch 45 \t Batch 140 \t Training Loss: 47.0436215536935\n",
      "Epoch 45 \t Batch 160 \t Training Loss: 47.18679602146149\n",
      "Epoch 45 \t Batch 180 \t Training Loss: 47.17874531216091\n",
      "Epoch 45 \t Batch 200 \t Training Loss: 47.11020067214966\n",
      "Epoch 45 \t Batch 220 \t Training Loss: 47.02335962815718\n",
      "Epoch 45 \t Batch 240 \t Training Loss: 47.02160660425822\n",
      "Epoch 45 \t Batch 260 \t Training Loss: 47.041522715641904\n",
      "Epoch 45 \t Batch 280 \t Training Loss: 47.00265686852591\n",
      "Epoch 45 \t Batch 300 \t Training Loss: 46.953140856424966\n",
      "Epoch 45 \t Batch 320 \t Training Loss: 46.95959132909775\n",
      "Epoch 45 \t Batch 340 \t Training Loss: 46.873584309746235\n",
      "Epoch 45 \t Batch 360 \t Training Loss: 46.889582580990265\n",
      "Epoch 45 \t Batch 380 \t Training Loss: 46.88438576146176\n",
      "Epoch 45 \t Batch 400 \t Training Loss: 46.923927011489866\n",
      "Epoch 45 \t Batch 420 \t Training Loss: 46.8892057237171\n",
      "Epoch 45 \t Batch 440 \t Training Loss: 46.84018845124678\n",
      "Epoch 45 \t Batch 460 \t Training Loss: 46.82844217549199\n",
      "Epoch 45 \t Batch 480 \t Training Loss: 46.74875450134277\n",
      "Epoch 45 \t Batch 500 \t Training Loss: 46.72293688964844\n",
      "Epoch 45 \t Batch 520 \t Training Loss: 46.74350957870483\n",
      "Epoch 45 \t Batch 540 \t Training Loss: 46.716439508508756\n",
      "Epoch 45 \t Batch 560 \t Training Loss: 46.705238819122314\n",
      "Epoch 45 \t Batch 580 \t Training Loss: 46.68115744097479\n",
      "Epoch 45 \t Batch 600 \t Training Loss: 46.66571407318115\n",
      "Epoch 45 \t Batch 620 \t Training Loss: 46.66017860289543\n",
      "Epoch 45 \t Batch 640 \t Training Loss: 46.660973846912384\n",
      "Epoch 45 \t Batch 660 \t Training Loss: 46.67731482765891\n",
      "Epoch 45 \t Batch 680 \t Training Loss: 46.70177373325124\n",
      "Epoch 45 \t Batch 700 \t Training Loss: 46.720005220685685\n",
      "Epoch 45 \t Batch 720 \t Training Loss: 46.703400701946684\n",
      "Epoch 45 \t Batch 740 \t Training Loss: 46.71447695654792\n",
      "Epoch 45 \t Batch 760 \t Training Loss: 46.731205769589074\n",
      "Epoch 45 \t Batch 780 \t Training Loss: 46.73765385945638\n",
      "Epoch 45 \t Batch 800 \t Training Loss: 46.79023950099945\n",
      "Epoch 45 \t Batch 820 \t Training Loss: 46.76810152937726\n",
      "Epoch 45 \t Batch 840 \t Training Loss: 46.74776401065645\n",
      "Epoch 45 \t Batch 860 \t Training Loss: 46.747408942289134\n",
      "Epoch 45 \t Batch 880 \t Training Loss: 46.77368233420632\n",
      "Epoch 45 \t Batch 900 \t Training Loss: 46.77512802971734\n",
      "Epoch 45 \t Batch 20 \t Validation Loss: 18.552016067504884\n",
      "Epoch 45 \t Batch 40 \t Validation Loss: 21.64874608516693\n",
      "Epoch 45 \t Batch 60 \t Validation Loss: 21.2054039478302\n",
      "Epoch 45 \t Batch 80 \t Validation Loss: 21.872207725048064\n",
      "Epoch 45 \t Batch 100 \t Validation Loss: 23.32839705467224\n",
      "Epoch 45 \t Batch 120 \t Validation Loss: 24.516795214017232\n",
      "Epoch 45 \t Batch 140 \t Validation Loss: 25.022946187428065\n",
      "Epoch 45 \t Batch 160 \t Validation Loss: 26.89970102906227\n",
      "Epoch 45 \t Batch 180 \t Validation Loss: 30.39840842352973\n",
      "Epoch 45 \t Batch 200 \t Validation Loss: 31.82656325340271\n",
      "Epoch 45 \t Batch 220 \t Validation Loss: 33.05390263470736\n",
      "Epoch 45 \t Batch 240 \t Validation Loss: 33.523893574873604\n",
      "Epoch 45 \t Batch 260 \t Validation Loss: 35.55252608519334\n",
      "Epoch 45 \t Batch 280 \t Validation Loss: 36.66547092710223\n",
      "Epoch 45 \t Batch 300 \t Validation Loss: 37.72413292566935\n",
      "Epoch 45 \t Batch 320 \t Validation Loss: 38.24412712156773\n",
      "Epoch 45 \t Batch 340 \t Validation Loss: 38.2037746850182\n",
      "Epoch 45 \t Batch 360 \t Validation Loss: 38.04512863953908\n",
      "Epoch 45 \t Batch 380 \t Validation Loss: 38.306067318665356\n",
      "Epoch 45 \t Batch 400 \t Validation Loss: 37.96343924283981\n",
      "Epoch 45 \t Batch 420 \t Validation Loss: 38.05345537321908\n",
      "Epoch 45 \t Batch 440 \t Validation Loss: 37.82748785235665\n",
      "Epoch 45 \t Batch 460 \t Validation Loss: 38.14910611899003\n",
      "Epoch 45 \t Batch 480 \t Validation Loss: 38.674390655756\n",
      "Epoch 45 \t Batch 500 \t Validation Loss: 38.424978372573854\n",
      "Epoch 45 \t Batch 520 \t Validation Loss: 38.21525572813474\n",
      "Epoch 45 \t Batch 540 \t Validation Loss: 37.98665926368148\n",
      "Epoch 45 \t Batch 560 \t Validation Loss: 37.779791615690506\n",
      "Epoch 45 \t Batch 580 \t Validation Loss: 37.47389785339092\n",
      "Epoch 45 \t Batch 600 \t Validation Loss: 37.716192072232566\n",
      "Epoch 45 Training Loss: 46.795898824377694 Validation Loss: 38.357820527894155\n",
      "Epoch 45 completed\n",
      "Epoch 46 \t Batch 20 \t Training Loss: 47.24376201629639\n",
      "Epoch 46 \t Batch 40 \t Training Loss: 47.03617763519287\n",
      "Epoch 46 \t Batch 60 \t Training Loss: 46.739397048950195\n",
      "Epoch 46 \t Batch 80 \t Training Loss: 46.70811576843262\n",
      "Epoch 46 \t Batch 100 \t Training Loss: 46.88158542633057\n",
      "Epoch 46 \t Batch 120 \t Training Loss: 46.78873793284098\n",
      "Epoch 46 \t Batch 140 \t Training Loss: 46.66591802324567\n",
      "Epoch 46 \t Batch 160 \t Training Loss: 46.72532939910889\n",
      "Epoch 46 \t Batch 180 \t Training Loss: 46.88969635433621\n",
      "Epoch 46 \t Batch 200 \t Training Loss: 46.779445018768314\n",
      "Epoch 46 \t Batch 220 \t Training Loss: 46.73550654324618\n",
      "Epoch 46 \t Batch 240 \t Training Loss: 46.71550186475118\n",
      "Epoch 46 \t Batch 260 \t Training Loss: 46.68510538247915\n",
      "Epoch 46 \t Batch 280 \t Training Loss: 46.73637067249843\n",
      "Epoch 46 \t Batch 300 \t Training Loss: 46.87056084950765\n",
      "Epoch 46 \t Batch 320 \t Training Loss: 46.75939965248108\n",
      "Epoch 46 \t Batch 340 \t Training Loss: 46.696221564797796\n",
      "Epoch 46 \t Batch 360 \t Training Loss: 46.726505247751874\n",
      "Epoch 46 \t Batch 380 \t Training Loss: 46.67643206746955\n",
      "Epoch 46 \t Batch 400 \t Training Loss: 46.664057884216305\n",
      "Epoch 46 \t Batch 420 \t Training Loss: 46.61342978704543\n",
      "Epoch 46 \t Batch 440 \t Training Loss: 46.6781832001426\n",
      "Epoch 46 \t Batch 460 \t Training Loss: 46.71728022202201\n",
      "Epoch 46 \t Batch 480 \t Training Loss: 46.73956178824107\n",
      "Epoch 46 \t Batch 500 \t Training Loss: 46.68338505554199\n",
      "Epoch 46 \t Batch 520 \t Training Loss: 46.685504597883956\n",
      "Epoch 46 \t Batch 540 \t Training Loss: 46.705015443872526\n",
      "Epoch 46 \t Batch 560 \t Training Loss: 46.6883241380964\n",
      "Epoch 46 \t Batch 580 \t Training Loss: 46.67848454179435\n",
      "Epoch 46 \t Batch 600 \t Training Loss: 46.69699190775553\n",
      "Epoch 46 \t Batch 620 \t Training Loss: 46.646602538324174\n",
      "Epoch 46 \t Batch 640 \t Training Loss: 46.65856045484543\n",
      "Epoch 46 \t Batch 660 \t Training Loss: 46.66430530548096\n",
      "Epoch 46 \t Batch 680 \t Training Loss: 46.65480131822474\n",
      "Epoch 46 \t Batch 700 \t Training Loss: 46.674725014822826\n",
      "Epoch 46 \t Batch 720 \t Training Loss: 46.694864474402536\n",
      "Epoch 46 \t Batch 740 \t Training Loss: 46.73886139843915\n",
      "Epoch 46 \t Batch 760 \t Training Loss: 46.79092102552715\n",
      "Epoch 46 \t Batch 780 \t Training Loss: 46.7785853214753\n",
      "Epoch 46 \t Batch 800 \t Training Loss: 46.80973096370697\n",
      "Epoch 46 \t Batch 820 \t Training Loss: 46.789622804595204\n",
      "Epoch 46 \t Batch 840 \t Training Loss: 46.781663744790215\n",
      "Epoch 46 \t Batch 860 \t Training Loss: 46.78744966373887\n",
      "Epoch 46 \t Batch 880 \t Training Loss: 46.77411596124823\n",
      "Epoch 46 \t Batch 900 \t Training Loss: 46.747037391662595\n",
      "Epoch 46 \t Batch 20 \t Validation Loss: 26.615356349945067\n",
      "Epoch 46 \t Batch 40 \t Validation Loss: 26.59970784187317\n",
      "Epoch 46 \t Batch 60 \t Validation Loss: 27.07505400975545\n",
      "Epoch 46 \t Batch 80 \t Validation Loss: 27.308977258205413\n",
      "Epoch 46 \t Batch 100 \t Validation Loss: 27.856975317001343\n",
      "Epoch 46 \t Batch 120 \t Validation Loss: 28.479950292905173\n",
      "Epoch 46 \t Batch 140 \t Validation Loss: 28.560900081907\n",
      "Epoch 46 \t Batch 160 \t Validation Loss: 30.083449214696884\n",
      "Epoch 46 \t Batch 180 \t Validation Loss: 33.41115875244141\n",
      "Epoch 46 \t Batch 200 \t Validation Loss: 34.62761371612549\n",
      "Epoch 46 \t Batch 220 \t Validation Loss: 35.68968830975619\n",
      "Epoch 46 \t Batch 240 \t Validation Loss: 35.98924305041631\n",
      "Epoch 46 \t Batch 260 \t Validation Loss: 37.935481056800256\n",
      "Epoch 46 \t Batch 280 \t Validation Loss: 38.91772543021611\n",
      "Epoch 46 \t Batch 300 \t Validation Loss: 39.921007261276245\n",
      "Epoch 46 \t Batch 320 \t Validation Loss: 40.34029842913151\n",
      "Epoch 46 \t Batch 340 \t Validation Loss: 40.190243942597334\n",
      "Epoch 46 \t Batch 360 \t Validation Loss: 39.9634337398741\n",
      "Epoch 46 \t Batch 380 \t Validation Loss: 40.10506775755631\n",
      "Epoch 46 \t Batch 400 \t Validation Loss: 39.62051204919815\n",
      "Epoch 46 \t Batch 420 \t Validation Loss: 39.579588867369154\n",
      "Epoch 46 \t Batch 440 \t Validation Loss: 39.2101377660578\n",
      "Epoch 46 \t Batch 460 \t Validation Loss: 39.376736591173255\n",
      "Epoch 46 \t Batch 480 \t Validation Loss: 39.82072096268336\n",
      "Epoch 46 \t Batch 500 \t Validation Loss: 39.5078616065979\n",
      "Epoch 46 \t Batch 520 \t Validation Loss: 39.220936325880196\n",
      "Epoch 46 \t Batch 540 \t Validation Loss: 38.928045772623136\n",
      "Epoch 46 \t Batch 560 \t Validation Loss: 38.6954306449209\n",
      "Epoch 46 \t Batch 580 \t Validation Loss: 38.42079680705893\n",
      "Epoch 46 \t Batch 600 \t Validation Loss: 38.60282359282176\n",
      "Epoch 46 Training Loss: 46.73375264274905 Validation Loss: 39.24095914116153\n",
      "Epoch 46 completed\n",
      "Epoch 47 \t Batch 20 \t Training Loss: 42.99777069091797\n",
      "Epoch 47 \t Batch 40 \t Training Loss: 44.561836338043214\n",
      "Epoch 47 \t Batch 60 \t Training Loss: 45.29446887969971\n",
      "Epoch 47 \t Batch 80 \t Training Loss: 45.45425238609314\n",
      "Epoch 47 \t Batch 100 \t Training Loss: 45.83446315765381\n",
      "Epoch 47 \t Batch 120 \t Training Loss: 45.7970547358195\n",
      "Epoch 47 \t Batch 140 \t Training Loss: 45.763624463762554\n",
      "Epoch 47 \t Batch 160 \t Training Loss: 45.87785186767578\n",
      "Epoch 47 \t Batch 180 \t Training Loss: 45.954990005493165\n",
      "Epoch 47 \t Batch 200 \t Training Loss: 46.07194492340088\n",
      "Epoch 47 \t Batch 220 \t Training Loss: 45.99254869981245\n",
      "Epoch 47 \t Batch 240 \t Training Loss: 46.21223724683126\n",
      "Epoch 47 \t Batch 260 \t Training Loss: 46.28804770249587\n",
      "Epoch 47 \t Batch 280 \t Training Loss: 46.3833199773516\n",
      "Epoch 47 \t Batch 300 \t Training Loss: 46.49294645945231\n",
      "Epoch 47 \t Batch 320 \t Training Loss: 46.48609701395035\n",
      "Epoch 47 \t Batch 340 \t Training Loss: 46.55009307861328\n",
      "Epoch 47 \t Batch 360 \t Training Loss: 46.56926623450385\n",
      "Epoch 47 \t Batch 380 \t Training Loss: 46.55719617542468\n",
      "Epoch 47 \t Batch 400 \t Training Loss: 46.58777017593384\n",
      "Epoch 47 \t Batch 420 \t Training Loss: 46.629348200843445\n",
      "Epoch 47 \t Batch 440 \t Training Loss: 46.593786525726316\n",
      "Epoch 47 \t Batch 460 \t Training Loss: 46.58745494510816\n",
      "Epoch 47 \t Batch 480 \t Training Loss: 46.55607773462931\n",
      "Epoch 47 \t Batch 500 \t Training Loss: 46.519803970336916\n",
      "Epoch 47 \t Batch 520 \t Training Loss: 46.55090408325195\n",
      "Epoch 47 \t Batch 540 \t Training Loss: 46.57740057486075\n",
      "Epoch 47 \t Batch 560 \t Training Loss: 46.59122110094343\n",
      "Epoch 47 \t Batch 580 \t Training Loss: 46.51405501530088\n",
      "Epoch 47 \t Batch 600 \t Training Loss: 46.548632265726724\n",
      "Epoch 47 \t Batch 620 \t Training Loss: 46.546313365813226\n",
      "Epoch 47 \t Batch 640 \t Training Loss: 46.5511118888855\n",
      "Epoch 47 \t Batch 660 \t Training Loss: 46.58299151333895\n",
      "Epoch 47 \t Batch 680 \t Training Loss: 46.61992479773129\n",
      "Epoch 47 \t Batch 700 \t Training Loss: 46.62670181819371\n",
      "Epoch 47 \t Batch 720 \t Training Loss: 46.649105066723294\n",
      "Epoch 47 \t Batch 740 \t Training Loss: 46.62847706562764\n",
      "Epoch 47 \t Batch 760 \t Training Loss: 46.59062859886571\n",
      "Epoch 47 \t Batch 780 \t Training Loss: 46.59865590609037\n",
      "Epoch 47 \t Batch 800 \t Training Loss: 46.60726449012756\n",
      "Epoch 47 \t Batch 820 \t Training Loss: 46.655116900002085\n",
      "Epoch 47 \t Batch 840 \t Training Loss: 46.6629025777181\n",
      "Epoch 47 \t Batch 860 \t Training Loss: 46.657642506444176\n",
      "Epoch 47 \t Batch 880 \t Training Loss: 46.69472896402532\n",
      "Epoch 47 \t Batch 900 \t Training Loss: 46.70712331559923\n",
      "Epoch 47 \t Batch 20 \t Validation Loss: 19.98214168548584\n",
      "Epoch 47 \t Batch 40 \t Validation Loss: 22.166286063194274\n",
      "Epoch 47 \t Batch 60 \t Validation Loss: 21.866629854838052\n",
      "Epoch 47 \t Batch 80 \t Validation Loss: 22.484451174736023\n",
      "Epoch 47 \t Batch 100 \t Validation Loss: 23.628099880218507\n",
      "Epoch 47 \t Batch 120 \t Validation Loss: 24.69045666853587\n",
      "Epoch 47 \t Batch 140 \t Validation Loss: 25.161645664487565\n",
      "Epoch 47 \t Batch 160 \t Validation Loss: 27.028639167547226\n",
      "Epoch 47 \t Batch 180 \t Validation Loss: 30.584448703130086\n",
      "Epoch 47 \t Batch 200 \t Validation Loss: 31.98442945957184\n",
      "Epoch 47 \t Batch 220 \t Validation Loss: 33.17031954418529\n",
      "Epoch 47 \t Batch 240 \t Validation Loss: 33.63484186728795\n",
      "Epoch 47 \t Batch 260 \t Validation Loss: 35.668070756472076\n",
      "Epoch 47 \t Batch 280 \t Validation Loss: 36.77366854463305\n",
      "Epoch 47 \t Batch 300 \t Validation Loss: 37.85374164263408\n",
      "Epoch 47 \t Batch 320 \t Validation Loss: 38.35815636515618\n",
      "Epoch 47 \t Batch 340 \t Validation Loss: 38.310192944021786\n",
      "Epoch 47 \t Batch 360 \t Validation Loss: 38.15727790196737\n",
      "Epoch 47 \t Batch 380 \t Validation Loss: 38.375954688222784\n",
      "Epoch 47 \t Batch 400 \t Validation Loss: 37.99619025707245\n",
      "Epoch 47 \t Batch 420 \t Validation Loss: 38.04501534643627\n",
      "Epoch 47 \t Batch 440 \t Validation Loss: 37.75137712088498\n",
      "Epoch 47 \t Batch 460 \t Validation Loss: 38.002379780230314\n",
      "Epoch 47 \t Batch 480 \t Validation Loss: 38.51282443801562\n",
      "Epoch 47 \t Batch 500 \t Validation Loss: 38.25963774299622\n",
      "Epoch 47 \t Batch 520 \t Validation Loss: 38.034605585611786\n",
      "Epoch 47 \t Batch 540 \t Validation Loss: 37.805826828214855\n",
      "Epoch 47 \t Batch 560 \t Validation Loss: 37.61161178009851\n",
      "Epoch 47 \t Batch 580 \t Validation Loss: 37.38841968240409\n",
      "Epoch 47 \t Batch 600 \t Validation Loss: 37.61354422410329\n",
      "Epoch 47 Training Loss: 46.71858022309181 Validation Loss: 38.26870638828773\n",
      "Epoch 47 completed\n",
      "Epoch 48 \t Batch 20 \t Training Loss: 47.56666641235351\n",
      "Epoch 48 \t Batch 40 \t Training Loss: 46.84981660842895\n",
      "Epoch 48 \t Batch 60 \t Training Loss: 46.76412321726481\n",
      "Epoch 48 \t Batch 80 \t Training Loss: 46.58155889511109\n",
      "Epoch 48 \t Batch 100 \t Training Loss: 46.445137939453126\n",
      "Epoch 48 \t Batch 120 \t Training Loss: 46.41772321065267\n",
      "Epoch 48 \t Batch 140 \t Training Loss: 46.602365030561174\n",
      "Epoch 48 \t Batch 160 \t Training Loss: 46.561255025863645\n",
      "Epoch 48 \t Batch 180 \t Training Loss: 46.61955892774794\n",
      "Epoch 48 \t Batch 200 \t Training Loss: 46.66329490661621\n",
      "Epoch 48 \t Batch 220 \t Training Loss: 46.692674827575686\n",
      "Epoch 48 \t Batch 240 \t Training Loss: 46.634365479151406\n",
      "Epoch 48 \t Batch 260 \t Training Loss: 46.67771132542537\n",
      "Epoch 48 \t Batch 280 \t Training Loss: 46.674259649004256\n",
      "Epoch 48 \t Batch 300 \t Training Loss: 46.71172828674317\n",
      "Epoch 48 \t Batch 320 \t Training Loss: 46.67665812969208\n",
      "Epoch 48 \t Batch 340 \t Training Loss: 46.70227329029756\n",
      "Epoch 48 \t Batch 360 \t Training Loss: 46.70282668007745\n",
      "Epoch 48 \t Batch 380 \t Training Loss: 46.72375684035452\n",
      "Epoch 48 \t Batch 400 \t Training Loss: 46.67310796737671\n",
      "Epoch 48 \t Batch 420 \t Training Loss: 46.66522773561024\n",
      "Epoch 48 \t Batch 440 \t Training Loss: 46.708851502158424\n",
      "Epoch 48 \t Batch 460 \t Training Loss: 46.72892022340194\n",
      "Epoch 48 \t Batch 480 \t Training Loss: 46.705082599322004\n",
      "Epoch 48 \t Batch 500 \t Training Loss: 46.7537974243164\n",
      "Epoch 48 \t Batch 520 \t Training Loss: 46.769905757904056\n",
      "Epoch 48 \t Batch 540 \t Training Loss: 46.775721323931656\n",
      "Epoch 48 \t Batch 560 \t Training Loss: 46.76246970721653\n",
      "Epoch 48 \t Batch 580 \t Training Loss: 46.77145959919897\n",
      "Epoch 48 \t Batch 600 \t Training Loss: 46.738406874338786\n",
      "Epoch 48 \t Batch 620 \t Training Loss: 46.72735424041748\n",
      "Epoch 48 \t Batch 640 \t Training Loss: 46.636365646123885\n",
      "Epoch 48 \t Batch 660 \t Training Loss: 46.61384891163219\n",
      "Epoch 48 \t Batch 680 \t Training Loss: 46.58990878497853\n",
      "Epoch 48 \t Batch 700 \t Training Loss: 46.57405343736921\n",
      "Epoch 48 \t Batch 720 \t Training Loss: 46.5847987651825\n",
      "Epoch 48 \t Batch 740 \t Training Loss: 46.617649279413996\n",
      "Epoch 48 \t Batch 760 \t Training Loss: 46.66227878269396\n",
      "Epoch 48 \t Batch 780 \t Training Loss: 46.660705082233136\n",
      "Epoch 48 \t Batch 800 \t Training Loss: 46.64742565631867\n",
      "Epoch 48 \t Batch 820 \t Training Loss: 46.59701783250018\n",
      "Epoch 48 \t Batch 840 \t Training Loss: 46.64223294939313\n",
      "Epoch 48 \t Batch 860 \t Training Loss: 46.64082609220993\n",
      "Epoch 48 \t Batch 880 \t Training Loss: 46.67948691628196\n",
      "Epoch 48 \t Batch 900 \t Training Loss: 46.67889557308621\n",
      "Epoch 48 \t Batch 20 \t Validation Loss: 17.677493619918824\n",
      "Epoch 48 \t Batch 40 \t Validation Loss: 20.54289665222168\n",
      "Epoch 48 \t Batch 60 \t Validation Loss: 20.083160622914633\n",
      "Epoch 48 \t Batch 80 \t Validation Loss: 20.87639936208725\n",
      "Epoch 48 \t Batch 100 \t Validation Loss: 22.622843046188354\n",
      "Epoch 48 \t Batch 120 \t Validation Loss: 23.860774326324464\n",
      "Epoch 48 \t Batch 140 \t Validation Loss: 24.523770114353724\n",
      "Epoch 48 \t Batch 160 \t Validation Loss: 26.609107542037965\n",
      "Epoch 48 \t Batch 180 \t Validation Loss: 30.448389938142565\n",
      "Epoch 48 \t Batch 200 \t Validation Loss: 31.987946925163268\n",
      "Epoch 48 \t Batch 220 \t Validation Loss: 33.298003287748855\n",
      "Epoch 48 \t Batch 240 \t Validation Loss: 33.85102844635646\n",
      "Epoch 48 \t Batch 260 \t Validation Loss: 35.994262941067035\n",
      "Epoch 48 \t Batch 280 \t Validation Loss: 37.14047460896628\n",
      "Epoch 48 \t Batch 300 \t Validation Loss: 38.36755007743835\n",
      "Epoch 48 \t Batch 320 \t Validation Loss: 38.92287303805351\n",
      "Epoch 48 \t Batch 340 \t Validation Loss: 38.863925344803754\n",
      "Epoch 48 \t Batch 360 \t Validation Loss: 38.75619503127204\n",
      "Epoch 48 \t Batch 380 \t Validation Loss: 38.97678256787752\n",
      "Epoch 48 \t Batch 400 \t Validation Loss: 38.56140800476074\n",
      "Epoch 48 \t Batch 420 \t Validation Loss: 38.540419678460985\n",
      "Epoch 48 \t Batch 440 \t Validation Loss: 38.22999620871111\n",
      "Epoch 48 \t Batch 460 \t Validation Loss: 38.465454918405285\n",
      "Epoch 48 \t Batch 480 \t Validation Loss: 38.957941289742784\n",
      "Epoch 48 \t Batch 500 \t Validation Loss: 38.67170822906494\n",
      "Epoch 48 \t Batch 520 \t Validation Loss: 38.498375210395224\n",
      "Epoch 48 \t Batch 540 \t Validation Loss: 38.27741839444196\n",
      "Epoch 48 \t Batch 560 \t Validation Loss: 38.10433147634779\n",
      "Epoch 48 \t Batch 580 \t Validation Loss: 37.94579463498346\n",
      "Epoch 48 \t Batch 600 \t Validation Loss: 38.15857511520386\n",
      "Epoch 48 Training Loss: 46.67005513980578 Validation Loss: 38.81517273110229\n",
      "Epoch 48 completed\n",
      "Epoch 49 \t Batch 20 \t Training Loss: 45.91601905822754\n",
      "Epoch 49 \t Batch 40 \t Training Loss: 46.70466613769531\n",
      "Epoch 49 \t Batch 60 \t Training Loss: 46.846298408508304\n",
      "Epoch 49 \t Batch 80 \t Training Loss: 46.31158652305603\n",
      "Epoch 49 \t Batch 100 \t Training Loss: 46.46557456970215\n",
      "Epoch 49 \t Batch 120 \t Training Loss: 46.61584870020548\n",
      "Epoch 49 \t Batch 140 \t Training Loss: 46.43630782536098\n",
      "Epoch 49 \t Batch 160 \t Training Loss: 46.63485772609711\n",
      "Epoch 49 \t Batch 180 \t Training Loss: 46.671966552734375\n",
      "Epoch 49 \t Batch 200 \t Training Loss: 46.759208812713624\n",
      "Epoch 49 \t Batch 220 \t Training Loss: 46.64847429448908\n",
      "Epoch 49 \t Batch 240 \t Training Loss: 46.741565752029416\n",
      "Epoch 49 \t Batch 260 \t Training Loss: 46.77535759852483\n",
      "Epoch 49 \t Batch 280 \t Training Loss: 46.8185416494097\n",
      "Epoch 49 \t Batch 300 \t Training Loss: 46.807272593180336\n",
      "Epoch 49 \t Batch 320 \t Training Loss: 46.80243390798569\n",
      "Epoch 49 \t Batch 340 \t Training Loss: 46.644898930717915\n",
      "Epoch 49 \t Batch 360 \t Training Loss: 46.66153195699056\n",
      "Epoch 49 \t Batch 380 \t Training Loss: 46.626702880859376\n",
      "Epoch 49 \t Batch 400 \t Training Loss: 46.60830966949463\n",
      "Epoch 49 \t Batch 420 \t Training Loss: 46.714045261201406\n",
      "Epoch 49 \t Batch 440 \t Training Loss: 46.715773565118965\n",
      "Epoch 49 \t Batch 460 \t Training Loss: 46.731680604685906\n",
      "Epoch 49 \t Batch 480 \t Training Loss: 46.71156236330668\n",
      "Epoch 49 \t Batch 500 \t Training Loss: 46.73494998931885\n",
      "Epoch 49 \t Batch 520 \t Training Loss: 46.756833289219784\n",
      "Epoch 49 \t Batch 540 \t Training Loss: 46.754561516090675\n",
      "Epoch 49 \t Batch 560 \t Training Loss: 46.743676342282974\n",
      "Epoch 49 \t Batch 580 \t Training Loss: 46.71081523237557\n",
      "Epoch 49 \t Batch 600 \t Training Loss: 46.669610563913984\n",
      "Epoch 49 \t Batch 620 \t Training Loss: 46.69012475782825\n",
      "Epoch 49 \t Batch 640 \t Training Loss: 46.6804652094841\n",
      "Epoch 49 \t Batch 660 \t Training Loss: 46.688996002890846\n",
      "Epoch 49 \t Batch 680 \t Training Loss: 46.65840872035307\n",
      "Epoch 49 \t Batch 700 \t Training Loss: 46.68318842751639\n",
      "Epoch 49 \t Batch 720 \t Training Loss: 46.65681013001336\n",
      "Epoch 49 \t Batch 740 \t Training Loss: 46.6469662331246\n",
      "Epoch 49 \t Batch 760 \t Training Loss: 46.63887732154445\n",
      "Epoch 49 \t Batch 780 \t Training Loss: 46.63213661389473\n",
      "Epoch 49 \t Batch 800 \t Training Loss: 46.62184763908386\n",
      "Epoch 49 \t Batch 820 \t Training Loss: 46.61578648730022\n",
      "Epoch 49 \t Batch 840 \t Training Loss: 46.63459481284732\n",
      "Epoch 49 \t Batch 860 \t Training Loss: 46.62796186402787\n",
      "Epoch 49 \t Batch 880 \t Training Loss: 46.63583473292264\n",
      "Epoch 49 \t Batch 900 \t Training Loss: 46.63887552049425\n",
      "Epoch 49 \t Batch 20 \t Validation Loss: 20.419879245758057\n",
      "Epoch 49 \t Batch 40 \t Validation Loss: 23.418496942520143\n",
      "Epoch 49 \t Batch 60 \t Validation Loss: 22.762157996495564\n",
      "Epoch 49 \t Batch 80 \t Validation Loss: 23.155158507823945\n",
      "Epoch 49 \t Batch 100 \t Validation Loss: 24.278619680404663\n",
      "Epoch 49 \t Batch 120 \t Validation Loss: 25.320845286051433\n",
      "Epoch 49 \t Batch 140 \t Validation Loss: 25.791585649762833\n",
      "Epoch 49 \t Batch 160 \t Validation Loss: 27.709561502933504\n",
      "Epoch 49 \t Batch 180 \t Validation Loss: 31.1613563908471\n",
      "Epoch 49 \t Batch 200 \t Validation Loss: 32.535508599281314\n",
      "Epoch 49 \t Batch 220 \t Validation Loss: 33.79096342867071\n",
      "Epoch 49 \t Batch 240 \t Validation Loss: 34.266243990262346\n",
      "Epoch 49 \t Batch 260 \t Validation Loss: 36.32344395197355\n",
      "Epoch 49 \t Batch 280 \t Validation Loss: 37.4536887713841\n",
      "Epoch 49 \t Batch 300 \t Validation Loss: 38.53123945871989\n",
      "Epoch 49 \t Batch 320 \t Validation Loss: 39.03560274243355\n",
      "Epoch 49 \t Batch 340 \t Validation Loss: 38.96459194071153\n",
      "Epoch 49 \t Batch 360 \t Validation Loss: 38.822244008382164\n",
      "Epoch 49 \t Batch 380 \t Validation Loss: 39.066754160429305\n",
      "Epoch 49 \t Batch 400 \t Validation Loss: 38.67666574954987\n",
      "Epoch 49 \t Batch 420 \t Validation Loss: 38.68318143799191\n",
      "Epoch 49 \t Batch 440 \t Validation Loss: 38.41183699477803\n",
      "Epoch 49 \t Batch 460 \t Validation Loss: 38.68993132840032\n",
      "Epoch 49 \t Batch 480 \t Validation Loss: 39.162737399339676\n",
      "Epoch 49 \t Batch 500 \t Validation Loss: 38.88990337181092\n",
      "Epoch 49 \t Batch 520 \t Validation Loss: 38.751741368954\n",
      "Epoch 49 \t Batch 540 \t Validation Loss: 38.53033652128997\n",
      "Epoch 49 \t Batch 560 \t Validation Loss: 38.38120327336448\n",
      "Epoch 49 \t Batch 580 \t Validation Loss: 38.20284509987667\n",
      "Epoch 49 \t Batch 600 \t Validation Loss: 38.418372042973836\n",
      "Epoch 49 Training Loss: 46.654452736791114 Validation Loss: 39.11247006329623\n",
      "Epoch 49 completed\n",
      "Epoch 50 \t Batch 20 \t Training Loss: 46.727013969421385\n",
      "Epoch 50 \t Batch 40 \t Training Loss: 46.72191495895386\n",
      "Epoch 50 \t Batch 60 \t Training Loss: 46.582969665527344\n",
      "Epoch 50 \t Batch 80 \t Training Loss: 46.872300910949704\n",
      "Epoch 50 \t Batch 100 \t Training Loss: 46.698958778381346\n",
      "Epoch 50 \t Batch 120 \t Training Loss: 46.482538572947185\n",
      "Epoch 50 \t Batch 140 \t Training Loss: 46.38230446406773\n",
      "Epoch 50 \t Batch 160 \t Training Loss: 46.449993467330934\n",
      "Epoch 50 \t Batch 180 \t Training Loss: 46.58441403706868\n",
      "Epoch 50 \t Batch 200 \t Training Loss: 46.73642183303833\n",
      "Epoch 50 \t Batch 220 \t Training Loss: 46.74471643621271\n",
      "Epoch 50 \t Batch 240 \t Training Loss: 46.6504207611084\n",
      "Epoch 50 \t Batch 260 \t Training Loss: 46.61164945455698\n",
      "Epoch 50 \t Batch 280 \t Training Loss: 46.666925988878525\n",
      "Epoch 50 \t Batch 300 \t Training Loss: 46.730147183736165\n",
      "Epoch 50 \t Batch 320 \t Training Loss: 46.714576435089114\n",
      "Epoch 50 \t Batch 340 \t Training Loss: 46.70120557897231\n",
      "Epoch 50 \t Batch 360 \t Training Loss: 46.69119281768799\n",
      "Epoch 50 \t Batch 380 \t Training Loss: 46.69720619603207\n",
      "Epoch 50 \t Batch 400 \t Training Loss: 46.71899991035461\n",
      "Epoch 50 \t Batch 420 \t Training Loss: 46.6681204477946\n",
      "Epoch 50 \t Batch 440 \t Training Loss: 46.715057815204965\n",
      "Epoch 50 \t Batch 460 \t Training Loss: 46.65926745870839\n",
      "Epoch 50 \t Batch 480 \t Training Loss: 46.6496924718221\n",
      "Epoch 50 \t Batch 500 \t Training Loss: 46.63652606201172\n",
      "Epoch 50 \t Batch 520 \t Training Loss: 46.62070447481596\n",
      "Epoch 50 \t Batch 540 \t Training Loss: 46.61257582770453\n",
      "Epoch 50 \t Batch 560 \t Training Loss: 46.66634246962411\n",
      "Epoch 50 \t Batch 580 \t Training Loss: 46.61024899976007\n",
      "Epoch 50 \t Batch 600 \t Training Loss: 46.62460741043091\n",
      "Epoch 50 \t Batch 620 \t Training Loss: 46.59351204902895\n",
      "Epoch 50 \t Batch 640 \t Training Loss: 46.59182057976723\n",
      "Epoch 50 \t Batch 660 \t Training Loss: 46.60946188839999\n",
      "Epoch 50 \t Batch 680 \t Training Loss: 46.57582394656013\n",
      "Epoch 50 \t Batch 700 \t Training Loss: 46.604420585632326\n",
      "Epoch 50 \t Batch 720 \t Training Loss: 46.59409419695536\n",
      "Epoch 50 \t Batch 740 \t Training Loss: 46.630210335190235\n",
      "Epoch 50 \t Batch 760 \t Training Loss: 46.67366015785619\n",
      "Epoch 50 \t Batch 780 \t Training Loss: 46.60626990000407\n",
      "Epoch 50 \t Batch 800 \t Training Loss: 46.5829595041275\n",
      "Epoch 50 \t Batch 820 \t Training Loss: 46.58244846157911\n",
      "Epoch 50 \t Batch 840 \t Training Loss: 46.59362627665202\n",
      "Epoch 50 \t Batch 860 \t Training Loss: 46.59995434339656\n",
      "Epoch 50 \t Batch 880 \t Training Loss: 46.62986345291138\n",
      "Epoch 50 \t Batch 900 \t Training Loss: 46.622630526224775\n",
      "Epoch 50 \t Batch 20 \t Validation Loss: 22.696275091171266\n",
      "Epoch 50 \t Batch 40 \t Validation Loss: 22.693975806236267\n",
      "Epoch 50 \t Batch 60 \t Validation Loss: 22.854882383346556\n",
      "Epoch 50 \t Batch 80 \t Validation Loss: 23.1587127327919\n",
      "Epoch 50 \t Batch 100 \t Validation Loss: 24.126933126449586\n",
      "Epoch 50 \t Batch 120 \t Validation Loss: 25.07842311064402\n",
      "Epoch 50 \t Batch 140 \t Validation Loss: 25.543183067866735\n",
      "Epoch 50 \t Batch 160 \t Validation Loss: 27.61677612066269\n",
      "Epoch 50 \t Batch 180 \t Validation Loss: 31.630142238405014\n",
      "Epoch 50 \t Batch 200 \t Validation Loss: 33.22701557636261\n",
      "Epoch 50 \t Batch 220 \t Validation Loss: 34.65615149844776\n",
      "Epoch 50 \t Batch 240 \t Validation Loss: 35.241632064183555\n",
      "Epoch 50 \t Batch 260 \t Validation Loss: 37.4547416650332\n",
      "Epoch 50 \t Batch 280 \t Validation Loss: 38.61424393313272\n",
      "Epoch 50 \t Batch 300 \t Validation Loss: 39.87250199953715\n",
      "Epoch 50 \t Batch 320 \t Validation Loss: 40.43715761601925\n",
      "Epoch 50 \t Batch 340 \t Validation Loss: 40.33430858780356\n",
      "Epoch 50 \t Batch 360 \t Validation Loss: 40.19451943238576\n",
      "Epoch 50 \t Batch 380 \t Validation Loss: 40.39209452428316\n",
      "Epoch 50 \t Batch 400 \t Validation Loss: 39.91147533178329\n",
      "Epoch 50 \t Batch 420 \t Validation Loss: 39.8297118414016\n",
      "Epoch 50 \t Batch 440 \t Validation Loss: 39.43496700200168\n",
      "Epoch 50 \t Batch 460 \t Validation Loss: 39.599901327879536\n",
      "Epoch 50 \t Batch 480 \t Validation Loss: 40.05981076161067\n",
      "Epoch 50 \t Batch 500 \t Validation Loss: 39.74829291534424\n",
      "Epoch 50 \t Batch 520 \t Validation Loss: 39.48023921159597\n",
      "Epoch 50 \t Batch 540 \t Validation Loss: 39.24184240765042\n",
      "Epoch 50 \t Batch 560 \t Validation Loss: 39.02737737042563\n",
      "Epoch 50 \t Batch 580 \t Validation Loss: 38.797835432249926\n",
      "Epoch 50 \t Batch 600 \t Validation Loss: 39.015779647827145\n",
      "Epoch 50 Training Loss: 46.61601309739906 Validation Loss: 39.66878223109555\n",
      "Epoch 50 completed\n",
      "Epoch 51 \t Batch 20 \t Training Loss: 48.43357906341553\n",
      "Epoch 51 \t Batch 40 \t Training Loss: 47.37748727798462\n",
      "Epoch 51 \t Batch 60 \t Training Loss: 46.958000055948894\n",
      "Epoch 51 \t Batch 80 \t Training Loss: 47.1345862865448\n",
      "Epoch 51 \t Batch 100 \t Training Loss: 47.04159069061279\n",
      "Epoch 51 \t Batch 120 \t Training Loss: 46.963529141743976\n",
      "Epoch 51 \t Batch 140 \t Training Loss: 46.8562591280256\n",
      "Epoch 51 \t Batch 160 \t Training Loss: 46.754844427108765\n",
      "Epoch 51 \t Batch 180 \t Training Loss: 46.68179155985514\n",
      "Epoch 51 \t Batch 200 \t Training Loss: 46.73003856658936\n",
      "Epoch 51 \t Batch 220 \t Training Loss: 46.665513732216574\n",
      "Epoch 51 \t Batch 240 \t Training Loss: 46.587997229894\n",
      "Epoch 51 \t Batch 260 \t Training Loss: 46.62041909144475\n",
      "Epoch 51 \t Batch 280 \t Training Loss: 46.6256701196943\n",
      "Epoch 51 \t Batch 300 \t Training Loss: 46.62006907145182\n",
      "Epoch 51 \t Batch 320 \t Training Loss: 46.689766430854796\n",
      "Epoch 51 \t Batch 340 \t Training Loss: 46.78933650746065\n",
      "Epoch 51 \t Batch 360 \t Training Loss: 46.77110579808553\n",
      "Epoch 51 \t Batch 380 \t Training Loss: 46.69596106880589\n",
      "Epoch 51 \t Batch 400 \t Training Loss: 46.69229893684387\n",
      "Epoch 51 \t Batch 420 \t Training Loss: 46.651731427510576\n",
      "Epoch 51 \t Batch 440 \t Training Loss: 46.57667914303866\n",
      "Epoch 51 \t Batch 460 \t Training Loss: 46.63627339238706\n",
      "Epoch 51 \t Batch 480 \t Training Loss: 46.578052314122516\n",
      "Epoch 51 \t Batch 500 \t Training Loss: 46.55488708496094\n",
      "Epoch 51 \t Batch 520 \t Training Loss: 46.574316699688254\n",
      "Epoch 51 \t Batch 540 \t Training Loss: 46.606450963903356\n",
      "Epoch 51 \t Batch 560 \t Training Loss: 46.590113271985736\n",
      "Epoch 51 \t Batch 580 \t Training Loss: 46.53734434719743\n",
      "Epoch 51 \t Batch 600 \t Training Loss: 46.501013634999595\n",
      "Epoch 51 \t Batch 620 \t Training Loss: 46.560310179187404\n",
      "Epoch 51 \t Batch 640 \t Training Loss: 46.61492267847061\n",
      "Epoch 51 \t Batch 660 \t Training Loss: 46.61091405695135\n",
      "Epoch 51 \t Batch 680 \t Training Loss: 46.61326725342695\n",
      "Epoch 51 \t Batch 700 \t Training Loss: 46.59736400059291\n",
      "Epoch 51 \t Batch 720 \t Training Loss: 46.61780092981127\n",
      "Epoch 51 \t Batch 740 \t Training Loss: 46.603723294026146\n",
      "Epoch 51 \t Batch 760 \t Training Loss: 46.58522169213546\n",
      "Epoch 51 \t Batch 780 \t Training Loss: 46.58096720866668\n",
      "Epoch 51 \t Batch 800 \t Training Loss: 46.54301261425018\n",
      "Epoch 51 \t Batch 820 \t Training Loss: 46.52696237331483\n",
      "Epoch 51 \t Batch 840 \t Training Loss: 46.550854646591915\n",
      "Epoch 51 \t Batch 860 \t Training Loss: 46.550018244011454\n",
      "Epoch 51 \t Batch 880 \t Training Loss: 46.54453180919994\n",
      "Epoch 51 \t Batch 900 \t Training Loss: 46.56453397538927\n",
      "Epoch 51 \t Batch 20 \t Validation Loss: 23.886378526687622\n",
      "Epoch 51 \t Batch 40 \t Validation Loss: 26.049198865890503\n",
      "Epoch 51 \t Batch 60 \t Validation Loss: 25.766035970052084\n",
      "Epoch 51 \t Batch 80 \t Validation Loss: 26.040073919296265\n",
      "Epoch 51 \t Batch 100 \t Validation Loss: 26.829592914581298\n",
      "Epoch 51 \t Batch 120 \t Validation Loss: 27.58093778292338\n",
      "Epoch 51 \t Batch 140 \t Validation Loss: 27.69926914487566\n",
      "Epoch 51 \t Batch 160 \t Validation Loss: 29.090611362457274\n",
      "Epoch 51 \t Batch 180 \t Validation Loss: 32.1009442753262\n",
      "Epoch 51 \t Batch 200 \t Validation Loss: 33.231101722717284\n",
      "Epoch 51 \t Batch 220 \t Validation Loss: 34.15129520242864\n",
      "Epoch 51 \t Batch 240 \t Validation Loss: 34.39356655279796\n",
      "Epoch 51 \t Batch 260 \t Validation Loss: 36.2586811872629\n",
      "Epoch 51 \t Batch 280 \t Validation Loss: 37.24920509542738\n",
      "Epoch 51 \t Batch 300 \t Validation Loss: 38.12640327135722\n",
      "Epoch 51 \t Batch 320 \t Validation Loss: 38.508029508590695\n",
      "Epoch 51 \t Batch 340 \t Validation Loss: 38.40428400039673\n",
      "Epoch 51 \t Batch 360 \t Validation Loss: 38.15783415105608\n",
      "Epoch 51 \t Batch 380 \t Validation Loss: 38.33198573714808\n",
      "Epoch 51 \t Batch 400 \t Validation Loss: 37.942072422504424\n",
      "Epoch 51 \t Batch 420 \t Validation Loss: 37.978525148119246\n",
      "Epoch 51 \t Batch 440 \t Validation Loss: 37.69507624452764\n",
      "Epoch 51 \t Batch 460 \t Validation Loss: 37.929885399859884\n",
      "Epoch 51 \t Batch 480 \t Validation Loss: 38.41749689976374\n",
      "Epoch 51 \t Batch 500 \t Validation Loss: 38.1322642288208\n",
      "Epoch 51 \t Batch 520 \t Validation Loss: 37.88693804374108\n",
      "Epoch 51 \t Batch 540 \t Validation Loss: 37.64476864779437\n",
      "Epoch 51 \t Batch 560 \t Validation Loss: 37.45492060525077\n",
      "Epoch 51 \t Batch 580 \t Validation Loss: 37.18222812455276\n",
      "Epoch 51 \t Batch 600 \t Validation Loss: 37.43037721633911\n",
      "Epoch 51 Training Loss: 46.57778925256095 Validation Loss: 38.10573543821062\n",
      "Epoch 51 completed\n",
      "Epoch 52 \t Batch 20 \t Training Loss: 45.40262088775635\n",
      "Epoch 52 \t Batch 40 \t Training Loss: 46.208899688720706\n",
      "Epoch 52 \t Batch 60 \t Training Loss: 46.3907896677653\n",
      "Epoch 52 \t Batch 80 \t Training Loss: 46.78020215034485\n",
      "Epoch 52 \t Batch 100 \t Training Loss: 46.787100563049314\n",
      "Epoch 52 \t Batch 120 \t Training Loss: 46.663661066691084\n",
      "Epoch 52 \t Batch 140 \t Training Loss: 46.71400506155832\n",
      "Epoch 52 \t Batch 160 \t Training Loss: 46.6394145488739\n",
      "Epoch 52 \t Batch 180 \t Training Loss: 46.74122827317979\n",
      "Epoch 52 \t Batch 200 \t Training Loss: 46.66451755523681\n",
      "Epoch 52 \t Batch 220 \t Training Loss: 46.632687256552956\n",
      "Epoch 52 \t Batch 240 \t Training Loss: 46.55357316335042\n",
      "Epoch 52 \t Batch 260 \t Training Loss: 46.589319860018215\n",
      "Epoch 52 \t Batch 280 \t Training Loss: 46.64428724561419\n",
      "Epoch 52 \t Batch 300 \t Training Loss: 46.721130447387694\n",
      "Epoch 52 \t Batch 320 \t Training Loss: 46.741295635700226\n",
      "Epoch 52 \t Batch 340 \t Training Loss: 46.77364187801585\n",
      "Epoch 52 \t Batch 360 \t Training Loss: 46.84089918136597\n",
      "Epoch 52 \t Batch 380 \t Training Loss: 46.804423563103924\n",
      "Epoch 52 \t Batch 400 \t Training Loss: 46.72749216079712\n",
      "Epoch 52 \t Batch 420 \t Training Loss: 46.70291666303362\n",
      "Epoch 52 \t Batch 440 \t Training Loss: 46.709974210912534\n",
      "Epoch 52 \t Batch 460 \t Training Loss: 46.74821562559708\n",
      "Epoch 52 \t Batch 480 \t Training Loss: 46.65407569408417\n",
      "Epoch 52 \t Batch 500 \t Training Loss: 46.676597885131834\n",
      "Epoch 52 \t Batch 520 \t Training Loss: 46.60949463477502\n",
      "Epoch 52 \t Batch 540 \t Training Loss: 46.65768499727602\n",
      "Epoch 52 \t Batch 560 \t Training Loss: 46.66435545512608\n",
      "Epoch 52 \t Batch 580 \t Training Loss: 46.66357805317846\n",
      "Epoch 52 \t Batch 600 \t Training Loss: 46.63574191411336\n",
      "Epoch 52 \t Batch 620 \t Training Loss: 46.64320124349287\n",
      "Epoch 52 \t Batch 640 \t Training Loss: 46.61307032704353\n",
      "Epoch 52 \t Batch 660 \t Training Loss: 46.610147938583836\n",
      "Epoch 52 \t Batch 680 \t Training Loss: 46.60227773329791\n",
      "Epoch 52 \t Batch 700 \t Training Loss: 46.62071702139718\n",
      "Epoch 52 \t Batch 720 \t Training Loss: 46.64360949728224\n",
      "Epoch 52 \t Batch 740 \t Training Loss: 46.64056207296011\n",
      "Epoch 52 \t Batch 760 \t Training Loss: 46.64700108578331\n",
      "Epoch 52 \t Batch 780 \t Training Loss: 46.60980725899721\n",
      "Epoch 52 \t Batch 800 \t Training Loss: 46.59376038074493\n",
      "Epoch 52 \t Batch 820 \t Training Loss: 46.541218204033086\n",
      "Epoch 52 \t Batch 840 \t Training Loss: 46.5511689776466\n",
      "Epoch 52 \t Batch 860 \t Training Loss: 46.550059695576515\n",
      "Epoch 52 \t Batch 880 \t Training Loss: 46.56997092420404\n",
      "Epoch 52 \t Batch 900 \t Training Loss: 46.584171795315214\n",
      "Epoch 52 \t Batch 20 \t Validation Loss: 20.788611078262328\n",
      "Epoch 52 \t Batch 40 \t Validation Loss: 24.281185698509216\n",
      "Epoch 52 \t Batch 60 \t Validation Loss: 23.312256161371867\n",
      "Epoch 52 \t Batch 80 \t Validation Loss: 23.680285680294038\n",
      "Epoch 52 \t Batch 100 \t Validation Loss: 24.803353872299194\n",
      "Epoch 52 \t Batch 120 \t Validation Loss: 25.90300648212433\n",
      "Epoch 52 \t Batch 140 \t Validation Loss: 26.310420179367064\n",
      "Epoch 52 \t Batch 160 \t Validation Loss: 27.99580745100975\n",
      "Epoch 52 \t Batch 180 \t Validation Loss: 31.46671765115526\n",
      "Epoch 52 \t Batch 200 \t Validation Loss: 32.78290516376495\n",
      "Epoch 52 \t Batch 220 \t Validation Loss: 33.84474427916787\n",
      "Epoch 52 \t Batch 240 \t Validation Loss: 34.23643493254979\n",
      "Epoch 52 \t Batch 260 \t Validation Loss: 36.18839565057021\n",
      "Epoch 52 \t Batch 280 \t Validation Loss: 37.2033497265407\n",
      "Epoch 52 \t Batch 300 \t Validation Loss: 38.27864756266276\n",
      "Epoch 52 \t Batch 320 \t Validation Loss: 38.74677491188049\n",
      "Epoch 52 \t Batch 340 \t Validation Loss: 38.66947926353006\n",
      "Epoch 52 \t Batch 360 \t Validation Loss: 38.49323715633816\n",
      "Epoch 52 \t Batch 380 \t Validation Loss: 38.65724639892578\n",
      "Epoch 52 \t Batch 400 \t Validation Loss: 38.268239846229555\n",
      "Epoch 52 \t Batch 420 \t Validation Loss: 38.28989198321388\n",
      "Epoch 52 \t Batch 440 \t Validation Loss: 37.98575288599188\n",
      "Epoch 52 \t Batch 460 \t Validation Loss: 38.1636668122333\n",
      "Epoch 52 \t Batch 480 \t Validation Loss: 38.65513143936793\n",
      "Epoch 52 \t Batch 500 \t Validation Loss: 38.366106834411625\n",
      "Epoch 52 \t Batch 520 \t Validation Loss: 38.12212041708139\n",
      "Epoch 52 \t Batch 540 \t Validation Loss: 37.89730663652773\n",
      "Epoch 52 \t Batch 560 \t Validation Loss: 37.7135511466435\n",
      "Epoch 52 \t Batch 580 \t Validation Loss: 37.47151064050609\n",
      "Epoch 52 \t Batch 600 \t Validation Loss: 37.711475655237834\n",
      "Epoch 52 Training Loss: 46.56848886670827 Validation Loss: 38.39320414716547\n",
      "Epoch 52 completed\n",
      "Epoch 53 \t Batch 20 \t Training Loss: 47.64127941131592\n",
      "Epoch 53 \t Batch 40 \t Training Loss: 46.77404308319092\n",
      "Epoch 53 \t Batch 60 \t Training Loss: 46.37952632904053\n",
      "Epoch 53 \t Batch 80 \t Training Loss: 46.13612403869629\n",
      "Epoch 53 \t Batch 100 \t Training Loss: 46.21079368591309\n",
      "Epoch 53 \t Batch 120 \t Training Loss: 46.09955304463704\n",
      "Epoch 53 \t Batch 140 \t Training Loss: 45.99694578988211\n",
      "Epoch 53 \t Batch 160 \t Training Loss: 45.922847056388854\n",
      "Epoch 53 \t Batch 180 \t Training Loss: 45.849578878614636\n",
      "Epoch 53 \t Batch 200 \t Training Loss: 46.009033584594725\n",
      "Epoch 53 \t Batch 220 \t Training Loss: 46.14712196696888\n",
      "Epoch 53 \t Batch 240 \t Training Loss: 46.216132465998335\n",
      "Epoch 53 \t Batch 260 \t Training Loss: 46.146571056659404\n",
      "Epoch 53 \t Batch 280 \t Training Loss: 46.140755912235804\n",
      "Epoch 53 \t Batch 300 \t Training Loss: 46.248630650838216\n",
      "Epoch 53 \t Batch 320 \t Training Loss: 46.263650715351105\n",
      "Epoch 53 \t Batch 340 \t Training Loss: 46.23256913353415\n",
      "Epoch 53 \t Batch 360 \t Training Loss: 46.23860922919379\n",
      "Epoch 53 \t Batch 380 \t Training Loss: 46.38346041629189\n",
      "Epoch 53 \t Batch 400 \t Training Loss: 46.386098642349246\n",
      "Epoch 53 \t Batch 420 \t Training Loss: 46.452072334289554\n",
      "Epoch 53 \t Batch 440 \t Training Loss: 46.42936528812755\n",
      "Epoch 53 \t Batch 460 \t Training Loss: 46.461895337312114\n",
      "Epoch 53 \t Batch 480 \t Training Loss: 46.46168315410614\n",
      "Epoch 53 \t Batch 500 \t Training Loss: 46.49215551757813\n",
      "Epoch 53 \t Batch 520 \t Training Loss: 46.49034959352934\n",
      "Epoch 53 \t Batch 540 \t Training Loss: 46.459938741613314\n",
      "Epoch 53 \t Batch 560 \t Training Loss: 46.49289566448757\n",
      "Epoch 53 \t Batch 580 \t Training Loss: 46.51796811202477\n",
      "Epoch 53 \t Batch 600 \t Training Loss: 46.54698209126791\n",
      "Epoch 53 \t Batch 620 \t Training Loss: 46.52819254475255\n",
      "Epoch 53 \t Batch 640 \t Training Loss: 46.5314720928669\n",
      "Epoch 53 \t Batch 660 \t Training Loss: 46.52310799107407\n",
      "Epoch 53 \t Batch 680 \t Training Loss: 46.5438824597527\n",
      "Epoch 53 \t Batch 700 \t Training Loss: 46.54193915230887\n",
      "Epoch 53 \t Batch 720 \t Training Loss: 46.575790357589725\n",
      "Epoch 53 \t Batch 740 \t Training Loss: 46.63277955441862\n",
      "Epoch 53 \t Batch 760 \t Training Loss: 46.61882466767964\n",
      "Epoch 53 \t Batch 780 \t Training Loss: 46.62586786319048\n",
      "Epoch 53 \t Batch 800 \t Training Loss: 46.62104789733887\n",
      "Epoch 53 \t Batch 820 \t Training Loss: 46.57385466505841\n",
      "Epoch 53 \t Batch 840 \t Training Loss: 46.574831658317926\n",
      "Epoch 53 \t Batch 860 \t Training Loss: 46.57254722062932\n",
      "Epoch 53 \t Batch 880 \t Training Loss: 46.55744589458812\n",
      "Epoch 53 \t Batch 900 \t Training Loss: 46.575127860175236\n",
      "Epoch 53 \t Batch 20 \t Validation Loss: 26.691840600967407\n",
      "Epoch 53 \t Batch 40 \t Validation Loss: 26.443907690048217\n",
      "Epoch 53 \t Batch 60 \t Validation Loss: 26.685507361094157\n",
      "Epoch 53 \t Batch 80 \t Validation Loss: 26.83793807029724\n",
      "Epoch 53 \t Batch 100 \t Validation Loss: 27.519690475463868\n",
      "Epoch 53 \t Batch 120 \t Validation Loss: 28.221689589818318\n",
      "Epoch 53 \t Batch 140 \t Validation Loss: 28.320814432416643\n",
      "Epoch 53 \t Batch 160 \t Validation Loss: 29.988115775585175\n",
      "Epoch 53 \t Batch 180 \t Validation Loss: 33.39953373803033\n",
      "Epoch 53 \t Batch 200 \t Validation Loss: 34.71753529548645\n",
      "Epoch 53 \t Batch 220 \t Validation Loss: 35.81824649464\n",
      "Epoch 53 \t Batch 240 \t Validation Loss: 36.17325710058212\n",
      "Epoch 53 \t Batch 260 \t Validation Loss: 38.13590934093182\n",
      "Epoch 53 \t Batch 280 \t Validation Loss: 39.12468509674072\n",
      "Epoch 53 \t Batch 300 \t Validation Loss: 40.185054175059\n",
      "Epoch 53 \t Batch 320 \t Validation Loss: 40.641201198101044\n",
      "Epoch 53 \t Batch 340 \t Validation Loss: 40.499871758853686\n",
      "Epoch 53 \t Batch 360 \t Validation Loss: 40.297848304112755\n",
      "Epoch 53 \t Batch 380 \t Validation Loss: 40.480023660157855\n",
      "Epoch 53 \t Batch 400 \t Validation Loss: 40.018011274337766\n",
      "Epoch 53 \t Batch 420 \t Validation Loss: 40.00671166238331\n",
      "Epoch 53 \t Batch 440 \t Validation Loss: 39.6642459934408\n",
      "Epoch 53 \t Batch 460 \t Validation Loss: 39.91885666017947\n",
      "Epoch 53 \t Batch 480 \t Validation Loss: 40.36793625156085\n",
      "Epoch 53 \t Batch 500 \t Validation Loss: 40.08706478691101\n",
      "Epoch 53 \t Batch 520 \t Validation Loss: 39.89495418438545\n",
      "Epoch 53 \t Batch 540 \t Validation Loss: 39.556854482933325\n",
      "Epoch 53 \t Batch 560 \t Validation Loss: 39.26152810539518\n",
      "Epoch 53 \t Batch 580 \t Validation Loss: 38.90350865495616\n",
      "Epoch 53 \t Batch 600 \t Validation Loss: 39.046994182268776\n",
      "Epoch 53 Training Loss: 46.56846750939555 Validation Loss: 39.6457574042407\n",
      "Epoch 53 completed\n",
      "Epoch 54 \t Batch 20 \t Training Loss: 47.667758560180665\n",
      "Epoch 54 \t Batch 40 \t Training Loss: 47.66159830093384\n",
      "Epoch 54 \t Batch 60 \t Training Loss: 46.99982846577962\n",
      "Epoch 54 \t Batch 80 \t Training Loss: 46.72148313522339\n",
      "Epoch 54 \t Batch 100 \t Training Loss: 46.89821361541748\n",
      "Epoch 54 \t Batch 120 \t Training Loss: 46.718360137939456\n",
      "Epoch 54 \t Batch 140 \t Training Loss: 46.61458653041294\n",
      "Epoch 54 \t Batch 160 \t Training Loss: 46.59121589660644\n",
      "Epoch 54 \t Batch 180 \t Training Loss: 46.405451435512965\n",
      "Epoch 54 \t Batch 200 \t Training Loss: 46.314462947845456\n",
      "Epoch 54 \t Batch 220 \t Training Loss: 46.31959134882147\n",
      "Epoch 54 \t Batch 240 \t Training Loss: 46.372849766413374\n",
      "Epoch 54 \t Batch 260 \t Training Loss: 46.45788663717417\n",
      "Epoch 54 \t Batch 280 \t Training Loss: 46.479161616734096\n",
      "Epoch 54 \t Batch 300 \t Training Loss: 46.36841899871826\n",
      "Epoch 54 \t Batch 320 \t Training Loss: 46.32357729673386\n",
      "Epoch 54 \t Batch 340 \t Training Loss: 46.373059665455536\n",
      "Epoch 54 \t Batch 360 \t Training Loss: 46.43617861005995\n",
      "Epoch 54 \t Batch 380 \t Training Loss: 46.41984758879009\n",
      "Epoch 54 \t Batch 400 \t Training Loss: 46.39131345748901\n",
      "Epoch 54 \t Batch 420 \t Training Loss: 46.31893536703927\n",
      "Epoch 54 \t Batch 440 \t Training Loss: 46.35794508673928\n",
      "Epoch 54 \t Batch 460 \t Training Loss: 46.36936447309411\n",
      "Epoch 54 \t Batch 480 \t Training Loss: 46.36070144971212\n",
      "Epoch 54 \t Batch 500 \t Training Loss: 46.36945290374756\n",
      "Epoch 54 \t Batch 520 \t Training Loss: 46.33552298912635\n",
      "Epoch 54 \t Batch 540 \t Training Loss: 46.316573503282335\n",
      "Epoch 54 \t Batch 560 \t Training Loss: 46.37481610434396\n",
      "Epoch 54 \t Batch 580 \t Training Loss: 46.3829820369852\n",
      "Epoch 54 \t Batch 600 \t Training Loss: 46.401163221995034\n",
      "Epoch 54 \t Batch 620 \t Training Loss: 46.40640404608942\n",
      "Epoch 54 \t Batch 640 \t Training Loss: 46.36586334109306\n",
      "Epoch 54 \t Batch 660 \t Training Loss: 46.34469117829294\n",
      "Epoch 54 \t Batch 680 \t Training Loss: 46.35369743459365\n",
      "Epoch 54 \t Batch 700 \t Training Loss: 46.37453396388462\n",
      "Epoch 54 \t Batch 720 \t Training Loss: 46.36188941531711\n",
      "Epoch 54 \t Batch 740 \t Training Loss: 46.35600099305849\n",
      "Epoch 54 \t Batch 760 \t Training Loss: 46.38333821045725\n",
      "Epoch 54 \t Batch 780 \t Training Loss: 46.42898858877329\n",
      "Epoch 54 \t Batch 800 \t Training Loss: 46.48896370887756\n",
      "Epoch 54 \t Batch 820 \t Training Loss: 46.484857149821956\n",
      "Epoch 54 \t Batch 840 \t Training Loss: 46.497678902035666\n",
      "Epoch 54 \t Batch 860 \t Training Loss: 46.4631426655969\n",
      "Epoch 54 \t Batch 880 \t Training Loss: 46.49808329668912\n",
      "Epoch 54 \t Batch 900 \t Training Loss: 46.530172466701934\n",
      "Epoch 54 \t Batch 20 \t Validation Loss: 21.546897315979002\n",
      "Epoch 54 \t Batch 40 \t Validation Loss: 23.799302434921266\n",
      "Epoch 54 \t Batch 60 \t Validation Loss: 23.492695077260336\n",
      "Epoch 54 \t Batch 80 \t Validation Loss: 23.876809763908387\n",
      "Epoch 54 \t Batch 100 \t Validation Loss: 24.909204177856445\n",
      "Epoch 54 \t Batch 120 \t Validation Loss: 25.98621629079183\n",
      "Epoch 54 \t Batch 140 \t Validation Loss: 26.369454710824147\n",
      "Epoch 54 \t Batch 160 \t Validation Loss: 28.17697055339813\n",
      "Epoch 54 \t Batch 180 \t Validation Loss: 31.68193811310662\n",
      "Epoch 54 \t Batch 200 \t Validation Loss: 33.06451491832733\n",
      "Epoch 54 \t Batch 220 \t Validation Loss: 34.270150457728995\n",
      "Epoch 54 \t Batch 240 \t Validation Loss: 34.70233551263809\n",
      "Epoch 54 \t Batch 260 \t Validation Loss: 36.74419820492084\n",
      "Epoch 54 \t Batch 280 \t Validation Loss: 37.847773276056564\n",
      "Epoch 54 \t Batch 300 \t Validation Loss: 38.90989661852519\n",
      "Epoch 54 \t Batch 320 \t Validation Loss: 39.38397531807423\n",
      "Epoch 54 \t Batch 340 \t Validation Loss: 39.29258857895346\n",
      "Epoch 54 \t Batch 360 \t Validation Loss: 39.121736545032924\n",
      "Epoch 54 \t Batch 380 \t Validation Loss: 39.34825557156613\n",
      "Epoch 54 \t Batch 400 \t Validation Loss: 38.941638553142546\n",
      "Epoch 54 \t Batch 420 \t Validation Loss: 38.96426386606126\n",
      "Epoch 54 \t Batch 440 \t Validation Loss: 38.659422397613525\n",
      "Epoch 54 \t Batch 460 \t Validation Loss: 38.89728645241779\n",
      "Epoch 54 \t Batch 480 \t Validation Loss: 39.37278960943222\n",
      "Epoch 54 \t Batch 500 \t Validation Loss: 39.093207069396975\n",
      "Epoch 54 \t Batch 520 \t Validation Loss: 38.87490670130803\n",
      "Epoch 54 \t Batch 540 \t Validation Loss: 38.607734542422826\n",
      "Epoch 54 \t Batch 560 \t Validation Loss: 38.3908004113606\n",
      "Epoch 54 \t Batch 580 \t Validation Loss: 38.15709546516682\n",
      "Epoch 54 \t Batch 600 \t Validation Loss: 38.36015513737996\n",
      "Epoch 54 Training Loss: 46.52015756338737 Validation Loss: 39.0160101388956\n",
      "Epoch 54 completed\n",
      "Epoch 55 \t Batch 20 \t Training Loss: 46.91788558959961\n",
      "Epoch 55 \t Batch 40 \t Training Loss: 46.978044509887695\n",
      "Epoch 55 \t Batch 60 \t Training Loss: 46.71119702657064\n",
      "Epoch 55 \t Batch 80 \t Training Loss: 46.64101552963257\n",
      "Epoch 55 \t Batch 100 \t Training Loss: 46.5855961227417\n",
      "Epoch 55 \t Batch 120 \t Training Loss: 46.546176719665525\n",
      "Epoch 55 \t Batch 140 \t Training Loss: 46.3235597882952\n",
      "Epoch 55 \t Batch 160 \t Training Loss: 46.16173872947693\n",
      "Epoch 55 \t Batch 180 \t Training Loss: 46.230422274271646\n",
      "Epoch 55 \t Batch 200 \t Training Loss: 46.41609321594238\n",
      "Epoch 55 \t Batch 220 \t Training Loss: 46.29671840667724\n",
      "Epoch 55 \t Batch 240 \t Training Loss: 46.203259547551475\n",
      "Epoch 55 \t Batch 260 \t Training Loss: 46.303149311359114\n",
      "Epoch 55 \t Batch 280 \t Training Loss: 46.27594902856009\n",
      "Epoch 55 \t Batch 300 \t Training Loss: 46.232815945943194\n",
      "Epoch 55 \t Batch 320 \t Training Loss: 46.38134799003601\n",
      "Epoch 55 \t Batch 340 \t Training Loss: 46.34797690896427\n",
      "Epoch 55 \t Batch 360 \t Training Loss: 46.311643271976045\n",
      "Epoch 55 \t Batch 380 \t Training Loss: 46.29657706210488\n",
      "Epoch 55 \t Batch 400 \t Training Loss: 46.297527141571045\n",
      "Epoch 55 \t Batch 420 \t Training Loss: 46.3653252919515\n",
      "Epoch 55 \t Batch 440 \t Training Loss: 46.45859337720004\n",
      "Epoch 55 \t Batch 460 \t Training Loss: 46.47557498268459\n",
      "Epoch 55 \t Batch 480 \t Training Loss: 46.512052512168886\n",
      "Epoch 55 \t Batch 500 \t Training Loss: 46.53112967681885\n",
      "Epoch 55 \t Batch 520 \t Training Loss: 46.59716241543109\n",
      "Epoch 55 \t Batch 540 \t Training Loss: 46.52580755021837\n",
      "Epoch 55 \t Batch 560 \t Training Loss: 46.574415063858034\n",
      "Epoch 55 \t Batch 580 \t Training Loss: 46.54341167581492\n",
      "Epoch 55 \t Batch 600 \t Training Loss: 46.57039164861043\n",
      "Epoch 55 \t Batch 620 \t Training Loss: 46.56155985555341\n",
      "Epoch 55 \t Batch 640 \t Training Loss: 46.55388171672821\n",
      "Epoch 55 \t Batch 660 \t Training Loss: 46.549251689332905\n",
      "Epoch 55 \t Batch 680 \t Training Loss: 46.50672623129452\n",
      "Epoch 55 \t Batch 700 \t Training Loss: 46.534008004324775\n",
      "Epoch 55 \t Batch 720 \t Training Loss: 46.538125069936115\n",
      "Epoch 55 \t Batch 740 \t Training Loss: 46.54664262823157\n",
      "Epoch 55 \t Batch 760 \t Training Loss: 46.54921291250932\n",
      "Epoch 55 \t Batch 780 \t Training Loss: 46.52392170245831\n",
      "Epoch 55 \t Batch 800 \t Training Loss: 46.517579774856564\n",
      "Epoch 55 \t Batch 820 \t Training Loss: 46.50661197755395\n",
      "Epoch 55 \t Batch 840 \t Training Loss: 46.464795630318775\n",
      "Epoch 55 \t Batch 860 \t Training Loss: 46.49611925080765\n",
      "Epoch 55 \t Batch 880 \t Training Loss: 46.47516006122936\n",
      "Epoch 55 \t Batch 900 \t Training Loss: 46.489276555379234\n",
      "Epoch 55 \t Batch 20 \t Validation Loss: 24.590807914733887\n",
      "Epoch 55 \t Batch 40 \t Validation Loss: 27.336683082580567\n",
      "Epoch 55 \t Batch 60 \t Validation Loss: 26.750344022115073\n",
      "Epoch 55 \t Batch 80 \t Validation Loss: 27.408206462860107\n",
      "Epoch 55 \t Batch 100 \t Validation Loss: 28.23732656478882\n",
      "Epoch 55 \t Batch 120 \t Validation Loss: 28.908177042007445\n",
      "Epoch 55 \t Batch 140 \t Validation Loss: 28.90013858250209\n",
      "Epoch 55 \t Batch 160 \t Validation Loss: 30.074763917922972\n",
      "Epoch 55 \t Batch 180 \t Validation Loss: 32.7906791528066\n",
      "Epoch 55 \t Batch 200 \t Validation Loss: 33.61805688381195\n",
      "Epoch 55 \t Batch 220 \t Validation Loss: 34.317747033726086\n",
      "Epoch 55 \t Batch 240 \t Validation Loss: 34.44538562297821\n",
      "Epoch 55 \t Batch 260 \t Validation Loss: 36.04767755728501\n",
      "Epoch 55 \t Batch 280 \t Validation Loss: 36.80309813703809\n",
      "Epoch 55 \t Batch 300 \t Validation Loss: 37.70100012143453\n",
      "Epoch 55 \t Batch 320 \t Validation Loss: 38.0562142431736\n",
      "Epoch 55 \t Batch 340 \t Validation Loss: 37.94650619170245\n",
      "Epoch 55 \t Batch 360 \t Validation Loss: 37.74362203015222\n",
      "Epoch 55 \t Batch 380 \t Validation Loss: 37.899570050992466\n",
      "Epoch 55 \t Batch 400 \t Validation Loss: 37.53075865507126\n",
      "Epoch 55 \t Batch 420 \t Validation Loss: 37.59053839274815\n",
      "Epoch 55 \t Batch 440 \t Validation Loss: 37.32200253660029\n",
      "Epoch 55 \t Batch 460 \t Validation Loss: 37.592636361329454\n",
      "Epoch 55 \t Batch 480 \t Validation Loss: 38.09224395751953\n",
      "Epoch 55 \t Batch 500 \t Validation Loss: 37.810129123687744\n",
      "Epoch 55 \t Batch 520 \t Validation Loss: 37.61453870259798\n",
      "Epoch 55 \t Batch 540 \t Validation Loss: 37.39605615403917\n",
      "Epoch 55 \t Batch 560 \t Validation Loss: 37.214380863734654\n",
      "Epoch 55 \t Batch 580 \t Validation Loss: 36.963947993311386\n",
      "Epoch 55 \t Batch 600 \t Validation Loss: 37.21163489023844\n",
      "Epoch 55 Training Loss: 46.47953406181044 Validation Loss: 37.90423962977025\n",
      "Epoch 55 completed\n",
      "Epoch 56 \t Batch 20 \t Training Loss: 46.25765056610108\n",
      "Epoch 56 \t Batch 40 \t Training Loss: 46.030233192443845\n",
      "Epoch 56 \t Batch 60 \t Training Loss: 46.051088396708174\n",
      "Epoch 56 \t Batch 80 \t Training Loss: 46.08020858764648\n",
      "Epoch 56 \t Batch 100 \t Training Loss: 46.32205898284912\n",
      "Epoch 56 \t Batch 120 \t Training Loss: 46.32961467107137\n",
      "Epoch 56 \t Batch 140 \t Training Loss: 46.29857918875558\n",
      "Epoch 56 \t Batch 160 \t Training Loss: 46.22489635944366\n",
      "Epoch 56 \t Batch 180 \t Training Loss: 46.40514229668511\n",
      "Epoch 56 \t Batch 200 \t Training Loss: 46.46077140808106\n",
      "Epoch 56 \t Batch 220 \t Training Loss: 46.41619798486883\n",
      "Epoch 56 \t Batch 240 \t Training Loss: 46.52614351908366\n",
      "Epoch 56 \t Batch 260 \t Training Loss: 46.45091696519118\n",
      "Epoch 56 \t Batch 280 \t Training Loss: 46.50082115445818\n",
      "Epoch 56 \t Batch 300 \t Training Loss: 46.47810540517171\n",
      "Epoch 56 \t Batch 320 \t Training Loss: 46.466915380954745\n",
      "Epoch 56 \t Batch 340 \t Training Loss: 46.5510555267334\n",
      "Epoch 56 \t Batch 360 \t Training Loss: 46.60775460137261\n",
      "Epoch 56 \t Batch 380 \t Training Loss: 46.56851877915232\n",
      "Epoch 56 \t Batch 400 \t Training Loss: 46.4934516620636\n",
      "Epoch 56 \t Batch 420 \t Training Loss: 46.47917602175758\n",
      "Epoch 56 \t Batch 440 \t Training Loss: 46.45518353202126\n",
      "Epoch 56 \t Batch 460 \t Training Loss: 46.422944939654805\n",
      "Epoch 56 \t Batch 480 \t Training Loss: 46.43789180914561\n",
      "Epoch 56 \t Batch 500 \t Training Loss: 46.44673206329346\n",
      "Epoch 56 \t Batch 520 \t Training Loss: 46.431204399695766\n",
      "Epoch 56 \t Batch 540 \t Training Loss: 46.4435209274292\n",
      "Epoch 56 \t Batch 560 \t Training Loss: 46.450005762917655\n",
      "Epoch 56 \t Batch 580 \t Training Loss: 46.45367292206863\n",
      "Epoch 56 \t Batch 600 \t Training Loss: 46.45271681467692\n",
      "Epoch 56 \t Batch 620 \t Training Loss: 46.43185724443005\n",
      "Epoch 56 \t Batch 640 \t Training Loss: 46.432801419496535\n",
      "Epoch 56 \t Batch 660 \t Training Loss: 46.50255610148112\n",
      "Epoch 56 \t Batch 680 \t Training Loss: 46.48399346295525\n",
      "Epoch 56 \t Batch 700 \t Training Loss: 46.48272571563721\n",
      "Epoch 56 \t Batch 720 \t Training Loss: 46.46646318435669\n",
      "Epoch 56 \t Batch 740 \t Training Loss: 46.507203421721584\n",
      "Epoch 56 \t Batch 760 \t Training Loss: 46.498727793442576\n",
      "Epoch 56 \t Batch 780 \t Training Loss: 46.49223163800362\n",
      "Epoch 56 \t Batch 800 \t Training Loss: 46.46441659450531\n",
      "Epoch 56 \t Batch 820 \t Training Loss: 46.46166768888148\n",
      "Epoch 56 \t Batch 840 \t Training Loss: 46.451365520840596\n",
      "Epoch 56 \t Batch 860 \t Training Loss: 46.470511667118515\n",
      "Epoch 56 \t Batch 880 \t Training Loss: 46.477551373568446\n",
      "Epoch 56 \t Batch 900 \t Training Loss: 46.451320843166776\n",
      "Epoch 56 \t Batch 20 \t Validation Loss: 20.986906957626342\n",
      "Epoch 56 \t Batch 40 \t Validation Loss: 23.766689348220826\n",
      "Epoch 56 \t Batch 60 \t Validation Loss: 23.332371044158936\n",
      "Epoch 56 \t Batch 80 \t Validation Loss: 23.86223566532135\n",
      "Epoch 56 \t Batch 100 \t Validation Loss: 25.073881187438964\n",
      "Epoch 56 \t Batch 120 \t Validation Loss: 26.177971839904785\n",
      "Epoch 56 \t Batch 140 \t Validation Loss: 26.56786012649536\n",
      "Epoch 56 \t Batch 160 \t Validation Loss: 28.20516619682312\n",
      "Epoch 56 \t Batch 180 \t Validation Loss: 31.538449329800077\n",
      "Epoch 56 \t Batch 200 \t Validation Loss: 32.77799700737\n",
      "Epoch 56 \t Batch 220 \t Validation Loss: 33.87881465391679\n",
      "Epoch 56 \t Batch 240 \t Validation Loss: 34.267324447631836\n",
      "Epoch 56 \t Batch 260 \t Validation Loss: 36.19809324924763\n",
      "Epoch 56 \t Batch 280 \t Validation Loss: 37.24242235251835\n",
      "Epoch 56 \t Batch 300 \t Validation Loss: 38.27390777269999\n",
      "Epoch 56 \t Batch 320 \t Validation Loss: 38.733773812651634\n",
      "Epoch 56 \t Batch 340 \t Validation Loss: 38.65567544768838\n",
      "Epoch 56 \t Batch 360 \t Validation Loss: 38.46461661921607\n",
      "Epoch 56 \t Batch 380 \t Validation Loss: 38.66021648959109\n",
      "Epoch 56 \t Batch 400 \t Validation Loss: 38.26443439245224\n",
      "Epoch 56 \t Batch 420 \t Validation Loss: 38.28908313342503\n",
      "Epoch 56 \t Batch 440 \t Validation Loss: 38.0048469435085\n",
      "Epoch 56 \t Batch 460 \t Validation Loss: 38.24114902123161\n",
      "Epoch 56 \t Batch 480 \t Validation Loss: 38.70722806652387\n",
      "Epoch 56 \t Batch 500 \t Validation Loss: 38.426158464431765\n",
      "Epoch 56 \t Batch 520 \t Validation Loss: 38.194714137224054\n",
      "Epoch 56 \t Batch 540 \t Validation Loss: 37.961640126616864\n",
      "Epoch 56 \t Batch 560 \t Validation Loss: 37.78217567546027\n",
      "Epoch 56 \t Batch 580 \t Validation Loss: 37.52263704661665\n",
      "Epoch 56 \t Batch 600 \t Validation Loss: 37.75875984668732\n",
      "Epoch 56 Training Loss: 46.46471714115455 Validation Loss: 38.419878156154184\n",
      "Epoch 56 completed\n",
      "Epoch 57 \t Batch 20 \t Training Loss: 46.442027473449706\n",
      "Epoch 57 \t Batch 40 \t Training Loss: 46.16105661392212\n",
      "Epoch 57 \t Batch 60 \t Training Loss: 46.31503410339356\n",
      "Epoch 57 \t Batch 80 \t Training Loss: 46.36052207946777\n",
      "Epoch 57 \t Batch 100 \t Training Loss: 46.17845329284668\n",
      "Epoch 57 \t Batch 120 \t Training Loss: 46.22155605951945\n",
      "Epoch 57 \t Batch 140 \t Training Loss: 46.34341294424875\n",
      "Epoch 57 \t Batch 160 \t Training Loss: 46.49665064811707\n",
      "Epoch 57 \t Batch 180 \t Training Loss: 46.58763126797146\n",
      "Epoch 57 \t Batch 200 \t Training Loss: 46.68916416168213\n",
      "Epoch 57 \t Batch 220 \t Training Loss: 46.77410465587269\n",
      "Epoch 57 \t Batch 240 \t Training Loss: 46.71705932617188\n",
      "Epoch 57 \t Batch 260 \t Training Loss: 46.65873245826134\n",
      "Epoch 57 \t Batch 280 \t Training Loss: 46.656158106667654\n",
      "Epoch 57 \t Batch 300 \t Training Loss: 46.56432159423828\n",
      "Epoch 57 \t Batch 320 \t Training Loss: 46.61424095630646\n",
      "Epoch 57 \t Batch 340 \t Training Loss: 46.5784132789163\n",
      "Epoch 57 \t Batch 360 \t Training Loss: 46.57206605275472\n",
      "Epoch 57 \t Batch 380 \t Training Loss: 46.496835768850225\n",
      "Epoch 57 \t Batch 400 \t Training Loss: 46.51668299674988\n",
      "Epoch 57 \t Batch 420 \t Training Loss: 46.49775763012114\n",
      "Epoch 57 \t Batch 440 \t Training Loss: 46.48455490632491\n",
      "Epoch 57 \t Batch 460 \t Training Loss: 46.43313664146092\n",
      "Epoch 57 \t Batch 480 \t Training Loss: 46.468121973673504\n",
      "Epoch 57 \t Batch 500 \t Training Loss: 46.46099171447754\n",
      "Epoch 57 \t Batch 520 \t Training Loss: 46.44348279512845\n",
      "Epoch 57 \t Batch 540 \t Training Loss: 46.420807661833585\n",
      "Epoch 57 \t Batch 560 \t Training Loss: 46.4239972455161\n",
      "Epoch 57 \t Batch 580 \t Training Loss: 46.43674094101478\n",
      "Epoch 57 \t Batch 600 \t Training Loss: 46.47964544932047\n",
      "Epoch 57 \t Batch 620 \t Training Loss: 46.4815456574963\n",
      "Epoch 57 \t Batch 640 \t Training Loss: 46.4106124162674\n",
      "Epoch 57 \t Batch 660 \t Training Loss: 46.45514481284402\n",
      "Epoch 57 \t Batch 680 \t Training Loss: 46.45025717230404\n",
      "Epoch 57 \t Batch 700 \t Training Loss: 46.43105359213693\n",
      "Epoch 57 \t Batch 720 \t Training Loss: 46.4378986676534\n",
      "Epoch 57 \t Batch 740 \t Training Loss: 46.42095685391813\n",
      "Epoch 57 \t Batch 760 \t Training Loss: 46.41326749701249\n",
      "Epoch 57 \t Batch 780 \t Training Loss: 46.401597712590146\n",
      "Epoch 57 \t Batch 800 \t Training Loss: 46.42010652542114\n",
      "Epoch 57 \t Batch 820 \t Training Loss: 46.43750139096888\n",
      "Epoch 57 \t Batch 840 \t Training Loss: 46.39115164620536\n",
      "Epoch 57 \t Batch 860 \t Training Loss: 46.38780917677769\n",
      "Epoch 57 \t Batch 880 \t Training Loss: 46.38298697905107\n",
      "Epoch 57 \t Batch 900 \t Training Loss: 46.42483800676134\n",
      "Epoch 57 \t Batch 20 \t Validation Loss: 17.183215188980103\n",
      "Epoch 57 \t Batch 40 \t Validation Loss: 20.56971800327301\n",
      "Epoch 57 \t Batch 60 \t Validation Loss: 20.201913944880168\n",
      "Epoch 57 \t Batch 80 \t Validation Loss: 21.036703443527223\n",
      "Epoch 57 \t Batch 100 \t Validation Loss: 22.719984302520754\n",
      "Epoch 57 \t Batch 120 \t Validation Loss: 24.0592720190684\n",
      "Epoch 57 \t Batch 140 \t Validation Loss: 24.61264509473528\n",
      "Epoch 57 \t Batch 160 \t Validation Loss: 26.41375414133072\n",
      "Epoch 57 \t Batch 180 \t Validation Loss: 29.665169996685453\n",
      "Epoch 57 \t Batch 200 \t Validation Loss: 30.917907128334047\n",
      "Epoch 57 \t Batch 220 \t Validation Loss: 32.0080631906336\n",
      "Epoch 57 \t Batch 240 \t Validation Loss: 32.40963747501373\n",
      "Epoch 57 \t Batch 260 \t Validation Loss: 34.33542624253493\n",
      "Epoch 57 \t Batch 280 \t Validation Loss: 35.39262113571167\n",
      "Epoch 57 \t Batch 300 \t Validation Loss: 36.41360193252564\n",
      "Epoch 57 \t Batch 320 \t Validation Loss: 36.89510422348976\n",
      "Epoch 57 \t Batch 340 \t Validation Loss: 36.88265895843506\n",
      "Epoch 57 \t Batch 360 \t Validation Loss: 36.73266149891747\n",
      "Epoch 57 \t Batch 380 \t Validation Loss: 36.972836020118315\n",
      "Epoch 57 \t Batch 400 \t Validation Loss: 36.64778930425644\n",
      "Epoch 57 \t Batch 420 \t Validation Loss: 36.730504814783735\n",
      "Epoch 57 \t Batch 440 \t Validation Loss: 36.505161109837616\n",
      "Epoch 57 \t Batch 460 \t Validation Loss: 36.80510509947072\n",
      "Epoch 57 \t Batch 480 \t Validation Loss: 37.329126971960065\n",
      "Epoch 57 \t Batch 500 \t Validation Loss: 37.087033708572385\n",
      "Epoch 57 \t Batch 520 \t Validation Loss: 36.897619362977835\n",
      "Epoch 57 \t Batch 540 \t Validation Loss: 36.69523830237212\n",
      "Epoch 57 \t Batch 560 \t Validation Loss: 36.52853760549\n",
      "Epoch 57 \t Batch 580 \t Validation Loss: 36.289463564445235\n",
      "Epoch 57 \t Batch 600 \t Validation Loss: 36.54969113826752\n",
      "Epoch 57 Training Loss: 46.45156435202096 Validation Loss: 37.22137459996459\n",
      "Epoch 57 completed\n",
      "Epoch 58 \t Batch 20 \t Training Loss: 47.58055934906006\n",
      "Epoch 58 \t Batch 40 \t Training Loss: 46.96147232055664\n",
      "Epoch 58 \t Batch 60 \t Training Loss: 47.21244500478109\n",
      "Epoch 58 \t Batch 80 \t Training Loss: 46.86720376014709\n",
      "Epoch 58 \t Batch 100 \t Training Loss: 46.69171977996826\n",
      "Epoch 58 \t Batch 120 \t Training Loss: 46.6187240600586\n",
      "Epoch 58 \t Batch 140 \t Training Loss: 46.51604987553188\n",
      "Epoch 58 \t Batch 160 \t Training Loss: 46.6093533039093\n",
      "Epoch 58 \t Batch 180 \t Training Loss: 46.71386142306858\n",
      "Epoch 58 \t Batch 200 \t Training Loss: 46.637792892456055\n",
      "Epoch 58 \t Batch 220 \t Training Loss: 46.58266459378329\n",
      "Epoch 58 \t Batch 240 \t Training Loss: 46.51052241325378\n",
      "Epoch 58 \t Batch 260 \t Training Loss: 46.45297073951134\n",
      "Epoch 58 \t Batch 280 \t Training Loss: 46.53708062853132\n",
      "Epoch 58 \t Batch 300 \t Training Loss: 46.611064427693684\n",
      "Epoch 58 \t Batch 320 \t Training Loss: 46.627510488033295\n",
      "Epoch 58 \t Batch 340 \t Training Loss: 46.53146284888773\n",
      "Epoch 58 \t Batch 360 \t Training Loss: 46.570018461015486\n",
      "Epoch 58 \t Batch 380 \t Training Loss: 46.50154370759663\n",
      "Epoch 58 \t Batch 400 \t Training Loss: 46.541968755722046\n",
      "Epoch 58 \t Batch 420 \t Training Loss: 46.59754585992722\n",
      "Epoch 58 \t Batch 440 \t Training Loss: 46.497553712671454\n",
      "Epoch 58 \t Batch 460 \t Training Loss: 46.47437069105065\n",
      "Epoch 58 \t Batch 480 \t Training Loss: 46.49794534842173\n",
      "Epoch 58 \t Batch 500 \t Training Loss: 46.47818259429932\n",
      "Epoch 58 \t Batch 520 \t Training Loss: 46.38481504733746\n",
      "Epoch 58 \t Batch 540 \t Training Loss: 46.41286222669813\n",
      "Epoch 58 \t Batch 560 \t Training Loss: 46.42837519645691\n",
      "Epoch 58 \t Batch 580 \t Training Loss: 46.48260478315682\n",
      "Epoch 58 \t Batch 600 \t Training Loss: 46.4791180229187\n",
      "Epoch 58 \t Batch 620 \t Training Loss: 46.42234386936311\n",
      "Epoch 58 \t Batch 640 \t Training Loss: 46.42931448817253\n",
      "Epoch 58 \t Batch 660 \t Training Loss: 46.385668534943555\n",
      "Epoch 58 \t Batch 680 \t Training Loss: 46.37412466161391\n",
      "Epoch 58 \t Batch 700 \t Training Loss: 46.40197174617222\n",
      "Epoch 58 \t Batch 720 \t Training Loss: 46.40053546163771\n",
      "Epoch 58 \t Batch 740 \t Training Loss: 46.38128582722432\n",
      "Epoch 58 \t Batch 760 \t Training Loss: 46.35358180999756\n",
      "Epoch 58 \t Batch 780 \t Training Loss: 46.359442016405936\n",
      "Epoch 58 \t Batch 800 \t Training Loss: 46.333615942001344\n",
      "Epoch 58 \t Batch 820 \t Training Loss: 46.36644689048209\n",
      "Epoch 58 \t Batch 840 \t Training Loss: 46.379671441941035\n",
      "Epoch 58 \t Batch 860 \t Training Loss: 46.37055820198946\n",
      "Epoch 58 \t Batch 880 \t Training Loss: 46.40762928615917\n",
      "Epoch 58 \t Batch 900 \t Training Loss: 46.414310150146484\n",
      "Epoch 58 \t Batch 20 \t Validation Loss: 24.37369637489319\n",
      "Epoch 58 \t Batch 40 \t Validation Loss: 25.536075091362\n",
      "Epoch 58 \t Batch 60 \t Validation Loss: 25.565055497487386\n",
      "Epoch 58 \t Batch 80 \t Validation Loss: 26.089318728446962\n",
      "Epoch 58 \t Batch 100 \t Validation Loss: 26.653897094726563\n",
      "Epoch 58 \t Batch 120 \t Validation Loss: 27.438046630223592\n",
      "Epoch 58 \t Batch 140 \t Validation Loss: 27.638830525534495\n",
      "Epoch 58 \t Batch 160 \t Validation Loss: 29.304746079444886\n",
      "Epoch 58 \t Batch 180 \t Validation Loss: 32.689586575826006\n",
      "Epoch 58 \t Batch 200 \t Validation Loss: 33.944670867919925\n",
      "Epoch 58 \t Batch 220 \t Validation Loss: 35.110844586112286\n",
      "Epoch 58 \t Batch 240 \t Validation Loss: 35.531477717558545\n",
      "Epoch 58 \t Batch 260 \t Validation Loss: 37.497191762924196\n",
      "Epoch 58 \t Batch 280 \t Validation Loss: 38.530385681561064\n",
      "Epoch 58 \t Batch 300 \t Validation Loss: 39.58773236910502\n",
      "Epoch 58 \t Batch 320 \t Validation Loss: 40.02315876185894\n",
      "Epoch 58 \t Batch 340 \t Validation Loss: 39.90195394123302\n",
      "Epoch 58 \t Batch 360 \t Validation Loss: 39.715877662764655\n",
      "Epoch 58 \t Batch 380 \t Validation Loss: 39.901113853956524\n",
      "Epoch 58 \t Batch 400 \t Validation Loss: 39.45057763814926\n",
      "Epoch 58 \t Batch 420 \t Validation Loss: 39.401180737359184\n",
      "Epoch 58 \t Batch 440 \t Validation Loss: 39.04635904702273\n",
      "Epoch 58 \t Batch 460 \t Validation Loss: 39.27420711724655\n",
      "Epoch 58 \t Batch 480 \t Validation Loss: 39.709717426697416\n",
      "Epoch 58 \t Batch 500 \t Validation Loss: 39.40531431388855\n",
      "Epoch 58 \t Batch 520 \t Validation Loss: 39.161897408045256\n",
      "Epoch 58 \t Batch 540 \t Validation Loss: 38.917832175007575\n",
      "Epoch 58 \t Batch 560 \t Validation Loss: 38.73468441792897\n",
      "Epoch 58 \t Batch 580 \t Validation Loss: 38.54064843901273\n",
      "Epoch 58 \t Batch 600 \t Validation Loss: 38.74371407667796\n",
      "Epoch 58 Training Loss: 46.404938742534675 Validation Loss: 39.440338671981515\n",
      "Epoch 58 completed\n",
      "Epoch 59 \t Batch 20 \t Training Loss: 45.66048946380615\n",
      "Epoch 59 \t Batch 40 \t Training Loss: 46.53673143386841\n",
      "Epoch 59 \t Batch 60 \t Training Loss: 46.451798057556154\n",
      "Epoch 59 \t Batch 80 \t Training Loss: 46.141370344161984\n",
      "Epoch 59 \t Batch 100 \t Training Loss: 46.28034286499023\n",
      "Epoch 59 \t Batch 120 \t Training Loss: 46.134983730316165\n",
      "Epoch 59 \t Batch 140 \t Training Loss: 46.215861320495605\n",
      "Epoch 59 \t Batch 160 \t Training Loss: 46.32805247306824\n",
      "Epoch 59 \t Batch 180 \t Training Loss: 46.38621139526367\n",
      "Epoch 59 \t Batch 200 \t Training Loss: 46.367072162628176\n",
      "Epoch 59 \t Batch 220 \t Training Loss: 46.3293265949596\n",
      "Epoch 59 \t Batch 240 \t Training Loss: 46.28379068374634\n",
      "Epoch 59 \t Batch 260 \t Training Loss: 46.329890456566446\n",
      "Epoch 59 \t Batch 280 \t Training Loss: 46.374223123277936\n",
      "Epoch 59 \t Batch 300 \t Training Loss: 46.43102738698324\n",
      "Epoch 59 \t Batch 320 \t Training Loss: 46.48626304864884\n",
      "Epoch 59 \t Batch 340 \t Training Loss: 46.46614853354061\n",
      "Epoch 59 \t Batch 360 \t Training Loss: 46.54370420244005\n",
      "Epoch 59 \t Batch 380 \t Training Loss: 46.54239421643709\n",
      "Epoch 59 \t Batch 400 \t Training Loss: 46.46300402641296\n",
      "Epoch 59 \t Batch 420 \t Training Loss: 46.50255481175014\n",
      "Epoch 59 \t Batch 440 \t Training Loss: 46.50437554446134\n",
      "Epoch 59 \t Batch 460 \t Training Loss: 46.51282659613568\n",
      "Epoch 59 \t Batch 480 \t Training Loss: 46.483120210965474\n",
      "Epoch 59 \t Batch 500 \t Training Loss: 46.54095593261719\n",
      "Epoch 59 \t Batch 520 \t Training Loss: 46.54359288582435\n",
      "Epoch 59 \t Batch 540 \t Training Loss: 46.570269132543494\n",
      "Epoch 59 \t Batch 560 \t Training Loss: 46.566883884157455\n",
      "Epoch 59 \t Batch 580 \t Training Loss: 46.54602480921252\n",
      "Epoch 59 \t Batch 600 \t Training Loss: 46.51297681172689\n",
      "Epoch 59 \t Batch 620 \t Training Loss: 46.4656129652454\n",
      "Epoch 59 \t Batch 640 \t Training Loss: 46.43382051587105\n",
      "Epoch 59 \t Batch 660 \t Training Loss: 46.42676518758138\n",
      "Epoch 59 \t Batch 680 \t Training Loss: 46.39070229249842\n",
      "Epoch 59 \t Batch 700 \t Training Loss: 46.389239371163505\n",
      "Epoch 59 \t Batch 720 \t Training Loss: 46.37682560814751\n",
      "Epoch 59 \t Batch 740 \t Training Loss: 46.39981056419579\n",
      "Epoch 59 \t Batch 760 \t Training Loss: 46.37596127861425\n",
      "Epoch 59 \t Batch 780 \t Training Loss: 46.356823701124924\n",
      "Epoch 59 \t Batch 800 \t Training Loss: 46.367411437034605\n",
      "Epoch 59 \t Batch 820 \t Training Loss: 46.378557000509126\n",
      "Epoch 59 \t Batch 840 \t Training Loss: 46.36204725901286\n",
      "Epoch 59 \t Batch 860 \t Training Loss: 46.35734770575235\n",
      "Epoch 59 \t Batch 880 \t Training Loss: 46.360630403865464\n",
      "Epoch 59 \t Batch 900 \t Training Loss: 46.39719139946832\n",
      "Epoch 59 \t Batch 20 \t Validation Loss: 15.394275569915772\n",
      "Epoch 59 \t Batch 40 \t Validation Loss: 18.345996022224426\n",
      "Epoch 59 \t Batch 60 \t Validation Loss: 17.969136555989582\n",
      "Epoch 59 \t Batch 80 \t Validation Loss: 18.601155853271486\n",
      "Epoch 59 \t Batch 100 \t Validation Loss: 20.548289604187012\n",
      "Epoch 59 \t Batch 120 \t Validation Loss: 22.103086630503338\n",
      "Epoch 59 \t Batch 140 \t Validation Loss: 22.89487017222813\n",
      "Epoch 59 \t Batch 160 \t Validation Loss: 24.904875230789184\n",
      "Epoch 59 \t Batch 180 \t Validation Loss: 28.35375272432963\n",
      "Epoch 59 \t Batch 200 \t Validation Loss: 29.78093903064728\n",
      "Epoch 59 \t Batch 220 \t Validation Loss: 31.072689312154598\n",
      "Epoch 59 \t Batch 240 \t Validation Loss: 31.619529946645102\n",
      "Epoch 59 \t Batch 260 \t Validation Loss: 33.65211477279663\n",
      "Epoch 59 \t Batch 280 \t Validation Loss: 34.78767104489463\n",
      "Epoch 59 \t Batch 300 \t Validation Loss: 35.8463920434316\n",
      "Epoch 59 \t Batch 320 \t Validation Loss: 36.40841164290905\n",
      "Epoch 59 \t Batch 340 \t Validation Loss: 36.42245994175182\n",
      "Epoch 59 \t Batch 360 \t Validation Loss: 36.306601119041446\n",
      "Epoch 59 \t Batch 380 \t Validation Loss: 36.60888797107496\n",
      "Epoch 59 \t Batch 400 \t Validation Loss: 36.29559393167496\n",
      "Epoch 59 \t Batch 420 \t Validation Loss: 36.40633976573036\n",
      "Epoch 59 \t Batch 440 \t Validation Loss: 36.187642299045216\n",
      "Epoch 59 \t Batch 460 \t Validation Loss: 36.50366193315257\n",
      "Epoch 59 \t Batch 480 \t Validation Loss: 37.04427314400673\n",
      "Epoch 59 \t Batch 500 \t Validation Loss: 36.794820108413695\n",
      "Epoch 59 \t Batch 520 \t Validation Loss: 36.60343498816857\n",
      "Epoch 59 \t Batch 540 \t Validation Loss: 36.432621761604594\n",
      "Epoch 59 \t Batch 560 \t Validation Loss: 36.29874388149806\n",
      "Epoch 59 \t Batch 580 \t Validation Loss: 36.04522307823444\n",
      "Epoch 59 \t Batch 600 \t Validation Loss: 36.376012341181436\n",
      "Epoch 59 Training Loss: 46.396511481398456 Validation Loss: 37.0703143361327\n",
      "Epoch 59 completed\n",
      "Epoch 60 \t Batch 20 \t Training Loss: 46.64633045196533\n",
      "Epoch 60 \t Batch 40 \t Training Loss: 46.63817911148071\n",
      "Epoch 60 \t Batch 60 \t Training Loss: 46.93136342366537\n",
      "Epoch 60 \t Batch 80 \t Training Loss: 46.395929622650144\n",
      "Epoch 60 \t Batch 100 \t Training Loss: 46.508834495544434\n",
      "Epoch 60 \t Batch 120 \t Training Loss: 46.335171413421634\n",
      "Epoch 60 \t Batch 140 \t Training Loss: 46.23544594900949\n",
      "Epoch 60 \t Batch 160 \t Training Loss: 46.13858604431152\n",
      "Epoch 60 \t Batch 180 \t Training Loss: 46.11622066497803\n",
      "Epoch 60 \t Batch 200 \t Training Loss: 46.14001184463501\n",
      "Epoch 60 \t Batch 220 \t Training Loss: 46.15787988142534\n",
      "Epoch 60 \t Batch 240 \t Training Loss: 46.2388009707133\n",
      "Epoch 60 \t Batch 260 \t Training Loss: 46.241008142324596\n",
      "Epoch 60 \t Batch 280 \t Training Loss: 46.307258646828785\n",
      "Epoch 60 \t Batch 300 \t Training Loss: 46.34133324940999\n",
      "Epoch 60 \t Batch 320 \t Training Loss: 46.37148529291153\n",
      "Epoch 60 \t Batch 340 \t Training Loss: 46.44725648094626\n",
      "Epoch 60 \t Batch 360 \t Training Loss: 46.39733397165934\n",
      "Epoch 60 \t Batch 380 \t Training Loss: 46.431123924255374\n",
      "Epoch 60 \t Batch 400 \t Training Loss: 46.43262690544128\n",
      "Epoch 60 \t Batch 420 \t Training Loss: 46.43255687895275\n",
      "Epoch 60 \t Batch 440 \t Training Loss: 46.41452165950428\n",
      "Epoch 60 \t Batch 460 \t Training Loss: 46.424732954605766\n",
      "Epoch 60 \t Batch 480 \t Training Loss: 46.42812904516856\n",
      "Epoch 60 \t Batch 500 \t Training Loss: 46.431344528198245\n",
      "Epoch 60 \t Batch 520 \t Training Loss: 46.41133637795082\n",
      "Epoch 60 \t Batch 540 \t Training Loss: 46.41085774457013\n",
      "Epoch 60 \t Batch 560 \t Training Loss: 46.42621785572597\n",
      "Epoch 60 \t Batch 580 \t Training Loss: 46.411348296856055\n",
      "Epoch 60 \t Batch 600 \t Training Loss: 46.40900084813436\n",
      "Epoch 60 \t Batch 620 \t Training Loss: 46.40185233085386\n",
      "Epoch 60 \t Batch 640 \t Training Loss: 46.36113973855972\n",
      "Epoch 60 \t Batch 660 \t Training Loss: 46.370730457883894\n",
      "Epoch 60 \t Batch 680 \t Training Loss: 46.34769390330595\n",
      "Epoch 60 \t Batch 700 \t Training Loss: 46.36782895224435\n",
      "Epoch 60 \t Batch 720 \t Training Loss: 46.40937773386637\n",
      "Epoch 60 \t Batch 740 \t Training Loss: 46.39363277538403\n",
      "Epoch 60 \t Batch 760 \t Training Loss: 46.38811032144647\n",
      "Epoch 60 \t Batch 780 \t Training Loss: 46.41466236603566\n",
      "Epoch 60 \t Batch 800 \t Training Loss: 46.39033420562744\n",
      "Epoch 60 \t Batch 820 \t Training Loss: 46.38482505054009\n",
      "Epoch 60 \t Batch 840 \t Training Loss: 46.35975720995948\n",
      "Epoch 60 \t Batch 860 \t Training Loss: 46.34958201563636\n",
      "Epoch 60 \t Batch 880 \t Training Loss: 46.31983891833912\n",
      "Epoch 60 \t Batch 900 \t Training Loss: 46.32241325378418\n",
      "Epoch 60 \t Batch 20 \t Validation Loss: 15.906209325790405\n",
      "Epoch 60 \t Batch 40 \t Validation Loss: 18.647393465042114\n",
      "Epoch 60 \t Batch 60 \t Validation Loss: 18.433572483062743\n",
      "Epoch 60 \t Batch 80 \t Validation Loss: 19.247196280956267\n",
      "Epoch 60 \t Batch 100 \t Validation Loss: 21.271606779098512\n",
      "Epoch 60 \t Batch 120 \t Validation Loss: 22.670640166600545\n",
      "Epoch 60 \t Batch 140 \t Validation Loss: 23.48776544843401\n",
      "Epoch 60 \t Batch 160 \t Validation Loss: 25.777830874919893\n",
      "Epoch 60 \t Batch 180 \t Validation Loss: 29.665930710898504\n",
      "Epoch 60 \t Batch 200 \t Validation Loss: 31.395599331855774\n",
      "Epoch 60 \t Batch 220 \t Validation Loss: 32.878279360857874\n",
      "Epoch 60 \t Batch 240 \t Validation Loss: 33.49276419878006\n",
      "Epoch 60 \t Batch 260 \t Validation Loss: 35.764562181326056\n",
      "Epoch 60 \t Batch 280 \t Validation Loss: 37.031198852402824\n",
      "Epoch 60 \t Batch 300 \t Validation Loss: 38.21385909080505\n",
      "Epoch 60 \t Batch 320 \t Validation Loss: 38.76398166716099\n",
      "Epoch 60 \t Batch 340 \t Validation Loss: 38.74551814303678\n",
      "Epoch 60 \t Batch 360 \t Validation Loss: 38.66114458243052\n",
      "Epoch 60 \t Batch 380 \t Validation Loss: 38.96390082961634\n",
      "Epoch 60 \t Batch 400 \t Validation Loss: 38.59626517534256\n",
      "Epoch 60 \t Batch 420 \t Validation Loss: 38.62487698736645\n",
      "Epoch 60 \t Batch 440 \t Validation Loss: 38.33886510025371\n",
      "Epoch 60 \t Batch 460 \t Validation Loss: 38.627271082090296\n",
      "Epoch 60 \t Batch 480 \t Validation Loss: 39.135196977853774\n",
      "Epoch 60 \t Batch 500 \t Validation Loss: 38.892688756942746\n",
      "Epoch 60 \t Batch 520 \t Validation Loss: 38.711805591216454\n",
      "Epoch 60 \t Batch 540 \t Validation Loss: 38.52152448760138\n",
      "Epoch 60 \t Batch 560 \t Validation Loss: 38.376081315108706\n",
      "Epoch 60 \t Batch 580 \t Validation Loss: 38.25751316629607\n",
      "Epoch 60 \t Batch 600 \t Validation Loss: 38.50270215511322\n",
      "Epoch 60 Training Loss: 46.34846573541573 Validation Loss: 39.27690009482495\n",
      "Epoch 60 completed\n",
      "Epoch 61 \t Batch 20 \t Training Loss: 45.34423007965088\n",
      "Epoch 61 \t Batch 40 \t Training Loss: 46.42462215423584\n",
      "Epoch 61 \t Batch 60 \t Training Loss: 46.20954418182373\n",
      "Epoch 61 \t Batch 80 \t Training Loss: 46.10588083267212\n",
      "Epoch 61 \t Batch 100 \t Training Loss: 45.94990264892578\n",
      "Epoch 61 \t Batch 120 \t Training Loss: 45.82960316340129\n",
      "Epoch 61 \t Batch 140 \t Training Loss: 45.7958169392177\n",
      "Epoch 61 \t Batch 160 \t Training Loss: 45.90291578769684\n",
      "Epoch 61 \t Batch 180 \t Training Loss: 45.94779733022054\n",
      "Epoch 61 \t Batch 200 \t Training Loss: 46.01554773330688\n",
      "Epoch 61 \t Batch 220 \t Training Loss: 46.13818426999179\n",
      "Epoch 61 \t Batch 240 \t Training Loss: 46.106102021535236\n",
      "Epoch 61 \t Batch 260 \t Training Loss: 46.01405222966121\n",
      "Epoch 61 \t Batch 280 \t Training Loss: 45.93381438936506\n",
      "Epoch 61 \t Batch 300 \t Training Loss: 45.95829430898031\n",
      "Epoch 61 \t Batch 320 \t Training Loss: 45.97089149951935\n",
      "Epoch 61 \t Batch 340 \t Training Loss: 45.93250387977152\n",
      "Epoch 61 \t Batch 360 \t Training Loss: 46.06680526733398\n",
      "Epoch 61 \t Batch 380 \t Training Loss: 46.073771105314556\n",
      "Epoch 61 \t Batch 400 \t Training Loss: 46.19539939880371\n",
      "Epoch 61 \t Batch 420 \t Training Loss: 46.24434971582322\n",
      "Epoch 61 \t Batch 440 \t Training Loss: 46.29686714519154\n",
      "Epoch 61 \t Batch 460 \t Training Loss: 46.35354225324548\n",
      "Epoch 61 \t Batch 480 \t Training Loss: 46.34121197064717\n",
      "Epoch 61 \t Batch 500 \t Training Loss: 46.41993444061279\n",
      "Epoch 61 \t Batch 520 \t Training Loss: 46.34027550770686\n",
      "Epoch 61 \t Batch 540 \t Training Loss: 46.301608883893046\n",
      "Epoch 61 \t Batch 560 \t Training Loss: 46.26198603766305\n",
      "Epoch 61 \t Batch 580 \t Training Loss: 46.22827685125943\n",
      "Epoch 61 \t Batch 600 \t Training Loss: 46.18932704925537\n",
      "Epoch 61 \t Batch 620 \t Training Loss: 46.194590020948844\n",
      "Epoch 61 \t Batch 640 \t Training Loss: 46.29694812893867\n",
      "Epoch 61 \t Batch 660 \t Training Loss: 46.289125997369936\n",
      "Epoch 61 \t Batch 680 \t Training Loss: 46.30724280862247\n",
      "Epoch 61 \t Batch 700 \t Training Loss: 46.32841110229492\n",
      "Epoch 61 \t Batch 720 \t Training Loss: 46.298503732681276\n",
      "Epoch 61 \t Batch 740 \t Training Loss: 46.32159929017763\n",
      "Epoch 61 \t Batch 760 \t Training Loss: 46.300617037321395\n",
      "Epoch 61 \t Batch 780 \t Training Loss: 46.259030689337315\n",
      "Epoch 61 \t Batch 800 \t Training Loss: 46.280947213172915\n",
      "Epoch 61 \t Batch 820 \t Training Loss: 46.28764659602468\n",
      "Epoch 61 \t Batch 840 \t Training Loss: 46.298725119091216\n",
      "Epoch 61 \t Batch 860 \t Training Loss: 46.330868725444\n",
      "Epoch 61 \t Batch 880 \t Training Loss: 46.35398897691206\n",
      "Epoch 61 \t Batch 900 \t Training Loss: 46.35050162845188\n",
      "Epoch 61 \t Batch 20 \t Validation Loss: 17.227399349212646\n",
      "Epoch 61 \t Batch 40 \t Validation Loss: 20.495351099967955\n",
      "Epoch 61 \t Batch 60 \t Validation Loss: 19.738951031366984\n",
      "Epoch 61 \t Batch 80 \t Validation Loss: 20.37495502233505\n",
      "Epoch 61 \t Batch 100 \t Validation Loss: 22.138878355026247\n",
      "Epoch 61 \t Batch 120 \t Validation Loss: 23.453190716107688\n",
      "Epoch 61 \t Batch 140 \t Validation Loss: 24.079055070877075\n",
      "Epoch 61 \t Batch 160 \t Validation Loss: 25.792891043424607\n",
      "Epoch 61 \t Batch 180 \t Validation Loss: 28.895010418362087\n",
      "Epoch 61 \t Batch 200 \t Validation Loss: 30.22252311706543\n",
      "Epoch 61 \t Batch 220 \t Validation Loss: 31.225389810041946\n",
      "Epoch 61 \t Batch 240 \t Validation Loss: 31.57732105255127\n",
      "Epoch 61 \t Batch 260 \t Validation Loss: 33.47301139831543\n",
      "Epoch 61 \t Batch 280 \t Validation Loss: 34.50679012026106\n",
      "Epoch 61 \t Batch 300 \t Validation Loss: 35.44004097620646\n",
      "Epoch 61 \t Batch 320 \t Validation Loss: 35.89064507186413\n",
      "Epoch 61 \t Batch 340 \t Validation Loss: 35.92585297472337\n",
      "Epoch 61 \t Batch 360 \t Validation Loss: 35.76639793448978\n",
      "Epoch 61 \t Batch 380 \t Validation Loss: 36.033537736691926\n",
      "Epoch 61 \t Batch 400 \t Validation Loss: 35.780897524356845\n",
      "Epoch 61 \t Batch 420 \t Validation Loss: 35.92867966606504\n",
      "Epoch 61 \t Batch 440 \t Validation Loss: 35.77065523320978\n",
      "Epoch 61 \t Batch 460 \t Validation Loss: 36.15625778281171\n",
      "Epoch 61 \t Batch 480 \t Validation Loss: 36.7032457391421\n",
      "Epoch 61 \t Batch 500 \t Validation Loss: 36.48976919174194\n",
      "Epoch 61 \t Batch 520 \t Validation Loss: 36.41147624529325\n",
      "Epoch 61 \t Batch 540 \t Validation Loss: 36.22080209166916\n",
      "Epoch 61 \t Batch 560 \t Validation Loss: 36.09618284021105\n",
      "Epoch 61 \t Batch 580 \t Validation Loss: 35.928110231202226\n",
      "Epoch 61 \t Batch 600 \t Validation Loss: 36.20019090970357\n",
      "Epoch 61 Training Loss: 46.342116477575274 Validation Loss: 36.926680704215904\n",
      "Epoch 61 completed\n",
      "Epoch 62 \t Batch 20 \t Training Loss: 45.27164173126221\n",
      "Epoch 62 \t Batch 40 \t Training Loss: 46.138266468048094\n",
      "Epoch 62 \t Batch 60 \t Training Loss: 46.54865678151449\n",
      "Epoch 62 \t Batch 80 \t Training Loss: 46.94145965576172\n",
      "Epoch 62 \t Batch 100 \t Training Loss: 46.68976089477539\n",
      "Epoch 62 \t Batch 120 \t Training Loss: 46.705738639831544\n",
      "Epoch 62 \t Batch 140 \t Training Loss: 46.51673744746617\n",
      "Epoch 62 \t Batch 160 \t Training Loss: 46.37042841911316\n",
      "Epoch 62 \t Batch 180 \t Training Loss: 46.38152741326226\n",
      "Epoch 62 \t Batch 200 \t Training Loss: 46.38281904220581\n",
      "Epoch 62 \t Batch 220 \t Training Loss: 46.309774260087444\n",
      "Epoch 62 \t Batch 240 \t Training Loss: 46.36601831118266\n",
      "Epoch 62 \t Batch 260 \t Training Loss: 46.35848959409274\n",
      "Epoch 62 \t Batch 280 \t Training Loss: 46.28874827793666\n",
      "Epoch 62 \t Batch 300 \t Training Loss: 46.294182192484534\n",
      "Epoch 62 \t Batch 320 \t Training Loss: 46.325763034820554\n",
      "Epoch 62 \t Batch 340 \t Training Loss: 46.343087925630456\n",
      "Epoch 62 \t Batch 360 \t Training Loss: 46.3793704032898\n",
      "Epoch 62 \t Batch 380 \t Training Loss: 46.38016261050576\n",
      "Epoch 62 \t Batch 400 \t Training Loss: 46.299787940979\n",
      "Epoch 62 \t Batch 420 \t Training Loss: 46.338773909069246\n",
      "Epoch 62 \t Batch 440 \t Training Loss: 46.37538845755837\n",
      "Epoch 62 \t Batch 460 \t Training Loss: 46.42676393260127\n",
      "Epoch 62 \t Batch 480 \t Training Loss: 46.35681233406067\n",
      "Epoch 62 \t Batch 500 \t Training Loss: 46.3729254989624\n",
      "Epoch 62 \t Batch 520 \t Training Loss: 46.343870221651514\n",
      "Epoch 62 \t Batch 540 \t Training Loss: 46.31118594982006\n",
      "Epoch 62 \t Batch 560 \t Training Loss: 46.33471543448312\n",
      "Epoch 62 \t Batch 580 \t Training Loss: 46.354049281416266\n",
      "Epoch 62 \t Batch 600 \t Training Loss: 46.37926122029622\n",
      "Epoch 62 \t Batch 620 \t Training Loss: 46.35704468142602\n",
      "Epoch 62 \t Batch 640 \t Training Loss: 46.37918176054954\n",
      "Epoch 62 \t Batch 660 \t Training Loss: 46.34118086496989\n",
      "Epoch 62 \t Batch 680 \t Training Loss: 46.304967106089876\n",
      "Epoch 62 \t Batch 700 \t Training Loss: 46.30481573922294\n",
      "Epoch 62 \t Batch 720 \t Training Loss: 46.30367758538988\n",
      "Epoch 62 \t Batch 740 \t Training Loss: 46.325533335917704\n",
      "Epoch 62 \t Batch 760 \t Training Loss: 46.29163449437995\n",
      "Epoch 62 \t Batch 780 \t Training Loss: 46.31079540252686\n",
      "Epoch 62 \t Batch 800 \t Training Loss: 46.30000826835632\n",
      "Epoch 62 \t Batch 820 \t Training Loss: 46.3137749136948\n",
      "Epoch 62 \t Batch 840 \t Training Loss: 46.306601115635466\n",
      "Epoch 62 \t Batch 860 \t Training Loss: 46.3393731183784\n",
      "Epoch 62 \t Batch 880 \t Training Loss: 46.32163136655634\n",
      "Epoch 62 \t Batch 900 \t Training Loss: 46.30596271514892\n",
      "Epoch 62 \t Batch 20 \t Validation Loss: 19.188430738449096\n",
      "Epoch 62 \t Batch 40 \t Validation Loss: 21.453757810592652\n",
      "Epoch 62 \t Batch 60 \t Validation Loss: 21.076463635762533\n",
      "Epoch 62 \t Batch 80 \t Validation Loss: 21.62386189699173\n",
      "Epoch 62 \t Batch 100 \t Validation Loss: 23.010763177871706\n",
      "Epoch 62 \t Batch 120 \t Validation Loss: 24.103934772809346\n",
      "Epoch 62 \t Batch 140 \t Validation Loss: 24.606542934690204\n",
      "Epoch 62 \t Batch 160 \t Validation Loss: 26.369946998357772\n",
      "Epoch 62 \t Batch 180 \t Validation Loss: 29.712095250023737\n",
      "Epoch 62 \t Batch 200 \t Validation Loss: 31.076852006912233\n",
      "Epoch 62 \t Batch 220 \t Validation Loss: 32.18596280704845\n",
      "Epoch 62 \t Batch 240 \t Validation Loss: 32.606899964809415\n",
      "Epoch 62 \t Batch 260 \t Validation Loss: 34.56884034963755\n",
      "Epoch 62 \t Batch 280 \t Validation Loss: 35.642030354908535\n",
      "Epoch 62 \t Batch 300 \t Validation Loss: 36.64782474517822\n",
      "Epoch 62 \t Batch 320 \t Validation Loss: 37.125139421224596\n",
      "Epoch 62 \t Batch 340 \t Validation Loss: 37.09814321854535\n",
      "Epoch 62 \t Batch 360 \t Validation Loss: 36.929858424928454\n",
      "Epoch 62 \t Batch 380 \t Validation Loss: 37.17209579066226\n",
      "Epoch 62 \t Batch 400 \t Validation Loss: 36.83532765388489\n",
      "Epoch 62 \t Batch 420 \t Validation Loss: 36.90470967292786\n",
      "Epoch 62 \t Batch 440 \t Validation Loss: 36.65860631032424\n",
      "Epoch 62 \t Batch 460 \t Validation Loss: 36.98041120819423\n",
      "Epoch 62 \t Batch 480 \t Validation Loss: 37.50848827958107\n",
      "Epoch 62 \t Batch 500 \t Validation Loss: 37.25748746681214\n",
      "Epoch 62 \t Batch 520 \t Validation Loss: 37.094159265664906\n",
      "Epoch 62 \t Batch 540 \t Validation Loss: 36.883547132986564\n",
      "Epoch 62 \t Batch 560 \t Validation Loss: 36.69672884941101\n",
      "Epoch 62 \t Batch 580 \t Validation Loss: 36.44031683165451\n",
      "Epoch 62 \t Batch 600 \t Validation Loss: 36.7029932975769\n",
      "Epoch 62 Training Loss: 46.3172102192886 Validation Loss: 37.38380230866469\n",
      "Epoch 62 completed\n",
      "Epoch 63 \t Batch 20 \t Training Loss: 46.86773242950439\n",
      "Epoch 63 \t Batch 40 \t Training Loss: 46.47904605865479\n",
      "Epoch 63 \t Batch 60 \t Training Loss: 46.3564562479655\n",
      "Epoch 63 \t Batch 80 \t Training Loss: 46.148784971237184\n",
      "Epoch 63 \t Batch 100 \t Training Loss: 46.34793697357178\n",
      "Epoch 63 \t Batch 120 \t Training Loss: 46.45968723297119\n",
      "Epoch 63 \t Batch 140 \t Training Loss: 46.35463406699044\n",
      "Epoch 63 \t Batch 160 \t Training Loss: 46.40276110172272\n",
      "Epoch 63 \t Batch 180 \t Training Loss: 46.32182473076715\n",
      "Epoch 63 \t Batch 200 \t Training Loss: 46.40293043136597\n",
      "Epoch 63 \t Batch 220 \t Training Loss: 46.45831635215065\n",
      "Epoch 63 \t Batch 240 \t Training Loss: 46.40232327779134\n",
      "Epoch 63 \t Batch 260 \t Training Loss: 46.210877932035004\n",
      "Epoch 63 \t Batch 280 \t Training Loss: 46.28735719408308\n",
      "Epoch 63 \t Batch 300 \t Training Loss: 46.27156167348226\n",
      "Epoch 63 \t Batch 320 \t Training Loss: 46.411958682537076\n",
      "Epoch 63 \t Batch 340 \t Training Loss: 46.31669563966639\n",
      "Epoch 63 \t Batch 360 \t Training Loss: 46.37359636094835\n",
      "Epoch 63 \t Batch 380 \t Training Loss: 46.3804739901894\n",
      "Epoch 63 \t Batch 400 \t Training Loss: 46.356711769104\n",
      "Epoch 63 \t Batch 420 \t Training Loss: 46.41857122693743\n",
      "Epoch 63 \t Batch 440 \t Training Loss: 46.401817321777344\n",
      "Epoch 63 \t Batch 460 \t Training Loss: 46.38100975700047\n",
      "Epoch 63 \t Batch 480 \t Training Loss: 46.35290411313375\n",
      "Epoch 63 \t Batch 500 \t Training Loss: 46.33147408294678\n",
      "Epoch 63 \t Batch 520 \t Training Loss: 46.33074444257296\n",
      "Epoch 63 \t Batch 540 \t Training Loss: 46.32965189615886\n",
      "Epoch 63 \t Batch 560 \t Training Loss: 46.30896449770246\n",
      "Epoch 63 \t Batch 580 \t Training Loss: 46.34558170581686\n",
      "Epoch 63 \t Batch 600 \t Training Loss: 46.344575061798096\n",
      "Epoch 63 \t Batch 620 \t Training Loss: 46.30644668456047\n",
      "Epoch 63 \t Batch 640 \t Training Loss: 46.33479889631271\n",
      "Epoch 63 \t Batch 660 \t Training Loss: 46.29782236850623\n",
      "Epoch 63 \t Batch 680 \t Training Loss: 46.332033219056974\n",
      "Epoch 63 \t Batch 700 \t Training Loss: 46.34767613002232\n",
      "Epoch 63 \t Batch 720 \t Training Loss: 46.33544928232829\n",
      "Epoch 63 \t Batch 740 \t Training Loss: 46.37276082941004\n",
      "Epoch 63 \t Batch 760 \t Training Loss: 46.35328061957108\n",
      "Epoch 63 \t Batch 780 \t Training Loss: 46.30515922155136\n",
      "Epoch 63 \t Batch 800 \t Training Loss: 46.30968525409698\n",
      "Epoch 63 \t Batch 820 \t Training Loss: 46.290765078474834\n",
      "Epoch 63 \t Batch 840 \t Training Loss: 46.28706180481684\n",
      "Epoch 63 \t Batch 860 \t Training Loss: 46.29380065119544\n",
      "Epoch 63 \t Batch 880 \t Training Loss: 46.28024600202387\n",
      "Epoch 63 \t Batch 900 \t Training Loss: 46.28670286814372\n",
      "Epoch 63 \t Batch 20 \t Validation Loss: 17.89199275970459\n",
      "Epoch 63 \t Batch 40 \t Validation Loss: 20.498099064826967\n",
      "Epoch 63 \t Batch 60 \t Validation Loss: 20.009263912836712\n",
      "Epoch 63 \t Batch 80 \t Validation Loss: 20.514654922485352\n",
      "Epoch 63 \t Batch 100 \t Validation Loss: 22.218338565826414\n",
      "Epoch 63 \t Batch 120 \t Validation Loss: 23.60347462495168\n",
      "Epoch 63 \t Batch 140 \t Validation Loss: 24.287751027515956\n",
      "Epoch 63 \t Batch 160 \t Validation Loss: 26.427940994501114\n",
      "Epoch 63 \t Batch 180 \t Validation Loss: 30.32178800370958\n",
      "Epoch 63 \t Batch 200 \t Validation Loss: 31.96952799797058\n",
      "Epoch 63 \t Batch 220 \t Validation Loss: 33.34445312673395\n",
      "Epoch 63 \t Batch 240 \t Validation Loss: 33.936094303925834\n",
      "Epoch 63 \t Batch 260 \t Validation Loss: 36.10066014436575\n",
      "Epoch 63 \t Batch 280 \t Validation Loss: 37.268909328324455\n",
      "Epoch 63 \t Batch 300 \t Validation Loss: 38.53386162757874\n",
      "Epoch 63 \t Batch 320 \t Validation Loss: 39.11628873646259\n",
      "Epoch 63 \t Batch 340 \t Validation Loss: 39.06893406194799\n",
      "Epoch 63 \t Batch 360 \t Validation Loss: 38.957426857948306\n",
      "Epoch 63 \t Batch 380 \t Validation Loss: 39.20123623044867\n",
      "Epoch 63 \t Batch 400 \t Validation Loss: 38.802620027065274\n",
      "Epoch 63 \t Batch 420 \t Validation Loss: 38.85229092325483\n",
      "Epoch 63 \t Batch 440 \t Validation Loss: 38.557074462283744\n",
      "Epoch 63 \t Batch 460 \t Validation Loss: 38.822812563440074\n",
      "Epoch 63 \t Batch 480 \t Validation Loss: 39.32719418803851\n",
      "Epoch 63 \t Batch 500 \t Validation Loss: 39.08182350349426\n",
      "Epoch 63 \t Batch 520 \t Validation Loss: 38.88157637852889\n",
      "Epoch 63 \t Batch 540 \t Validation Loss: 38.60743004657604\n",
      "Epoch 63 \t Batch 560 \t Validation Loss: 38.369527879783085\n",
      "Epoch 63 \t Batch 580 \t Validation Loss: 38.06333481854406\n",
      "Epoch 63 \t Batch 600 \t Validation Loss: 38.263044244448345\n",
      "Epoch 63 Training Loss: 46.30668672584525 Validation Loss: 38.903299794568646\n",
      "Epoch 63 completed\n",
      "Epoch 64 \t Batch 20 \t Training Loss: 44.956271171569824\n",
      "Epoch 64 \t Batch 40 \t Training Loss: 45.01478881835938\n",
      "Epoch 64 \t Batch 60 \t Training Loss: 45.709017817179365\n",
      "Epoch 64 \t Batch 80 \t Training Loss: 45.83777093887329\n",
      "Epoch 64 \t Batch 100 \t Training Loss: 45.60835609436035\n",
      "Epoch 64 \t Batch 120 \t Training Loss: 45.667824776967365\n",
      "Epoch 64 \t Batch 140 \t Training Loss: 45.69536448887416\n",
      "Epoch 64 \t Batch 160 \t Training Loss: 45.875370383262634\n",
      "Epoch 64 \t Batch 180 \t Training Loss: 45.713630273607045\n",
      "Epoch 64 \t Batch 200 \t Training Loss: 45.75734432220459\n",
      "Epoch 64 \t Batch 220 \t Training Loss: 45.87520080913197\n",
      "Epoch 64 \t Batch 240 \t Training Loss: 45.949364026387535\n",
      "Epoch 64 \t Batch 260 \t Training Loss: 46.09733073894794\n",
      "Epoch 64 \t Batch 280 \t Training Loss: 46.15975750514439\n",
      "Epoch 64 \t Batch 300 \t Training Loss: 46.09251784006754\n",
      "Epoch 64 \t Batch 320 \t Training Loss: 45.99534697532654\n",
      "Epoch 64 \t Batch 340 \t Training Loss: 46.039709663391115\n",
      "Epoch 64 \t Batch 360 \t Training Loss: 46.00901718139649\n",
      "Epoch 64 \t Batch 380 \t Training Loss: 46.04679940876208\n",
      "Epoch 64 \t Batch 400 \t Training Loss: 46.102455110549926\n",
      "Epoch 64 \t Batch 420 \t Training Loss: 46.11167873200916\n",
      "Epoch 64 \t Batch 440 \t Training Loss: 46.13883303728971\n",
      "Epoch 64 \t Batch 460 \t Training Loss: 46.14507253895635\n",
      "Epoch 64 \t Batch 480 \t Training Loss: 46.27179075082143\n",
      "Epoch 64 \t Batch 500 \t Training Loss: 46.2584944152832\n",
      "Epoch 64 \t Batch 520 \t Training Loss: 46.19574331870446\n",
      "Epoch 64 \t Batch 540 \t Training Loss: 46.12024875923439\n",
      "Epoch 64 \t Batch 560 \t Training Loss: 46.06552978243147\n",
      "Epoch 64 \t Batch 580 \t Training Loss: 46.08320122422843\n",
      "Epoch 64 \t Batch 600 \t Training Loss: 46.0762691561381\n",
      "Epoch 64 \t Batch 620 \t Training Loss: 46.107600661247005\n",
      "Epoch 64 \t Batch 640 \t Training Loss: 46.13350207209587\n",
      "Epoch 64 \t Batch 660 \t Training Loss: 46.18572831587358\n",
      "Epoch 64 \t Batch 680 \t Training Loss: 46.183421443490424\n",
      "Epoch 64 \t Batch 700 \t Training Loss: 46.18232597896031\n",
      "Epoch 64 \t Batch 720 \t Training Loss: 46.172473928663464\n",
      "Epoch 64 \t Batch 740 \t Training Loss: 46.19916246775034\n",
      "Epoch 64 \t Batch 760 \t Training Loss: 46.23007527903506\n",
      "Epoch 64 \t Batch 780 \t Training Loss: 46.2027897859231\n",
      "Epoch 64 \t Batch 800 \t Training Loss: 46.23704240322113\n",
      "Epoch 64 \t Batch 820 \t Training Loss: 46.21460980671208\n",
      "Epoch 64 \t Batch 840 \t Training Loss: 46.186636093684605\n",
      "Epoch 64 \t Batch 860 \t Training Loss: 46.1856287357419\n",
      "Epoch 64 \t Batch 880 \t Training Loss: 46.194737165624446\n",
      "Epoch 64 \t Batch 900 \t Training Loss: 46.25281966315375\n",
      "Epoch 64 \t Batch 20 \t Validation Loss: 22.568461322784422\n",
      "Epoch 64 \t Batch 40 \t Validation Loss: 24.25785925388336\n",
      "Epoch 64 \t Batch 60 \t Validation Loss: 24.047367922465007\n",
      "Epoch 64 \t Batch 80 \t Validation Loss: 24.470228290557863\n",
      "Epoch 64 \t Batch 100 \t Validation Loss: 25.37796148300171\n",
      "Epoch 64 \t Batch 120 \t Validation Loss: 26.09241773287455\n",
      "Epoch 64 \t Batch 140 \t Validation Loss: 26.401518862588066\n",
      "Epoch 64 \t Batch 160 \t Validation Loss: 28.170365941524505\n",
      "Epoch 64 \t Batch 180 \t Validation Loss: 31.57765032450358\n",
      "Epoch 64 \t Batch 200 \t Validation Loss: 32.850034952163696\n",
      "Epoch 64 \t Batch 220 \t Validation Loss: 33.9593411619013\n",
      "Epoch 64 \t Batch 240 \t Validation Loss: 34.35543525616328\n",
      "Epoch 64 \t Batch 260 \t Validation Loss: 36.28377371567946\n",
      "Epoch 64 \t Batch 280 \t Validation Loss: 37.2904424054282\n",
      "Epoch 64 \t Batch 300 \t Validation Loss: 38.393694426218666\n",
      "Epoch 64 \t Batch 320 \t Validation Loss: 38.88040375113487\n",
      "Epoch 64 \t Batch 340 \t Validation Loss: 38.80001857420977\n",
      "Epoch 64 \t Batch 360 \t Validation Loss: 38.654171400600006\n",
      "Epoch 64 \t Batch 380 \t Validation Loss: 38.86564603353801\n",
      "Epoch 64 \t Batch 400 \t Validation Loss: 38.4668916964531\n",
      "Epoch 64 \t Batch 420 \t Validation Loss: 38.48252712885539\n",
      "Epoch 64 \t Batch 440 \t Validation Loss: 38.18016330545599\n",
      "Epoch 64 \t Batch 460 \t Validation Loss: 38.460548388439676\n",
      "Epoch 64 \t Batch 480 \t Validation Loss: 38.95331262350082\n",
      "Epoch 64 \t Batch 500 \t Validation Loss: 38.68517626571655\n",
      "Epoch 64 \t Batch 520 \t Validation Loss: 38.51031597577609\n",
      "Epoch 64 \t Batch 540 \t Validation Loss: 38.29774584946809\n",
      "Epoch 64 \t Batch 560 \t Validation Loss: 38.111558594022476\n",
      "Epoch 64 \t Batch 580 \t Validation Loss: 37.85746106114881\n",
      "Epoch 64 \t Batch 600 \t Validation Loss: 38.10949468930562\n",
      "Epoch 64 Training Loss: 46.25231869946129 Validation Loss: 38.77772882077601\n",
      "Epoch 64 completed\n",
      "Epoch 65 \t Batch 20 \t Training Loss: 46.58571357727051\n",
      "Epoch 65 \t Batch 40 \t Training Loss: 46.90013465881348\n",
      "Epoch 65 \t Batch 60 \t Training Loss: 46.92923170725505\n",
      "Epoch 65 \t Batch 80 \t Training Loss: 46.573199129104616\n",
      "Epoch 65 \t Batch 100 \t Training Loss: 46.25824977874756\n",
      "Epoch 65 \t Batch 120 \t Training Loss: 46.1804305712382\n",
      "Epoch 65 \t Batch 140 \t Training Loss: 46.12864824022566\n",
      "Epoch 65 \t Batch 160 \t Training Loss: 46.24586501121521\n",
      "Epoch 65 \t Batch 180 \t Training Loss: 46.53625250922309\n",
      "Epoch 65 \t Batch 200 \t Training Loss: 46.54983325958252\n",
      "Epoch 65 \t Batch 220 \t Training Loss: 46.53840389251709\n",
      "Epoch 65 \t Batch 240 \t Training Loss: 46.557321055730185\n",
      "Epoch 65 \t Batch 260 \t Training Loss: 46.510769154475284\n",
      "Epoch 65 \t Batch 280 \t Training Loss: 46.52542435782296\n",
      "Epoch 65 \t Batch 300 \t Training Loss: 46.62058216094971\n",
      "Epoch 65 \t Batch 320 \t Training Loss: 46.58103305101395\n",
      "Epoch 65 \t Batch 340 \t Training Loss: 46.50955314636231\n",
      "Epoch 65 \t Batch 360 \t Training Loss: 46.592449559105766\n",
      "Epoch 65 \t Batch 380 \t Training Loss: 46.514192400480574\n",
      "Epoch 65 \t Batch 400 \t Training Loss: 46.55045678138733\n",
      "Epoch 65 \t Batch 420 \t Training Loss: 46.54605160667783\n",
      "Epoch 65 \t Batch 440 \t Training Loss: 46.48635387420654\n",
      "Epoch 65 \t Batch 460 \t Training Loss: 46.46651272981063\n",
      "Epoch 65 \t Batch 480 \t Training Loss: 46.40259169737498\n",
      "Epoch 65 \t Batch 500 \t Training Loss: 46.37284093475342\n",
      "Epoch 65 \t Batch 520 \t Training Loss: 46.379631101168115\n",
      "Epoch 65 \t Batch 540 \t Training Loss: 46.325974231296115\n",
      "Epoch 65 \t Batch 560 \t Training Loss: 46.29844241142273\n",
      "Epoch 65 \t Batch 580 \t Training Loss: 46.289637710308206\n",
      "Epoch 65 \t Batch 600 \t Training Loss: 46.322442849477135\n",
      "Epoch 65 \t Batch 620 \t Training Loss: 46.34008060578377\n",
      "Epoch 65 \t Batch 640 \t Training Loss: 46.33938783407211\n",
      "Epoch 65 \t Batch 660 \t Training Loss: 46.341012105074796\n",
      "Epoch 65 \t Batch 680 \t Training Loss: 46.35274253172033\n",
      "Epoch 65 \t Batch 700 \t Training Loss: 46.339255670819966\n",
      "Epoch 65 \t Batch 720 \t Training Loss: 46.332246436013115\n",
      "Epoch 65 \t Batch 740 \t Training Loss: 46.31463328696586\n",
      "Epoch 65 \t Batch 760 \t Training Loss: 46.26268081665039\n",
      "Epoch 65 \t Batch 780 \t Training Loss: 46.223347272628395\n",
      "Epoch 65 \t Batch 800 \t Training Loss: 46.1889598941803\n",
      "Epoch 65 \t Batch 820 \t Training Loss: 46.1689581289524\n",
      "Epoch 65 \t Batch 840 \t Training Loss: 46.18050038019816\n",
      "Epoch 65 \t Batch 860 \t Training Loss: 46.192122268676755\n",
      "Epoch 65 \t Batch 880 \t Training Loss: 46.218513753197406\n",
      "Epoch 65 \t Batch 900 \t Training Loss: 46.2307869720459\n",
      "Epoch 65 \t Batch 20 \t Validation Loss: 18.25260987281799\n",
      "Epoch 65 \t Batch 40 \t Validation Loss: 20.385224366188048\n",
      "Epoch 65 \t Batch 60 \t Validation Loss: 20.048917738596597\n",
      "Epoch 65 \t Batch 80 \t Validation Loss: 20.578765857219697\n",
      "Epoch 65 \t Batch 100 \t Validation Loss: 22.578389368057252\n",
      "Epoch 65 \t Batch 120 \t Validation Loss: 23.999680026372275\n",
      "Epoch 65 \t Batch 140 \t Validation Loss: 24.64933317729405\n",
      "Epoch 65 \t Batch 160 \t Validation Loss: 26.355065798759462\n",
      "Epoch 65 \t Batch 180 \t Validation Loss: 29.586310847600302\n",
      "Epoch 65 \t Batch 200 \t Validation Loss: 30.857752079963685\n",
      "Epoch 65 \t Batch 220 \t Validation Loss: 31.86666703224182\n",
      "Epoch 65 \t Batch 240 \t Validation Loss: 32.258324984709425\n",
      "Epoch 65 \t Batch 260 \t Validation Loss: 34.121567032887384\n",
      "Epoch 65 \t Batch 280 \t Validation Loss: 35.11225672108787\n",
      "Epoch 65 \t Batch 300 \t Validation Loss: 36.13623610496521\n",
      "Epoch 65 \t Batch 320 \t Validation Loss: 36.60268296599388\n",
      "Epoch 65 \t Batch 340 \t Validation Loss: 36.59815339481129\n",
      "Epoch 65 \t Batch 360 \t Validation Loss: 36.45516512129042\n",
      "Epoch 65 \t Batch 380 \t Validation Loss: 36.67919141367862\n",
      "Epoch 65 \t Batch 400 \t Validation Loss: 36.386816477775575\n",
      "Epoch 65 \t Batch 420 \t Validation Loss: 36.473531632196334\n",
      "Epoch 65 \t Batch 440 \t Validation Loss: 36.26809189969843\n",
      "Epoch 65 \t Batch 460 \t Validation Loss: 36.57531224955683\n",
      "Epoch 65 \t Batch 480 \t Validation Loss: 37.09944216410319\n",
      "Epoch 65 \t Batch 500 \t Validation Loss: 36.84127836990356\n",
      "Epoch 65 \t Batch 520 \t Validation Loss: 36.68217666882735\n",
      "Epoch 65 \t Batch 540 \t Validation Loss: 36.53568586243524\n",
      "Epoch 65 \t Batch 560 \t Validation Loss: 36.41898339305605\n",
      "Epoch 65 \t Batch 580 \t Validation Loss: 36.21308910271217\n",
      "Epoch 65 \t Batch 600 \t Validation Loss: 36.52705919424693\n",
      "Epoch 65 Training Loss: 46.23376726922188 Validation Loss: 37.237875813013545\n",
      "Epoch 65 completed\n",
      "Epoch 66 \t Batch 20 \t Training Loss: 46.90915069580078\n",
      "Epoch 66 \t Batch 40 \t Training Loss: 45.46186408996582\n",
      "Epoch 66 \t Batch 60 \t Training Loss: 45.797385597229\n",
      "Epoch 66 \t Batch 80 \t Training Loss: 45.920849323272705\n",
      "Epoch 66 \t Batch 100 \t Training Loss: 46.12014144897461\n",
      "Epoch 66 \t Batch 120 \t Training Loss: 46.33043270111084\n",
      "Epoch 66 \t Batch 140 \t Training Loss: 46.382685034615655\n",
      "Epoch 66 \t Batch 160 \t Training Loss: 46.27957379817963\n",
      "Epoch 66 \t Batch 180 \t Training Loss: 46.47069784800212\n",
      "Epoch 66 \t Batch 200 \t Training Loss: 46.317412109375\n",
      "Epoch 66 \t Batch 220 \t Training Loss: 46.20098824934526\n",
      "Epoch 66 \t Batch 240 \t Training Loss: 46.34984563191732\n",
      "Epoch 66 \t Batch 260 \t Training Loss: 46.27136620741624\n",
      "Epoch 66 \t Batch 280 \t Training Loss: 46.22799295697894\n",
      "Epoch 66 \t Batch 300 \t Training Loss: 46.2283736928304\n",
      "Epoch 66 \t Batch 320 \t Training Loss: 46.25640732049942\n",
      "Epoch 66 \t Batch 340 \t Training Loss: 46.19079934288474\n",
      "Epoch 66 \t Batch 360 \t Training Loss: 46.19876601960924\n",
      "Epoch 66 \t Batch 380 \t Training Loss: 46.20308576885023\n",
      "Epoch 66 \t Batch 400 \t Training Loss: 46.21125597953797\n",
      "Epoch 66 \t Batch 420 \t Training Loss: 46.18806963421049\n",
      "Epoch 66 \t Batch 440 \t Training Loss: 46.16419883207841\n",
      "Epoch 66 \t Batch 460 \t Training Loss: 46.17937651095183\n",
      "Epoch 66 \t Batch 480 \t Training Loss: 46.13293316364288\n",
      "Epoch 66 \t Batch 500 \t Training Loss: 46.14347665405273\n",
      "Epoch 66 \t Batch 520 \t Training Loss: 46.14148415051974\n",
      "Epoch 66 \t Batch 540 \t Training Loss: 46.09072785554109\n",
      "Epoch 66 \t Batch 560 \t Training Loss: 46.08784055028643\n",
      "Epoch 66 \t Batch 580 \t Training Loss: 46.07095965681405\n",
      "Epoch 66 \t Batch 600 \t Training Loss: 46.06403868357341\n",
      "Epoch 66 \t Batch 620 \t Training Loss: 46.02907282306302\n",
      "Epoch 66 \t Batch 640 \t Training Loss: 46.06502861380577\n",
      "Epoch 66 \t Batch 660 \t Training Loss: 46.0920818271059\n",
      "Epoch 66 \t Batch 680 \t Training Loss: 46.074865268258485\n",
      "Epoch 66 \t Batch 700 \t Training Loss: 46.1336447089059\n",
      "Epoch 66 \t Batch 720 \t Training Loss: 46.14617535803053\n",
      "Epoch 66 \t Batch 740 \t Training Loss: 46.14976964383512\n",
      "Epoch 66 \t Batch 760 \t Training Loss: 46.18488329335263\n",
      "Epoch 66 \t Batch 780 \t Training Loss: 46.186794711381964\n",
      "Epoch 66 \t Batch 800 \t Training Loss: 46.21693521976471\n",
      "Epoch 66 \t Batch 820 \t Training Loss: 46.19660372850372\n",
      "Epoch 66 \t Batch 840 \t Training Loss: 46.18954671678089\n",
      "Epoch 66 \t Batch 860 \t Training Loss: 46.19707800842995\n",
      "Epoch 66 \t Batch 880 \t Training Loss: 46.21063873984597\n",
      "Epoch 66 \t Batch 900 \t Training Loss: 46.19766934288872\n",
      "Epoch 66 \t Batch 20 \t Validation Loss: 24.13394112586975\n",
      "Epoch 66 \t Batch 40 \t Validation Loss: 25.367668390274048\n",
      "Epoch 66 \t Batch 60 \t Validation Loss: 25.17095683415731\n",
      "Epoch 66 \t Batch 80 \t Validation Loss: 25.70315055847168\n",
      "Epoch 66 \t Batch 100 \t Validation Loss: 26.269308624267577\n",
      "Epoch 66 \t Batch 120 \t Validation Loss: 26.860144639015196\n",
      "Epoch 66 \t Batch 140 \t Validation Loss: 27.083640595844813\n",
      "Epoch 66 \t Batch 160 \t Validation Loss: 28.994061917066574\n",
      "Epoch 66 \t Batch 180 \t Validation Loss: 32.755832539664375\n",
      "Epoch 66 \t Batch 200 \t Validation Loss: 34.31854853153229\n",
      "Epoch 66 \t Batch 220 \t Validation Loss: 35.553771136023784\n",
      "Epoch 66 \t Batch 240 \t Validation Loss: 36.020182712872824\n",
      "Epoch 66 \t Batch 260 \t Validation Loss: 38.08691300979027\n",
      "Epoch 66 \t Batch 280 \t Validation Loss: 39.15668761730194\n",
      "Epoch 66 \t Batch 300 \t Validation Loss: 40.36047911643982\n",
      "Epoch 66 \t Batch 320 \t Validation Loss: 40.85999037921429\n",
      "Epoch 66 \t Batch 340 \t Validation Loss: 40.75495377428391\n",
      "Epoch 66 \t Batch 360 \t Validation Loss: 40.58419472111596\n",
      "Epoch 66 \t Batch 380 \t Validation Loss: 40.773212058920606\n",
      "Epoch 66 \t Batch 400 \t Validation Loss: 40.33174322366715\n",
      "Epoch 66 \t Batch 420 \t Validation Loss: 40.33748101052784\n",
      "Epoch 66 \t Batch 440 \t Validation Loss: 39.986017476428636\n",
      "Epoch 66 \t Batch 460 \t Validation Loss: 40.21904411937879\n",
      "Epoch 66 \t Batch 480 \t Validation Loss: 40.67564515074094\n",
      "Epoch 66 \t Batch 500 \t Validation Loss: 40.41128080940246\n",
      "Epoch 66 \t Batch 520 \t Validation Loss: 40.18270392234509\n",
      "Epoch 66 \t Batch 540 \t Validation Loss: 39.867247650358415\n",
      "Epoch 66 \t Batch 560 \t Validation Loss: 39.60784056016377\n",
      "Epoch 66 \t Batch 580 \t Validation Loss: 39.31777009141856\n",
      "Epoch 66 \t Batch 600 \t Validation Loss: 39.463444177309675\n",
      "Epoch 66 Training Loss: 46.19297842527034 Validation Loss: 40.07451515847986\n",
      "Epoch 66 completed\n",
      "Epoch 67 \t Batch 20 \t Training Loss: 45.98468723297119\n",
      "Epoch 67 \t Batch 40 \t Training Loss: 45.81256113052368\n",
      "Epoch 67 \t Batch 60 \t Training Loss: 45.86428407033284\n",
      "Epoch 67 \t Batch 80 \t Training Loss: 45.85236458778381\n",
      "Epoch 67 \t Batch 100 \t Training Loss: 45.87497398376465\n",
      "Epoch 67 \t Batch 120 \t Training Loss: 45.79243233998616\n",
      "Epoch 67 \t Batch 140 \t Training Loss: 45.634529277256554\n",
      "Epoch 67 \t Batch 160 \t Training Loss: 45.80088105201721\n",
      "Epoch 67 \t Batch 180 \t Training Loss: 45.89597076839871\n",
      "Epoch 67 \t Batch 200 \t Training Loss: 45.95928176879883\n",
      "Epoch 67 \t Batch 220 \t Training Loss: 45.98837047923695\n",
      "Epoch 67 \t Batch 240 \t Training Loss: 46.01716237068176\n",
      "Epoch 67 \t Batch 260 \t Training Loss: 45.94305791121263\n",
      "Epoch 67 \t Batch 280 \t Training Loss: 45.99411189215524\n",
      "Epoch 67 \t Batch 300 \t Training Loss: 46.05366133371989\n",
      "Epoch 67 \t Batch 320 \t Training Loss: 46.17226763963699\n",
      "Epoch 67 \t Batch 340 \t Training Loss: 46.178481831270105\n",
      "Epoch 67 \t Batch 360 \t Training Loss: 46.190031645033095\n",
      "Epoch 67 \t Batch 380 \t Training Loss: 46.16308817612497\n",
      "Epoch 67 \t Batch 400 \t Training Loss: 46.20719082832336\n",
      "Epoch 67 \t Batch 420 \t Training Loss: 46.259567042759485\n",
      "Epoch 67 \t Batch 440 \t Training Loss: 46.18081156990745\n",
      "Epoch 67 \t Batch 460 \t Training Loss: 46.139435262265415\n",
      "Epoch 67 \t Batch 480 \t Training Loss: 46.19152443408966\n",
      "Epoch 67 \t Batch 500 \t Training Loss: 46.16635236358643\n",
      "Epoch 67 \t Batch 520 \t Training Loss: 46.1401403940641\n",
      "Epoch 67 \t Batch 540 \t Training Loss: 46.18322546923602\n",
      "Epoch 67 \t Batch 560 \t Training Loss: 46.21887405940465\n",
      "Epoch 67 \t Batch 580 \t Training Loss: 46.167826231594745\n",
      "Epoch 67 \t Batch 600 \t Training Loss: 46.116758785247804\n",
      "Epoch 67 \t Batch 620 \t Training Loss: 46.109264706027126\n",
      "Epoch 67 \t Batch 640 \t Training Loss: 46.1033048927784\n",
      "Epoch 67 \t Batch 660 \t Training Loss: 46.1107296221184\n",
      "Epoch 67 \t Batch 680 \t Training Loss: 46.07682365529678\n",
      "Epoch 67 \t Batch 700 \t Training Loss: 46.09217657906669\n",
      "Epoch 67 \t Batch 720 \t Training Loss: 46.090262264675566\n",
      "Epoch 67 \t Batch 740 \t Training Loss: 46.15447249025912\n",
      "Epoch 67 \t Batch 760 \t Training Loss: 46.17478107151232\n",
      "Epoch 67 \t Batch 780 \t Training Loss: 46.170305217840735\n",
      "Epoch 67 \t Batch 800 \t Training Loss: 46.17679421424866\n",
      "Epoch 67 \t Batch 820 \t Training Loss: 46.174612054592224\n",
      "Epoch 67 \t Batch 840 \t Training Loss: 46.131577182951425\n",
      "Epoch 67 \t Batch 860 \t Training Loss: 46.12193568030069\n",
      "Epoch 67 \t Batch 880 \t Training Loss: 46.15183628255671\n",
      "Epoch 67 \t Batch 900 \t Training Loss: 46.17207663642036\n",
      "Epoch 67 \t Batch 20 \t Validation Loss: 27.360041522979735\n",
      "Epoch 67 \t Batch 40 \t Validation Loss: 27.25009753704071\n",
      "Epoch 67 \t Batch 60 \t Validation Loss: 27.40855180422465\n",
      "Epoch 67 \t Batch 80 \t Validation Loss: 27.51366468667984\n",
      "Epoch 67 \t Batch 100 \t Validation Loss: 27.79432801246643\n",
      "Epoch 67 \t Batch 120 \t Validation Loss: 28.263786005973817\n",
      "Epoch 67 \t Batch 140 \t Validation Loss: 28.266404376711165\n",
      "Epoch 67 \t Batch 160 \t Validation Loss: 29.95033993124962\n",
      "Epoch 67 \t Batch 180 \t Validation Loss: 33.421216911739776\n",
      "Epoch 67 \t Batch 200 \t Validation Loss: 34.66269082069397\n",
      "Epoch 67 \t Batch 220 \t Validation Loss: 35.86764685890891\n",
      "Epoch 67 \t Batch 240 \t Validation Loss: 36.26903772354126\n",
      "Epoch 67 \t Batch 260 \t Validation Loss: 38.305969043878406\n",
      "Epoch 67 \t Batch 280 \t Validation Loss: 39.33005639484951\n",
      "Epoch 67 \t Batch 300 \t Validation Loss: 40.378961181640626\n",
      "Epoch 67 \t Batch 320 \t Validation Loss: 40.845495593547824\n",
      "Epoch 67 \t Batch 340 \t Validation Loss: 40.70307338378009\n",
      "Epoch 67 \t Batch 360 \t Validation Loss: 40.470470666885376\n",
      "Epoch 67 \t Batch 380 \t Validation Loss: 40.65274254648309\n",
      "Epoch 67 \t Batch 400 \t Validation Loss: 40.15108026027679\n",
      "Epoch 67 \t Batch 420 \t Validation Loss: 40.10773730959211\n",
      "Epoch 67 \t Batch 440 \t Validation Loss: 39.74365147677335\n",
      "Epoch 67 \t Batch 460 \t Validation Loss: 39.985327301854674\n",
      "Epoch 67 \t Batch 480 \t Validation Loss: 40.42041020790736\n",
      "Epoch 67 \t Batch 500 \t Validation Loss: 40.103175857543945\n",
      "Epoch 67 \t Batch 520 \t Validation Loss: 39.84882847529191\n",
      "Epoch 67 \t Batch 540 \t Validation Loss: 39.56227555274963\n",
      "Epoch 67 \t Batch 560 \t Validation Loss: 39.2913152882031\n",
      "Epoch 67 \t Batch 580 \t Validation Loss: 38.91546817648\n",
      "Epoch 67 \t Batch 600 \t Validation Loss: 39.13882672945658\n",
      "Epoch 67 Training Loss: 46.1779223892395 Validation Loss: 39.745309077300035\n",
      "Epoch 67 completed\n",
      "Epoch 68 \t Batch 20 \t Training Loss: 46.30909442901611\n",
      "Epoch 68 \t Batch 40 \t Training Loss: 45.756150531768796\n",
      "Epoch 68 \t Batch 60 \t Training Loss: 45.71549441019694\n",
      "Epoch 68 \t Batch 80 \t Training Loss: 45.908285188674924\n",
      "Epoch 68 \t Batch 100 \t Training Loss: 45.94674175262451\n",
      "Epoch 68 \t Batch 120 \t Training Loss: 45.768852392832436\n",
      "Epoch 68 \t Batch 140 \t Training Loss: 45.867090116228376\n",
      "Epoch 68 \t Batch 160 \t Training Loss: 45.935150146484375\n",
      "Epoch 68 \t Batch 180 \t Training Loss: 45.91309725443522\n",
      "Epoch 68 \t Batch 200 \t Training Loss: 46.03244417190552\n",
      "Epoch 68 \t Batch 220 \t Training Loss: 46.0287070534446\n",
      "Epoch 68 \t Batch 240 \t Training Loss: 46.019217268625894\n",
      "Epoch 68 \t Batch 260 \t Training Loss: 46.054039353590746\n",
      "Epoch 68 \t Batch 280 \t Training Loss: 46.01345169884818\n",
      "Epoch 68 \t Batch 300 \t Training Loss: 46.02675392150879\n",
      "Epoch 68 \t Batch 320 \t Training Loss: 46.11161106824875\n",
      "Epoch 68 \t Batch 340 \t Training Loss: 46.10928241505342\n",
      "Epoch 68 \t Batch 360 \t Training Loss: 46.20633435779148\n",
      "Epoch 68 \t Batch 380 \t Training Loss: 46.164552748830694\n",
      "Epoch 68 \t Batch 400 \t Training Loss: 46.173514184951785\n",
      "Epoch 68 \t Batch 420 \t Training Loss: 46.157371230352496\n",
      "Epoch 68 \t Batch 440 \t Training Loss: 46.1597299749201\n",
      "Epoch 68 \t Batch 460 \t Training Loss: 46.201390283004095\n",
      "Epoch 68 \t Batch 480 \t Training Loss: 46.307024677594505\n",
      "Epoch 68 \t Batch 500 \t Training Loss: 46.364195960998536\n",
      "Epoch 68 \t Batch 520 \t Training Loss: 46.31433382767897\n",
      "Epoch 68 \t Batch 540 \t Training Loss: 46.28884617134377\n",
      "Epoch 68 \t Batch 560 \t Training Loss: 46.2759249482836\n",
      "Epoch 68 \t Batch 580 \t Training Loss: 46.285718233832\n",
      "Epoch 68 \t Batch 600 \t Training Loss: 46.28015627543132\n",
      "Epoch 68 \t Batch 620 \t Training Loss: 46.267011396346554\n",
      "Epoch 68 \t Batch 640 \t Training Loss: 46.22972425818443\n",
      "Epoch 68 \t Batch 660 \t Training Loss: 46.197893870960584\n",
      "Epoch 68 \t Batch 680 \t Training Loss: 46.17872403649723\n",
      "Epoch 68 \t Batch 700 \t Training Loss: 46.19530112675258\n",
      "Epoch 68 \t Batch 720 \t Training Loss: 46.203761148452756\n",
      "Epoch 68 \t Batch 740 \t Training Loss: 46.17235315683726\n",
      "Epoch 68 \t Batch 760 \t Training Loss: 46.17710258584273\n",
      "Epoch 68 \t Batch 780 \t Training Loss: 46.17936216501089\n",
      "Epoch 68 \t Batch 800 \t Training Loss: 46.170239396095276\n",
      "Epoch 68 \t Batch 820 \t Training Loss: 46.15951613914676\n",
      "Epoch 68 \t Batch 840 \t Training Loss: 46.13483490262713\n",
      "Epoch 68 \t Batch 860 \t Training Loss: 46.15243204249892\n",
      "Epoch 68 \t Batch 880 \t Training Loss: 46.15515838536349\n",
      "Epoch 68 \t Batch 900 \t Training Loss: 46.151688202752005\n",
      "Epoch 68 \t Batch 20 \t Validation Loss: 17.13194088935852\n",
      "Epoch 68 \t Batch 40 \t Validation Loss: 19.094373297691345\n",
      "Epoch 68 \t Batch 60 \t Validation Loss: 18.835528802871703\n",
      "Epoch 68 \t Batch 80 \t Validation Loss: 19.349822318553926\n",
      "Epoch 68 \t Batch 100 \t Validation Loss: 21.141306943893433\n",
      "Epoch 68 \t Batch 120 \t Validation Loss: 22.66875893274943\n",
      "Epoch 68 \t Batch 140 \t Validation Loss: 23.384479236602782\n",
      "Epoch 68 \t Batch 160 \t Validation Loss: 25.03137472867966\n",
      "Epoch 68 \t Batch 180 \t Validation Loss: 28.011856280432806\n",
      "Epoch 68 \t Batch 200 \t Validation Loss: 29.192538623809813\n",
      "Epoch 68 \t Batch 220 \t Validation Loss: 30.12224066474221\n",
      "Epoch 68 \t Batch 240 \t Validation Loss: 30.468599859873454\n",
      "Epoch 68 \t Batch 260 \t Validation Loss: 32.22954529982347\n",
      "Epoch 68 \t Batch 280 \t Validation Loss: 33.184015597615925\n",
      "Epoch 68 \t Batch 300 \t Validation Loss: 34.091068747838335\n",
      "Epoch 68 \t Batch 320 \t Validation Loss: 34.53881759941578\n",
      "Epoch 68 \t Batch 340 \t Validation Loss: 34.63248229587779\n",
      "Epoch 68 \t Batch 360 \t Validation Loss: 34.50994941658444\n",
      "Epoch 68 \t Batch 380 \t Validation Loss: 34.774491658963655\n",
      "Epoch 68 \t Batch 400 \t Validation Loss: 34.60768402338028\n",
      "Epoch 68 \t Batch 420 \t Validation Loss: 34.79387578737168\n",
      "Epoch 68 \t Batch 440 \t Validation Loss: 34.70573071349751\n",
      "Epoch 68 \t Batch 460 \t Validation Loss: 35.09686040670975\n",
      "Epoch 68 \t Batch 480 \t Validation Loss: 35.6693265457948\n",
      "Epoch 68 \t Batch 500 \t Validation Loss: 35.46305890846252\n",
      "Epoch 68 \t Batch 520 \t Validation Loss: 35.35138433346381\n",
      "Epoch 68 \t Batch 540 \t Validation Loss: 35.25396499810395\n",
      "Epoch 68 \t Batch 560 \t Validation Loss: 35.194209795338764\n",
      "Epoch 68 \t Batch 580 \t Validation Loss: 35.05278716416195\n",
      "Epoch 68 \t Batch 600 \t Validation Loss: 35.40114809830983\n",
      "Epoch 68 Training Loss: 46.15955737996907 Validation Loss: 36.14901712807742\n",
      "Validation Loss Decreased(22566.078581809998--->22267.79455089569) Saving The Model\n",
      "Epoch 68 completed\n",
      "Epoch 69 \t Batch 20 \t Training Loss: 47.25823783874512\n",
      "Epoch 69 \t Batch 40 \t Training Loss: 46.656387138366696\n",
      "Epoch 69 \t Batch 60 \t Training Loss: 46.100599924723305\n",
      "Epoch 69 \t Batch 80 \t Training Loss: 45.83345522880554\n",
      "Epoch 69 \t Batch 100 \t Training Loss: 46.21533466339111\n",
      "Epoch 69 \t Batch 120 \t Training Loss: 46.241819349924725\n",
      "Epoch 69 \t Batch 140 \t Training Loss: 46.49229052407401\n",
      "Epoch 69 \t Batch 160 \t Training Loss: 46.38465759754181\n",
      "Epoch 69 \t Batch 180 \t Training Loss: 46.31831005944146\n",
      "Epoch 69 \t Batch 200 \t Training Loss: 46.251552753448486\n",
      "Epoch 69 \t Batch 220 \t Training Loss: 46.365697028420186\n",
      "Epoch 69 \t Batch 240 \t Training Loss: 46.2425240834554\n",
      "Epoch 69 \t Batch 260 \t Training Loss: 46.25634062840388\n",
      "Epoch 69 \t Batch 280 \t Training Loss: 46.16768445968628\n",
      "Epoch 69 \t Batch 300 \t Training Loss: 46.19400143941243\n",
      "Epoch 69 \t Batch 320 \t Training Loss: 46.15489671230316\n",
      "Epoch 69 \t Batch 340 \t Training Loss: 46.15182340285357\n",
      "Epoch 69 \t Batch 360 \t Training Loss: 46.17990013758342\n",
      "Epoch 69 \t Batch 380 \t Training Loss: 46.21490734501889\n",
      "Epoch 69 \t Batch 400 \t Training Loss: 46.290935068130494\n",
      "Epoch 69 \t Batch 420 \t Training Loss: 46.30405863807315\n",
      "Epoch 69 \t Batch 440 \t Training Loss: 46.305036813562566\n",
      "Epoch 69 \t Batch 460 \t Training Loss: 46.217065753107484\n",
      "Epoch 69 \t Batch 480 \t Training Loss: 46.15645821889242\n",
      "Epoch 69 \t Batch 500 \t Training Loss: 46.13283856201172\n",
      "Epoch 69 \t Batch 520 \t Training Loss: 46.13916203425481\n",
      "Epoch 69 \t Batch 540 \t Training Loss: 46.13010631137424\n",
      "Epoch 69 \t Batch 560 \t Training Loss: 46.12457276753017\n",
      "Epoch 69 \t Batch 580 \t Training Loss: 46.120472355546624\n",
      "Epoch 69 \t Batch 600 \t Training Loss: 46.091169611612955\n",
      "Epoch 69 \t Batch 620 \t Training Loss: 46.08659580599877\n",
      "Epoch 69 \t Batch 640 \t Training Loss: 46.12286628484726\n",
      "Epoch 69 \t Batch 660 \t Training Loss: 46.11320343017578\n",
      "Epoch 69 \t Batch 680 \t Training Loss: 46.14377031887279\n",
      "Epoch 69 \t Batch 700 \t Training Loss: 46.12987977709089\n",
      "Epoch 69 \t Batch 720 \t Training Loss: 46.10653627183702\n",
      "Epoch 69 \t Batch 740 \t Training Loss: 46.121862865138695\n",
      "Epoch 69 \t Batch 760 \t Training Loss: 46.15610728514822\n",
      "Epoch 69 \t Batch 780 \t Training Loss: 46.16971550476857\n",
      "Epoch 69 \t Batch 800 \t Training Loss: 46.15132777690887\n",
      "Epoch 69 \t Batch 820 \t Training Loss: 46.15450942807081\n",
      "Epoch 69 \t Batch 840 \t Training Loss: 46.13046459924607\n",
      "Epoch 69 \t Batch 860 \t Training Loss: 46.1510554424552\n",
      "Epoch 69 \t Batch 880 \t Training Loss: 46.165511309016836\n",
      "Epoch 69 \t Batch 900 \t Training Loss: 46.16179796006944\n",
      "Epoch 69 \t Batch 20 \t Validation Loss: 17.559342908859254\n",
      "Epoch 69 \t Batch 40 \t Validation Loss: 21.097504687309264\n",
      "Epoch 69 \t Batch 60 \t Validation Loss: 20.364514207839967\n",
      "Epoch 69 \t Batch 80 \t Validation Loss: 21.05100977420807\n",
      "Epoch 69 \t Batch 100 \t Validation Loss: 22.489868888854982\n",
      "Epoch 69 \t Batch 120 \t Validation Loss: 23.70907831986745\n",
      "Epoch 69 \t Batch 140 \t Validation Loss: 24.454196541649953\n",
      "Epoch 69 \t Batch 160 \t Validation Loss: 26.590423148870467\n",
      "Epoch 69 \t Batch 180 \t Validation Loss: 30.28165422545539\n",
      "Epoch 69 \t Batch 200 \t Validation Loss: 31.861634345054625\n",
      "Epoch 69 \t Batch 220 \t Validation Loss: 33.19760850126093\n",
      "Epoch 69 \t Batch 240 \t Validation Loss: 33.78342283566793\n",
      "Epoch 69 \t Batch 260 \t Validation Loss: 35.87735234774076\n",
      "Epoch 69 \t Batch 280 \t Validation Loss: 37.02528784275055\n",
      "Epoch 69 \t Batch 300 \t Validation Loss: 38.15323142051697\n",
      "Epoch 69 \t Batch 320 \t Validation Loss: 38.69240681827068\n",
      "Epoch 69 \t Batch 340 \t Validation Loss: 38.70073082026313\n",
      "Epoch 69 \t Batch 360 \t Validation Loss: 38.63088867134518\n",
      "Epoch 69 \t Batch 380 \t Validation Loss: 38.916843843460086\n",
      "Epoch 69 \t Batch 400 \t Validation Loss: 38.61105843305588\n",
      "Epoch 69 \t Batch 420 \t Validation Loss: 38.63944143794832\n",
      "Epoch 69 \t Batch 440 \t Validation Loss: 38.39873800060966\n",
      "Epoch 69 \t Batch 460 \t Validation Loss: 38.6998245716095\n",
      "Epoch 69 \t Batch 480 \t Validation Loss: 39.19412225286166\n",
      "Epoch 69 \t Batch 500 \t Validation Loss: 38.94934649085999\n",
      "Epoch 69 \t Batch 520 \t Validation Loss: 38.81857260373923\n",
      "Epoch 69 \t Batch 540 \t Validation Loss: 38.635992730105365\n",
      "Epoch 69 \t Batch 560 \t Validation Loss: 38.494985377788545\n",
      "Epoch 69 \t Batch 580 \t Validation Loss: 38.328277119274794\n",
      "Epoch 69 \t Batch 600 \t Validation Loss: 38.56138361771902\n",
      "Epoch 69 Training Loss: 46.135728621820846 Validation Loss: 39.25690009222402\n",
      "Epoch 69 completed\n",
      "Epoch 70 \t Batch 20 \t Training Loss: 46.665808486938474\n",
      "Epoch 70 \t Batch 40 \t Training Loss: 46.53782052993775\n",
      "Epoch 70 \t Batch 60 \t Training Loss: 46.653780174255374\n",
      "Epoch 70 \t Batch 80 \t Training Loss: 46.24922180175781\n",
      "Epoch 70 \t Batch 100 \t Training Loss: 46.123890762329104\n",
      "Epoch 70 \t Batch 120 \t Training Loss: 46.21301240921021\n",
      "Epoch 70 \t Batch 140 \t Training Loss: 46.16918806348528\n",
      "Epoch 70 \t Batch 160 \t Training Loss: 46.14143342971802\n",
      "Epoch 70 \t Batch 180 \t Training Loss: 46.1326270421346\n",
      "Epoch 70 \t Batch 200 \t Training Loss: 46.13588449478149\n",
      "Epoch 70 \t Batch 220 \t Training Loss: 46.12569361600009\n",
      "Epoch 70 \t Batch 240 \t Training Loss: 46.068580118815106\n",
      "Epoch 70 \t Batch 260 \t Training Loss: 46.09591489938589\n",
      "Epoch 70 \t Batch 280 \t Training Loss: 46.07315951756069\n",
      "Epoch 70 \t Batch 300 \t Training Loss: 46.10428384145101\n",
      "Epoch 70 \t Batch 320 \t Training Loss: 46.165136539936064\n",
      "Epoch 70 \t Batch 340 \t Training Loss: 46.15181321536794\n",
      "Epoch 70 \t Batch 360 \t Training Loss: 46.165919134351945\n",
      "Epoch 70 \t Batch 380 \t Training Loss: 46.15435087304366\n",
      "Epoch 70 \t Batch 400 \t Training Loss: 46.24956542015076\n",
      "Epoch 70 \t Batch 420 \t Training Loss: 46.276954178583054\n",
      "Epoch 70 \t Batch 440 \t Training Loss: 46.29356321855025\n",
      "Epoch 70 \t Batch 460 \t Training Loss: 46.2662713755732\n",
      "Epoch 70 \t Batch 480 \t Training Loss: 46.31275641123454\n",
      "Epoch 70 \t Batch 500 \t Training Loss: 46.29530074310303\n",
      "Epoch 70 \t Batch 520 \t Training Loss: 46.29361268557035\n",
      "Epoch 70 \t Batch 540 \t Training Loss: 46.289818431712966\n",
      "Epoch 70 \t Batch 560 \t Training Loss: 46.281356470925466\n",
      "Epoch 70 \t Batch 580 \t Training Loss: 46.27419713776687\n",
      "Epoch 70 \t Batch 600 \t Training Loss: 46.29901357014974\n",
      "Epoch 70 \t Batch 620 \t Training Loss: 46.24575624773579\n",
      "Epoch 70 \t Batch 640 \t Training Loss: 46.18510257005691\n",
      "Epoch 70 \t Batch 660 \t Training Loss: 46.24077417778246\n",
      "Epoch 70 \t Batch 680 \t Training Loss: 46.20186273350435\n",
      "Epoch 70 \t Batch 700 \t Training Loss: 46.225287061418804\n",
      "Epoch 70 \t Batch 720 \t Training Loss: 46.225591044955785\n",
      "Epoch 70 \t Batch 740 \t Training Loss: 46.21659618583885\n",
      "Epoch 70 \t Batch 760 \t Training Loss: 46.233486351213955\n",
      "Epoch 70 \t Batch 780 \t Training Loss: 46.20081854111109\n",
      "Epoch 70 \t Batch 800 \t Training Loss: 46.18110249996185\n",
      "Epoch 70 \t Batch 820 \t Training Loss: 46.198725760855325\n",
      "Epoch 70 \t Batch 840 \t Training Loss: 46.17060461498442\n",
      "Epoch 70 \t Batch 860 \t Training Loss: 46.1707648432532\n",
      "Epoch 70 \t Batch 880 \t Training Loss: 46.15781266472556\n",
      "Epoch 70 \t Batch 900 \t Training Loss: 46.119675657484265\n",
      "Epoch 70 \t Batch 20 \t Validation Loss: 19.514902210235597\n",
      "Epoch 70 \t Batch 40 \t Validation Loss: 21.651131939888\n",
      "Epoch 70 \t Batch 60 \t Validation Loss: 21.42578059832255\n",
      "Epoch 70 \t Batch 80 \t Validation Loss: 22.32884030342102\n",
      "Epoch 70 \t Batch 100 \t Validation Loss: 23.738493194580077\n",
      "Epoch 70 \t Batch 120 \t Validation Loss: 24.906484190622965\n",
      "Epoch 70 \t Batch 140 \t Validation Loss: 25.370779282706124\n",
      "Epoch 70 \t Batch 160 \t Validation Loss: 27.24584184885025\n",
      "Epoch 70 \t Batch 180 \t Validation Loss: 30.823156107796564\n",
      "Epoch 70 \t Batch 200 \t Validation Loss: 32.23750127315521\n",
      "Epoch 70 \t Batch 220 \t Validation Loss: 33.50413167693398\n",
      "Epoch 70 \t Batch 240 \t Validation Loss: 34.011580828825636\n",
      "Epoch 70 \t Batch 260 \t Validation Loss: 36.04632106194129\n",
      "Epoch 70 \t Batch 280 \t Validation Loss: 37.14830341679709\n",
      "Epoch 70 \t Batch 300 \t Validation Loss: 38.25744417508443\n",
      "Epoch 70 \t Batch 320 \t Validation Loss: 38.77447000443935\n",
      "Epoch 70 \t Batch 340 \t Validation Loss: 38.72696812293109\n",
      "Epoch 70 \t Batch 360 \t Validation Loss: 38.57018596066369\n",
      "Epoch 70 \t Batch 380 \t Validation Loss: 38.79478740943106\n",
      "Epoch 70 \t Batch 400 \t Validation Loss: 38.403796331882475\n",
      "Epoch 70 \t Batch 420 \t Validation Loss: 38.45218931833903\n",
      "Epoch 70 \t Batch 440 \t Validation Loss: 38.17264433774081\n",
      "Epoch 70 \t Batch 460 \t Validation Loss: 38.4307686764261\n",
      "Epoch 70 \t Batch 480 \t Validation Loss: 38.92724945942561\n",
      "Epoch 70 \t Batch 500 \t Validation Loss: 38.65917298126221\n",
      "Epoch 70 \t Batch 520 \t Validation Loss: 38.41628370651832\n",
      "Epoch 70 \t Batch 540 \t Validation Loss: 38.170902969219064\n",
      "Epoch 70 \t Batch 560 \t Validation Loss: 37.96648144040789\n",
      "Epoch 70 \t Batch 580 \t Validation Loss: 37.66427674786798\n",
      "Epoch 70 \t Batch 600 \t Validation Loss: 37.90658599217733\n",
      "Epoch 70 Training Loss: 46.1040909079715 Validation Loss: 38.60813829186675\n",
      "Epoch 70 completed\n",
      "Epoch 71 \t Batch 20 \t Training Loss: 45.74554576873779\n",
      "Epoch 71 \t Batch 40 \t Training Loss: 46.08116283416748\n",
      "Epoch 71 \t Batch 60 \t Training Loss: 46.28903636932373\n",
      "Epoch 71 \t Batch 80 \t Training Loss: 46.13487467765808\n",
      "Epoch 71 \t Batch 100 \t Training Loss: 46.32139595031738\n",
      "Epoch 71 \t Batch 120 \t Training Loss: 46.23814223607381\n",
      "Epoch 71 \t Batch 140 \t Training Loss: 46.24696829659598\n",
      "Epoch 71 \t Batch 160 \t Training Loss: 46.155359983444214\n",
      "Epoch 71 \t Batch 180 \t Training Loss: 46.15694046020508\n",
      "Epoch 71 \t Batch 200 \t Training Loss: 46.11397613525391\n",
      "Epoch 71 \t Batch 220 \t Training Loss: 46.124088270013985\n",
      "Epoch 71 \t Batch 240 \t Training Loss: 46.04109547932943\n",
      "Epoch 71 \t Batch 260 \t Training Loss: 46.13063765305739\n",
      "Epoch 71 \t Batch 280 \t Training Loss: 46.10515493665423\n",
      "Epoch 71 \t Batch 300 \t Training Loss: 46.04088692982992\n",
      "Epoch 71 \t Batch 320 \t Training Loss: 46.01678028106689\n",
      "Epoch 71 \t Batch 340 \t Training Loss: 46.07551156212302\n",
      "Epoch 71 \t Batch 360 \t Training Loss: 46.05544271469116\n",
      "Epoch 71 \t Batch 380 \t Training Loss: 46.039319530286285\n",
      "Epoch 71 \t Batch 400 \t Training Loss: 45.96777263641357\n",
      "Epoch 71 \t Batch 420 \t Training Loss: 45.96387048448835\n",
      "Epoch 71 \t Batch 440 \t Training Loss: 46.0312219099565\n",
      "Epoch 71 \t Batch 460 \t Training Loss: 46.03322205750838\n",
      "Epoch 71 \t Batch 480 \t Training Loss: 46.003385655085246\n",
      "Epoch 71 \t Batch 500 \t Training Loss: 46.00662738800049\n",
      "Epoch 71 \t Batch 520 \t Training Loss: 46.02907798473652\n",
      "Epoch 71 \t Batch 540 \t Training Loss: 46.01957971078378\n",
      "Epoch 71 \t Batch 560 \t Training Loss: 45.97613946369716\n",
      "Epoch 71 \t Batch 580 \t Training Loss: 45.964127816825076\n",
      "Epoch 71 \t Batch 600 \t Training Loss: 45.998992379506426\n",
      "Epoch 71 \t Batch 620 \t Training Loss: 46.00154788109564\n",
      "Epoch 71 \t Batch 640 \t Training Loss: 46.03677521944046\n",
      "Epoch 71 \t Batch 660 \t Training Loss: 46.09056491273822\n",
      "Epoch 71 \t Batch 680 \t Training Loss: 46.088878816716814\n",
      "Epoch 71 \t Batch 700 \t Training Loss: 46.069229469299316\n",
      "Epoch 71 \t Batch 720 \t Training Loss: 46.05691984494527\n",
      "Epoch 71 \t Batch 740 \t Training Loss: 46.01777166418127\n",
      "Epoch 71 \t Batch 760 \t Training Loss: 46.04098120739585\n",
      "Epoch 71 \t Batch 780 \t Training Loss: 46.05386308034261\n",
      "Epoch 71 \t Batch 800 \t Training Loss: 46.045988721847536\n",
      "Epoch 71 \t Batch 820 \t Training Loss: 46.04794462715707\n",
      "Epoch 71 \t Batch 840 \t Training Loss: 46.03390428452265\n",
      "Epoch 71 \t Batch 860 \t Training Loss: 46.03757483016613\n",
      "Epoch 71 \t Batch 880 \t Training Loss: 46.01834169734608\n",
      "Epoch 71 \t Batch 900 \t Training Loss: 46.02346451229519\n",
      "Epoch 71 \t Batch 20 \t Validation Loss: 21.812334060668945\n",
      "Epoch 71 \t Batch 40 \t Validation Loss: 23.36904001235962\n",
      "Epoch 71 \t Batch 60 \t Validation Loss: 23.120281966527305\n",
      "Epoch 71 \t Batch 80 \t Validation Loss: 23.776695322990417\n",
      "Epoch 71 \t Batch 100 \t Validation Loss: 24.897098693847656\n",
      "Epoch 71 \t Batch 120 \t Validation Loss: 25.7350146373113\n",
      "Epoch 71 \t Batch 140 \t Validation Loss: 26.03505986758641\n",
      "Epoch 71 \t Batch 160 \t Validation Loss: 27.71576616168022\n",
      "Epoch 71 \t Batch 180 \t Validation Loss: 31.07258399327596\n",
      "Epoch 71 \t Batch 200 \t Validation Loss: 32.32552680015564\n",
      "Epoch 71 \t Batch 220 \t Validation Loss: 33.405151393196796\n",
      "Epoch 71 \t Batch 240 \t Validation Loss: 33.80425541003545\n",
      "Epoch 71 \t Batch 260 \t Validation Loss: 35.75631748346182\n",
      "Epoch 71 \t Batch 280 \t Validation Loss: 36.815900683403015\n",
      "Epoch 71 \t Batch 300 \t Validation Loss: 37.85030515988668\n",
      "Epoch 71 \t Batch 320 \t Validation Loss: 38.31450731754303\n",
      "Epoch 71 \t Batch 340 \t Validation Loss: 38.259056153016935\n",
      "Epoch 71 \t Batch 360 \t Validation Loss: 38.0643113984002\n",
      "Epoch 71 \t Batch 380 \t Validation Loss: 38.263159064242714\n",
      "Epoch 71 \t Batch 400 \t Validation Loss: 37.89239234447479\n",
      "Epoch 71 \t Batch 420 \t Validation Loss: 37.90558622905186\n",
      "Epoch 71 \t Batch 440 \t Validation Loss: 37.62841849760576\n",
      "Epoch 71 \t Batch 460 \t Validation Loss: 37.893803807963494\n",
      "Epoch 71 \t Batch 480 \t Validation Loss: 38.391422001520795\n",
      "Epoch 71 \t Batch 500 \t Validation Loss: 38.113831497192386\n",
      "Epoch 71 \t Batch 520 \t Validation Loss: 37.89897005374615\n",
      "Epoch 71 \t Batch 540 \t Validation Loss: 37.68509190877278\n",
      "Epoch 71 \t Batch 560 \t Validation Loss: 37.499448367527556\n",
      "Epoch 71 \t Batch 580 \t Validation Loss: 37.23026391391097\n",
      "Epoch 71 \t Batch 600 \t Validation Loss: 37.48772885004679\n",
      "Epoch 71 Training Loss: 46.03510606405909 Validation Loss: 38.14046421608368\n",
      "Epoch 71 completed\n",
      "Epoch 72 \t Batch 20 \t Training Loss: 46.72772693634033\n",
      "Epoch 72 \t Batch 40 \t Training Loss: 45.977992343902585\n",
      "Epoch 72 \t Batch 60 \t Training Loss: 46.13856474558512\n",
      "Epoch 72 \t Batch 80 \t Training Loss: 46.19408864974976\n",
      "Epoch 72 \t Batch 100 \t Training Loss: 46.34941562652588\n",
      "Epoch 72 \t Batch 120 \t Training Loss: 46.449722862243654\n",
      "Epoch 72 \t Batch 140 \t Training Loss: 46.6211101259504\n",
      "Epoch 72 \t Batch 160 \t Training Loss: 46.32423529624939\n",
      "Epoch 72 \t Batch 180 \t Training Loss: 46.24299059973823\n",
      "Epoch 72 \t Batch 200 \t Training Loss: 46.173807334899905\n",
      "Epoch 72 \t Batch 220 \t Training Loss: 46.180499614368784\n",
      "Epoch 72 \t Batch 240 \t Training Loss: 46.10563570658366\n",
      "Epoch 72 \t Batch 260 \t Training Loss: 46.15814380645752\n",
      "Epoch 72 \t Batch 280 \t Training Loss: 46.17582174028669\n",
      "Epoch 72 \t Batch 300 \t Training Loss: 46.2131808980306\n",
      "Epoch 72 \t Batch 320 \t Training Loss: 46.22053079605102\n",
      "Epoch 72 \t Batch 340 \t Training Loss: 46.253056806676526\n",
      "Epoch 72 \t Batch 360 \t Training Loss: 46.24730296664768\n",
      "Epoch 72 \t Batch 380 \t Training Loss: 46.248225934881916\n",
      "Epoch 72 \t Batch 400 \t Training Loss: 46.28792950630188\n",
      "Epoch 72 \t Batch 420 \t Training Loss: 46.295814450581865\n",
      "Epoch 72 \t Batch 440 \t Training Loss: 46.27206536206332\n",
      "Epoch 72 \t Batch 460 \t Training Loss: 46.2728469931561\n",
      "Epoch 72 \t Batch 480 \t Training Loss: 46.13182830810547\n",
      "Epoch 72 \t Batch 500 \t Training Loss: 46.07574115753174\n",
      "Epoch 72 \t Batch 520 \t Training Loss: 46.033980479607216\n",
      "Epoch 72 \t Batch 540 \t Training Loss: 46.06857449566876\n",
      "Epoch 72 \t Batch 560 \t Training Loss: 46.031660495485575\n",
      "Epoch 72 \t Batch 580 \t Training Loss: 46.074274286730535\n",
      "Epoch 72 \t Batch 600 \t Training Loss: 46.001813888549805\n",
      "Epoch 72 \t Batch 620 \t Training Loss: 45.97260349027572\n",
      "Epoch 72 \t Batch 640 \t Training Loss: 45.95887733101845\n",
      "Epoch 72 \t Batch 660 \t Training Loss: 45.97417151711204\n",
      "Epoch 72 \t Batch 680 \t Training Loss: 45.95057136872236\n",
      "Epoch 72 \t Batch 700 \t Training Loss: 45.948240618024556\n",
      "Epoch 72 \t Batch 720 \t Training Loss: 45.984221654468115\n",
      "Epoch 72 \t Batch 740 \t Training Loss: 46.001489742382155\n",
      "Epoch 72 \t Batch 760 \t Training Loss: 46.01158823716013\n",
      "Epoch 72 \t Batch 780 \t Training Loss: 45.98761029854799\n",
      "Epoch 72 \t Batch 800 \t Training Loss: 45.99415057182312\n",
      "Epoch 72 \t Batch 820 \t Training Loss: 45.98762658282024\n",
      "Epoch 72 \t Batch 840 \t Training Loss: 45.993533770243324\n",
      "Epoch 72 \t Batch 860 \t Training Loss: 46.0064763313116\n",
      "Epoch 72 \t Batch 880 \t Training Loss: 46.03000701557506\n",
      "Epoch 72 \t Batch 900 \t Training Loss: 46.02444527943929\n",
      "Epoch 72 \t Batch 20 \t Validation Loss: 21.351940059661864\n",
      "Epoch 72 \t Batch 40 \t Validation Loss: 22.245907044410707\n",
      "Epoch 72 \t Batch 60 \t Validation Loss: 22.401793877283733\n",
      "Epoch 72 \t Batch 80 \t Validation Loss: 22.871798551082613\n",
      "Epoch 72 \t Batch 100 \t Validation Loss: 24.29703791618347\n",
      "Epoch 72 \t Batch 120 \t Validation Loss: 25.31550752321879\n",
      "Epoch 72 \t Batch 140 \t Validation Loss: 25.711467157091413\n",
      "Epoch 72 \t Batch 160 \t Validation Loss: 27.43401074409485\n",
      "Epoch 72 \t Batch 180 \t Validation Loss: 30.913595056533815\n",
      "Epoch 72 \t Batch 200 \t Validation Loss: 32.24327575206757\n",
      "Epoch 72 \t Batch 220 \t Validation Loss: 33.423340038819745\n",
      "Epoch 72 \t Batch 240 \t Validation Loss: 33.88232024510702\n",
      "Epoch 72 \t Batch 260 \t Validation Loss: 35.86239582942083\n",
      "Epoch 72 \t Batch 280 \t Validation Loss: 36.95454283441816\n",
      "Epoch 72 \t Batch 300 \t Validation Loss: 38.0649982770284\n",
      "Epoch 72 \t Batch 320 \t Validation Loss: 38.53793489336967\n",
      "Epoch 72 \t Batch 340 \t Validation Loss: 38.45195635066313\n",
      "Epoch 72 \t Batch 360 \t Validation Loss: 38.25404314464993\n",
      "Epoch 72 \t Batch 380 \t Validation Loss: 38.441332054138186\n",
      "Epoch 72 \t Batch 400 \t Validation Loss: 38.025340156555174\n",
      "Epoch 72 \t Batch 420 \t Validation Loss: 38.048986357734314\n",
      "Epoch 72 \t Batch 440 \t Validation Loss: 37.77182727727023\n",
      "Epoch 72 \t Batch 460 \t Validation Loss: 37.99225013152413\n",
      "Epoch 72 \t Batch 480 \t Validation Loss: 38.46567192872365\n",
      "Epoch 72 \t Batch 500 \t Validation Loss: 38.16433973693848\n",
      "Epoch 72 \t Batch 520 \t Validation Loss: 37.918441457014815\n",
      "Epoch 72 \t Batch 540 \t Validation Loss: 37.669407516055635\n",
      "Epoch 72 \t Batch 560 \t Validation Loss: 37.46911295822689\n",
      "Epoch 72 \t Batch 580 \t Validation Loss: 37.19310002162539\n",
      "Epoch 72 \t Batch 600 \t Validation Loss: 37.42415651957194\n",
      "Epoch 72 Training Loss: 46.03045196699602 Validation Loss: 38.089132742448285\n",
      "Epoch 72 completed\n",
      "Epoch 73 \t Batch 20 \t Training Loss: 46.89062004089355\n",
      "Epoch 73 \t Batch 40 \t Training Loss: 46.501688957214355\n",
      "Epoch 73 \t Batch 60 \t Training Loss: 46.45048224131266\n",
      "Epoch 73 \t Batch 80 \t Training Loss: 46.31293225288391\n",
      "Epoch 73 \t Batch 100 \t Training Loss: 46.45606170654297\n",
      "Epoch 73 \t Batch 120 \t Training Loss: 46.31801274617513\n",
      "Epoch 73 \t Batch 140 \t Training Loss: 46.395682907104494\n",
      "Epoch 73 \t Batch 160 \t Training Loss: 46.34221346378327\n",
      "Epoch 73 \t Batch 180 \t Training Loss: 46.180485513475205\n",
      "Epoch 73 \t Batch 200 \t Training Loss: 46.25629377365112\n",
      "Epoch 73 \t Batch 220 \t Training Loss: 46.21293910633434\n",
      "Epoch 73 \t Batch 240 \t Training Loss: 46.274988412857056\n",
      "Epoch 73 \t Batch 260 \t Training Loss: 46.25827187758226\n",
      "Epoch 73 \t Batch 280 \t Training Loss: 46.26125834328788\n",
      "Epoch 73 \t Batch 300 \t Training Loss: 46.299518559773766\n",
      "Epoch 73 \t Batch 320 \t Training Loss: 46.248018217086795\n",
      "Epoch 73 \t Batch 340 \t Training Loss: 46.205596160888675\n",
      "Epoch 73 \t Batch 360 \t Training Loss: 46.152988794114854\n",
      "Epoch 73 \t Batch 380 \t Training Loss: 46.11939139115183\n",
      "Epoch 73 \t Batch 400 \t Training Loss: 46.165704298019406\n",
      "Epoch 73 \t Batch 420 \t Training Loss: 46.10821834745861\n",
      "Epoch 73 \t Batch 440 \t Training Loss: 46.06461555307562\n",
      "Epoch 73 \t Batch 460 \t Training Loss: 46.03541968801747\n",
      "Epoch 73 \t Batch 480 \t Training Loss: 45.99515967369079\n",
      "Epoch 73 \t Batch 500 \t Training Loss: 45.99716667175293\n",
      "Epoch 73 \t Batch 520 \t Training Loss: 45.98688073525062\n",
      "Epoch 73 \t Batch 540 \t Training Loss: 46.00693460393835\n",
      "Epoch 73 \t Batch 560 \t Training Loss: 45.968938323429654\n",
      "Epoch 73 \t Batch 580 \t Training Loss: 45.9792368066722\n",
      "Epoch 73 \t Batch 600 \t Training Loss: 45.97586852391561\n",
      "Epoch 73 \t Batch 620 \t Training Loss: 45.98382819391066\n",
      "Epoch 73 \t Batch 640 \t Training Loss: 45.978526616096495\n",
      "Epoch 73 \t Batch 660 \t Training Loss: 45.98784998113459\n",
      "Epoch 73 \t Batch 680 \t Training Loss: 45.97429796106675\n",
      "Epoch 73 \t Batch 700 \t Training Loss: 45.997480109078545\n",
      "Epoch 73 \t Batch 720 \t Training Loss: 45.989296701219345\n",
      "Epoch 73 \t Batch 740 \t Training Loss: 46.00564787065661\n",
      "Epoch 73 \t Batch 760 \t Training Loss: 46.01033855739393\n",
      "Epoch 73 \t Batch 780 \t Training Loss: 46.01465550936185\n",
      "Epoch 73 \t Batch 800 \t Training Loss: 46.02197802066803\n",
      "Epoch 73 \t Batch 820 \t Training Loss: 46.06082740411526\n",
      "Epoch 73 \t Batch 840 \t Training Loss: 46.08010502769834\n",
      "Epoch 73 \t Batch 860 \t Training Loss: 46.09914779663086\n",
      "Epoch 73 \t Batch 880 \t Training Loss: 46.07811119773171\n",
      "Epoch 73 \t Batch 900 \t Training Loss: 46.070409821404354\n",
      "Epoch 73 \t Batch 20 \t Validation Loss: 16.964055156707765\n",
      "Epoch 73 \t Batch 40 \t Validation Loss: 20.216699802875517\n",
      "Epoch 73 \t Batch 60 \t Validation Loss: 19.803913489977518\n",
      "Epoch 73 \t Batch 80 \t Validation Loss: 20.437487441301347\n",
      "Epoch 73 \t Batch 100 \t Validation Loss: 22.113111863136293\n",
      "Epoch 73 \t Batch 120 \t Validation Loss: 23.67932545741399\n",
      "Epoch 73 \t Batch 140 \t Validation Loss: 24.43802434716906\n",
      "Epoch 73 \t Batch 160 \t Validation Loss: 26.59869660437107\n",
      "Epoch 73 \t Batch 180 \t Validation Loss: 30.582145852512785\n",
      "Epoch 73 \t Batch 200 \t Validation Loss: 32.21198242425918\n",
      "Epoch 73 \t Batch 220 \t Validation Loss: 33.640919609503314\n",
      "Epoch 73 \t Batch 240 \t Validation Loss: 34.25118136604627\n",
      "Epoch 73 \t Batch 260 \t Validation Loss: 36.50789026663853\n",
      "Epoch 73 \t Batch 280 \t Validation Loss: 37.73524947336742\n",
      "Epoch 73 \t Batch 300 \t Validation Loss: 38.995931073824565\n",
      "Epoch 73 \t Batch 320 \t Validation Loss: 39.563768507540225\n",
      "Epoch 73 \t Batch 340 \t Validation Loss: 39.50690800302169\n",
      "Epoch 73 \t Batch 360 \t Validation Loss: 39.37258837090598\n",
      "Epoch 73 \t Batch 380 \t Validation Loss: 39.598114672460056\n",
      "Epoch 73 \t Batch 400 \t Validation Loss: 39.162720428705214\n",
      "Epoch 73 \t Batch 420 \t Validation Loss: 39.164588429814295\n",
      "Epoch 73 \t Batch 440 \t Validation Loss: 38.82595315521414\n",
      "Epoch 73 \t Batch 460 \t Validation Loss: 39.05674209698387\n",
      "Epoch 73 \t Batch 480 \t Validation Loss: 39.53368159631888\n",
      "Epoch 73 \t Batch 500 \t Validation Loss: 39.25884946155548\n",
      "Epoch 73 \t Batch 520 \t Validation Loss: 39.00070952727244\n",
      "Epoch 73 \t Batch 540 \t Validation Loss: 38.74428030031699\n",
      "Epoch 73 \t Batch 560 \t Validation Loss: 38.51166906441961\n",
      "Epoch 73 \t Batch 580 \t Validation Loss: 38.236773498304956\n",
      "Epoch 73 \t Batch 600 \t Validation Loss: 38.444186516602834\n",
      "Epoch 73 Training Loss: 46.05444298913721 Validation Loss: 39.09440762114215\n",
      "Epoch 73 completed\n",
      "Epoch 74 \t Batch 20 \t Training Loss: 46.24335689544678\n",
      "Epoch 74 \t Batch 40 \t Training Loss: 46.31273603439331\n",
      "Epoch 74 \t Batch 60 \t Training Loss: 46.262793922424315\n",
      "Epoch 74 \t Batch 80 \t Training Loss: 46.427654123306276\n",
      "Epoch 74 \t Batch 100 \t Training Loss: 46.37709991455078\n",
      "Epoch 74 \t Batch 120 \t Training Loss: 46.24182252883911\n",
      "Epoch 74 \t Batch 140 \t Training Loss: 46.35232650211879\n",
      "Epoch 74 \t Batch 160 \t Training Loss: 46.096100378036496\n",
      "Epoch 74 \t Batch 180 \t Training Loss: 46.19987138112386\n",
      "Epoch 74 \t Batch 200 \t Training Loss: 46.1764786529541\n",
      "Epoch 74 \t Batch 220 \t Training Loss: 46.17739994742654\n",
      "Epoch 74 \t Batch 240 \t Training Loss: 46.21935796737671\n",
      "Epoch 74 \t Batch 260 \t Training Loss: 46.27883708660419\n",
      "Epoch 74 \t Batch 280 \t Training Loss: 46.37676830291748\n",
      "Epoch 74 \t Batch 300 \t Training Loss: 46.4519650777181\n",
      "Epoch 74 \t Batch 320 \t Training Loss: 46.34172282218933\n",
      "Epoch 74 \t Batch 340 \t Training Loss: 46.28489778182086\n",
      "Epoch 74 \t Batch 360 \t Training Loss: 46.37622283299764\n",
      "Epoch 74 \t Batch 380 \t Training Loss: 46.38546398564389\n",
      "Epoch 74 \t Batch 400 \t Training Loss: 46.34010263442993\n",
      "Epoch 74 \t Batch 420 \t Training Loss: 46.29789291563488\n",
      "Epoch 74 \t Batch 440 \t Training Loss: 46.24846628362482\n",
      "Epoch 74 \t Batch 460 \t Training Loss: 46.209209301160726\n",
      "Epoch 74 \t Batch 480 \t Training Loss: 46.16631433169047\n",
      "Epoch 74 \t Batch 500 \t Training Loss: 46.139054901123046\n",
      "Epoch 74 \t Batch 520 \t Training Loss: 46.119443878760706\n",
      "Epoch 74 \t Batch 540 \t Training Loss: 46.13590176193802\n",
      "Epoch 74 \t Batch 560 \t Training Loss: 46.11756700788226\n",
      "Epoch 74 \t Batch 580 \t Training Loss: 46.1215977241253\n",
      "Epoch 74 \t Batch 600 \t Training Loss: 46.133036092122396\n",
      "Epoch 74 \t Batch 620 \t Training Loss: 46.11700189651982\n",
      "Epoch 74 \t Batch 640 \t Training Loss: 46.12390891313553\n",
      "Epoch 74 \t Batch 660 \t Training Loss: 46.15130733721184\n",
      "Epoch 74 \t Batch 680 \t Training Loss: 46.13496552074657\n",
      "Epoch 74 \t Batch 700 \t Training Loss: 46.15918577466692\n",
      "Epoch 74 \t Batch 720 \t Training Loss: 46.09021500481499\n",
      "Epoch 74 \t Batch 740 \t Training Loss: 46.098678016662596\n",
      "Epoch 74 \t Batch 760 \t Training Loss: 46.07551509957565\n",
      "Epoch 74 \t Batch 780 \t Training Loss: 46.044804802919046\n",
      "Epoch 74 \t Batch 800 \t Training Loss: 46.03208611488342\n",
      "Epoch 74 \t Batch 820 \t Training Loss: 46.038673968431425\n",
      "Epoch 74 \t Batch 840 \t Training Loss: 46.0466627348037\n",
      "Epoch 74 \t Batch 860 \t Training Loss: 46.0308975663296\n",
      "Epoch 74 \t Batch 880 \t Training Loss: 45.9987250328064\n",
      "Epoch 74 \t Batch 900 \t Training Loss: 45.994277771843805\n",
      "Epoch 74 \t Batch 20 \t Validation Loss: 21.00343370437622\n",
      "Epoch 74 \t Batch 40 \t Validation Loss: 21.78372151851654\n",
      "Epoch 74 \t Batch 60 \t Validation Loss: 21.855440870920816\n",
      "Epoch 74 \t Batch 80 \t Validation Loss: 22.204053890705108\n",
      "Epoch 74 \t Batch 100 \t Validation Loss: 23.593043184280397\n",
      "Epoch 74 \t Batch 120 \t Validation Loss: 24.872613008817037\n",
      "Epoch 74 \t Batch 140 \t Validation Loss: 25.40922067505973\n",
      "Epoch 74 \t Batch 160 \t Validation Loss: 27.171321445703505\n",
      "Epoch 74 \t Batch 180 \t Validation Loss: 30.753376065360175\n",
      "Epoch 74 \t Batch 200 \t Validation Loss: 32.145162014961244\n",
      "Epoch 74 \t Batch 220 \t Validation Loss: 33.23616992343556\n",
      "Epoch 74 \t Batch 240 \t Validation Loss: 33.66158185799917\n",
      "Epoch 74 \t Batch 260 \t Validation Loss: 35.66410115315364\n",
      "Epoch 74 \t Batch 280 \t Validation Loss: 36.73082614966801\n",
      "Epoch 74 \t Batch 300 \t Validation Loss: 37.8460929775238\n",
      "Epoch 74 \t Batch 320 \t Validation Loss: 38.314531716704366\n",
      "Epoch 74 \t Batch 340 \t Validation Loss: 38.24636131454916\n",
      "Epoch 74 \t Batch 360 \t Validation Loss: 38.0685341808531\n",
      "Epoch 74 \t Batch 380 \t Validation Loss: 38.262141862668486\n",
      "Epoch 74 \t Batch 400 \t Validation Loss: 37.88318577528\n",
      "Epoch 74 \t Batch 420 \t Validation Loss: 37.90328045118423\n",
      "Epoch 74 \t Batch 440 \t Validation Loss: 37.61934146230871\n",
      "Epoch 74 \t Batch 460 \t Validation Loss: 37.83948265780573\n",
      "Epoch 74 \t Batch 480 \t Validation Loss: 38.34631843765577\n",
      "Epoch 74 \t Batch 500 \t Validation Loss: 38.05627500343323\n",
      "Epoch 74 \t Batch 520 \t Validation Loss: 37.82605336079231\n",
      "Epoch 74 \t Batch 540 \t Validation Loss: 37.61135319073995\n",
      "Epoch 74 \t Batch 560 \t Validation Loss: 37.454013124534065\n",
      "Epoch 74 \t Batch 580 \t Validation Loss: 37.268706514095435\n",
      "Epoch 74 \t Batch 600 \t Validation Loss: 37.51832220872243\n",
      "Epoch 74 Training Loss: 46.00759356466479 Validation Loss: 38.21308003617572\n",
      "Epoch 74 completed\n",
      "Epoch 75 \t Batch 20 \t Training Loss: 44.879541015625\n",
      "Epoch 75 \t Batch 40 \t Training Loss: 44.83848400115967\n",
      "Epoch 75 \t Batch 60 \t Training Loss: 45.04209912618001\n",
      "Epoch 75 \t Batch 80 \t Training Loss: 45.00201907157898\n",
      "Epoch 75 \t Batch 100 \t Training Loss: 44.90049125671387\n",
      "Epoch 75 \t Batch 120 \t Training Loss: 45.05032447179158\n",
      "Epoch 75 \t Batch 140 \t Training Loss: 45.24799807412284\n",
      "Epoch 75 \t Batch 160 \t Training Loss: 45.32981543540954\n",
      "Epoch 75 \t Batch 180 \t Training Loss: 45.38873920440674\n",
      "Epoch 75 \t Batch 200 \t Training Loss: 45.42277250289917\n",
      "Epoch 75 \t Batch 220 \t Training Loss: 45.39834778525613\n",
      "Epoch 75 \t Batch 240 \t Training Loss: 45.352941354115806\n",
      "Epoch 75 \t Batch 260 \t Training Loss: 45.41490164536696\n",
      "Epoch 75 \t Batch 280 \t Training Loss: 45.43596933909825\n",
      "Epoch 75 \t Batch 300 \t Training Loss: 45.527148729960125\n",
      "Epoch 75 \t Batch 320 \t Training Loss: 45.55863258838654\n",
      "Epoch 75 \t Batch 340 \t Training Loss: 45.57891979217529\n",
      "Epoch 75 \t Batch 360 \t Training Loss: 45.61549676259359\n",
      "Epoch 75 \t Batch 380 \t Training Loss: 45.75173547644364\n",
      "Epoch 75 \t Batch 400 \t Training Loss: 45.77624543190002\n",
      "Epoch 75 \t Batch 420 \t Training Loss: 45.81805333637056\n",
      "Epoch 75 \t Batch 440 \t Training Loss: 45.87511056553234\n",
      "Epoch 75 \t Batch 460 \t Training Loss: 45.877030704332434\n",
      "Epoch 75 \t Batch 480 \t Training Loss: 45.84284099737803\n",
      "Epoch 75 \t Batch 500 \t Training Loss: 45.8217816696167\n",
      "Epoch 75 \t Batch 520 \t Training Loss: 45.79636163711548\n",
      "Epoch 75 \t Batch 540 \t Training Loss: 45.80936503233733\n",
      "Epoch 75 \t Batch 560 \t Training Loss: 45.83661146163941\n",
      "Epoch 75 \t Batch 580 \t Training Loss: 45.87515671828697\n",
      "Epoch 75 \t Batch 600 \t Training Loss: 45.92621831258138\n",
      "Epoch 75 \t Batch 620 \t Training Loss: 45.91165199279785\n",
      "Epoch 75 \t Batch 640 \t Training Loss: 45.88986758589745\n",
      "Epoch 75 \t Batch 660 \t Training Loss: 45.9241445830374\n",
      "Epoch 75 \t Batch 680 \t Training Loss: 45.91225836136762\n",
      "Epoch 75 \t Batch 700 \t Training Loss: 45.940261028834755\n",
      "Epoch 75 \t Batch 720 \t Training Loss: 45.93694622251723\n",
      "Epoch 75 \t Batch 740 \t Training Loss: 45.943607252997325\n",
      "Epoch 75 \t Batch 760 \t Training Loss: 45.93465822119462\n",
      "Epoch 75 \t Batch 780 \t Training Loss: 45.97264508467454\n",
      "Epoch 75 \t Batch 800 \t Training Loss: 45.96786166667938\n",
      "Epoch 75 \t Batch 820 \t Training Loss: 45.98272906047542\n",
      "Epoch 75 \t Batch 840 \t Training Loss: 45.96704647881644\n",
      "Epoch 75 \t Batch 860 \t Training Loss: 45.9436889249225\n",
      "Epoch 75 \t Batch 880 \t Training Loss: 45.97191565253518\n",
      "Epoch 75 \t Batch 900 \t Training Loss: 45.954662259419756\n",
      "Epoch 75 \t Batch 20 \t Validation Loss: 25.994501495361327\n",
      "Epoch 75 \t Batch 40 \t Validation Loss: 26.50143449306488\n",
      "Epoch 75 \t Batch 60 \t Validation Loss: 26.32303156852722\n",
      "Epoch 75 \t Batch 80 \t Validation Loss: 26.47770447731018\n",
      "Epoch 75 \t Batch 100 \t Validation Loss: 27.021981201171876\n",
      "Epoch 75 \t Batch 120 \t Validation Loss: 27.611935289700828\n",
      "Epoch 75 \t Batch 140 \t Validation Loss: 27.74743381908962\n",
      "Epoch 75 \t Batch 160 \t Validation Loss: 29.193039590120314\n",
      "Epoch 75 \t Batch 180 \t Validation Loss: 32.34834877120124\n",
      "Epoch 75 \t Batch 200 \t Validation Loss: 33.470834403038026\n",
      "Epoch 75 \t Batch 220 \t Validation Loss: 34.3915613044392\n",
      "Epoch 75 \t Batch 240 \t Validation Loss: 34.68825887441635\n",
      "Epoch 75 \t Batch 260 \t Validation Loss: 36.491167167516856\n",
      "Epoch 75 \t Batch 280 \t Validation Loss: 37.37493270805904\n",
      "Epoch 75 \t Batch 300 \t Validation Loss: 38.416378761927284\n",
      "Epoch 75 \t Batch 320 \t Validation Loss: 38.85752310156822\n",
      "Epoch 75 \t Batch 340 \t Validation Loss: 38.75907328549553\n",
      "Epoch 75 \t Batch 360 \t Validation Loss: 38.558890671200224\n",
      "Epoch 75 \t Batch 380 \t Validation Loss: 38.72502152794286\n",
      "Epoch 75 \t Batch 400 \t Validation Loss: 38.33581276893616\n",
      "Epoch 75 \t Batch 420 \t Validation Loss: 38.33290835562207\n",
      "Epoch 75 \t Batch 440 \t Validation Loss: 38.03284013271332\n",
      "Epoch 75 \t Batch 460 \t Validation Loss: 38.27351623825405\n",
      "Epoch 75 \t Batch 480 \t Validation Loss: 38.767716974020004\n",
      "Epoch 75 \t Batch 500 \t Validation Loss: 38.465906732559205\n",
      "Epoch 75 \t Batch 520 \t Validation Loss: 38.26521771871126\n",
      "Epoch 75 \t Batch 540 \t Validation Loss: 38.030004536664045\n",
      "Epoch 75 \t Batch 560 \t Validation Loss: 37.833474608830045\n",
      "Epoch 75 \t Batch 580 \t Validation Loss: 37.60348803092693\n",
      "Epoch 75 \t Batch 600 \t Validation Loss: 37.8192138004303\n",
      "Epoch 75 Training Loss: 45.96114796146572 Validation Loss: 38.49151255867698\n",
      "Epoch 75 completed\n",
      "Epoch 76 \t Batch 20 \t Training Loss: 44.74197006225586\n",
      "Epoch 76 \t Batch 40 \t Training Loss: 45.56991558074951\n",
      "Epoch 76 \t Batch 60 \t Training Loss: 45.43417542775472\n",
      "Epoch 76 \t Batch 80 \t Training Loss: 45.67200403213501\n",
      "Epoch 76 \t Batch 100 \t Training Loss: 45.87525356292725\n",
      "Epoch 76 \t Batch 120 \t Training Loss: 45.78669315973918\n",
      "Epoch 76 \t Batch 140 \t Training Loss: 45.87885284423828\n",
      "Epoch 76 \t Batch 160 \t Training Loss: 45.70311694145202\n",
      "Epoch 76 \t Batch 180 \t Training Loss: 45.70541583167182\n",
      "Epoch 76 \t Batch 200 \t Training Loss: 45.697364044189456\n",
      "Epoch 76 \t Batch 220 \t Training Loss: 45.79436109716242\n",
      "Epoch 76 \t Batch 240 \t Training Loss: 45.87073726654053\n",
      "Epoch 76 \t Batch 260 \t Training Loss: 45.961567115783694\n",
      "Epoch 76 \t Batch 280 \t Training Loss: 45.95700454711914\n",
      "Epoch 76 \t Batch 300 \t Training Loss: 45.96196564992269\n",
      "Epoch 76 \t Batch 320 \t Training Loss: 45.836986446380614\n",
      "Epoch 76 \t Batch 340 \t Training Loss: 45.91619189767277\n",
      "Epoch 76 \t Batch 360 \t Training Loss: 45.95765161514282\n",
      "Epoch 76 \t Batch 380 \t Training Loss: 45.87073794917056\n",
      "Epoch 76 \t Batch 400 \t Training Loss: 45.84146018981934\n",
      "Epoch 76 \t Batch 420 \t Training Loss: 45.79127175467355\n",
      "Epoch 76 \t Batch 440 \t Training Loss: 45.750116443634035\n",
      "Epoch 76 \t Batch 460 \t Training Loss: 45.7615794306216\n",
      "Epoch 76 \t Batch 480 \t Training Loss: 45.77849686940511\n",
      "Epoch 76 \t Batch 500 \t Training Loss: 45.83100157165527\n",
      "Epoch 76 \t Batch 520 \t Training Loss: 45.88946951352633\n",
      "Epoch 76 \t Batch 540 \t Training Loss: 45.8713596979777\n",
      "Epoch 76 \t Batch 560 \t Training Loss: 45.872677721296036\n",
      "Epoch 76 \t Batch 580 \t Training Loss: 45.81740062976706\n",
      "Epoch 76 \t Batch 600 \t Training Loss: 45.804313036600746\n",
      "Epoch 76 \t Batch 620 \t Training Loss: 45.84986174183507\n",
      "Epoch 76 \t Batch 640 \t Training Loss: 45.88907681703567\n",
      "Epoch 76 \t Batch 660 \t Training Loss: 45.88045087294145\n",
      "Epoch 76 \t Batch 680 \t Training Loss: 45.89397425932043\n",
      "Epoch 76 \t Batch 700 \t Training Loss: 45.89146607535226\n",
      "Epoch 76 \t Batch 720 \t Training Loss: 45.890656312306724\n",
      "Epoch 76 \t Batch 740 \t Training Loss: 45.89195955121839\n",
      "Epoch 76 \t Batch 760 \t Training Loss: 45.913148854908194\n",
      "Epoch 76 \t Batch 780 \t Training Loss: 45.899828407091974\n",
      "Epoch 76 \t Batch 800 \t Training Loss: 45.852390117645264\n",
      "Epoch 76 \t Batch 820 \t Training Loss: 45.862861614692505\n",
      "Epoch 76 \t Batch 840 \t Training Loss: 45.91364670708066\n",
      "Epoch 76 \t Batch 860 \t Training Loss: 45.91458057137423\n",
      "Epoch 76 \t Batch 880 \t Training Loss: 45.9287407875061\n",
      "Epoch 76 \t Batch 900 \t Training Loss: 45.90816822052002\n",
      "Epoch 76 \t Batch 20 \t Validation Loss: 21.328369474411012\n",
      "Epoch 76 \t Batch 40 \t Validation Loss: 22.79219431877136\n",
      "Epoch 76 \t Batch 60 \t Validation Loss: 22.564936542510985\n",
      "Epoch 76 \t Batch 80 \t Validation Loss: 23.13907766342163\n",
      "Epoch 76 \t Batch 100 \t Validation Loss: 24.297067203521728\n",
      "Epoch 76 \t Batch 120 \t Validation Loss: 25.393853775660197\n",
      "Epoch 76 \t Batch 140 \t Validation Loss: 25.87550424848284\n",
      "Epoch 76 \t Batch 160 \t Validation Loss: 27.747476410865783\n",
      "Epoch 76 \t Batch 180 \t Validation Loss: 31.335653999116687\n",
      "Epoch 76 \t Batch 200 \t Validation Loss: 32.75990959644318\n",
      "Epoch 76 \t Batch 220 \t Validation Loss: 33.993661390651354\n",
      "Epoch 76 \t Batch 240 \t Validation Loss: 34.479746933778124\n",
      "Epoch 76 \t Batch 260 \t Validation Loss: 36.57486646725581\n",
      "Epoch 76 \t Batch 280 \t Validation Loss: 37.71029270717076\n",
      "Epoch 76 \t Batch 300 \t Validation Loss: 38.81459192276001\n",
      "Epoch 76 \t Batch 320 \t Validation Loss: 39.2686618745327\n",
      "Epoch 76 \t Batch 340 \t Validation Loss: 39.18177456575282\n",
      "Epoch 76 \t Batch 360 \t Validation Loss: 39.01898908085293\n",
      "Epoch 76 \t Batch 380 \t Validation Loss: 39.23816120248092\n",
      "Epoch 76 \t Batch 400 \t Validation Loss: 38.82813296794891\n",
      "Epoch 76 \t Batch 420 \t Validation Loss: 38.80046243213472\n",
      "Epoch 76 \t Batch 440 \t Validation Loss: 38.48919689872048\n",
      "Epoch 76 \t Batch 460 \t Validation Loss: 38.70801132865574\n",
      "Epoch 76 \t Batch 480 \t Validation Loss: 39.17663252353668\n",
      "Epoch 76 \t Batch 500 \t Validation Loss: 38.8648112487793\n",
      "Epoch 76 \t Batch 520 \t Validation Loss: 38.67212822987483\n",
      "Epoch 76 \t Batch 540 \t Validation Loss: 38.45753778881497\n",
      "Epoch 76 \t Batch 560 \t Validation Loss: 38.29891545772553\n",
      "Epoch 76 \t Batch 580 \t Validation Loss: 38.10408670491186\n",
      "Epoch 76 \t Batch 600 \t Validation Loss: 38.33839781125386\n",
      "Epoch 76 Training Loss: 45.93981200606118 Validation Loss: 39.02670118096587\n",
      "Epoch 76 completed\n",
      "Epoch 77 \t Batch 20 \t Training Loss: 46.293533897399904\n",
      "Epoch 77 \t Batch 40 \t Training Loss: 45.72378168106079\n",
      "Epoch 77 \t Batch 60 \t Training Loss: 45.58781960805257\n",
      "Epoch 77 \t Batch 80 \t Training Loss: 45.642314195632935\n",
      "Epoch 77 \t Batch 100 \t Training Loss: 45.74908393859863\n",
      "Epoch 77 \t Batch 120 \t Training Loss: 45.906432978312175\n",
      "Epoch 77 \t Batch 140 \t Training Loss: 45.882130541120254\n",
      "Epoch 77 \t Batch 160 \t Training Loss: 46.187704992294314\n",
      "Epoch 77 \t Batch 180 \t Training Loss: 46.067231051127116\n",
      "Epoch 77 \t Batch 200 \t Training Loss: 46.02602893829346\n",
      "Epoch 77 \t Batch 220 \t Training Loss: 45.952613067626956\n",
      "Epoch 77 \t Batch 240 \t Training Loss: 46.02946802775065\n",
      "Epoch 77 \t Batch 260 \t Training Loss: 46.02849148970384\n",
      "Epoch 77 \t Batch 280 \t Training Loss: 46.032537419455394\n",
      "Epoch 77 \t Batch 300 \t Training Loss: 46.028969332377116\n",
      "Epoch 77 \t Batch 320 \t Training Loss: 46.077040183544156\n",
      "Epoch 77 \t Batch 340 \t Training Loss: 46.02749404907227\n",
      "Epoch 77 \t Batch 360 \t Training Loss: 45.99792579015096\n",
      "Epoch 77 \t Batch 380 \t Training Loss: 45.90612673508493\n",
      "Epoch 77 \t Batch 400 \t Training Loss: 45.9090385723114\n",
      "Epoch 77 \t Batch 420 \t Training Loss: 45.86585838681176\n",
      "Epoch 77 \t Batch 440 \t Training Loss: 45.856694438240744\n",
      "Epoch 77 \t Batch 460 \t Training Loss: 45.878374049974525\n",
      "Epoch 77 \t Batch 480 \t Training Loss: 45.88160512447357\n",
      "Epoch 77 \t Batch 500 \t Training Loss: 45.91569275665283\n",
      "Epoch 77 \t Batch 520 \t Training Loss: 45.898229158841644\n",
      "Epoch 77 \t Batch 540 \t Training Loss: 45.8990557635272\n",
      "Epoch 77 \t Batch 560 \t Training Loss: 45.869857454299925\n",
      "Epoch 77 \t Batch 580 \t Training Loss: 45.83626449848043\n",
      "Epoch 77 \t Batch 600 \t Training Loss: 45.88065660476685\n",
      "Epoch 77 \t Batch 620 \t Training Loss: 45.90333685721121\n",
      "Epoch 77 \t Batch 640 \t Training Loss: 45.93212042450905\n",
      "Epoch 77 \t Batch 660 \t Training Loss: 45.92311584010269\n",
      "Epoch 77 \t Batch 680 \t Training Loss: 45.963355086831484\n",
      "Epoch 77 \t Batch 700 \t Training Loss: 45.94647212437221\n",
      "Epoch 77 \t Batch 720 \t Training Loss: 45.965591494242354\n",
      "Epoch 77 \t Batch 740 \t Training Loss: 45.97215802476213\n",
      "Epoch 77 \t Batch 760 \t Training Loss: 45.96282290408486\n",
      "Epoch 77 \t Batch 780 \t Training Loss: 45.93932053003556\n",
      "Epoch 77 \t Batch 800 \t Training Loss: 45.949118089675906\n",
      "Epoch 77 \t Batch 820 \t Training Loss: 45.952355570909454\n",
      "Epoch 77 \t Batch 840 \t Training Loss: 45.96361096245902\n",
      "Epoch 77 \t Batch 860 \t Training Loss: 45.915932216200716\n",
      "Epoch 77 \t Batch 880 \t Training Loss: 45.90665338689631\n",
      "Epoch 77 \t Batch 900 \t Training Loss: 45.926841523912216\n",
      "Epoch 77 \t Batch 20 \t Validation Loss: 14.378045463562012\n",
      "Epoch 77 \t Batch 40 \t Validation Loss: 17.544942688941955\n",
      "Epoch 77 \t Batch 60 \t Validation Loss: 17.265874973932902\n",
      "Epoch 77 \t Batch 80 \t Validation Loss: 18.184579455852507\n",
      "Epoch 77 \t Batch 100 \t Validation Loss: 20.446060190200807\n",
      "Epoch 77 \t Batch 120 \t Validation Loss: 22.02577633857727\n",
      "Epoch 77 \t Batch 140 \t Validation Loss: 22.951694079807826\n",
      "Epoch 77 \t Batch 160 \t Validation Loss: 24.94815477132797\n",
      "Epoch 77 \t Batch 180 \t Validation Loss: 28.231912485758464\n",
      "Epoch 77 \t Batch 200 \t Validation Loss: 29.591637029647828\n",
      "Epoch 77 \t Batch 220 \t Validation Loss: 30.69183287187056\n",
      "Epoch 77 \t Batch 240 \t Validation Loss: 31.153675774733227\n",
      "Epoch 77 \t Batch 260 \t Validation Loss: 33.10036887755761\n",
      "Epoch 77 \t Batch 280 \t Validation Loss: 34.19874109881265\n",
      "Epoch 77 \t Batch 300 \t Validation Loss: 35.22325825691223\n",
      "Epoch 77 \t Batch 320 \t Validation Loss: 35.69455129802227\n",
      "Epoch 77 \t Batch 340 \t Validation Loss: 35.7673660699059\n",
      "Epoch 77 \t Batch 360 \t Validation Loss: 35.70298267735375\n",
      "Epoch 77 \t Batch 380 \t Validation Loss: 36.01187600336577\n",
      "Epoch 77 \t Batch 400 \t Validation Loss: 35.81508511781693\n",
      "Epoch 77 \t Batch 420 \t Validation Loss: 35.9440091019585\n",
      "Epoch 77 \t Batch 440 \t Validation Loss: 35.80606518875469\n",
      "Epoch 77 \t Batch 460 \t Validation Loss: 36.16052640624668\n",
      "Epoch 77 \t Batch 480 \t Validation Loss: 36.70831427375476\n",
      "Epoch 77 \t Batch 500 \t Validation Loss: 36.4826486415863\n",
      "Epoch 77 \t Batch 520 \t Validation Loss: 36.4360226722864\n",
      "Epoch 77 \t Batch 540 \t Validation Loss: 36.324389943370115\n",
      "Epoch 77 \t Batch 560 \t Validation Loss: 36.2595300759588\n",
      "Epoch 77 \t Batch 580 \t Validation Loss: 36.194448295132865\n",
      "Epoch 77 \t Batch 600 \t Validation Loss: 36.48985560576121\n",
      "Epoch 77 Training Loss: 45.92353418947176 Validation Loss: 37.25162579641714\n",
      "Epoch 77 completed\n",
      "Epoch 78 \t Batch 20 \t Training Loss: 46.37069091796875\n",
      "Epoch 78 \t Batch 40 \t Training Loss: 46.38680648803711\n",
      "Epoch 78 \t Batch 60 \t Training Loss: 46.0965638478597\n",
      "Epoch 78 \t Batch 80 \t Training Loss: 46.090529441833496\n",
      "Epoch 78 \t Batch 100 \t Training Loss: 45.67766845703125\n",
      "Epoch 78 \t Batch 120 \t Training Loss: 45.69264612197876\n",
      "Epoch 78 \t Batch 140 \t Training Loss: 45.74647617340088\n",
      "Epoch 78 \t Batch 160 \t Training Loss: 45.69793701171875\n",
      "Epoch 78 \t Batch 180 \t Training Loss: 45.61192527347141\n",
      "Epoch 78 \t Batch 200 \t Training Loss: 45.7954549407959\n",
      "Epoch 78 \t Batch 220 \t Training Loss: 45.87101052024148\n",
      "Epoch 78 \t Batch 240 \t Training Loss: 45.91208769480387\n",
      "Epoch 78 \t Batch 260 \t Training Loss: 45.7472108400785\n",
      "Epoch 78 \t Batch 280 \t Training Loss: 45.65443992614746\n",
      "Epoch 78 \t Batch 300 \t Training Loss: 45.77997353871663\n",
      "Epoch 78 \t Batch 320 \t Training Loss: 45.889687061309814\n",
      "Epoch 78 \t Batch 340 \t Training Loss: 45.81699431924259\n",
      "Epoch 78 \t Batch 360 \t Training Loss: 45.83170644972059\n",
      "Epoch 78 \t Batch 380 \t Training Loss: 45.824011280662134\n",
      "Epoch 78 \t Batch 400 \t Training Loss: 45.86194738388062\n",
      "Epoch 78 \t Batch 420 \t Training Loss: 45.82525533040364\n",
      "Epoch 78 \t Batch 440 \t Training Loss: 45.840275062214246\n",
      "Epoch 78 \t Batch 460 \t Training Loss: 45.827216546431835\n",
      "Epoch 78 \t Batch 480 \t Training Loss: 45.894250837961835\n",
      "Epoch 78 \t Batch 500 \t Training Loss: 45.88184757232666\n",
      "Epoch 78 \t Batch 520 \t Training Loss: 45.926969066032996\n",
      "Epoch 78 \t Batch 540 \t Training Loss: 45.923230255974666\n",
      "Epoch 78 \t Batch 560 \t Training Loss: 45.90455631528582\n",
      "Epoch 78 \t Batch 580 \t Training Loss: 45.86327085823849\n",
      "Epoch 78 \t Batch 600 \t Training Loss: 45.86108992894491\n",
      "Epoch 78 \t Batch 620 \t Training Loss: 45.889080859768775\n",
      "Epoch 78 \t Batch 640 \t Training Loss: 45.88443056344986\n",
      "Epoch 78 \t Batch 660 \t Training Loss: 45.872817837108265\n",
      "Epoch 78 \t Batch 680 \t Training Loss: 45.86638304205502\n",
      "Epoch 78 \t Batch 700 \t Training Loss: 45.82797710963658\n",
      "Epoch 78 \t Batch 720 \t Training Loss: 45.838966867658826\n",
      "Epoch 78 \t Batch 740 \t Training Loss: 45.863758855252655\n",
      "Epoch 78 \t Batch 760 \t Training Loss: 45.886550401386465\n",
      "Epoch 78 \t Batch 780 \t Training Loss: 45.865377435928735\n",
      "Epoch 78 \t Batch 800 \t Training Loss: 45.87050443649292\n",
      "Epoch 78 \t Batch 820 \t Training Loss: 45.931915362288315\n",
      "Epoch 78 \t Batch 840 \t Training Loss: 45.92164779844738\n",
      "Epoch 78 \t Batch 860 \t Training Loss: 45.91257908399715\n",
      "Epoch 78 \t Batch 880 \t Training Loss: 45.92284505150535\n",
      "Epoch 78 \t Batch 900 \t Training Loss: 45.90609349568685\n",
      "Epoch 78 \t Batch 20 \t Validation Loss: 23.141295385360717\n",
      "Epoch 78 \t Batch 40 \t Validation Loss: 23.686216521263123\n",
      "Epoch 78 \t Batch 60 \t Validation Loss: 23.793142127990723\n",
      "Epoch 78 \t Batch 80 \t Validation Loss: 24.27726970911026\n",
      "Epoch 78 \t Batch 100 \t Validation Loss: 25.12379448890686\n",
      "Epoch 78 \t Batch 120 \t Validation Loss: 26.018994800249736\n",
      "Epoch 78 \t Batch 140 \t Validation Loss: 26.362347269058226\n",
      "Epoch 78 \t Batch 160 \t Validation Loss: 28.105773907899856\n",
      "Epoch 78 \t Batch 180 \t Validation Loss: 31.533372916115656\n",
      "Epoch 78 \t Batch 200 \t Validation Loss: 32.86515995502472\n",
      "Epoch 78 \t Batch 220 \t Validation Loss: 33.983436129309915\n",
      "Epoch 78 \t Batch 240 \t Validation Loss: 34.38745985428492\n",
      "Epoch 78 \t Batch 260 \t Validation Loss: 36.39013194671044\n",
      "Epoch 78 \t Batch 280 \t Validation Loss: 37.458427381515506\n",
      "Epoch 78 \t Batch 300 \t Validation Loss: 38.54609692891439\n",
      "Epoch 78 \t Batch 320 \t Validation Loss: 39.008868008852005\n",
      "Epoch 78 \t Batch 340 \t Validation Loss: 38.93080810098087\n",
      "Epoch 78 \t Batch 360 \t Validation Loss: 38.76126262876723\n",
      "Epoch 78 \t Batch 380 \t Validation Loss: 38.96400454169825\n",
      "Epoch 78 \t Batch 400 \t Validation Loss: 38.563194756507876\n",
      "Epoch 78 \t Batch 420 \t Validation Loss: 38.548025762467155\n",
      "Epoch 78 \t Batch 440 \t Validation Loss: 38.23764788020741\n",
      "Epoch 78 \t Batch 460 \t Validation Loss: 38.46658338049184\n",
      "Epoch 78 \t Batch 480 \t Validation Loss: 38.95334704319636\n",
      "Epoch 78 \t Batch 500 \t Validation Loss: 38.662114917755126\n",
      "Epoch 78 \t Batch 520 \t Validation Loss: 38.45055595361269\n",
      "Epoch 78 \t Batch 540 \t Validation Loss: 38.246688003893254\n",
      "Epoch 78 \t Batch 560 \t Validation Loss: 38.09278156587056\n",
      "Epoch 78 \t Batch 580 \t Validation Loss: 37.92373541634658\n",
      "Epoch 78 \t Batch 600 \t Validation Loss: 38.15303439617157\n",
      "Epoch 78 Training Loss: 45.92300360959385 Validation Loss: 38.85731239287884\n",
      "Epoch 78 completed\n",
      "Epoch 79 \t Batch 20 \t Training Loss: 44.31928672790527\n",
      "Epoch 79 \t Batch 40 \t Training Loss: 44.89905128479004\n",
      "Epoch 79 \t Batch 60 \t Training Loss: 45.64284076690674\n",
      "Epoch 79 \t Batch 80 \t Training Loss: 45.284605741500854\n",
      "Epoch 79 \t Batch 100 \t Training Loss: 45.34233261108398\n",
      "Epoch 79 \t Batch 120 \t Training Loss: 45.58667914072672\n",
      "Epoch 79 \t Batch 140 \t Training Loss: 45.681646728515624\n",
      "Epoch 79 \t Batch 160 \t Training Loss: 45.66210958957672\n",
      "Epoch 79 \t Batch 180 \t Training Loss: 45.664067628648546\n",
      "Epoch 79 \t Batch 200 \t Training Loss: 45.651189403533934\n",
      "Epoch 79 \t Batch 220 \t Training Loss: 45.81519607197155\n",
      "Epoch 79 \t Batch 240 \t Training Loss: 45.85562980969747\n",
      "Epoch 79 \t Batch 260 \t Training Loss: 45.85714357816256\n",
      "Epoch 79 \t Batch 280 \t Training Loss: 45.89452238082886\n",
      "Epoch 79 \t Batch 300 \t Training Loss: 45.869901504516605\n",
      "Epoch 79 \t Batch 320 \t Training Loss: 45.858856523036955\n",
      "Epoch 79 \t Batch 340 \t Training Loss: 45.859021400002874\n",
      "Epoch 79 \t Batch 360 \t Training Loss: 45.83531787660387\n",
      "Epoch 79 \t Batch 380 \t Training Loss: 45.82012127324155\n",
      "Epoch 79 \t Batch 400 \t Training Loss: 45.82050485610962\n",
      "Epoch 79 \t Batch 420 \t Training Loss: 45.856612323579334\n",
      "Epoch 79 \t Batch 440 \t Training Loss: 45.80923653515902\n",
      "Epoch 79 \t Batch 460 \t Training Loss: 45.81536900064219\n",
      "Epoch 79 \t Batch 480 \t Training Loss: 45.78611947695414\n",
      "Epoch 79 \t Batch 500 \t Training Loss: 45.79040087127686\n",
      "Epoch 79 \t Batch 520 \t Training Loss: 45.88325835741483\n",
      "Epoch 79 \t Batch 540 \t Training Loss: 45.90065046239782\n",
      "Epoch 79 \t Batch 560 \t Training Loss: 45.905310249328615\n",
      "Epoch 79 \t Batch 580 \t Training Loss: 45.91387905910097\n",
      "Epoch 79 \t Batch 600 \t Training Loss: 45.893077869415286\n",
      "Epoch 79 \t Batch 620 \t Training Loss: 45.861119282630185\n",
      "Epoch 79 \t Batch 640 \t Training Loss: 45.90591455101967\n",
      "Epoch 79 \t Batch 660 \t Training Loss: 45.88731842619\n",
      "Epoch 79 \t Batch 680 \t Training Loss: 45.881193374185\n",
      "Epoch 79 \t Batch 700 \t Training Loss: 45.883495363507954\n",
      "Epoch 79 \t Batch 720 \t Training Loss: 45.8932395723131\n",
      "Epoch 79 \t Batch 740 \t Training Loss: 45.88967543421565\n",
      "Epoch 79 \t Batch 760 \t Training Loss: 45.8756119025381\n",
      "Epoch 79 \t Batch 780 \t Training Loss: 45.889157662024864\n",
      "Epoch 79 \t Batch 800 \t Training Loss: 45.89262234210968\n",
      "Epoch 79 \t Batch 820 \t Training Loss: 45.92548818355653\n",
      "Epoch 79 \t Batch 840 \t Training Loss: 45.89360480535598\n",
      "Epoch 79 \t Batch 860 \t Training Loss: 45.89098986692207\n",
      "Epoch 79 \t Batch 880 \t Training Loss: 45.879443207654084\n",
      "Epoch 79 \t Batch 900 \t Training Loss: 45.86862558576796\n",
      "Epoch 79 \t Batch 20 \t Validation Loss: 24.505459117889405\n",
      "Epoch 79 \t Batch 40 \t Validation Loss: 25.739871859550476\n",
      "Epoch 79 \t Batch 60 \t Validation Loss: 25.633701292673745\n",
      "Epoch 79 \t Batch 80 \t Validation Loss: 26.127228307724\n",
      "Epoch 79 \t Batch 100 \t Validation Loss: 26.548246250152587\n",
      "Epoch 79 \t Batch 120 \t Validation Loss: 27.166061282157898\n",
      "Epoch 79 \t Batch 140 \t Validation Loss: 27.366454185758318\n",
      "Epoch 79 \t Batch 160 \t Validation Loss: 29.199920457601547\n",
      "Epoch 79 \t Batch 180 \t Validation Loss: 32.97826200591193\n",
      "Epoch 79 \t Batch 200 \t Validation Loss: 34.374302382469175\n",
      "Epoch 79 \t Batch 220 \t Validation Loss: 35.67226971279491\n",
      "Epoch 79 \t Batch 240 \t Validation Loss: 36.154651113351186\n",
      "Epoch 79 \t Batch 260 \t Validation Loss: 38.275872586323665\n",
      "Epoch 79 \t Batch 280 \t Validation Loss: 39.346978078569684\n",
      "Epoch 79 \t Batch 300 \t Validation Loss: 40.54159852345784\n",
      "Epoch 79 \t Batch 320 \t Validation Loss: 41.026812601089475\n",
      "Epoch 79 \t Batch 340 \t Validation Loss: 40.89420413970947\n",
      "Epoch 79 \t Batch 360 \t Validation Loss: 40.71713814735413\n",
      "Epoch 79 \t Batch 380 \t Validation Loss: 40.874605901617755\n",
      "Epoch 79 \t Batch 400 \t Validation Loss: 40.366842980384824\n",
      "Epoch 79 \t Batch 420 \t Validation Loss: 40.295734287443615\n",
      "Epoch 79 \t Batch 440 \t Validation Loss: 39.91335800777782\n",
      "Epoch 79 \t Batch 460 \t Validation Loss: 40.053142705171005\n",
      "Epoch 79 \t Batch 480 \t Validation Loss: 40.501685913403826\n",
      "Epoch 79 \t Batch 500 \t Validation Loss: 40.182213897705076\n",
      "Epoch 79 \t Batch 520 \t Validation Loss: 39.87506622167734\n",
      "Epoch 79 \t Batch 540 \t Validation Loss: 39.60469262864854\n",
      "Epoch 79 \t Batch 560 \t Validation Loss: 39.39041478293283\n",
      "Epoch 79 \t Batch 580 \t Validation Loss: 39.14028382794611\n",
      "Epoch 79 \t Batch 600 \t Validation Loss: 39.33143053372701\n",
      "Epoch 79 Training Loss: 45.87459812975502 Validation Loss: 39.98750731852147\n",
      "Epoch 79 completed\n",
      "Epoch 80 \t Batch 20 \t Training Loss: 46.84276237487793\n",
      "Epoch 80 \t Batch 40 \t Training Loss: 45.61250123977661\n",
      "Epoch 80 \t Batch 60 \t Training Loss: 45.49345830281575\n",
      "Epoch 80 \t Batch 80 \t Training Loss: 45.69582433700562\n",
      "Epoch 80 \t Batch 100 \t Training Loss: 45.67744491577148\n",
      "Epoch 80 \t Batch 120 \t Training Loss: 45.668172963460286\n",
      "Epoch 80 \t Batch 140 \t Training Loss: 45.667847415379114\n",
      "Epoch 80 \t Batch 160 \t Training Loss: 45.750315284729005\n",
      "Epoch 80 \t Batch 180 \t Training Loss: 45.64714643690321\n",
      "Epoch 80 \t Batch 200 \t Training Loss: 45.5355345916748\n",
      "Epoch 80 \t Batch 220 \t Training Loss: 45.69451359835538\n",
      "Epoch 80 \t Batch 240 \t Training Loss: 45.547596073150636\n",
      "Epoch 80 \t Batch 260 \t Training Loss: 45.60708092909593\n",
      "Epoch 80 \t Batch 280 \t Training Loss: 45.62642741884504\n",
      "Epoch 80 \t Batch 300 \t Training Loss: 45.637300720214846\n",
      "Epoch 80 \t Batch 320 \t Training Loss: 45.755859684944156\n",
      "Epoch 80 \t Batch 340 \t Training Loss: 45.76949210447424\n",
      "Epoch 80 \t Batch 360 \t Training Loss: 45.736685053507486\n",
      "Epoch 80 \t Batch 380 \t Training Loss: 45.83083743045204\n",
      "Epoch 80 \t Batch 400 \t Training Loss: 45.86470093727112\n",
      "Epoch 80 \t Batch 420 \t Training Loss: 45.88821095966158\n",
      "Epoch 80 \t Batch 440 \t Training Loss: 45.84156544425271\n",
      "Epoch 80 \t Batch 460 \t Training Loss: 45.82633851922077\n",
      "Epoch 80 \t Batch 480 \t Training Loss: 45.79972903728485\n",
      "Epoch 80 \t Batch 500 \t Training Loss: 45.81449897766113\n",
      "Epoch 80 \t Batch 520 \t Training Loss: 45.82648944121141\n",
      "Epoch 80 \t Batch 540 \t Training Loss: 45.89221691555447\n",
      "Epoch 80 \t Batch 560 \t Training Loss: 45.83277237074716\n",
      "Epoch 80 \t Batch 580 \t Training Loss: 45.78858209149591\n",
      "Epoch 80 \t Batch 600 \t Training Loss: 45.779629936218264\n",
      "Epoch 80 \t Batch 620 \t Training Loss: 45.76076120561169\n",
      "Epoch 80 \t Batch 640 \t Training Loss: 45.752476239204405\n",
      "Epoch 80 \t Batch 660 \t Training Loss: 45.758986305468014\n",
      "Epoch 80 \t Batch 680 \t Training Loss: 45.75603511473712\n",
      "Epoch 80 \t Batch 700 \t Training Loss: 45.786571001325335\n",
      "Epoch 80 \t Batch 720 \t Training Loss: 45.816999430126614\n",
      "Epoch 80 \t Batch 740 \t Training Loss: 45.81222657899599\n",
      "Epoch 80 \t Batch 760 \t Training Loss: 45.77717766510813\n",
      "Epoch 80 \t Batch 780 \t Training Loss: 45.80708269461607\n",
      "Epoch 80 \t Batch 800 \t Training Loss: 45.755749859809875\n",
      "Epoch 80 \t Batch 820 \t Training Loss: 45.771570359206784\n",
      "Epoch 80 \t Batch 840 \t Training Loss: 45.81147313345046\n",
      "Epoch 80 \t Batch 860 \t Training Loss: 45.77600267543349\n",
      "Epoch 80 \t Batch 880 \t Training Loss: 45.77801049839366\n",
      "Epoch 80 \t Batch 900 \t Training Loss: 45.78443463643392\n",
      "Epoch 80 \t Batch 20 \t Validation Loss: 18.24895052909851\n",
      "Epoch 80 \t Batch 40 \t Validation Loss: 20.633902168273927\n",
      "Epoch 80 \t Batch 60 \t Validation Loss: 20.17905395825704\n",
      "Epoch 80 \t Batch 80 \t Validation Loss: 20.87044734954834\n",
      "Epoch 80 \t Batch 100 \t Validation Loss: 22.69264362335205\n",
      "Epoch 80 \t Batch 120 \t Validation Loss: 23.863038023312885\n",
      "Epoch 80 \t Batch 140 \t Validation Loss: 24.51239378111703\n",
      "Epoch 80 \t Batch 160 \t Validation Loss: 26.54794251322746\n",
      "Epoch 80 \t Batch 180 \t Validation Loss: 30.121197313732573\n",
      "Epoch 80 \t Batch 200 \t Validation Loss: 31.6589657831192\n",
      "Epoch 80 \t Batch 220 \t Validation Loss: 32.94329139102589\n",
      "Epoch 80 \t Batch 240 \t Validation Loss: 33.48401700258255\n",
      "Epoch 80 \t Batch 260 \t Validation Loss: 35.53956635915316\n",
      "Epoch 80 \t Batch 280 \t Validation Loss: 36.649742589678084\n",
      "Epoch 80 \t Batch 300 \t Validation Loss: 37.75237822214763\n",
      "Epoch 80 \t Batch 320 \t Validation Loss: 38.281628674268724\n",
      "Epoch 80 \t Batch 340 \t Validation Loss: 38.25606089760275\n",
      "Epoch 80 \t Batch 360 \t Validation Loss: 38.134956979751585\n",
      "Epoch 80 \t Batch 380 \t Validation Loss: 38.3898145123532\n",
      "Epoch 80 \t Batch 400 \t Validation Loss: 38.03935447692871\n",
      "Epoch 80 \t Batch 420 \t Validation Loss: 38.08363767351423\n",
      "Epoch 80 \t Batch 440 \t Validation Loss: 37.819206940044054\n",
      "Epoch 80 \t Batch 460 \t Validation Loss: 38.09658846647843\n",
      "Epoch 80 \t Batch 480 \t Validation Loss: 38.59276715914408\n",
      "Epoch 80 \t Batch 500 \t Validation Loss: 38.33736678314209\n",
      "Epoch 80 \t Batch 520 \t Validation Loss: 38.15530181848086\n",
      "Epoch 80 \t Batch 540 \t Validation Loss: 37.963356758047034\n",
      "Epoch 80 \t Batch 560 \t Validation Loss: 37.82497284242085\n",
      "Epoch 80 \t Batch 580 \t Validation Loss: 37.65930718224624\n",
      "Epoch 80 \t Batch 600 \t Validation Loss: 37.9002214606603\n",
      "Epoch 80 Training Loss: 45.810637579099435 Validation Loss: 38.610081404834595\n",
      "Epoch 80 completed\n",
      "Epoch 81 \t Batch 20 \t Training Loss: 46.04818477630615\n",
      "Epoch 81 \t Batch 40 \t Training Loss: 46.81020240783691\n",
      "Epoch 81 \t Batch 60 \t Training Loss: 46.16439723968506\n",
      "Epoch 81 \t Batch 80 \t Training Loss: 45.6435884475708\n",
      "Epoch 81 \t Batch 100 \t Training Loss: 45.67534118652344\n",
      "Epoch 81 \t Batch 120 \t Training Loss: 45.781624507904056\n",
      "Epoch 81 \t Batch 140 \t Training Loss: 45.87035124642508\n",
      "Epoch 81 \t Batch 160 \t Training Loss: 45.853226232528684\n",
      "Epoch 81 \t Batch 180 \t Training Loss: 45.76093525356717\n",
      "Epoch 81 \t Batch 200 \t Training Loss: 45.784356842041014\n",
      "Epoch 81 \t Batch 220 \t Training Loss: 45.89432452808727\n",
      "Epoch 81 \t Batch 240 \t Training Loss: 45.95782335599264\n",
      "Epoch 81 \t Batch 260 \t Training Loss: 45.924136660649225\n",
      "Epoch 81 \t Batch 280 \t Training Loss: 45.86278907230922\n",
      "Epoch 81 \t Batch 300 \t Training Loss: 45.86819357554118\n",
      "Epoch 81 \t Batch 320 \t Training Loss: 45.88215461969376\n",
      "Epoch 81 \t Batch 340 \t Training Loss: 45.838814017351936\n",
      "Epoch 81 \t Batch 360 \t Training Loss: 45.76043710708618\n",
      "Epoch 81 \t Batch 380 \t Training Loss: 45.78123533349288\n",
      "Epoch 81 \t Batch 400 \t Training Loss: 45.799986896514895\n",
      "Epoch 81 \t Batch 420 \t Training Loss: 45.870357622419085\n",
      "Epoch 81 \t Batch 440 \t Training Loss: 45.813707022233444\n",
      "Epoch 81 \t Batch 460 \t Training Loss: 45.75268582053806\n",
      "Epoch 81 \t Batch 480 \t Training Loss: 45.79887065092723\n",
      "Epoch 81 \t Batch 500 \t Training Loss: 45.77074465179443\n",
      "Epoch 81 \t Batch 520 \t Training Loss: 45.78194649036114\n",
      "Epoch 81 \t Batch 540 \t Training Loss: 45.73552335103353\n",
      "Epoch 81 \t Batch 560 \t Training Loss: 45.80217125075204\n",
      "Epoch 81 \t Batch 580 \t Training Loss: 45.733471219293\n",
      "Epoch 81 \t Batch 600 \t Training Loss: 45.757476603190106\n",
      "Epoch 81 \t Batch 620 \t Training Loss: 45.811985065091044\n",
      "Epoch 81 \t Batch 640 \t Training Loss: 45.783957469463346\n",
      "Epoch 81 \t Batch 660 \t Training Loss: 45.81666174223929\n",
      "Epoch 81 \t Batch 680 \t Training Loss: 45.81715108647066\n",
      "Epoch 81 \t Batch 700 \t Training Loss: 45.815522646222796\n",
      "Epoch 81 \t Batch 720 \t Training Loss: 45.83690350850423\n",
      "Epoch 81 \t Batch 740 \t Training Loss: 45.83990379539696\n",
      "Epoch 81 \t Batch 760 \t Training Loss: 45.85343327271311\n",
      "Epoch 81 \t Batch 780 \t Training Loss: 45.85150387103741\n",
      "Epoch 81 \t Batch 800 \t Training Loss: 45.815081286430356\n",
      "Epoch 81 \t Batch 820 \t Training Loss: 45.81219522429676\n",
      "Epoch 81 \t Batch 840 \t Training Loss: 45.800835400535945\n",
      "Epoch 81 \t Batch 860 \t Training Loss: 45.79271312536195\n",
      "Epoch 81 \t Batch 880 \t Training Loss: 45.78254634250294\n",
      "Epoch 81 \t Batch 900 \t Training Loss: 45.79606600443522\n",
      "Epoch 81 \t Batch 20 \t Validation Loss: 17.72460150718689\n",
      "Epoch 81 \t Batch 40 \t Validation Loss: 20.19445412158966\n",
      "Epoch 81 \t Batch 60 \t Validation Loss: 19.819504960378012\n",
      "Epoch 81 \t Batch 80 \t Validation Loss: 20.504621303081514\n",
      "Epoch 81 \t Batch 100 \t Validation Loss: 22.303639001846314\n",
      "Epoch 81 \t Batch 120 \t Validation Loss: 23.537356503804524\n",
      "Epoch 81 \t Batch 140 \t Validation Loss: 24.220962946755545\n",
      "Epoch 81 \t Batch 160 \t Validation Loss: 26.312452006340028\n",
      "Epoch 81 \t Batch 180 \t Validation Loss: 30.023851304584078\n",
      "Epoch 81 \t Batch 200 \t Validation Loss: 31.580658164024353\n",
      "Epoch 81 \t Batch 220 \t Validation Loss: 32.905257723548196\n",
      "Epoch 81 \t Batch 240 \t Validation Loss: 33.46702487866084\n",
      "Epoch 81 \t Batch 260 \t Validation Loss: 35.60496661112859\n",
      "Epoch 81 \t Batch 280 \t Validation Loss: 36.76997992651803\n",
      "Epoch 81 \t Batch 300 \t Validation Loss: 37.945217018127444\n",
      "Epoch 81 \t Batch 320 \t Validation Loss: 38.49750018417835\n",
      "Epoch 81 \t Batch 340 \t Validation Loss: 38.47437050482806\n",
      "Epoch 81 \t Batch 360 \t Validation Loss: 38.361603718333775\n",
      "Epoch 81 \t Batch 380 \t Validation Loss: 38.61587604472512\n",
      "Epoch 81 \t Batch 400 \t Validation Loss: 38.221157815456394\n",
      "Epoch 81 \t Batch 420 \t Validation Loss: 38.25039253916059\n",
      "Epoch 81 \t Batch 440 \t Validation Loss: 37.95638336485082\n",
      "Epoch 81 \t Batch 460 \t Validation Loss: 38.2677040659863\n",
      "Epoch 81 \t Batch 480 \t Validation Loss: 38.77518963217735\n",
      "Epoch 81 \t Batch 500 \t Validation Loss: 38.53510758781433\n",
      "Epoch 81 \t Batch 520 \t Validation Loss: 38.33709051975837\n",
      "Epoch 81 \t Batch 540 \t Validation Loss: 38.124065549285326\n",
      "Epoch 81 \t Batch 560 \t Validation Loss: 37.942635057653696\n",
      "Epoch 81 \t Batch 580 \t Validation Loss: 37.71206408040277\n",
      "Epoch 81 \t Batch 600 \t Validation Loss: 37.94936857064565\n",
      "Epoch 81 Training Loss: 45.816841920043544 Validation Loss: 38.61205472729423\n",
      "Epoch 81 completed\n",
      "Epoch 82 \t Batch 20 \t Training Loss: 45.153595542907716\n",
      "Epoch 82 \t Batch 40 \t Training Loss: 45.26352834701538\n",
      "Epoch 82 \t Batch 60 \t Training Loss: 45.41664740244548\n",
      "Epoch 82 \t Batch 80 \t Training Loss: 45.24137940406799\n",
      "Epoch 82 \t Batch 100 \t Training Loss: 45.397225799560545\n",
      "Epoch 82 \t Batch 120 \t Training Loss: 45.50264056523641\n",
      "Epoch 82 \t Batch 140 \t Training Loss: 45.48165231432233\n",
      "Epoch 82 \t Batch 160 \t Training Loss: 45.53681223392486\n",
      "Epoch 82 \t Batch 180 \t Training Loss: 45.64280868106418\n",
      "Epoch 82 \t Batch 200 \t Training Loss: 45.76717163085937\n",
      "Epoch 82 \t Batch 220 \t Training Loss: 45.77685983831232\n",
      "Epoch 82 \t Batch 240 \t Training Loss: 45.763311306635536\n",
      "Epoch 82 \t Batch 260 \t Training Loss: 45.83830980154184\n",
      "Epoch 82 \t Batch 280 \t Training Loss: 45.86995692934309\n",
      "Epoch 82 \t Batch 300 \t Training Loss: 45.822825558980306\n",
      "Epoch 82 \t Batch 320 \t Training Loss: 45.9144146323204\n",
      "Epoch 82 \t Batch 340 \t Training Loss: 45.90376264908735\n",
      "Epoch 82 \t Batch 360 \t Training Loss: 45.9673865530226\n",
      "Epoch 82 \t Batch 380 \t Training Loss: 45.93043600383558\n",
      "Epoch 82 \t Batch 400 \t Training Loss: 45.93285276412964\n",
      "Epoch 82 \t Batch 420 \t Training Loss: 45.91070522126697\n",
      "Epoch 82 \t Batch 440 \t Training Loss: 45.872163625196976\n",
      "Epoch 82 \t Batch 460 \t Training Loss: 45.868029271001404\n",
      "Epoch 82 \t Batch 480 \t Training Loss: 45.861213000615436\n",
      "Epoch 82 \t Batch 500 \t Training Loss: 45.82627333068848\n",
      "Epoch 82 \t Batch 520 \t Training Loss: 45.86892336331881\n",
      "Epoch 82 \t Batch 540 \t Training Loss: 45.81376293323658\n",
      "Epoch 82 \t Batch 560 \t Training Loss: 45.82539993694851\n",
      "Epoch 82 \t Batch 580 \t Training Loss: 45.869159987877154\n",
      "Epoch 82 \t Batch 600 \t Training Loss: 45.8644407526652\n",
      "Epoch 82 \t Batch 620 \t Training Loss: 45.829413918525944\n",
      "Epoch 82 \t Batch 640 \t Training Loss: 45.82548747062683\n",
      "Epoch 82 \t Batch 660 \t Training Loss: 45.8448348941225\n",
      "Epoch 82 \t Batch 680 \t Training Loss: 45.844592722724464\n",
      "Epoch 82 \t Batch 700 \t Training Loss: 45.83368535723005\n",
      "Epoch 82 \t Batch 720 \t Training Loss: 45.79358303811815\n",
      "Epoch 82 \t Batch 740 \t Training Loss: 45.76930652309108\n",
      "Epoch 82 \t Batch 760 \t Training Loss: 45.76118921982614\n",
      "Epoch 82 \t Batch 780 \t Training Loss: 45.77707951374543\n",
      "Epoch 82 \t Batch 800 \t Training Loss: 45.763991417884824\n",
      "Epoch 82 \t Batch 820 \t Training Loss: 45.777733746970576\n",
      "Epoch 82 \t Batch 840 \t Training Loss: 45.79392221087501\n",
      "Epoch 82 \t Batch 860 \t Training Loss: 45.80357343540635\n",
      "Epoch 82 \t Batch 880 \t Training Loss: 45.780541224913165\n",
      "Epoch 82 \t Batch 900 \t Training Loss: 45.81449927859836\n",
      "Epoch 82 \t Batch 20 \t Validation Loss: 19.36070351600647\n",
      "Epoch 82 \t Batch 40 \t Validation Loss: 21.21952543258667\n",
      "Epoch 82 \t Batch 60 \t Validation Loss: 20.87761796315511\n",
      "Epoch 82 \t Batch 80 \t Validation Loss: 21.409016954898835\n",
      "Epoch 82 \t Batch 100 \t Validation Loss: 22.95933844566345\n",
      "Epoch 82 \t Batch 120 \t Validation Loss: 24.156989336013794\n",
      "Epoch 82 \t Batch 140 \t Validation Loss: 24.711299964359828\n",
      "Epoch 82 \t Batch 160 \t Validation Loss: 26.67109011411667\n",
      "Epoch 82 \t Batch 180 \t Validation Loss: 30.159489880667792\n",
      "Epoch 82 \t Batch 200 \t Validation Loss: 31.532069630622864\n",
      "Epoch 82 \t Batch 220 \t Validation Loss: 32.738566940481014\n",
      "Epoch 82 \t Batch 240 \t Validation Loss: 33.23610537449519\n",
      "Epoch 82 \t Batch 260 \t Validation Loss: 35.231617054572475\n",
      "Epoch 82 \t Batch 280 \t Validation Loss: 36.340211503846305\n",
      "Epoch 82 \t Batch 300 \t Validation Loss: 37.45967526435852\n",
      "Epoch 82 \t Batch 320 \t Validation Loss: 38.00438189804554\n",
      "Epoch 82 \t Batch 340 \t Validation Loss: 37.990875005722046\n",
      "Epoch 82 \t Batch 360 \t Validation Loss: 37.86535856458876\n",
      "Epoch 82 \t Batch 380 \t Validation Loss: 38.138759432340926\n",
      "Epoch 82 \t Batch 400 \t Validation Loss: 37.81450172901154\n",
      "Epoch 82 \t Batch 420 \t Validation Loss: 37.874078253337316\n",
      "Epoch 82 \t Batch 440 \t Validation Loss: 37.64136785810644\n",
      "Epoch 82 \t Batch 460 \t Validation Loss: 37.97424079438915\n",
      "Epoch 82 \t Batch 480 \t Validation Loss: 38.49072756171226\n",
      "Epoch 82 \t Batch 500 \t Validation Loss: 38.255841230392456\n",
      "Epoch 82 \t Batch 520 \t Validation Loss: 38.08629544698275\n",
      "Epoch 82 \t Batch 540 \t Validation Loss: 37.84576382460418\n",
      "Epoch 82 \t Batch 560 \t Validation Loss: 37.64906464985439\n",
      "Epoch 82 \t Batch 580 \t Validation Loss: 37.38087642275054\n",
      "Epoch 82 \t Batch 600 \t Validation Loss: 37.62039046287536\n",
      "Epoch 82 Training Loss: 45.806785263178 Validation Loss: 38.29324389742566\n",
      "Epoch 82 completed\n",
      "Epoch 83 \t Batch 20 \t Training Loss: 45.70782470703125\n",
      "Epoch 83 \t Batch 40 \t Training Loss: 45.795789051055905\n",
      "Epoch 83 \t Batch 60 \t Training Loss: 45.47934449513753\n",
      "Epoch 83 \t Batch 80 \t Training Loss: 45.49289875030517\n",
      "Epoch 83 \t Batch 100 \t Training Loss: 45.20906188964844\n",
      "Epoch 83 \t Batch 120 \t Training Loss: 45.14010636011759\n",
      "Epoch 83 \t Batch 140 \t Training Loss: 45.26040791102818\n",
      "Epoch 83 \t Batch 160 \t Training Loss: 45.25617690086365\n",
      "Epoch 83 \t Batch 180 \t Training Loss: 45.256476910909015\n",
      "Epoch 83 \t Batch 200 \t Training Loss: 45.31786243438721\n",
      "Epoch 83 \t Batch 220 \t Training Loss: 45.36168849251487\n",
      "Epoch 83 \t Batch 240 \t Training Loss: 45.44601612091064\n",
      "Epoch 83 \t Batch 260 \t Training Loss: 45.44189466329721\n",
      "Epoch 83 \t Batch 280 \t Training Loss: 45.46510253633772\n",
      "Epoch 83 \t Batch 300 \t Training Loss: 45.48854704538981\n",
      "Epoch 83 \t Batch 320 \t Training Loss: 45.53018723726272\n",
      "Epoch 83 \t Batch 340 \t Training Loss: 45.63242139255299\n",
      "Epoch 83 \t Batch 360 \t Training Loss: 45.578085305955675\n",
      "Epoch 83 \t Batch 380 \t Training Loss: 45.50583328447844\n",
      "Epoch 83 \t Batch 400 \t Training Loss: 45.46163866996765\n",
      "Epoch 83 \t Batch 420 \t Training Loss: 45.536721956162225\n",
      "Epoch 83 \t Batch 440 \t Training Loss: 45.611144655401056\n",
      "Epoch 83 \t Batch 460 \t Training Loss: 45.65433060189952\n",
      "Epoch 83 \t Batch 480 \t Training Loss: 45.638065775235496\n",
      "Epoch 83 \t Batch 500 \t Training Loss: 45.62085662841797\n",
      "Epoch 83 \t Batch 520 \t Training Loss: 45.657556570493256\n",
      "Epoch 83 \t Batch 540 \t Training Loss: 45.68720498968054\n",
      "Epoch 83 \t Batch 560 \t Training Loss: 45.69853641646249\n",
      "Epoch 83 \t Batch 580 \t Training Loss: 45.717730515578694\n",
      "Epoch 83 \t Batch 600 \t Training Loss: 45.680207640329996\n",
      "Epoch 83 \t Batch 620 \t Training Loss: 45.6950774100519\n",
      "Epoch 83 \t Batch 640 \t Training Loss: 45.66173837780953\n",
      "Epoch 83 \t Batch 660 \t Training Loss: 45.651804900891854\n",
      "Epoch 83 \t Batch 680 \t Training Loss: 45.681099717757284\n",
      "Epoch 83 \t Batch 700 \t Training Loss: 45.68388555254255\n",
      "Epoch 83 \t Batch 720 \t Training Loss: 45.665551376342776\n",
      "Epoch 83 \t Batch 740 \t Training Loss: 45.636751391436604\n",
      "Epoch 83 \t Batch 760 \t Training Loss: 45.655399262277705\n",
      "Epoch 83 \t Batch 780 \t Training Loss: 45.67068646748861\n",
      "Epoch 83 \t Batch 800 \t Training Loss: 45.707910451889035\n",
      "Epoch 83 \t Batch 820 \t Training Loss: 45.712527168087846\n",
      "Epoch 83 \t Batch 840 \t Training Loss: 45.74895068123227\n",
      "Epoch 83 \t Batch 860 \t Training Loss: 45.778267549913984\n",
      "Epoch 83 \t Batch 880 \t Training Loss: 45.807794445211236\n",
      "Epoch 83 \t Batch 900 \t Training Loss: 45.79413636525472\n",
      "Epoch 83 \t Batch 20 \t Validation Loss: 27.18476333618164\n",
      "Epoch 83 \t Batch 40 \t Validation Loss: 27.755480885505676\n",
      "Epoch 83 \t Batch 60 \t Validation Loss: 27.464978583653767\n",
      "Epoch 83 \t Batch 80 \t Validation Loss: 27.717893755435945\n",
      "Epoch 83 \t Batch 100 \t Validation Loss: 28.238446836471557\n",
      "Epoch 83 \t Batch 120 \t Validation Loss: 28.843607338269553\n",
      "Epoch 83 \t Batch 140 \t Validation Loss: 28.886218799863542\n",
      "Epoch 83 \t Batch 160 \t Validation Loss: 30.466708379983903\n",
      "Epoch 83 \t Batch 180 \t Validation Loss: 34.00397858089871\n",
      "Epoch 83 \t Batch 200 \t Validation Loss: 35.30526376724243\n",
      "Epoch 83 \t Batch 220 \t Validation Loss: 36.35312793905085\n",
      "Epoch 83 \t Batch 240 \t Validation Loss: 36.71777504682541\n",
      "Epoch 83 \t Batch 260 \t Validation Loss: 38.66130882776701\n",
      "Epoch 83 \t Batch 280 \t Validation Loss: 39.61986126559121\n",
      "Epoch 83 \t Batch 300 \t Validation Loss: 40.807806742986045\n",
      "Epoch 83 \t Batch 320 \t Validation Loss: 41.25750033855438\n",
      "Epoch 83 \t Batch 340 \t Validation Loss: 41.100867148006664\n",
      "Epoch 83 \t Batch 360 \t Validation Loss: 40.88590116500855\n",
      "Epoch 83 \t Batch 380 \t Validation Loss: 40.996512177115996\n",
      "Epoch 83 \t Batch 400 \t Validation Loss: 40.49839269638061\n",
      "Epoch 83 \t Batch 420 \t Validation Loss: 40.44978349095299\n",
      "Epoch 83 \t Batch 440 \t Validation Loss: 40.0737410957163\n",
      "Epoch 83 \t Batch 460 \t Validation Loss: 40.26259326727494\n",
      "Epoch 83 \t Batch 480 \t Validation Loss: 40.69693112572034\n",
      "Epoch 83 \t Batch 500 \t Validation Loss: 40.38217555427551\n",
      "Epoch 83 \t Batch 520 \t Validation Loss: 40.10749544913952\n",
      "Epoch 83 \t Batch 540 \t Validation Loss: 39.791562509536746\n",
      "Epoch 83 \t Batch 560 \t Validation Loss: 39.52885053668703\n",
      "Epoch 83 \t Batch 580 \t Validation Loss: 39.1914198694558\n",
      "Epoch 83 \t Batch 600 \t Validation Loss: 39.35760059197744\n",
      "Epoch 83 Training Loss: 45.77407961713319 Validation Loss: 39.974747931802426\n",
      "Epoch 83 completed\n",
      "Epoch 84 \t Batch 20 \t Training Loss: 45.07849407196045\n",
      "Epoch 84 \t Batch 40 \t Training Loss: 44.37182207107544\n",
      "Epoch 84 \t Batch 60 \t Training Loss: 44.55186538696289\n",
      "Epoch 84 \t Batch 80 \t Training Loss: 44.8941481590271\n",
      "Epoch 84 \t Batch 100 \t Training Loss: 44.9590270614624\n",
      "Epoch 84 \t Batch 120 \t Training Loss: 44.77654349009196\n",
      "Epoch 84 \t Batch 140 \t Training Loss: 45.03466900416783\n",
      "Epoch 84 \t Batch 160 \t Training Loss: 44.95976529121399\n",
      "Epoch 84 \t Batch 180 \t Training Loss: 45.09357003106011\n",
      "Epoch 84 \t Batch 200 \t Training Loss: 45.09432004928589\n",
      "Epoch 84 \t Batch 220 \t Training Loss: 45.06652414148504\n",
      "Epoch 84 \t Batch 240 \t Training Loss: 45.05484420458476\n",
      "Epoch 84 \t Batch 260 \t Training Loss: 45.047606585575984\n",
      "Epoch 84 \t Batch 280 \t Training Loss: 45.05825598580497\n",
      "Epoch 84 \t Batch 300 \t Training Loss: 45.13956251780192\n",
      "Epoch 84 \t Batch 320 \t Training Loss: 45.20044310092926\n",
      "Epoch 84 \t Batch 340 \t Training Loss: 45.28268968918744\n",
      "Epoch 84 \t Batch 360 \t Training Loss: 45.36357583999634\n",
      "Epoch 84 \t Batch 380 \t Training Loss: 45.411661399038216\n",
      "Epoch 84 \t Batch 400 \t Training Loss: 45.388386707305905\n",
      "Epoch 84 \t Batch 420 \t Training Loss: 45.40591475168864\n",
      "Epoch 84 \t Batch 440 \t Training Loss: 45.47113730690696\n",
      "Epoch 84 \t Batch 460 \t Training Loss: 45.55180308300516\n",
      "Epoch 84 \t Batch 480 \t Training Loss: 45.54013872941335\n",
      "Epoch 84 \t Batch 500 \t Training Loss: 45.60926345062256\n",
      "Epoch 84 \t Batch 520 \t Training Loss: 45.630297264686\n",
      "Epoch 84 \t Batch 540 \t Training Loss: 45.580065670719854\n",
      "Epoch 84 \t Batch 560 \t Training Loss: 45.55747931344168\n",
      "Epoch 84 \t Batch 580 \t Training Loss: 45.558379955949455\n",
      "Epoch 84 \t Batch 600 \t Training Loss: 45.5818572807312\n",
      "Epoch 84 \t Batch 620 \t Training Loss: 45.58213374230169\n",
      "Epoch 84 \t Batch 640 \t Training Loss: 45.57772404551506\n",
      "Epoch 84 \t Batch 660 \t Training Loss: 45.60550384521484\n",
      "Epoch 84 \t Batch 680 \t Training Loss: 45.62506565206191\n",
      "Epoch 84 \t Batch 700 \t Training Loss: 45.64979454585484\n",
      "Epoch 84 \t Batch 720 \t Training Loss: 45.64727299478319\n",
      "Epoch 84 \t Batch 740 \t Training Loss: 45.65492517625963\n",
      "Epoch 84 \t Batch 760 \t Training Loss: 45.66922475915206\n",
      "Epoch 84 \t Batch 780 \t Training Loss: 45.66805610167675\n",
      "Epoch 84 \t Batch 800 \t Training Loss: 45.69465487957001\n",
      "Epoch 84 \t Batch 820 \t Training Loss: 45.6913357525337\n",
      "Epoch 84 \t Batch 840 \t Training Loss: 45.6852132661002\n",
      "Epoch 84 \t Batch 860 \t Training Loss: 45.68677221342575\n",
      "Epoch 84 \t Batch 880 \t Training Loss: 45.73607498515736\n",
      "Epoch 84 \t Batch 900 \t Training Loss: 45.713693364461264\n",
      "Epoch 84 \t Batch 20 \t Validation Loss: 23.15762243270874\n",
      "Epoch 84 \t Batch 40 \t Validation Loss: 24.518169403076172\n",
      "Epoch 84 \t Batch 60 \t Validation Loss: 24.200014114379883\n",
      "Epoch 84 \t Batch 80 \t Validation Loss: 24.428268826007844\n",
      "Epoch 84 \t Batch 100 \t Validation Loss: 25.46262608528137\n",
      "Epoch 84 \t Batch 120 \t Validation Loss: 26.358112184206643\n",
      "Epoch 84 \t Batch 140 \t Validation Loss: 26.673442929131642\n",
      "Epoch 84 \t Batch 160 \t Validation Loss: 28.35919595360756\n",
      "Epoch 84 \t Batch 180 \t Validation Loss: 31.90900379286872\n",
      "Epoch 84 \t Batch 200 \t Validation Loss: 33.32644546985626\n",
      "Epoch 84 \t Batch 220 \t Validation Loss: 34.447929438677704\n",
      "Epoch 84 \t Batch 240 \t Validation Loss: 34.862146429220836\n",
      "Epoch 84 \t Batch 260 \t Validation Loss: 36.88135509857764\n",
      "Epoch 84 \t Batch 280 \t Validation Loss: 37.93527478490557\n",
      "Epoch 84 \t Batch 300 \t Validation Loss: 39.073317495981854\n",
      "Epoch 84 \t Batch 320 \t Validation Loss: 39.539323672652245\n",
      "Epoch 84 \t Batch 340 \t Validation Loss: 39.45076298152699\n",
      "Epoch 84 \t Batch 360 \t Validation Loss: 39.23778292602963\n",
      "Epoch 84 \t Batch 380 \t Validation Loss: 39.432676187314485\n",
      "Epoch 84 \t Batch 400 \t Validation Loss: 39.025357034206394\n",
      "Epoch 84 \t Batch 420 \t Validation Loss: 39.029319047927856\n",
      "Epoch 84 \t Batch 440 \t Validation Loss: 38.715813955393706\n",
      "Epoch 84 \t Batch 460 \t Validation Loss: 38.95407762320146\n",
      "Epoch 84 \t Batch 480 \t Validation Loss: 39.42061394254367\n",
      "Epoch 84 \t Batch 500 \t Validation Loss: 39.12607527351379\n",
      "Epoch 84 \t Batch 520 \t Validation Loss: 38.89771328889407\n",
      "Epoch 84 \t Batch 540 \t Validation Loss: 38.65679583196287\n",
      "Epoch 84 \t Batch 560 \t Validation Loss: 38.45344518082482\n",
      "Epoch 84 \t Batch 580 \t Validation Loss: 38.19999166850386\n",
      "Epoch 84 \t Batch 600 \t Validation Loss: 38.415758894284565\n",
      "Epoch 84 Training Loss: 45.72377776267614 Validation Loss: 39.077351478787214\n",
      "Epoch 84 completed\n",
      "Epoch 85 \t Batch 20 \t Training Loss: 45.30704975128174\n",
      "Epoch 85 \t Batch 40 \t Training Loss: 45.40926122665405\n",
      "Epoch 85 \t Batch 60 \t Training Loss: 45.63639920552571\n",
      "Epoch 85 \t Batch 80 \t Training Loss: 45.87326588630676\n",
      "Epoch 85 \t Batch 100 \t Training Loss: 45.89472408294678\n",
      "Epoch 85 \t Batch 120 \t Training Loss: 45.756081899007164\n",
      "Epoch 85 \t Batch 140 \t Training Loss: 45.77312831878662\n",
      "Epoch 85 \t Batch 160 \t Training Loss: 45.82743022441864\n",
      "Epoch 85 \t Batch 180 \t Training Loss: 45.810061645507815\n",
      "Epoch 85 \t Batch 200 \t Training Loss: 45.676182708740235\n",
      "Epoch 85 \t Batch 220 \t Training Loss: 45.56624370921742\n",
      "Epoch 85 \t Batch 240 \t Training Loss: 45.70571753184001\n",
      "Epoch 85 \t Batch 260 \t Training Loss: 45.76534940279447\n",
      "Epoch 85 \t Batch 280 \t Training Loss: 45.61587987627302\n",
      "Epoch 85 \t Batch 300 \t Training Loss: 45.65856093088786\n",
      "Epoch 85 \t Batch 320 \t Training Loss: 45.64311598539352\n",
      "Epoch 85 \t Batch 340 \t Training Loss: 45.60880195393282\n",
      "Epoch 85 \t Batch 360 \t Training Loss: 45.538663895924884\n",
      "Epoch 85 \t Batch 380 \t Training Loss: 45.531695667066074\n",
      "Epoch 85 \t Batch 400 \t Training Loss: 45.551142349243165\n",
      "Epoch 85 \t Batch 420 \t Training Loss: 45.51972496396019\n",
      "Epoch 85 \t Batch 440 \t Training Loss: 45.56111177964644\n",
      "Epoch 85 \t Batch 460 \t Training Loss: 45.622499051301375\n",
      "Epoch 85 \t Batch 480 \t Training Loss: 45.65924151738485\n",
      "Epoch 85 \t Batch 500 \t Training Loss: 45.683641548156736\n",
      "Epoch 85 \t Batch 520 \t Training Loss: 45.69444873516376\n",
      "Epoch 85 \t Batch 540 \t Training Loss: 45.744882887381095\n",
      "Epoch 85 \t Batch 560 \t Training Loss: 45.74170685495649\n",
      "Epoch 85 \t Batch 580 \t Training Loss: 45.74311184718691\n",
      "Epoch 85 \t Batch 600 \t Training Loss: 45.757654126485185\n",
      "Epoch 85 \t Batch 620 \t Training Loss: 45.766272661762855\n",
      "Epoch 85 \t Batch 640 \t Training Loss: 45.7674655854702\n",
      "Epoch 85 \t Batch 660 \t Training Loss: 45.793433385906795\n",
      "Epoch 85 \t Batch 680 \t Training Loss: 45.74287034764009\n",
      "Epoch 85 \t Batch 700 \t Training Loss: 45.73504159109933\n",
      "Epoch 85 \t Batch 720 \t Training Loss: 45.72037666108873\n",
      "Epoch 85 \t Batch 740 \t Training Loss: 45.72007444742564\n",
      "Epoch 85 \t Batch 760 \t Training Loss: 45.725968481365\n",
      "Epoch 85 \t Batch 780 \t Training Loss: 45.68101685841878\n",
      "Epoch 85 \t Batch 800 \t Training Loss: 45.72339293956757\n",
      "Epoch 85 \t Batch 820 \t Training Loss: 45.74179341851211\n",
      "Epoch 85 \t Batch 840 \t Training Loss: 45.741167222885856\n",
      "Epoch 85 \t Batch 860 \t Training Loss: 45.72110945235851\n",
      "Epoch 85 \t Batch 880 \t Training Loss: 45.72985419793562\n",
      "Epoch 85 \t Batch 900 \t Training Loss: 45.75747117784288\n",
      "Epoch 85 \t Batch 20 \t Validation Loss: 17.009098958969116\n",
      "Epoch 85 \t Batch 40 \t Validation Loss: 19.538953185081482\n",
      "Epoch 85 \t Batch 60 \t Validation Loss: 19.390314801534018\n",
      "Epoch 85 \t Batch 80 \t Validation Loss: 20.283128917217255\n",
      "Epoch 85 \t Batch 100 \t Validation Loss: 22.21344069480896\n",
      "Epoch 85 \t Batch 120 \t Validation Loss: 23.728316362698873\n",
      "Epoch 85 \t Batch 140 \t Validation Loss: 24.60102756364005\n",
      "Epoch 85 \t Batch 160 \t Validation Loss: 26.69353511929512\n",
      "Epoch 85 \t Batch 180 \t Validation Loss: 30.368707609176635\n",
      "Epoch 85 \t Batch 200 \t Validation Loss: 31.905035700798035\n",
      "Epoch 85 \t Batch 220 \t Validation Loss: 33.173936276002365\n",
      "Epoch 85 \t Batch 240 \t Validation Loss: 33.76195100943247\n",
      "Epoch 85 \t Batch 260 \t Validation Loss: 35.8145611506242\n",
      "Epoch 85 \t Batch 280 \t Validation Loss: 36.930067907060895\n",
      "Epoch 85 \t Batch 300 \t Validation Loss: 38.07795345306396\n",
      "Epoch 85 \t Batch 320 \t Validation Loss: 38.59624108672142\n",
      "Epoch 85 \t Batch 340 \t Validation Loss: 38.59389899197747\n",
      "Epoch 85 \t Batch 360 \t Validation Loss: 38.51764514181349\n",
      "Epoch 85 \t Batch 380 \t Validation Loss: 38.77663462789435\n",
      "Epoch 85 \t Batch 400 \t Validation Loss: 38.4447128868103\n",
      "Epoch 85 \t Batch 420 \t Validation Loss: 38.468694400787356\n",
      "Epoch 85 \t Batch 440 \t Validation Loss: 38.2139517957514\n",
      "Epoch 85 \t Batch 460 \t Validation Loss: 38.50066777519558\n",
      "Epoch 85 \t Batch 480 \t Validation Loss: 38.99148943026861\n",
      "Epoch 85 \t Batch 500 \t Validation Loss: 38.726322792053224\n",
      "Epoch 85 \t Batch 520 \t Validation Loss: 38.600107310368465\n",
      "Epoch 85 \t Batch 540 \t Validation Loss: 38.44830914956552\n",
      "Epoch 85 \t Batch 560 \t Validation Loss: 38.339820293017794\n",
      "Epoch 85 \t Batch 580 \t Validation Loss: 38.22409881723338\n",
      "Epoch 85 \t Batch 600 \t Validation Loss: 38.47911460876465\n",
      "Epoch 85 Training Loss: 45.73966896780965 Validation Loss: 39.236847047681934\n",
      "Epoch 85 completed\n",
      "Epoch 86 \t Batch 20 \t Training Loss: 44.519876098632814\n",
      "Epoch 86 \t Batch 40 \t Training Loss: 45.02533798217773\n",
      "Epoch 86 \t Batch 60 \t Training Loss: 45.66111793518066\n",
      "Epoch 86 \t Batch 80 \t Training Loss: 45.67886085510254\n",
      "Epoch 86 \t Batch 100 \t Training Loss: 45.38380428314209\n",
      "Epoch 86 \t Batch 120 \t Training Loss: 45.42264340718587\n",
      "Epoch 86 \t Batch 140 \t Training Loss: 45.66752708980015\n",
      "Epoch 86 \t Batch 160 \t Training Loss: 45.6499440908432\n",
      "Epoch 86 \t Batch 180 \t Training Loss: 45.62086279127333\n",
      "Epoch 86 \t Batch 200 \t Training Loss: 45.53777841567993\n",
      "Epoch 86 \t Batch 220 \t Training Loss: 45.41249708695845\n",
      "Epoch 86 \t Batch 240 \t Training Loss: 45.4178152402242\n",
      "Epoch 86 \t Batch 260 \t Training Loss: 45.45564397665171\n",
      "Epoch 86 \t Batch 280 \t Training Loss: 45.497687462397984\n",
      "Epoch 86 \t Batch 300 \t Training Loss: 45.53730242411296\n",
      "Epoch 86 \t Batch 320 \t Training Loss: 45.5643443107605\n",
      "Epoch 86 \t Batch 340 \t Training Loss: 45.58174232034122\n",
      "Epoch 86 \t Batch 360 \t Training Loss: 45.56985126071506\n",
      "Epoch 86 \t Batch 380 \t Training Loss: 45.56784126884059\n",
      "Epoch 86 \t Batch 400 \t Training Loss: 45.62779948234558\n",
      "Epoch 86 \t Batch 420 \t Training Loss: 45.65722042265392\n",
      "Epoch 86 \t Batch 440 \t Training Loss: 45.65754632949829\n",
      "Epoch 86 \t Batch 460 \t Training Loss: 45.707040372102156\n",
      "Epoch 86 \t Batch 480 \t Training Loss: 45.69785188039144\n",
      "Epoch 86 \t Batch 500 \t Training Loss: 45.698276756286624\n",
      "Epoch 86 \t Batch 520 \t Training Loss: 45.67459886257465\n",
      "Epoch 86 \t Batch 540 \t Training Loss: 45.72038438585069\n",
      "Epoch 86 \t Batch 560 \t Training Loss: 45.71472761290414\n",
      "Epoch 86 \t Batch 580 \t Training Loss: 45.73511564320531\n",
      "Epoch 86 \t Batch 600 \t Training Loss: 45.73643344243367\n",
      "Epoch 86 \t Batch 620 \t Training Loss: 45.74055895036267\n",
      "Epoch 86 \t Batch 640 \t Training Loss: 45.67896192669868\n",
      "Epoch 86 \t Batch 660 \t Training Loss: 45.68893059239243\n",
      "Epoch 86 \t Batch 680 \t Training Loss: 45.67598176282995\n",
      "Epoch 86 \t Batch 700 \t Training Loss: 45.696871577671594\n",
      "Epoch 86 \t Batch 720 \t Training Loss: 45.674138932757906\n",
      "Epoch 86 \t Batch 740 \t Training Loss: 45.68417144466091\n",
      "Epoch 86 \t Batch 760 \t Training Loss: 45.69935965788992\n",
      "Epoch 86 \t Batch 780 \t Training Loss: 45.69025011307154\n",
      "Epoch 86 \t Batch 800 \t Training Loss: 45.68542520046234\n",
      "Epoch 86 \t Batch 820 \t Training Loss: 45.670232363445\n",
      "Epoch 86 \t Batch 840 \t Training Loss: 45.68639365150815\n",
      "Epoch 86 \t Batch 860 \t Training Loss: 45.665761623826135\n",
      "Epoch 86 \t Batch 880 \t Training Loss: 45.678220142017715\n",
      "Epoch 86 \t Batch 900 \t Training Loss: 45.6739258617825\n",
      "Epoch 86 \t Batch 20 \t Validation Loss: 17.41769905090332\n",
      "Epoch 86 \t Batch 40 \t Validation Loss: 19.55734815597534\n",
      "Epoch 86 \t Batch 60 \t Validation Loss: 19.322488673528035\n",
      "Epoch 86 \t Batch 80 \t Validation Loss: 19.942937326431274\n",
      "Epoch 86 \t Batch 100 \t Validation Loss: 21.79227731704712\n",
      "Epoch 86 \t Batch 120 \t Validation Loss: 23.322898197174073\n",
      "Epoch 86 \t Batch 140 \t Validation Loss: 24.11478020804269\n",
      "Epoch 86 \t Batch 160 \t Validation Loss: 26.131515061855318\n",
      "Epoch 86 \t Batch 180 \t Validation Loss: 30.011997413635253\n",
      "Epoch 86 \t Batch 200 \t Validation Loss: 31.525997409820558\n",
      "Epoch 86 \t Batch 220 \t Validation Loss: 32.759502931074664\n",
      "Epoch 86 \t Batch 240 \t Validation Loss: 33.29306746323903\n",
      "Epoch 86 \t Batch 260 \t Validation Loss: 35.34730782142052\n",
      "Epoch 86 \t Batch 280 \t Validation Loss: 36.443624888147625\n",
      "Epoch 86 \t Batch 300 \t Validation Loss: 37.751857131322225\n",
      "Epoch 86 \t Batch 320 \t Validation Loss: 38.34502512216568\n",
      "Epoch 86 \t Batch 340 \t Validation Loss: 38.325011545069074\n",
      "Epoch 86 \t Batch 360 \t Validation Loss: 38.22401178677877\n",
      "Epoch 86 \t Batch 380 \t Validation Loss: 38.43322730817293\n",
      "Epoch 86 \t Batch 400 \t Validation Loss: 38.07208635807037\n",
      "Epoch 86 \t Batch 420 \t Validation Loss: 38.10018543515886\n",
      "Epoch 86 \t Batch 440 \t Validation Loss: 37.82811519882896\n",
      "Epoch 86 \t Batch 460 \t Validation Loss: 38.11180797245191\n",
      "Epoch 86 \t Batch 480 \t Validation Loss: 38.63341158231099\n",
      "Epoch 86 \t Batch 500 \t Validation Loss: 38.37302647018433\n",
      "Epoch 86 \t Batch 520 \t Validation Loss: 38.174689610187826\n",
      "Epoch 86 \t Batch 540 \t Validation Loss: 37.94758574697706\n",
      "Epoch 86 \t Batch 560 \t Validation Loss: 37.74809045961925\n",
      "Epoch 86 \t Batch 580 \t Validation Loss: 37.49175616132802\n",
      "Epoch 86 \t Batch 600 \t Validation Loss: 37.724733541806536\n",
      "Epoch 86 Training Loss: 45.65769646913951 Validation Loss: 38.396714927314164\n",
      "Epoch 86 completed\n",
      "Epoch 87 \t Batch 20 \t Training Loss: 44.54667644500732\n",
      "Epoch 87 \t Batch 40 \t Training Loss: 45.15454044342041\n",
      "Epoch 87 \t Batch 60 \t Training Loss: 45.38034184773763\n",
      "Epoch 87 \t Batch 80 \t Training Loss: 45.72237281799316\n",
      "Epoch 87 \t Batch 100 \t Training Loss: 45.71283500671387\n",
      "Epoch 87 \t Batch 120 \t Training Loss: 45.489662901560465\n",
      "Epoch 87 \t Batch 140 \t Training Loss: 45.33814577375139\n",
      "Epoch 87 \t Batch 160 \t Training Loss: 45.27358605861664\n",
      "Epoch 87 \t Batch 180 \t Training Loss: 45.257878091600205\n",
      "Epoch 87 \t Batch 200 \t Training Loss: 45.322135028839114\n",
      "Epoch 87 \t Batch 220 \t Training Loss: 45.436708346280184\n",
      "Epoch 87 \t Batch 240 \t Training Loss: 45.49454544385274\n",
      "Epoch 87 \t Batch 260 \t Training Loss: 45.43763907505916\n",
      "Epoch 87 \t Batch 280 \t Training Loss: 45.422538362230576\n",
      "Epoch 87 \t Batch 300 \t Training Loss: 45.407847023010255\n",
      "Epoch 87 \t Batch 320 \t Training Loss: 45.4108241558075\n",
      "Epoch 87 \t Batch 340 \t Training Loss: 45.406271732554714\n",
      "Epoch 87 \t Batch 360 \t Training Loss: 45.40727059046427\n",
      "Epoch 87 \t Batch 380 \t Training Loss: 45.31873176976254\n",
      "Epoch 87 \t Batch 400 \t Training Loss: 45.35280389785767\n",
      "Epoch 87 \t Batch 420 \t Training Loss: 45.47363177708217\n",
      "Epoch 87 \t Batch 440 \t Training Loss: 45.386188038912685\n",
      "Epoch 87 \t Batch 460 \t Training Loss: 45.437715596738066\n",
      "Epoch 87 \t Batch 480 \t Training Loss: 45.450212295850115\n",
      "Epoch 87 \t Batch 500 \t Training Loss: 45.50478125\n",
      "Epoch 87 \t Batch 520 \t Training Loss: 45.531904440659744\n",
      "Epoch 87 \t Batch 540 \t Training Loss: 45.56754424483688\n",
      "Epoch 87 \t Batch 560 \t Training Loss: 45.53243524006435\n",
      "Epoch 87 \t Batch 580 \t Training Loss: 45.49963810361665\n",
      "Epoch 87 \t Batch 600 \t Training Loss: 45.474799925486245\n",
      "Epoch 87 \t Batch 620 \t Training Loss: 45.46613209632135\n",
      "Epoch 87 \t Batch 640 \t Training Loss: 45.51688039302826\n",
      "Epoch 87 \t Batch 660 \t Training Loss: 45.553388982830626\n",
      "Epoch 87 \t Batch 680 \t Training Loss: 45.561088864943564\n",
      "Epoch 87 \t Batch 700 \t Training Loss: 45.554188755580356\n",
      "Epoch 87 \t Batch 720 \t Training Loss: 45.567831669913396\n",
      "Epoch 87 \t Batch 740 \t Training Loss: 45.584554522746316\n",
      "Epoch 87 \t Batch 760 \t Training Loss: 45.618801352852266\n",
      "Epoch 87 \t Batch 780 \t Training Loss: 45.609125704643056\n",
      "Epoch 87 \t Batch 800 \t Training Loss: 45.61974722385406\n",
      "Epoch 87 \t Batch 820 \t Training Loss: 45.61763862516822\n",
      "Epoch 87 \t Batch 840 \t Training Loss: 45.63533346085321\n",
      "Epoch 87 \t Batch 860 \t Training Loss: 45.63890889189964\n",
      "Epoch 87 \t Batch 880 \t Training Loss: 45.62553647648205\n",
      "Epoch 87 \t Batch 900 \t Training Loss: 45.63152759128147\n",
      "Epoch 87 \t Batch 20 \t Validation Loss: 25.400287246704103\n",
      "Epoch 87 \t Batch 40 \t Validation Loss: 26.527376675605773\n",
      "Epoch 87 \t Batch 60 \t Validation Loss: 26.25367194811503\n",
      "Epoch 87 \t Batch 80 \t Validation Loss: 26.57095127105713\n",
      "Epoch 87 \t Batch 100 \t Validation Loss: 27.273268241882324\n",
      "Epoch 87 \t Batch 120 \t Validation Loss: 27.874476369222005\n",
      "Epoch 87 \t Batch 140 \t Validation Loss: 28.0564500944955\n",
      "Epoch 87 \t Batch 160 \t Validation Loss: 29.83002381324768\n",
      "Epoch 87 \t Batch 180 \t Validation Loss: 33.353353050020004\n",
      "Epoch 87 \t Batch 200 \t Validation Loss: 34.63976171970367\n",
      "Epoch 87 \t Batch 220 \t Validation Loss: 35.79039520350369\n",
      "Epoch 87 \t Batch 240 \t Validation Loss: 36.17682124376297\n",
      "Epoch 87 \t Batch 260 \t Validation Loss: 38.167776412230275\n",
      "Epoch 87 \t Batch 280 \t Validation Loss: 39.19540538447244\n",
      "Epoch 87 \t Batch 300 \t Validation Loss: 40.297955675125124\n",
      "Epoch 87 \t Batch 320 \t Validation Loss: 40.73381526172161\n",
      "Epoch 87 \t Batch 340 \t Validation Loss: 40.61302880960352\n",
      "Epoch 87 \t Batch 360 \t Validation Loss: 40.44146586259206\n",
      "Epoch 87 \t Batch 380 \t Validation Loss: 40.575841148276076\n",
      "Epoch 87 \t Batch 400 \t Validation Loss: 40.08853627443314\n",
      "Epoch 87 \t Batch 420 \t Validation Loss: 40.01545411291576\n",
      "Epoch 87 \t Batch 440 \t Validation Loss: 39.64691320982846\n",
      "Epoch 87 \t Batch 460 \t Validation Loss: 39.86164792309636\n",
      "Epoch 87 \t Batch 480 \t Validation Loss: 40.30780557195346\n",
      "Epoch 87 \t Batch 500 \t Validation Loss: 40.014329084396365\n",
      "Epoch 87 \t Batch 520 \t Validation Loss: 39.757422199616066\n",
      "Epoch 87 \t Batch 540 \t Validation Loss: 39.49675159630952\n",
      "Epoch 87 \t Batch 560 \t Validation Loss: 39.327119396414076\n",
      "Epoch 87 \t Batch 580 \t Validation Loss: 39.11484914812549\n",
      "Epoch 87 \t Batch 600 \t Validation Loss: 39.315881024996436\n",
      "Epoch 87 Training Loss: 45.646087929362686 Validation Loss: 39.973047843227135\n",
      "Epoch 87 completed\n",
      "Epoch 88 \t Batch 20 \t Training Loss: 45.29569282531738\n",
      "Epoch 88 \t Batch 40 \t Training Loss: 45.89940280914307\n",
      "Epoch 88 \t Batch 60 \t Training Loss: 45.67570972442627\n",
      "Epoch 88 \t Batch 80 \t Training Loss: 45.51950368881226\n",
      "Epoch 88 \t Batch 100 \t Training Loss: 45.36676937103272\n",
      "Epoch 88 \t Batch 120 \t Training Loss: 45.33398100535075\n",
      "Epoch 88 \t Batch 140 \t Training Loss: 45.35377750396729\n",
      "Epoch 88 \t Batch 160 \t Training Loss: 45.41133136749268\n",
      "Epoch 88 \t Batch 180 \t Training Loss: 45.36068325042724\n",
      "Epoch 88 \t Batch 200 \t Training Loss: 45.35868942260742\n",
      "Epoch 88 \t Batch 220 \t Training Loss: 45.3506668437611\n",
      "Epoch 88 \t Batch 240 \t Training Loss: 45.33003565470378\n",
      "Epoch 88 \t Batch 260 \t Training Loss: 45.29283915299636\n",
      "Epoch 88 \t Batch 280 \t Training Loss: 45.28964125769479\n",
      "Epoch 88 \t Batch 300 \t Training Loss: 45.398428179423014\n",
      "Epoch 88 \t Batch 320 \t Training Loss: 45.35805398225784\n",
      "Epoch 88 \t Batch 340 \t Training Loss: 45.446475264605354\n",
      "Epoch 88 \t Batch 360 \t Training Loss: 45.49707815382216\n",
      "Epoch 88 \t Batch 380 \t Training Loss: 45.579075883564194\n",
      "Epoch 88 \t Batch 400 \t Training Loss: 45.65651490211487\n",
      "Epoch 88 \t Batch 420 \t Training Loss: 45.629813848223\n",
      "Epoch 88 \t Batch 440 \t Training Loss: 45.714007169550115\n",
      "Epoch 88 \t Batch 460 \t Training Loss: 45.702453132297684\n",
      "Epoch 88 \t Batch 480 \t Training Loss: 45.723177989323936\n",
      "Epoch 88 \t Batch 500 \t Training Loss: 45.65721952819824\n",
      "Epoch 88 \t Batch 520 \t Training Loss: 45.645751351576585\n",
      "Epoch 88 \t Batch 540 \t Training Loss: 45.62492193999114\n",
      "Epoch 88 \t Batch 560 \t Training Loss: 45.607774591445924\n",
      "Epoch 88 \t Batch 580 \t Training Loss: 45.54493505543676\n",
      "Epoch 88 \t Batch 600 \t Training Loss: 45.5357710202535\n",
      "Epoch 88 \t Batch 620 \t Training Loss: 45.55839713311965\n",
      "Epoch 88 \t Batch 640 \t Training Loss: 45.55031172037125\n",
      "Epoch 88 \t Batch 660 \t Training Loss: 45.52595620588823\n",
      "Epoch 88 \t Batch 680 \t Training Loss: 45.505084492178526\n",
      "Epoch 88 \t Batch 700 \t Training Loss: 45.54733660016741\n",
      "Epoch 88 \t Batch 720 \t Training Loss: 45.554689444435965\n",
      "Epoch 88 \t Batch 740 \t Training Loss: 45.562454708202466\n",
      "Epoch 88 \t Batch 760 \t Training Loss: 45.52830858732525\n",
      "Epoch 88 \t Batch 780 \t Training Loss: 45.558273877853004\n",
      "Epoch 88 \t Batch 800 \t Training Loss: 45.58445562362671\n",
      "Epoch 88 \t Batch 820 \t Training Loss: 45.59235314625065\n",
      "Epoch 88 \t Batch 840 \t Training Loss: 45.60511320659092\n",
      "Epoch 88 \t Batch 860 \t Training Loss: 45.610961794298746\n",
      "Epoch 88 \t Batch 880 \t Training Loss: 45.64920001896945\n",
      "Epoch 88 \t Batch 900 \t Training Loss: 45.65240111456977\n",
      "Epoch 88 \t Batch 20 \t Validation Loss: 16.358016633987425\n",
      "Epoch 88 \t Batch 40 \t Validation Loss: 17.641740918159485\n",
      "Epoch 88 \t Batch 60 \t Validation Loss: 17.749022054672242\n",
      "Epoch 88 \t Batch 80 \t Validation Loss: 18.450997269153596\n",
      "Epoch 88 \t Batch 100 \t Validation Loss: 20.35404631614685\n",
      "Epoch 88 \t Batch 120 \t Validation Loss: 21.884292260805765\n",
      "Epoch 88 \t Batch 140 \t Validation Loss: 22.763461419514247\n",
      "Epoch 88 \t Batch 160 \t Validation Loss: 24.95206623673439\n",
      "Epoch 88 \t Batch 180 \t Validation Loss: 28.775516976250543\n",
      "Epoch 88 \t Batch 200 \t Validation Loss: 30.42258006095886\n",
      "Epoch 88 \t Batch 220 \t Validation Loss: 31.758374058116566\n",
      "Epoch 88 \t Batch 240 \t Validation Loss: 32.36313805977503\n",
      "Epoch 88 \t Batch 260 \t Validation Loss: 34.52138105539175\n",
      "Epoch 88 \t Batch 280 \t Validation Loss: 35.72163031441825\n",
      "Epoch 88 \t Batch 300 \t Validation Loss: 36.95802335103353\n",
      "Epoch 88 \t Batch 320 \t Validation Loss: 37.53197935819626\n",
      "Epoch 88 \t Batch 340 \t Validation Loss: 37.55091881471522\n",
      "Epoch 88 \t Batch 360 \t Validation Loss: 37.45962536070082\n",
      "Epoch 88 \t Batch 380 \t Validation Loss: 37.75537661502236\n",
      "Epoch 88 \t Batch 400 \t Validation Loss: 37.43099478244781\n",
      "Epoch 88 \t Batch 420 \t Validation Loss: 37.52923308781215\n",
      "Epoch 88 \t Batch 440 \t Validation Loss: 37.28799024061723\n",
      "Epoch 88 \t Batch 460 \t Validation Loss: 37.593299218882684\n",
      "Epoch 88 \t Batch 480 \t Validation Loss: 38.14094607035319\n",
      "Epoch 88 \t Batch 500 \t Validation Loss: 37.91468815231323\n",
      "Epoch 88 \t Batch 520 \t Validation Loss: 37.72677719409649\n",
      "Epoch 88 \t Batch 540 \t Validation Loss: 37.52066245255647\n",
      "Epoch 88 \t Batch 560 \t Validation Loss: 37.363037831442696\n",
      "Epoch 88 \t Batch 580 \t Validation Loss: 37.16429083922814\n",
      "Epoch 88 \t Batch 600 \t Validation Loss: 37.41490391413371\n",
      "Epoch 88 Training Loss: 45.650407244100954 Validation Loss: 38.11154803672394\n",
      "Epoch 88 completed\n",
      "Epoch 89 \t Batch 20 \t Training Loss: 44.030601501464844\n",
      "Epoch 89 \t Batch 40 \t Training Loss: 45.000084400177\n",
      "Epoch 89 \t Batch 60 \t Training Loss: 45.35302848815918\n",
      "Epoch 89 \t Batch 80 \t Training Loss: 45.2057186126709\n",
      "Epoch 89 \t Batch 100 \t Training Loss: 44.876411514282225\n",
      "Epoch 89 \t Batch 120 \t Training Loss: 44.865628210703534\n",
      "Epoch 89 \t Batch 140 \t Training Loss: 44.98931743076869\n",
      "Epoch 89 \t Batch 160 \t Training Loss: 44.95183973312378\n",
      "Epoch 89 \t Batch 180 \t Training Loss: 44.97518130408393\n",
      "Epoch 89 \t Batch 200 \t Training Loss: 45.314303455352785\n",
      "Epoch 89 \t Batch 220 \t Training Loss: 45.25163295052268\n",
      "Epoch 89 \t Batch 240 \t Training Loss: 45.28126537005107\n",
      "Epoch 89 \t Batch 260 \t Training Loss: 45.3559564443735\n",
      "Epoch 89 \t Batch 280 \t Training Loss: 45.360990728650776\n",
      "Epoch 89 \t Batch 300 \t Training Loss: 45.37030632019043\n",
      "Epoch 89 \t Batch 320 \t Training Loss: 45.37708123922348\n",
      "Epoch 89 \t Batch 340 \t Training Loss: 45.4142193625955\n",
      "Epoch 89 \t Batch 360 \t Training Loss: 45.41271871990628\n",
      "Epoch 89 \t Batch 380 \t Training Loss: 45.453961422568874\n",
      "Epoch 89 \t Batch 400 \t Training Loss: 45.48697796821594\n",
      "Epoch 89 \t Batch 420 \t Training Loss: 45.47195059458415\n",
      "Epoch 89 \t Batch 440 \t Training Loss: 45.60137480822476\n",
      "Epoch 89 \t Batch 460 \t Training Loss: 45.60697767838188\n",
      "Epoch 89 \t Batch 480 \t Training Loss: 45.58839348157247\n",
      "Epoch 89 \t Batch 500 \t Training Loss: 45.54125454711914\n",
      "Epoch 89 \t Batch 520 \t Training Loss: 45.510582087590144\n",
      "Epoch 89 \t Batch 540 \t Training Loss: 45.50726926591661\n",
      "Epoch 89 \t Batch 560 \t Training Loss: 45.52115715571812\n",
      "Epoch 89 \t Batch 580 \t Training Loss: 45.554234458660254\n",
      "Epoch 89 \t Batch 600 \t Training Loss: 45.57617255528768\n",
      "Epoch 89 \t Batch 620 \t Training Loss: 45.60208710085961\n",
      "Epoch 89 \t Batch 640 \t Training Loss: 45.5658414542675\n",
      "Epoch 89 \t Batch 660 \t Training Loss: 45.59108220302697\n",
      "Epoch 89 \t Batch 680 \t Training Loss: 45.6282213715946\n",
      "Epoch 89 \t Batch 700 \t Training Loss: 45.6418586403983\n",
      "Epoch 89 \t Batch 720 \t Training Loss: 45.6820481883155\n",
      "Epoch 89 \t Batch 740 \t Training Loss: 45.68185805243415\n",
      "Epoch 89 \t Batch 760 \t Training Loss: 45.672235955690084\n",
      "Epoch 89 \t Batch 780 \t Training Loss: 45.62625418442946\n",
      "Epoch 89 \t Batch 800 \t Training Loss: 45.624906339645385\n",
      "Epoch 89 \t Batch 820 \t Training Loss: 45.5893130418731\n",
      "Epoch 89 \t Batch 840 \t Training Loss: 45.57897421518962\n",
      "Epoch 89 \t Batch 860 \t Training Loss: 45.60439462439958\n",
      "Epoch 89 \t Batch 880 \t Training Loss: 45.61284794373946\n",
      "Epoch 89 \t Batch 900 \t Training Loss: 45.59973729027642\n",
      "Epoch 89 \t Batch 20 \t Validation Loss: 25.864369297027586\n",
      "Epoch 89 \t Batch 40 \t Validation Loss: 26.055181860923767\n",
      "Epoch 89 \t Batch 60 \t Validation Loss: 26.029791084925332\n",
      "Epoch 89 \t Batch 80 \t Validation Loss: 26.382115375995635\n",
      "Epoch 89 \t Batch 100 \t Validation Loss: 27.127025098800658\n",
      "Epoch 89 \t Batch 120 \t Validation Loss: 27.83450187842051\n",
      "Epoch 89 \t Batch 140 \t Validation Loss: 28.15359170777457\n",
      "Epoch 89 \t Batch 160 \t Validation Loss: 30.122358936071397\n",
      "Epoch 89 \t Batch 180 \t Validation Loss: 33.99052753448486\n",
      "Epoch 89 \t Batch 200 \t Validation Loss: 35.56828867912292\n",
      "Epoch 89 \t Batch 220 \t Validation Loss: 36.7780728080056\n",
      "Epoch 89 \t Batch 240 \t Validation Loss: 37.240775406360626\n",
      "Epoch 89 \t Batch 260 \t Validation Loss: 39.333548776920026\n",
      "Epoch 89 \t Batch 280 \t Validation Loss: 40.400764897891456\n",
      "Epoch 89 \t Batch 300 \t Validation Loss: 41.657998870213824\n",
      "Epoch 89 \t Batch 320 \t Validation Loss: 42.16046216785908\n",
      "Epoch 89 \t Batch 340 \t Validation Loss: 42.0325238199795\n",
      "Epoch 89 \t Batch 360 \t Validation Loss: 41.86904645231035\n",
      "Epoch 89 \t Batch 380 \t Validation Loss: 42.05412826287119\n",
      "Epoch 89 \t Batch 400 \t Validation Loss: 41.57502574205399\n",
      "Epoch 89 \t Batch 420 \t Validation Loss: 41.507939486276534\n",
      "Epoch 89 \t Batch 440 \t Validation Loss: 41.14450789581645\n",
      "Epoch 89 \t Batch 460 \t Validation Loss: 41.41160556751749\n",
      "Epoch 89 \t Batch 480 \t Validation Loss: 41.819444662332536\n",
      "Epoch 89 \t Batch 500 \t Validation Loss: 41.519224859237674\n",
      "Epoch 89 \t Batch 520 \t Validation Loss: 41.3691628401096\n",
      "Epoch 89 \t Batch 540 \t Validation Loss: 41.12722424577784\n",
      "Epoch 89 \t Batch 560 \t Validation Loss: 40.968397894927435\n",
      "Epoch 89 \t Batch 580 \t Validation Loss: 40.78350054806676\n",
      "Epoch 89 \t Batch 600 \t Validation Loss: 40.96714859803517\n",
      "Epoch 89 Training Loss: 45.596363566823975 Validation Loss: 41.63192394337097\n",
      "Epoch 89 completed\n",
      "Epoch 90 \t Batch 20 \t Training Loss: 44.81874370574951\n",
      "Epoch 90 \t Batch 40 \t Training Loss: 44.99598350524902\n",
      "Epoch 90 \t Batch 60 \t Training Loss: 44.80134359995524\n",
      "Epoch 90 \t Batch 80 \t Training Loss: 45.02772846221924\n",
      "Epoch 90 \t Batch 100 \t Training Loss: 45.343843612670895\n",
      "Epoch 90 \t Batch 120 \t Training Loss: 45.386124006907146\n",
      "Epoch 90 \t Batch 140 \t Training Loss: 45.41341590881348\n",
      "Epoch 90 \t Batch 160 \t Training Loss: 45.40115411281586\n",
      "Epoch 90 \t Batch 180 \t Training Loss: 45.414154370625816\n",
      "Epoch 90 \t Batch 200 \t Training Loss: 45.537098770141604\n",
      "Epoch 90 \t Batch 220 \t Training Loss: 45.57237767306241\n",
      "Epoch 90 \t Batch 240 \t Training Loss: 45.54016772905985\n",
      "Epoch 90 \t Batch 260 \t Training Loss: 45.58434853186974\n",
      "Epoch 90 \t Batch 280 \t Training Loss: 45.65145695550101\n",
      "Epoch 90 \t Batch 300 \t Training Loss: 45.6763285446167\n",
      "Epoch 90 \t Batch 320 \t Training Loss: 45.683862280845645\n",
      "Epoch 90 \t Batch 340 \t Training Loss: 45.660128481247845\n",
      "Epoch 90 \t Batch 360 \t Training Loss: 45.6973849190606\n",
      "Epoch 90 \t Batch 380 \t Training Loss: 45.747184823688706\n",
      "Epoch 90 \t Batch 400 \t Training Loss: 45.793795022964474\n",
      "Epoch 90 \t Batch 420 \t Training Loss: 45.77615174793062\n",
      "Epoch 90 \t Batch 440 \t Training Loss: 45.73844718066129\n",
      "Epoch 90 \t Batch 460 \t Training Loss: 45.7568885139797\n",
      "Epoch 90 \t Batch 480 \t Training Loss: 45.773268127441405\n",
      "Epoch 90 \t Batch 500 \t Training Loss: 45.727643821716306\n",
      "Epoch 90 \t Batch 520 \t Training Loss: 45.78590373259324\n",
      "Epoch 90 \t Batch 540 \t Training Loss: 45.77589994359899\n",
      "Epoch 90 \t Batch 560 \t Training Loss: 45.728779138837545\n",
      "Epoch 90 \t Batch 580 \t Training Loss: 45.764436083826524\n",
      "Epoch 90 \t Batch 600 \t Training Loss: 45.74859958648682\n",
      "Epoch 90 \t Batch 620 \t Training Loss: 45.7326286808137\n",
      "Epoch 90 \t Batch 640 \t Training Loss: 45.720540231466295\n",
      "Epoch 90 \t Batch 660 \t Training Loss: 45.700045481595126\n",
      "Epoch 90 \t Batch 680 \t Training Loss: 45.687190712199495\n",
      "Epoch 90 \t Batch 700 \t Training Loss: 45.652502365112305\n",
      "Epoch 90 \t Batch 720 \t Training Loss: 45.64332969983419\n",
      "Epoch 90 \t Batch 740 \t Training Loss: 45.642634268064754\n",
      "Epoch 90 \t Batch 760 \t Training Loss: 45.619583024476704\n",
      "Epoch 90 \t Batch 780 \t Training Loss: 45.59973249190893\n",
      "Epoch 90 \t Batch 800 \t Training Loss: 45.56959406375885\n",
      "Epoch 90 \t Batch 820 \t Training Loss: 45.59751912093744\n",
      "Epoch 90 \t Batch 840 \t Training Loss: 45.578353795551116\n",
      "Epoch 90 \t Batch 860 \t Training Loss: 45.62196083512417\n",
      "Epoch 90 \t Batch 880 \t Training Loss: 45.64033445444974\n",
      "Epoch 90 \t Batch 900 \t Training Loss: 45.6119419309828\n",
      "Epoch 90 \t Batch 20 \t Validation Loss: 20.017207193374634\n",
      "Epoch 90 \t Batch 40 \t Validation Loss: 21.843656039237977\n",
      "Epoch 90 \t Batch 60 \t Validation Loss: 21.496853415171305\n",
      "Epoch 90 \t Batch 80 \t Validation Loss: 21.888790893554688\n",
      "Epoch 90 \t Batch 100 \t Validation Loss: 23.347168159484863\n",
      "Epoch 90 \t Batch 120 \t Validation Loss: 24.594283485412596\n",
      "Epoch 90 \t Batch 140 \t Validation Loss: 25.20080108642578\n",
      "Epoch 90 \t Batch 160 \t Validation Loss: 27.171089160442353\n",
      "Epoch 90 \t Batch 180 \t Validation Loss: 30.866326136059232\n",
      "Epoch 90 \t Batch 200 \t Validation Loss: 32.377581839561465\n",
      "Epoch 90 \t Batch 220 \t Validation Loss: 33.557178258895874\n",
      "Epoch 90 \t Batch 240 \t Validation Loss: 34.027252332369486\n",
      "Epoch 90 \t Batch 260 \t Validation Loss: 36.091144279333264\n",
      "Epoch 90 \t Batch 280 \t Validation Loss: 37.209135460853574\n",
      "Epoch 90 \t Batch 300 \t Validation Loss: 38.385069093704224\n",
      "Epoch 90 \t Batch 320 \t Validation Loss: 38.91216844320297\n",
      "Epoch 90 \t Batch 340 \t Validation Loss: 38.866797250859875\n",
      "Epoch 90 \t Batch 360 \t Validation Loss: 38.722995318306815\n",
      "Epoch 90 \t Batch 380 \t Validation Loss: 38.95220123592176\n",
      "Epoch 90 \t Batch 400 \t Validation Loss: 38.57662864208221\n",
      "Epoch 90 \t Batch 420 \t Validation Loss: 38.60542004903157\n",
      "Epoch 90 \t Batch 440 \t Validation Loss: 38.339673800901934\n",
      "Epoch 90 \t Batch 460 \t Validation Loss: 38.673791275853695\n",
      "Epoch 90 \t Batch 480 \t Validation Loss: 39.177261586983995\n",
      "Epoch 90 \t Batch 500 \t Validation Loss: 38.92050717163086\n",
      "Epoch 90 \t Batch 520 \t Validation Loss: 38.75711351541372\n",
      "Epoch 90 \t Batch 540 \t Validation Loss: 38.51808292600844\n",
      "Epoch 90 \t Batch 560 \t Validation Loss: 38.30803404194968\n",
      "Epoch 90 \t Batch 580 \t Validation Loss: 38.044680855191984\n",
      "Epoch 90 \t Batch 600 \t Validation Loss: 38.27071581204732\n",
      "Epoch 90 Training Loss: 45.589324231496164 Validation Loss: 38.946231938027715\n",
      "Epoch 90 completed\n",
      "Epoch 91 \t Batch 20 \t Training Loss: 45.42807178497314\n",
      "Epoch 91 \t Batch 40 \t Training Loss: 45.39374408721924\n",
      "Epoch 91 \t Batch 60 \t Training Loss: 45.19995606740316\n",
      "Epoch 91 \t Batch 80 \t Training Loss: 45.245230484008786\n",
      "Epoch 91 \t Batch 100 \t Training Loss: 45.68171100616455\n",
      "Epoch 91 \t Batch 120 \t Training Loss: 45.779224681854245\n",
      "Epoch 91 \t Batch 140 \t Training Loss: 45.8149859564645\n",
      "Epoch 91 \t Batch 160 \t Training Loss: 45.702534437179565\n",
      "Epoch 91 \t Batch 180 \t Training Loss: 45.73339680565728\n",
      "Epoch 91 \t Batch 200 \t Training Loss: 45.72650255203247\n",
      "Epoch 91 \t Batch 220 \t Training Loss: 45.70890643379905\n",
      "Epoch 91 \t Batch 240 \t Training Loss: 45.82902553876241\n",
      "Epoch 91 \t Batch 260 \t Training Loss: 45.93680468339186\n",
      "Epoch 91 \t Batch 280 \t Training Loss: 45.85068411145892\n",
      "Epoch 91 \t Batch 300 \t Training Loss: 45.79501319885254\n",
      "Epoch 91 \t Batch 320 \t Training Loss: 45.745267391204834\n",
      "Epoch 91 \t Batch 340 \t Training Loss: 45.74795446956859\n",
      "Epoch 91 \t Batch 360 \t Training Loss: 45.736461808946395\n",
      "Epoch 91 \t Batch 380 \t Training Loss: 45.670704811497735\n",
      "Epoch 91 \t Batch 400 \t Training Loss: 45.58790347099304\n",
      "Epoch 91 \t Batch 420 \t Training Loss: 45.528637949625654\n",
      "Epoch 91 \t Batch 440 \t Training Loss: 45.57059020996094\n",
      "Epoch 91 \t Batch 460 \t Training Loss: 45.54356079101562\n",
      "Epoch 91 \t Batch 480 \t Training Loss: 45.55473128954569\n",
      "Epoch 91 \t Batch 500 \t Training Loss: 45.55746939849853\n",
      "Epoch 91 \t Batch 520 \t Training Loss: 45.5349324959975\n",
      "Epoch 91 \t Batch 540 \t Training Loss: 45.543046682852285\n",
      "Epoch 91 \t Batch 560 \t Training Loss: 45.527376011439735\n",
      "Epoch 91 \t Batch 580 \t Training Loss: 45.54282955301219\n",
      "Epoch 91 \t Batch 600 \t Training Loss: 45.55657287597656\n",
      "Epoch 91 \t Batch 620 \t Training Loss: 45.566113127431564\n",
      "Epoch 91 \t Batch 640 \t Training Loss: 45.54298576116562\n",
      "Epoch 91 \t Batch 660 \t Training Loss: 45.534595501061645\n",
      "Epoch 91 \t Batch 680 \t Training Loss: 45.50509490966797\n",
      "Epoch 91 \t Batch 700 \t Training Loss: 45.461558069501606\n",
      "Epoch 91 \t Batch 720 \t Training Loss: 45.428544759750366\n",
      "Epoch 91 \t Batch 740 \t Training Loss: 45.45912998818063\n",
      "Epoch 91 \t Batch 760 \t Training Loss: 45.488236768622144\n",
      "Epoch 91 \t Batch 780 \t Training Loss: 45.49072778652876\n",
      "Epoch 91 \t Batch 800 \t Training Loss: 45.5092053604126\n",
      "Epoch 91 \t Batch 820 \t Training Loss: 45.538554149720724\n",
      "Epoch 91 \t Batch 840 \t Training Loss: 45.52843495777675\n",
      "Epoch 91 \t Batch 860 \t Training Loss: 45.54325511843659\n",
      "Epoch 91 \t Batch 880 \t Training Loss: 45.54015788598494\n",
      "Epoch 91 \t Batch 900 \t Training Loss: 45.52637842390272\n",
      "Epoch 91 \t Batch 20 \t Validation Loss: 19.55736036300659\n",
      "Epoch 91 \t Batch 40 \t Validation Loss: 22.149989128112793\n",
      "Epoch 91 \t Batch 60 \t Validation Loss: 21.887890783945718\n",
      "Epoch 91 \t Batch 80 \t Validation Loss: 22.600972032547\n",
      "Epoch 91 \t Batch 100 \t Validation Loss: 23.989184188842774\n",
      "Epoch 91 \t Batch 120 \t Validation Loss: 25.174297491709392\n",
      "Epoch 91 \t Batch 140 \t Validation Loss: 25.7476845741272\n",
      "Epoch 91 \t Batch 160 \t Validation Loss: 27.699048268795014\n",
      "Epoch 91 \t Batch 180 \t Validation Loss: 31.331709988911946\n",
      "Epoch 91 \t Batch 200 \t Validation Loss: 32.73900480270386\n",
      "Epoch 91 \t Batch 220 \t Validation Loss: 33.95976226980036\n",
      "Epoch 91 \t Batch 240 \t Validation Loss: 34.43927841981252\n",
      "Epoch 91 \t Batch 260 \t Validation Loss: 36.468480924459605\n",
      "Epoch 91 \t Batch 280 \t Validation Loss: 37.54895485128675\n",
      "Epoch 91 \t Batch 300 \t Validation Loss: 38.71630825996399\n",
      "Epoch 91 \t Batch 320 \t Validation Loss: 39.22157327234745\n",
      "Epoch 91 \t Batch 340 \t Validation Loss: 39.17425466144786\n",
      "Epoch 91 \t Batch 360 \t Validation Loss: 39.04261224799686\n",
      "Epoch 91 \t Batch 380 \t Validation Loss: 39.25242689283271\n",
      "Epoch 91 \t Batch 400 \t Validation Loss: 38.85995098352432\n",
      "Epoch 91 \t Batch 420 \t Validation Loss: 38.87625682694571\n",
      "Epoch 91 \t Batch 440 \t Validation Loss: 38.570750620148395\n",
      "Epoch 91 \t Batch 460 \t Validation Loss: 38.81671650720679\n",
      "Epoch 91 \t Batch 480 \t Validation Loss: 39.29526415467262\n",
      "Epoch 91 \t Batch 500 \t Validation Loss: 39.03118992805481\n",
      "Epoch 91 \t Batch 520 \t Validation Loss: 38.8022693835772\n",
      "Epoch 91 \t Batch 540 \t Validation Loss: 38.56652992566426\n",
      "Epoch 91 \t Batch 560 \t Validation Loss: 38.38927081823349\n",
      "Epoch 91 \t Batch 580 \t Validation Loss: 38.1672385955679\n",
      "Epoch 91 \t Batch 600 \t Validation Loss: 38.38467433452606\n",
      "Epoch 91 Training Loss: 45.5411518059484 Validation Loss: 39.05801583420146\n",
      "Epoch 91 completed\n",
      "Epoch 92 \t Batch 20 \t Training Loss: 44.773813819885255\n",
      "Epoch 92 \t Batch 40 \t Training Loss: 45.614386558532715\n",
      "Epoch 92 \t Batch 60 \t Training Loss: 45.921861521402995\n",
      "Epoch 92 \t Batch 80 \t Training Loss: 45.81808161735535\n",
      "Epoch 92 \t Batch 100 \t Training Loss: 46.044313201904295\n",
      "Epoch 92 \t Batch 120 \t Training Loss: 45.49509830474854\n",
      "Epoch 92 \t Batch 140 \t Training Loss: 45.45214143480573\n",
      "Epoch 92 \t Batch 160 \t Training Loss: 45.345831108093265\n",
      "Epoch 92 \t Batch 180 \t Training Loss: 45.30829283396403\n",
      "Epoch 92 \t Batch 200 \t Training Loss: 45.32786144256592\n",
      "Epoch 92 \t Batch 220 \t Training Loss: 45.44085429798473\n",
      "Epoch 92 \t Batch 240 \t Training Loss: 45.394592142105104\n",
      "Epoch 92 \t Batch 260 \t Training Loss: 45.42402527148907\n",
      "Epoch 92 \t Batch 280 \t Training Loss: 45.36426842553275\n",
      "Epoch 92 \t Batch 300 \t Training Loss: 45.43316232045492\n",
      "Epoch 92 \t Batch 320 \t Training Loss: 45.384948062896726\n",
      "Epoch 92 \t Batch 340 \t Training Loss: 45.471378326416016\n",
      "Epoch 92 \t Batch 360 \t Training Loss: 45.50766667260064\n",
      "Epoch 92 \t Batch 380 \t Training Loss: 45.48697141346179\n",
      "Epoch 92 \t Batch 400 \t Training Loss: 45.52224245071411\n",
      "Epoch 92 \t Batch 420 \t Training Loss: 45.506997889564154\n",
      "Epoch 92 \t Batch 440 \t Training Loss: 45.51960266286677\n",
      "Epoch 92 \t Batch 460 \t Training Loss: 45.517677273957624\n",
      "Epoch 92 \t Batch 480 \t Training Loss: 45.51922729810079\n",
      "Epoch 92 \t Batch 500 \t Training Loss: 45.55582145690918\n",
      "Epoch 92 \t Batch 520 \t Training Loss: 45.466449488126315\n",
      "Epoch 92 \t Batch 540 \t Training Loss: 45.46445364069056\n",
      "Epoch 92 \t Batch 560 \t Training Loss: 45.40907004901341\n",
      "Epoch 92 \t Batch 580 \t Training Loss: 45.444228869471054\n",
      "Epoch 92 \t Batch 600 \t Training Loss: 45.4492720413208\n",
      "Epoch 92 \t Batch 620 \t Training Loss: 45.470400194967944\n",
      "Epoch 92 \t Batch 640 \t Training Loss: 45.45697036981583\n",
      "Epoch 92 \t Batch 660 \t Training Loss: 45.43159948406797\n",
      "Epoch 92 \t Batch 680 \t Training Loss: 45.45073040793924\n",
      "Epoch 92 \t Batch 700 \t Training Loss: 45.44714048113142\n",
      "Epoch 92 \t Batch 720 \t Training Loss: 45.445361937416926\n",
      "Epoch 92 \t Batch 740 \t Training Loss: 45.47842072667302\n",
      "Epoch 92 \t Batch 760 \t Training Loss: 45.48118593316329\n",
      "Epoch 92 \t Batch 780 \t Training Loss: 45.49292017129751\n",
      "Epoch 92 \t Batch 800 \t Training Loss: 45.50186253547668\n",
      "Epoch 92 \t Batch 820 \t Training Loss: 45.52660021898223\n",
      "Epoch 92 \t Batch 840 \t Training Loss: 45.509156681242445\n",
      "Epoch 92 \t Batch 860 \t Training Loss: 45.53021220717319\n",
      "Epoch 92 \t Batch 880 \t Training Loss: 45.53321296951987\n",
      "Epoch 92 \t Batch 900 \t Training Loss: 45.54098345438639\n",
      "Epoch 92 \t Batch 20 \t Validation Loss: 19.450779533386232\n",
      "Epoch 92 \t Batch 40 \t Validation Loss: 21.132974004745485\n",
      "Epoch 92 \t Batch 60 \t Validation Loss: 20.785579125086468\n",
      "Epoch 92 \t Batch 80 \t Validation Loss: 21.24469678401947\n",
      "Epoch 92 \t Batch 100 \t Validation Loss: 22.93930118560791\n",
      "Epoch 92 \t Batch 120 \t Validation Loss: 24.27617745399475\n",
      "Epoch 92 \t Batch 140 \t Validation Loss: 24.994787079947336\n",
      "Epoch 92 \t Batch 160 \t Validation Loss: 27.037239289283754\n",
      "Epoch 92 \t Batch 180 \t Validation Loss: 30.92184082137214\n",
      "Epoch 92 \t Batch 200 \t Validation Loss: 32.48767131328583\n",
      "Epoch 92 \t Batch 220 \t Validation Loss: 33.73412107554349\n",
      "Epoch 92 \t Batch 240 \t Validation Loss: 34.26456983884176\n",
      "Epoch 92 \t Batch 260 \t Validation Loss: 36.37786414806659\n",
      "Epoch 92 \t Batch 280 \t Validation Loss: 37.538097170421054\n",
      "Epoch 92 \t Batch 300 \t Validation Loss: 38.79105148951213\n",
      "Epoch 92 \t Batch 320 \t Validation Loss: 39.32701670825482\n",
      "Epoch 92 \t Batch 340 \t Validation Loss: 39.277245378494264\n",
      "Epoch 92 \t Batch 360 \t Validation Loss: 39.15343013074663\n",
      "Epoch 92 \t Batch 380 \t Validation Loss: 39.38102028746354\n",
      "Epoch 92 \t Batch 400 \t Validation Loss: 38.998764855861666\n",
      "Epoch 92 \t Batch 420 \t Validation Loss: 38.98066226187206\n",
      "Epoch 92 \t Batch 440 \t Validation Loss: 38.67750989957289\n",
      "Epoch 92 \t Batch 460 \t Validation Loss: 38.94915282829948\n",
      "Epoch 92 \t Batch 480 \t Validation Loss: 39.42247211734454\n",
      "Epoch 92 \t Batch 500 \t Validation Loss: 39.129109869003294\n",
      "Epoch 92 \t Batch 520 \t Validation Loss: 38.94512744500087\n",
      "Epoch 92 \t Batch 540 \t Validation Loss: 38.741596887729784\n",
      "Epoch 92 \t Batch 560 \t Validation Loss: 38.56188054255077\n",
      "Epoch 92 \t Batch 580 \t Validation Loss: 38.37210287390084\n",
      "Epoch 92 \t Batch 600 \t Validation Loss: 38.600466701189674\n",
      "Epoch 92 Training Loss: 45.53619298238141 Validation Loss: 39.2913647208895\n",
      "Epoch 92 completed\n",
      "Epoch 93 \t Batch 20 \t Training Loss: 45.018510055541995\n",
      "Epoch 93 \t Batch 40 \t Training Loss: 45.35976457595825\n",
      "Epoch 93 \t Batch 60 \t Training Loss: 45.21757221221924\n",
      "Epoch 93 \t Batch 80 \t Training Loss: 45.21620502471924\n",
      "Epoch 93 \t Batch 100 \t Training Loss: 45.463347358703615\n",
      "Epoch 93 \t Batch 120 \t Training Loss: 45.5882851600647\n",
      "Epoch 93 \t Batch 140 \t Training Loss: 45.364033045087545\n",
      "Epoch 93 \t Batch 160 \t Training Loss: 45.2395254611969\n",
      "Epoch 93 \t Batch 180 \t Training Loss: 45.28494926028782\n",
      "Epoch 93 \t Batch 200 \t Training Loss: 45.14325511932373\n",
      "Epoch 93 \t Batch 220 \t Training Loss: 45.15956625504927\n",
      "Epoch 93 \t Batch 240 \t Training Loss: 45.172517935434975\n",
      "Epoch 93 \t Batch 260 \t Training Loss: 45.25329513549805\n",
      "Epoch 93 \t Batch 280 \t Training Loss: 45.27341387612479\n",
      "Epoch 93 \t Batch 300 \t Training Loss: 45.27354911804199\n",
      "Epoch 93 \t Batch 320 \t Training Loss: 45.29787976741791\n",
      "Epoch 93 \t Batch 340 \t Training Loss: 45.361458901798024\n",
      "Epoch 93 \t Batch 360 \t Training Loss: 45.44245055516561\n",
      "Epoch 93 \t Batch 380 \t Training Loss: 45.36397910871004\n",
      "Epoch 93 \t Batch 400 \t Training Loss: 45.397386121749875\n",
      "Epoch 93 \t Batch 420 \t Training Loss: 45.458725983755926\n",
      "Epoch 93 \t Batch 440 \t Training Loss: 45.44576705585826\n",
      "Epoch 93 \t Batch 460 \t Training Loss: 45.53808082912279\n",
      "Epoch 93 \t Batch 480 \t Training Loss: 45.52040471235911\n",
      "Epoch 93 \t Batch 500 \t Training Loss: 45.49951309204101\n",
      "Epoch 93 \t Batch 520 \t Training Loss: 45.44481823260968\n",
      "Epoch 93 \t Batch 540 \t Training Loss: 45.453194074277526\n",
      "Epoch 93 \t Batch 560 \t Training Loss: 45.4709491457258\n",
      "Epoch 93 \t Batch 580 \t Training Loss: 45.47956790266366\n",
      "Epoch 93 \t Batch 600 \t Training Loss: 45.40883421579997\n",
      "Epoch 93 \t Batch 620 \t Training Loss: 45.38229145542268\n",
      "Epoch 93 \t Batch 640 \t Training Loss: 45.349940770864485\n",
      "Epoch 93 \t Batch 660 \t Training Loss: 45.31627632487904\n",
      "Epoch 93 \t Batch 680 \t Training Loss: 45.31007679770975\n",
      "Epoch 93 \t Batch 700 \t Training Loss: 45.36787229810442\n",
      "Epoch 93 \t Batch 720 \t Training Loss: 45.413230917188855\n",
      "Epoch 93 \t Batch 740 \t Training Loss: 45.462269401550294\n",
      "Epoch 93 \t Batch 760 \t Training Loss: 45.46740188598633\n",
      "Epoch 93 \t Batch 780 \t Training Loss: 45.46052886278201\n",
      "Epoch 93 \t Batch 800 \t Training Loss: 45.46599604129791\n",
      "Epoch 93 \t Batch 820 \t Training Loss: 45.457932937435984\n",
      "Epoch 93 \t Batch 840 \t Training Loss: 45.46135733468192\n",
      "Epoch 93 \t Batch 860 \t Training Loss: 45.446500831426576\n",
      "Epoch 93 \t Batch 880 \t Training Loss: 45.45740532875061\n",
      "Epoch 93 \t Batch 900 \t Training Loss: 45.477395384046766\n",
      "Epoch 93 \t Batch 20 \t Validation Loss: 16.577826690673827\n",
      "Epoch 93 \t Batch 40 \t Validation Loss: 18.30140039920807\n",
      "Epoch 93 \t Batch 60 \t Validation Loss: 18.335973596572877\n",
      "Epoch 93 \t Batch 80 \t Validation Loss: 19.138326573371888\n",
      "Epoch 93 \t Batch 100 \t Validation Loss: 21.15421188354492\n",
      "Epoch 93 \t Batch 120 \t Validation Loss: 22.60129455725352\n",
      "Epoch 93 \t Batch 140 \t Validation Loss: 23.41893192699977\n",
      "Epoch 93 \t Batch 160 \t Validation Loss: 25.609045451879503\n",
      "Epoch 93 \t Batch 180 \t Validation Loss: 29.535808346006604\n",
      "Epoch 93 \t Batch 200 \t Validation Loss: 31.216110014915465\n",
      "Epoch 93 \t Batch 220 \t Validation Loss: 32.545579056306316\n",
      "Epoch 93 \t Batch 240 \t Validation Loss: 33.13326816956202\n",
      "Epoch 93 \t Batch 260 \t Validation Loss: 35.33126386128939\n",
      "Epoch 93 \t Batch 280 \t Validation Loss: 36.517748733929224\n",
      "Epoch 93 \t Batch 300 \t Validation Loss: 37.78722544670105\n",
      "Epoch 93 \t Batch 320 \t Validation Loss: 38.35022920370102\n",
      "Epoch 93 \t Batch 340 \t Validation Loss: 38.349927414164824\n",
      "Epoch 93 \t Batch 360 \t Validation Loss: 38.24584992196825\n",
      "Epoch 93 \t Batch 380 \t Validation Loss: 38.51703538894653\n",
      "Epoch 93 \t Batch 400 \t Validation Loss: 38.15003995418549\n",
      "Epoch 93 \t Batch 420 \t Validation Loss: 38.20011406853085\n",
      "Epoch 93 \t Batch 440 \t Validation Loss: 37.934185734662144\n",
      "Epoch 93 \t Batch 460 \t Validation Loss: 38.31133422436921\n",
      "Epoch 93 \t Batch 480 \t Validation Loss: 38.83567987283071\n",
      "Epoch 93 \t Batch 500 \t Validation Loss: 38.604064929962156\n",
      "Epoch 93 \t Batch 520 \t Validation Loss: 38.4733588548807\n",
      "Epoch 93 \t Batch 540 \t Validation Loss: 38.2503638126232\n",
      "Epoch 93 \t Batch 560 \t Validation Loss: 38.073913884162906\n",
      "Epoch 93 \t Batch 580 \t Validation Loss: 37.871516638788684\n",
      "Epoch 93 \t Batch 600 \t Validation Loss: 38.1107107702891\n",
      "Epoch 93 Training Loss: 45.49421305432834 Validation Loss: 38.79421827700231\n",
      "Epoch 93 completed\n",
      "Epoch 94 \t Batch 20 \t Training Loss: 46.132891464233396\n",
      "Epoch 94 \t Batch 40 \t Training Loss: 45.70885791778564\n",
      "Epoch 94 \t Batch 60 \t Training Loss: 45.66023298899333\n",
      "Epoch 94 \t Batch 80 \t Training Loss: 45.84678916931152\n",
      "Epoch 94 \t Batch 100 \t Training Loss: 45.73708599090576\n",
      "Epoch 94 \t Batch 120 \t Training Loss: 45.764967600504555\n",
      "Epoch 94 \t Batch 140 \t Training Loss: 45.747864205496654\n",
      "Epoch 94 \t Batch 160 \t Training Loss: 45.66119377613067\n",
      "Epoch 94 \t Batch 180 \t Training Loss: 45.775464905632866\n",
      "Epoch 94 \t Batch 200 \t Training Loss: 45.65703071594238\n",
      "Epoch 94 \t Batch 220 \t Training Loss: 45.65364322662354\n",
      "Epoch 94 \t Batch 240 \t Training Loss: 45.67160520553589\n",
      "Epoch 94 \t Batch 260 \t Training Loss: 45.62044908083402\n",
      "Epoch 94 \t Batch 280 \t Training Loss: 45.49544497898647\n",
      "Epoch 94 \t Batch 300 \t Training Loss: 45.56657543182373\n",
      "Epoch 94 \t Batch 320 \t Training Loss: 45.59124571084976\n",
      "Epoch 94 \t Batch 340 \t Training Loss: 45.525420200123506\n",
      "Epoch 94 \t Batch 360 \t Training Loss: 45.44126779768202\n",
      "Epoch 94 \t Batch 380 \t Training Loss: 45.44912815093994\n",
      "Epoch 94 \t Batch 400 \t Training Loss: 45.42920742034912\n",
      "Epoch 94 \t Batch 420 \t Training Loss: 45.50212057204474\n",
      "Epoch 94 \t Batch 440 \t Training Loss: 45.51543043309992\n",
      "Epoch 94 \t Batch 460 \t Training Loss: 45.53126569001571\n",
      "Epoch 94 \t Batch 480 \t Training Loss: 45.449526437123616\n",
      "Epoch 94 \t Batch 500 \t Training Loss: 45.41175093841553\n",
      "Epoch 94 \t Batch 520 \t Training Loss: 45.3869889992934\n",
      "Epoch 94 \t Batch 540 \t Training Loss: 45.41916980743408\n",
      "Epoch 94 \t Batch 560 \t Training Loss: 45.46576176370893\n",
      "Epoch 94 \t Batch 580 \t Training Loss: 45.443411636352536\n",
      "Epoch 94 \t Batch 600 \t Training Loss: 45.426130835215254\n",
      "Epoch 94 \t Batch 620 \t Training Loss: 45.42019986183413\n",
      "Epoch 94 \t Batch 640 \t Training Loss: 45.409251815080644\n",
      "Epoch 94 \t Batch 660 \t Training Loss: 45.446689461216785\n",
      "Epoch 94 \t Batch 680 \t Training Loss: 45.46212179520551\n",
      "Epoch 94 \t Batch 700 \t Training Loss: 45.43830241612026\n",
      "Epoch 94 \t Batch 720 \t Training Loss: 45.450551732381186\n",
      "Epoch 94 \t Batch 740 \t Training Loss: 45.44172788568445\n",
      "Epoch 94 \t Batch 760 \t Training Loss: 45.463417429673044\n",
      "Epoch 94 \t Batch 780 \t Training Loss: 45.46334111629388\n",
      "Epoch 94 \t Batch 800 \t Training Loss: 45.46565841674805\n",
      "Epoch 94 \t Batch 820 \t Training Loss: 45.468182210224434\n",
      "Epoch 94 \t Batch 840 \t Training Loss: 45.49978331157139\n",
      "Epoch 94 \t Batch 860 \t Training Loss: 45.49778298666311\n",
      "Epoch 94 \t Batch 880 \t Training Loss: 45.47674491188743\n",
      "Epoch 94 \t Batch 900 \t Training Loss: 45.51081630706787\n",
      "Epoch 94 \t Batch 20 \t Validation Loss: 18.502012634277342\n",
      "Epoch 94 \t Batch 40 \t Validation Loss: 20.383555316925047\n",
      "Epoch 94 \t Batch 60 \t Validation Loss: 20.03500065803528\n",
      "Epoch 94 \t Batch 80 \t Validation Loss: 21.065187907218935\n",
      "Epoch 94 \t Batch 100 \t Validation Loss: 23.03994342803955\n",
      "Epoch 94 \t Batch 120 \t Validation Loss: 24.41765371958415\n",
      "Epoch 94 \t Batch 140 \t Validation Loss: 25.130398178100585\n",
      "Epoch 94 \t Batch 160 \t Validation Loss: 27.27551885843277\n",
      "Epoch 94 \t Batch 180 \t Validation Loss: 31.07911574045817\n",
      "Epoch 94 \t Batch 200 \t Validation Loss: 32.619626684188844\n",
      "Epoch 94 \t Batch 220 \t Validation Loss: 33.9333009633151\n",
      "Epoch 94 \t Batch 240 \t Validation Loss: 34.4904500246048\n",
      "Epoch 94 \t Batch 260 \t Validation Loss: 36.602411754314716\n",
      "Epoch 94 \t Batch 280 \t Validation Loss: 37.729980383600505\n",
      "Epoch 94 \t Batch 300 \t Validation Loss: 38.966076644261676\n",
      "Epoch 94 \t Batch 320 \t Validation Loss: 39.49924187958241\n",
      "Epoch 94 \t Batch 340 \t Validation Loss: 39.47049450313344\n",
      "Epoch 94 \t Batch 360 \t Validation Loss: 39.381510869661966\n",
      "Epoch 94 \t Batch 380 \t Validation Loss: 39.644067796907926\n",
      "Epoch 94 \t Batch 400 \t Validation Loss: 39.297197296619416\n",
      "Epoch 94 \t Batch 420 \t Validation Loss: 39.30663134029933\n",
      "Epoch 94 \t Batch 440 \t Validation Loss: 39.0676763642918\n",
      "Epoch 94 \t Batch 460 \t Validation Loss: 39.3913585725038\n",
      "Epoch 94 \t Batch 480 \t Validation Loss: 39.87889185945193\n",
      "Epoch 94 \t Batch 500 \t Validation Loss: 39.61798481559753\n",
      "Epoch 94 \t Batch 520 \t Validation Loss: 39.5351888858355\n",
      "Epoch 94 \t Batch 540 \t Validation Loss: 39.37492296607406\n",
      "Epoch 94 \t Batch 560 \t Validation Loss: 39.28002536807742\n",
      "Epoch 94 \t Batch 580 \t Validation Loss: 39.15127961553376\n",
      "Epoch 94 \t Batch 600 \t Validation Loss: 39.4188173977534\n",
      "Epoch 94 Training Loss: 45.512146678329685 Validation Loss: 40.12632512581813\n",
      "Epoch 94 completed\n",
      "Epoch 95 \t Batch 20 \t Training Loss: 44.32570304870605\n",
      "Epoch 95 \t Batch 40 \t Training Loss: 44.88257884979248\n",
      "Epoch 95 \t Batch 60 \t Training Loss: 44.753885459899905\n",
      "Epoch 95 \t Batch 80 \t Training Loss: 44.971109580993655\n",
      "Epoch 95 \t Batch 100 \t Training Loss: 44.91152374267578\n",
      "Epoch 95 \t Batch 120 \t Training Loss: 44.82989495595296\n",
      "Epoch 95 \t Batch 140 \t Training Loss: 45.054328319004604\n",
      "Epoch 95 \t Batch 160 \t Training Loss: 45.030503678321836\n",
      "Epoch 95 \t Batch 180 \t Training Loss: 45.17298927307129\n",
      "Epoch 95 \t Batch 200 \t Training Loss: 45.14982088088989\n",
      "Epoch 95 \t Batch 220 \t Training Loss: 45.01674478704279\n",
      "Epoch 95 \t Batch 240 \t Training Loss: 45.068212588628136\n",
      "Epoch 95 \t Batch 260 \t Training Loss: 45.1059418458205\n",
      "Epoch 95 \t Batch 280 \t Training Loss: 45.160825851985386\n",
      "Epoch 95 \t Batch 300 \t Training Loss: 45.14570194244385\n",
      "Epoch 95 \t Batch 320 \t Training Loss: 45.15058917999268\n",
      "Epoch 95 \t Batch 340 \t Training Loss: 45.144897629232965\n",
      "Epoch 95 \t Batch 360 \t Training Loss: 45.174483638339574\n",
      "Epoch 95 \t Batch 380 \t Training Loss: 45.15430314917313\n",
      "Epoch 95 \t Batch 400 \t Training Loss: 45.24241150856018\n",
      "Epoch 95 \t Batch 420 \t Training Loss: 45.27582835242862\n",
      "Epoch 95 \t Batch 440 \t Training Loss: 45.20661718195135\n",
      "Epoch 95 \t Batch 460 \t Training Loss: 45.210803570954695\n",
      "Epoch 95 \t Batch 480 \t Training Loss: 45.27050297260284\n",
      "Epoch 95 \t Batch 500 \t Training Loss: 45.29483815765381\n",
      "Epoch 95 \t Batch 520 \t Training Loss: 45.33970061815702\n",
      "Epoch 95 \t Batch 540 \t Training Loss: 45.352868984363695\n",
      "Epoch 95 \t Batch 560 \t Training Loss: 45.33783460344587\n",
      "Epoch 95 \t Batch 580 \t Training Loss: 45.345422244894095\n",
      "Epoch 95 \t Batch 600 \t Training Loss: 45.32527523676554\n",
      "Epoch 95 \t Batch 620 \t Training Loss: 45.32423536854406\n",
      "Epoch 95 \t Batch 640 \t Training Loss: 45.33016214370728\n",
      "Epoch 95 \t Batch 660 \t Training Loss: 45.3967523690426\n",
      "Epoch 95 \t Batch 680 \t Training Loss: 45.352895416932945\n",
      "Epoch 95 \t Batch 700 \t Training Loss: 45.371465339660645\n",
      "Epoch 95 \t Batch 720 \t Training Loss: 45.34218307071262\n",
      "Epoch 95 \t Batch 740 \t Training Loss: 45.36739043158454\n",
      "Epoch 95 \t Batch 760 \t Training Loss: 45.394885374370375\n",
      "Epoch 95 \t Batch 780 \t Training Loss: 45.40818096063076\n",
      "Epoch 95 \t Batch 800 \t Training Loss: 45.39018371105194\n",
      "Epoch 95 \t Batch 820 \t Training Loss: 45.450119362807854\n",
      "Epoch 95 \t Batch 840 \t Training Loss: 45.4810228120713\n",
      "Epoch 95 \t Batch 860 \t Training Loss: 45.48974742445835\n",
      "Epoch 95 \t Batch 880 \t Training Loss: 45.48231254924428\n",
      "Epoch 95 \t Batch 900 \t Training Loss: 45.49483922746447\n",
      "Epoch 95 \t Batch 20 \t Validation Loss: 14.297022438049316\n",
      "Epoch 95 \t Batch 40 \t Validation Loss: 15.900587224960328\n",
      "Epoch 95 \t Batch 60 \t Validation Loss: 16.02248231569926\n",
      "Epoch 95 \t Batch 80 \t Validation Loss: 16.865116703510285\n",
      "Epoch 95 \t Batch 100 \t Validation Loss: 18.996319627761842\n",
      "Epoch 95 \t Batch 120 \t Validation Loss: 20.714546704292296\n",
      "Epoch 95 \t Batch 140 \t Validation Loss: 21.773823486055647\n",
      "Epoch 95 \t Batch 160 \t Validation Loss: 24.258455330133437\n",
      "Epoch 95 \t Batch 180 \t Validation Loss: 28.340892632802326\n",
      "Epoch 95 \t Batch 200 \t Validation Loss: 30.096688375473022\n",
      "Epoch 95 \t Batch 220 \t Validation Loss: 31.64304842515425\n",
      "Epoch 95 \t Batch 240 \t Validation Loss: 32.35463214317958\n",
      "Epoch 95 \t Batch 260 \t Validation Loss: 34.63398298483629\n",
      "Epoch 95 \t Batch 280 \t Validation Loss: 35.921232547078816\n",
      "Epoch 95 \t Batch 300 \t Validation Loss: 37.23921403566996\n",
      "Epoch 95 \t Batch 320 \t Validation Loss: 37.87965634167195\n",
      "Epoch 95 \t Batch 340 \t Validation Loss: 37.92707513640909\n",
      "Epoch 95 \t Batch 360 \t Validation Loss: 37.90258613957299\n",
      "Epoch 95 \t Batch 380 \t Validation Loss: 38.22063375523216\n",
      "Epoch 95 \t Batch 400 \t Validation Loss: 37.88428116559982\n",
      "Epoch 95 \t Batch 420 \t Validation Loss: 37.932514297394526\n",
      "Epoch 95 \t Batch 440 \t Validation Loss: 37.67421699220484\n",
      "Epoch 95 \t Batch 460 \t Validation Loss: 37.99990755371425\n",
      "Epoch 95 \t Batch 480 \t Validation Loss: 38.553493609031044\n",
      "Epoch 95 \t Batch 500 \t Validation Loss: 38.33748731803894\n",
      "Epoch 95 \t Batch 520 \t Validation Loss: 38.17640514923976\n",
      "Epoch 95 \t Batch 540 \t Validation Loss: 37.966502530486494\n",
      "Epoch 95 \t Batch 560 \t Validation Loss: 37.78990873779569\n",
      "Epoch 95 \t Batch 580 \t Validation Loss: 37.55879800237458\n",
      "Epoch 95 \t Batch 600 \t Validation Loss: 37.81290060202281\n",
      "Epoch 95 Training Loss: 45.46428022311844 Validation Loss: 38.503167166338336\n",
      "Epoch 95 completed\n",
      "Epoch 96 \t Batch 20 \t Training Loss: 44.93174819946289\n",
      "Epoch 96 \t Batch 40 \t Training Loss: 45.159930038452146\n",
      "Epoch 96 \t Batch 60 \t Training Loss: 45.528279050191244\n",
      "Epoch 96 \t Batch 80 \t Training Loss: 45.475528192520144\n",
      "Epoch 96 \t Batch 100 \t Training Loss: 45.48362075805664\n",
      "Epoch 96 \t Batch 120 \t Training Loss: 45.459729480743405\n",
      "Epoch 96 \t Batch 140 \t Training Loss: 45.412496430533274\n",
      "Epoch 96 \t Batch 160 \t Training Loss: 45.227100706100465\n",
      "Epoch 96 \t Batch 180 \t Training Loss: 45.27911656697591\n",
      "Epoch 96 \t Batch 200 \t Training Loss: 45.30584926605225\n",
      "Epoch 96 \t Batch 220 \t Training Loss: 45.34407891360196\n",
      "Epoch 96 \t Batch 240 \t Training Loss: 45.457589864730835\n",
      "Epoch 96 \t Batch 260 \t Training Loss: 45.465970934354345\n",
      "Epoch 96 \t Batch 280 \t Training Loss: 45.45547387259347\n",
      "Epoch 96 \t Batch 300 \t Training Loss: 45.47967399597168\n",
      "Epoch 96 \t Batch 320 \t Training Loss: 45.389621591567995\n",
      "Epoch 96 \t Batch 340 \t Training Loss: 45.42273194930133\n",
      "Epoch 96 \t Batch 360 \t Training Loss: 45.459143829345706\n",
      "Epoch 96 \t Batch 380 \t Training Loss: 45.4884760806435\n",
      "Epoch 96 \t Batch 400 \t Training Loss: 45.51267256736755\n",
      "Epoch 96 \t Batch 420 \t Training Loss: 45.461492729187015\n",
      "Epoch 96 \t Batch 440 \t Training Loss: 45.43716600591486\n",
      "Epoch 96 \t Batch 460 \t Training Loss: 45.42969520402991\n",
      "Epoch 96 \t Batch 480 \t Training Loss: 45.47760090827942\n",
      "Epoch 96 \t Batch 500 \t Training Loss: 45.466218437194826\n",
      "Epoch 96 \t Batch 520 \t Training Loss: 45.491499321277324\n",
      "Epoch 96 \t Batch 540 \t Training Loss: 45.524665959676106\n",
      "Epoch 96 \t Batch 560 \t Training Loss: 45.5283484186445\n",
      "Epoch 96 \t Batch 580 \t Training Loss: 45.501655473380254\n",
      "Epoch 96 \t Batch 600 \t Training Loss: 45.43034622192383\n",
      "Epoch 96 \t Batch 620 \t Training Loss: 45.43141494874031\n",
      "Epoch 96 \t Batch 640 \t Training Loss: 45.42524825930595\n",
      "Epoch 96 \t Batch 660 \t Training Loss: 45.415747399763625\n",
      "Epoch 96 \t Batch 680 \t Training Loss: 45.41542308470782\n",
      "Epoch 96 \t Batch 700 \t Training Loss: 45.43564507075718\n",
      "Epoch 96 \t Batch 720 \t Training Loss: 45.44378996425205\n",
      "Epoch 96 \t Batch 740 \t Training Loss: 45.45506218575142\n",
      "Epoch 96 \t Batch 760 \t Training Loss: 45.45109510923687\n",
      "Epoch 96 \t Batch 780 \t Training Loss: 45.46099151220077\n",
      "Epoch 96 \t Batch 800 \t Training Loss: 45.45315508365631\n",
      "Epoch 96 \t Batch 820 \t Training Loss: 45.44878371168927\n",
      "Epoch 96 \t Batch 840 \t Training Loss: 45.42664442062378\n",
      "Epoch 96 \t Batch 860 \t Training Loss: 45.445533002809036\n",
      "Epoch 96 \t Batch 880 \t Training Loss: 45.43398475646973\n",
      "Epoch 96 \t Batch 900 \t Training Loss: 45.423832444085015\n",
      "Epoch 96 \t Batch 20 \t Validation Loss: 22.584913539886475\n",
      "Epoch 96 \t Batch 40 \t Validation Loss: 24.2214848279953\n",
      "Epoch 96 \t Batch 60 \t Validation Loss: 23.90294426282247\n",
      "Epoch 96 \t Batch 80 \t Validation Loss: 24.41649661064148\n",
      "Epoch 96 \t Batch 100 \t Validation Loss: 25.536332092285157\n",
      "Epoch 96 \t Batch 120 \t Validation Loss: 26.554959948857626\n",
      "Epoch 96 \t Batch 140 \t Validation Loss: 26.96248162133353\n",
      "Epoch 96 \t Batch 160 \t Validation Loss: 28.79696943759918\n",
      "Epoch 96 \t Batch 180 \t Validation Loss: 32.27693929672241\n",
      "Epoch 96 \t Batch 200 \t Validation Loss: 33.62486825942993\n",
      "Epoch 96 \t Batch 220 \t Validation Loss: 34.73890020197088\n",
      "Epoch 96 \t Batch 240 \t Validation Loss: 35.13245258331299\n",
      "Epoch 96 \t Batch 260 \t Validation Loss: 37.13934075648968\n",
      "Epoch 96 \t Batch 280 \t Validation Loss: 38.220929397855485\n",
      "Epoch 96 \t Batch 300 \t Validation Loss: 39.311584803263344\n",
      "Epoch 96 \t Batch 320 \t Validation Loss: 39.780968913435935\n",
      "Epoch 96 \t Batch 340 \t Validation Loss: 39.700320813235116\n",
      "Epoch 96 \t Batch 360 \t Validation Loss: 39.550038615862526\n",
      "Epoch 96 \t Batch 380 \t Validation Loss: 39.76048126973604\n",
      "Epoch 96 \t Batch 400 \t Validation Loss: 39.361014378070834\n",
      "Epoch 96 \t Batch 420 \t Validation Loss: 39.33327303159805\n",
      "Epoch 96 \t Batch 440 \t Validation Loss: 39.037846220623365\n",
      "Epoch 96 \t Batch 460 \t Validation Loss: 39.340680203230484\n",
      "Epoch 96 \t Batch 480 \t Validation Loss: 39.824458529551826\n",
      "Epoch 96 \t Batch 500 \t Validation Loss: 39.55408678245544\n",
      "Epoch 96 \t Batch 520 \t Validation Loss: 39.39681576398703\n",
      "Epoch 96 \t Batch 540 \t Validation Loss: 39.16933799496403\n",
      "Epoch 96 \t Batch 560 \t Validation Loss: 38.99644117525646\n",
      "Epoch 96 \t Batch 580 \t Validation Loss: 38.780888384786145\n",
      "Epoch 96 \t Batch 600 \t Validation Loss: 38.99866685390472\n",
      "Epoch 96 Training Loss: 45.42446359033283 Validation Loss: 39.68615844807068\n",
      "Epoch 96 completed\n",
      "Epoch 97 \t Batch 20 \t Training Loss: 44.33877677917481\n",
      "Epoch 97 \t Batch 40 \t Training Loss: 45.12610311508179\n",
      "Epoch 97 \t Batch 60 \t Training Loss: 44.538926951090495\n",
      "Epoch 97 \t Batch 80 \t Training Loss: 45.23946657180786\n",
      "Epoch 97 \t Batch 100 \t Training Loss: 45.07101417541504\n",
      "Epoch 97 \t Batch 120 \t Training Loss: 45.23459952672322\n",
      "Epoch 97 \t Batch 140 \t Training Loss: 45.266232763017925\n",
      "Epoch 97 \t Batch 160 \t Training Loss: 45.280167722702025\n",
      "Epoch 97 \t Batch 180 \t Training Loss: 45.36321551005046\n",
      "Epoch 97 \t Batch 200 \t Training Loss: 45.275839500427246\n",
      "Epoch 97 \t Batch 220 \t Training Loss: 45.21367992054332\n",
      "Epoch 97 \t Batch 240 \t Training Loss: 45.15836653709412\n",
      "Epoch 97 \t Batch 260 \t Training Loss: 45.20396888439472\n",
      "Epoch 97 \t Batch 280 \t Training Loss: 45.2294324193682\n",
      "Epoch 97 \t Batch 300 \t Training Loss: 45.34178164164225\n",
      "Epoch 97 \t Batch 320 \t Training Loss: 45.31926646232605\n",
      "Epoch 97 \t Batch 340 \t Training Loss: 45.20244286481072\n",
      "Epoch 97 \t Batch 360 \t Training Loss: 45.211329778035484\n",
      "Epoch 97 \t Batch 380 \t Training Loss: 45.299185772946004\n",
      "Epoch 97 \t Batch 400 \t Training Loss: 45.26688020706177\n",
      "Epoch 97 \t Batch 420 \t Training Loss: 45.32120546613421\n",
      "Epoch 97 \t Batch 440 \t Training Loss: 45.357488198713824\n",
      "Epoch 97 \t Batch 460 \t Training Loss: 45.373387427951975\n",
      "Epoch 97 \t Batch 480 \t Training Loss: 45.34002078374227\n",
      "Epoch 97 \t Batch 500 \t Training Loss: 45.34104261016846\n",
      "Epoch 97 \t Batch 520 \t Training Loss: 45.37827072143555\n",
      "Epoch 97 \t Batch 540 \t Training Loss: 45.399063399985984\n",
      "Epoch 97 \t Batch 560 \t Training Loss: 45.424044779368806\n",
      "Epoch 97 \t Batch 580 \t Training Loss: 45.427660100213416\n",
      "Epoch 97 \t Batch 600 \t Training Loss: 45.40658582051595\n",
      "Epoch 97 \t Batch 620 \t Training Loss: 45.41154655333488\n",
      "Epoch 97 \t Batch 640 \t Training Loss: 45.41152885556221\n",
      "Epoch 97 \t Batch 660 \t Training Loss: 45.41849613767682\n",
      "Epoch 97 \t Batch 680 \t Training Loss: 45.398542544421026\n",
      "Epoch 97 \t Batch 700 \t Training Loss: 45.38701309749058\n",
      "Epoch 97 \t Batch 720 \t Training Loss: 45.364110204908584\n",
      "Epoch 97 \t Batch 740 \t Training Loss: 45.40374729053394\n",
      "Epoch 97 \t Batch 760 \t Training Loss: 45.437148059041874\n",
      "Epoch 97 \t Batch 780 \t Training Loss: 45.4757266215789\n",
      "Epoch 97 \t Batch 800 \t Training Loss: 45.45906830310822\n",
      "Epoch 97 \t Batch 820 \t Training Loss: 45.444501565142374\n",
      "Epoch 97 \t Batch 840 \t Training Loss: 45.45222503117153\n",
      "Epoch 97 \t Batch 860 \t Training Loss: 45.478304778697876\n",
      "Epoch 97 \t Batch 880 \t Training Loss: 45.48287120298906\n",
      "Epoch 97 \t Batch 900 \t Training Loss: 45.43755837334527\n",
      "Epoch 97 \t Batch 20 \t Validation Loss: 17.797243356704712\n",
      "Epoch 97 \t Batch 40 \t Validation Loss: 19.94359211921692\n",
      "Epoch 97 \t Batch 60 \t Validation Loss: 19.60562523206075\n",
      "Epoch 97 \t Batch 80 \t Validation Loss: 20.434200096130372\n",
      "Epoch 97 \t Batch 100 \t Validation Loss: 22.10488374710083\n",
      "Epoch 97 \t Batch 120 \t Validation Loss: 23.328765948613484\n",
      "Epoch 97 \t Batch 140 \t Validation Loss: 24.082487065451485\n",
      "Epoch 97 \t Batch 160 \t Validation Loss: 26.31617681980133\n",
      "Epoch 97 \t Batch 180 \t Validation Loss: 30.28668931855096\n",
      "Epoch 97 \t Batch 200 \t Validation Loss: 31.860527215003966\n",
      "Epoch 97 \t Batch 220 \t Validation Loss: 33.2230292970484\n",
      "Epoch 97 \t Batch 240 \t Validation Loss: 33.83143944342931\n",
      "Epoch 97 \t Batch 260 \t Validation Loss: 36.00869852946355\n",
      "Epoch 97 \t Batch 280 \t Validation Loss: 37.21113208362034\n",
      "Epoch 97 \t Batch 300 \t Validation Loss: 38.48753946304321\n",
      "Epoch 97 \t Batch 320 \t Validation Loss: 39.064585340023044\n",
      "Epoch 97 \t Batch 340 \t Validation Loss: 39.045684309566724\n",
      "Epoch 97 \t Batch 360 \t Validation Loss: 38.9876344203949\n",
      "Epoch 97 \t Batch 380 \t Validation Loss: 39.248898581454625\n",
      "Epoch 97 \t Batch 400 \t Validation Loss: 38.885245795249936\n",
      "Epoch 97 \t Batch 420 \t Validation Loss: 38.87739225115095\n",
      "Epoch 97 \t Batch 440 \t Validation Loss: 38.61292033628984\n",
      "Epoch 97 \t Batch 460 \t Validation Loss: 38.89325874577398\n",
      "Epoch 97 \t Batch 480 \t Validation Loss: 39.38646511634191\n",
      "Epoch 97 \t Batch 500 \t Validation Loss: 39.09829113006592\n",
      "Epoch 97 \t Batch 520 \t Validation Loss: 38.93679490823012\n",
      "Epoch 97 \t Batch 540 \t Validation Loss: 38.770037739365186\n",
      "Epoch 97 \t Batch 560 \t Validation Loss: 38.63104505879539\n",
      "Epoch 97 \t Batch 580 \t Validation Loss: 38.49959910162564\n",
      "Epoch 97 \t Batch 600 \t Validation Loss: 38.74351149559021\n",
      "Epoch 97 Training Loss: 45.415551941683404 Validation Loss: 39.44098085552067\n",
      "Epoch 97 completed\n",
      "Epoch 98 \t Batch 20 \t Training Loss: 45.96418418884277\n",
      "Epoch 98 \t Batch 40 \t Training Loss: 45.732877540588376\n",
      "Epoch 98 \t Batch 60 \t Training Loss: 45.449398803710935\n",
      "Epoch 98 \t Batch 80 \t Training Loss: 45.37627382278443\n",
      "Epoch 98 \t Batch 100 \t Training Loss: 45.430812492370606\n",
      "Epoch 98 \t Batch 120 \t Training Loss: 45.487403551737465\n",
      "Epoch 98 \t Batch 140 \t Training Loss: 45.283847781590055\n",
      "Epoch 98 \t Batch 160 \t Training Loss: 45.23357493877411\n",
      "Epoch 98 \t Batch 180 \t Training Loss: 45.35646673838298\n",
      "Epoch 98 \t Batch 200 \t Training Loss: 45.29731433868408\n",
      "Epoch 98 \t Batch 220 \t Training Loss: 45.383097700639205\n",
      "Epoch 98 \t Batch 240 \t Training Loss: 45.46067266464233\n",
      "Epoch 98 \t Batch 260 \t Training Loss: 45.451097752497745\n",
      "Epoch 98 \t Batch 280 \t Training Loss: 45.4652498790196\n",
      "Epoch 98 \t Batch 300 \t Training Loss: 45.49245454152425\n",
      "Epoch 98 \t Batch 320 \t Training Loss: 45.41381009817123\n",
      "Epoch 98 \t Batch 340 \t Training Loss: 45.464799555610206\n",
      "Epoch 98 \t Batch 360 \t Training Loss: 45.427597745259604\n",
      "Epoch 98 \t Batch 380 \t Training Loss: 45.49481936003033\n",
      "Epoch 98 \t Batch 400 \t Training Loss: 45.46155309677124\n",
      "Epoch 98 \t Batch 420 \t Training Loss: 45.47938045320057\n",
      "Epoch 98 \t Batch 440 \t Training Loss: 45.51129478107799\n",
      "Epoch 98 \t Batch 460 \t Training Loss: 45.482658377937646\n",
      "Epoch 98 \t Batch 480 \t Training Loss: 45.471096436182656\n",
      "Epoch 98 \t Batch 500 \t Training Loss: 45.442028419494626\n",
      "Epoch 98 \t Batch 520 \t Training Loss: 45.4116927587069\n",
      "Epoch 98 \t Batch 540 \t Training Loss: 45.462909401787655\n",
      "Epoch 98 \t Batch 560 \t Training Loss: 45.434362282071795\n",
      "Epoch 98 \t Batch 580 \t Training Loss: 45.40845993962781\n",
      "Epoch 98 \t Batch 600 \t Training Loss: 45.46940289815267\n",
      "Epoch 98 \t Batch 620 \t Training Loss: 45.47640533447266\n",
      "Epoch 98 \t Batch 640 \t Training Loss: 45.40329941511154\n",
      "Epoch 98 \t Batch 660 \t Training Loss: 45.43285013256651\n",
      "Epoch 98 \t Batch 680 \t Training Loss: 45.4026590571684\n",
      "Epoch 98 \t Batch 700 \t Training Loss: 45.37279537745884\n",
      "Epoch 98 \t Batch 720 \t Training Loss: 45.39323587417603\n",
      "Epoch 98 \t Batch 740 \t Training Loss: 45.40227959091599\n",
      "Epoch 98 \t Batch 760 \t Training Loss: 45.427041952233566\n",
      "Epoch 98 \t Batch 780 \t Training Loss: 45.42608150090927\n",
      "Epoch 98 \t Batch 800 \t Training Loss: 45.399156141281125\n",
      "Epoch 98 \t Batch 820 \t Training Loss: 45.38098493901695\n",
      "Epoch 98 \t Batch 840 \t Training Loss: 45.39102385384696\n",
      "Epoch 98 \t Batch 860 \t Training Loss: 45.38355790071709\n",
      "Epoch 98 \t Batch 880 \t Training Loss: 45.38019886450334\n",
      "Epoch 98 \t Batch 900 \t Training Loss: 45.389111760457354\n",
      "Epoch 98 \t Batch 20 \t Validation Loss: 18.685698318481446\n",
      "Epoch 98 \t Batch 40 \t Validation Loss: 19.744087862968446\n",
      "Epoch 98 \t Batch 60 \t Validation Loss: 19.70773809750875\n",
      "Epoch 98 \t Batch 80 \t Validation Loss: 20.27344833612442\n",
      "Epoch 98 \t Batch 100 \t Validation Loss: 21.93164059638977\n",
      "Epoch 98 \t Batch 120 \t Validation Loss: 23.382484102249144\n",
      "Epoch 98 \t Batch 140 \t Validation Loss: 24.053234563555037\n",
      "Epoch 98 \t Batch 160 \t Validation Loss: 26.169996857643127\n",
      "Epoch 98 \t Batch 180 \t Validation Loss: 30.069144105911256\n",
      "Epoch 98 \t Batch 200 \t Validation Loss: 31.686783099174498\n",
      "Epoch 98 \t Batch 220 \t Validation Loss: 33.021272732994774\n",
      "Epoch 98 \t Batch 240 \t Validation Loss: 33.58292632500331\n",
      "Epoch 98 \t Batch 260 \t Validation Loss: 35.76521147214449\n",
      "Epoch 98 \t Batch 280 \t Validation Loss: 36.96768414633615\n",
      "Epoch 98 \t Batch 300 \t Validation Loss: 38.21089095433553\n",
      "Epoch 98 \t Batch 320 \t Validation Loss: 38.76952206194401\n",
      "Epoch 98 \t Batch 340 \t Validation Loss: 38.75354128725388\n",
      "Epoch 98 \t Batch 360 \t Validation Loss: 38.62085656854841\n",
      "Epoch 98 \t Batch 380 \t Validation Loss: 38.858099001332334\n",
      "Epoch 98 \t Batch 400 \t Validation Loss: 38.49084922552109\n",
      "Epoch 98 \t Batch 420 \t Validation Loss: 38.563688216890604\n",
      "Epoch 98 \t Batch 440 \t Validation Loss: 38.317412638664244\n",
      "Epoch 98 \t Batch 460 \t Validation Loss: 38.61351321469183\n",
      "Epoch 98 \t Batch 480 \t Validation Loss: 39.12858367562294\n",
      "Epoch 98 \t Batch 500 \t Validation Loss: 38.88503618812561\n",
      "Epoch 98 \t Batch 520 \t Validation Loss: 38.666602008159344\n",
      "Epoch 98 \t Batch 540 \t Validation Loss: 38.366458110456115\n",
      "Epoch 98 \t Batch 560 \t Validation Loss: 38.12984128849847\n",
      "Epoch 98 \t Batch 580 \t Validation Loss: 37.79464310448745\n",
      "Epoch 98 \t Batch 600 \t Validation Loss: 38.00118233839671\n",
      "Epoch 98 Training Loss: 45.39516617453605 Validation Loss: 38.63543320476235\n",
      "Epoch 98 completed\n",
      "Epoch 99 \t Batch 20 \t Training Loss: 45.11396865844726\n",
      "Epoch 99 \t Batch 40 \t Training Loss: 45.176993465423585\n",
      "Epoch 99 \t Batch 60 \t Training Loss: 45.264308802286784\n",
      "Epoch 99 \t Batch 80 \t Training Loss: 45.44330105781555\n",
      "Epoch 99 \t Batch 100 \t Training Loss: 45.74763954162598\n",
      "Epoch 99 \t Batch 120 \t Training Loss: 45.6991348584493\n",
      "Epoch 99 \t Batch 140 \t Training Loss: 45.6866039276123\n",
      "Epoch 99 \t Batch 160 \t Training Loss: 45.714147782325746\n",
      "Epoch 99 \t Batch 180 \t Training Loss: 45.69516444736057\n",
      "Epoch 99 \t Batch 200 \t Training Loss: 45.71498107910156\n",
      "Epoch 99 \t Batch 220 \t Training Loss: 45.5552113793113\n",
      "Epoch 99 \t Batch 240 \t Training Loss: 45.57002158164978\n",
      "Epoch 99 \t Batch 260 \t Training Loss: 45.43930187225342\n",
      "Epoch 99 \t Batch 280 \t Training Loss: 45.45575299944196\n",
      "Epoch 99 \t Batch 300 \t Training Loss: 45.46855800628662\n",
      "Epoch 99 \t Batch 320 \t Training Loss: 45.4252214550972\n",
      "Epoch 99 \t Batch 340 \t Training Loss: 45.4638282214894\n",
      "Epoch 99 \t Batch 360 \t Training Loss: 45.45222680833604\n",
      "Epoch 99 \t Batch 380 \t Training Loss: 45.473967491953\n",
      "Epoch 99 \t Batch 400 \t Training Loss: 45.514893684387204\n",
      "Epoch 99 \t Batch 420 \t Training Loss: 45.42014272780646\n",
      "Epoch 99 \t Batch 440 \t Training Loss: 45.47580298510465\n",
      "Epoch 99 \t Batch 460 \t Training Loss: 45.4105849970942\n",
      "Epoch 99 \t Batch 480 \t Training Loss: 45.35112617015839\n",
      "Epoch 99 \t Batch 500 \t Training Loss: 45.35275131988526\n",
      "Epoch 99 \t Batch 520 \t Training Loss: 45.40094755612887\n",
      "Epoch 99 \t Batch 540 \t Training Loss: 45.40651671091715\n",
      "Epoch 99 \t Batch 560 \t Training Loss: 45.39983980996268\n",
      "Epoch 99 \t Batch 580 \t Training Loss: 45.40907297463253\n",
      "Epoch 99 \t Batch 600 \t Training Loss: 45.45670510609945\n",
      "Epoch 99 \t Batch 620 \t Training Loss: 45.416998038753384\n",
      "Epoch 99 \t Batch 640 \t Training Loss: 45.43659843206406\n",
      "Epoch 99 \t Batch 660 \t Training Loss: 45.452489968502164\n",
      "Epoch 99 \t Batch 680 \t Training Loss: 45.50641303903916\n",
      "Epoch 99 \t Batch 700 \t Training Loss: 45.434427860804966\n",
      "Epoch 99 \t Batch 720 \t Training Loss: 45.420914014180504\n",
      "Epoch 99 \t Batch 740 \t Training Loss: 45.41780776462039\n",
      "Epoch 99 \t Batch 760 \t Training Loss: 45.40848871532239\n",
      "Epoch 99 \t Batch 780 \t Training Loss: 45.359002533937115\n",
      "Epoch 99 \t Batch 800 \t Training Loss: 45.3679200220108\n",
      "Epoch 99 \t Batch 820 \t Training Loss: 45.305839026846535\n",
      "Epoch 99 \t Batch 840 \t Training Loss: 45.29389725185576\n",
      "Epoch 99 \t Batch 860 \t Training Loss: 45.28907222747803\n",
      "Epoch 99 \t Batch 880 \t Training Loss: 45.29506166631525\n",
      "Epoch 99 \t Batch 900 \t Training Loss: 45.31667402903239\n",
      "Epoch 99 \t Batch 20 \t Validation Loss: 14.838663339614868\n",
      "Epoch 99 \t Batch 40 \t Validation Loss: 16.84849610328674\n",
      "Epoch 99 \t Batch 60 \t Validation Loss: 16.731521209081013\n",
      "Epoch 99 \t Batch 80 \t Validation Loss: 17.727350091934206\n",
      "Epoch 99 \t Batch 100 \t Validation Loss: 19.884855651855467\n",
      "Epoch 99 \t Batch 120 \t Validation Loss: 21.7084100484848\n",
      "Epoch 99 \t Batch 140 \t Validation Loss: 22.79510942867824\n",
      "Epoch 99 \t Batch 160 \t Validation Loss: 25.33739053606987\n",
      "Epoch 99 \t Batch 180 \t Validation Loss: 29.61657264497545\n",
      "Epoch 99 \t Batch 200 \t Validation Loss: 31.48357731819153\n",
      "Epoch 99 \t Batch 220 \t Validation Loss: 32.99593854383989\n",
      "Epoch 99 \t Batch 240 \t Validation Loss: 33.76138490835826\n",
      "Epoch 99 \t Batch 260 \t Validation Loss: 36.05157122612\n",
      "Epoch 99 \t Batch 280 \t Validation Loss: 37.31705042294094\n",
      "Epoch 99 \t Batch 300 \t Validation Loss: 38.64695176442464\n",
      "Epoch 99 \t Batch 320 \t Validation Loss: 39.24206702113152\n",
      "Epoch 99 \t Batch 340 \t Validation Loss: 39.247477638020236\n",
      "Epoch 99 \t Batch 360 \t Validation Loss: 39.21809721522861\n",
      "Epoch 99 \t Batch 380 \t Validation Loss: 39.45315670214201\n",
      "Epoch 99 \t Batch 400 \t Validation Loss: 39.06680383682251\n",
      "Epoch 99 \t Batch 420 \t Validation Loss: 39.08802006131127\n",
      "Epoch 99 \t Batch 440 \t Validation Loss: 38.77128194028681\n",
      "Epoch 99 \t Batch 460 \t Validation Loss: 39.02498118773751\n",
      "Epoch 99 \t Batch 480 \t Validation Loss: 39.498647268613176\n",
      "Epoch 99 \t Batch 500 \t Validation Loss: 39.25066024017334\n",
      "Epoch 99 \t Batch 520 \t Validation Loss: 39.047392969865065\n",
      "Epoch 99 \t Batch 540 \t Validation Loss: 38.858897120864306\n",
      "Epoch 99 \t Batch 560 \t Validation Loss: 38.766557042939326\n",
      "Epoch 99 \t Batch 580 \t Validation Loss: 38.6128785560871\n",
      "Epoch 99 \t Batch 600 \t Validation Loss: 38.859541479746504\n",
      "Epoch 99 Training Loss: 45.35973621480728 Validation Loss: 39.551417601573\n",
      "Epoch 99 completed\n",
      "Epoch 100 \t Batch 20 \t Training Loss: 44.81452388763428\n",
      "Epoch 100 \t Batch 40 \t Training Loss: 45.43367176055908\n",
      "Epoch 100 \t Batch 60 \t Training Loss: 45.47930380503337\n",
      "Epoch 100 \t Batch 80 \t Training Loss: 45.28748841285706\n",
      "Epoch 100 \t Batch 100 \t Training Loss: 45.102560920715334\n",
      "Epoch 100 \t Batch 120 \t Training Loss: 45.20312576293945\n",
      "Epoch 100 \t Batch 140 \t Training Loss: 45.17543204171317\n",
      "Epoch 100 \t Batch 160 \t Training Loss: 45.24473886489868\n",
      "Epoch 100 \t Batch 180 \t Training Loss: 45.138334104749894\n",
      "Epoch 100 \t Batch 200 \t Training Loss: 45.100824203491214\n",
      "Epoch 100 \t Batch 220 \t Training Loss: 45.303048411282624\n",
      "Epoch 100 \t Batch 240 \t Training Loss: 45.27571598688761\n",
      "Epoch 100 \t Batch 260 \t Training Loss: 45.303281534635104\n",
      "Epoch 100 \t Batch 280 \t Training Loss: 45.36017144066947\n",
      "Epoch 100 \t Batch 300 \t Training Loss: 45.31464018503825\n",
      "Epoch 100 \t Batch 320 \t Training Loss: 45.3812029838562\n",
      "Epoch 100 \t Batch 340 \t Training Loss: 45.366002957961136\n",
      "Epoch 100 \t Batch 360 \t Training Loss: 45.406161562601724\n",
      "Epoch 100 \t Batch 380 \t Training Loss: 45.40650131827906\n",
      "Epoch 100 \t Batch 400 \t Training Loss: 45.40096429824829\n",
      "Epoch 100 \t Batch 420 \t Training Loss: 45.381333759852815\n",
      "Epoch 100 \t Batch 440 \t Training Loss: 45.395298671722415\n",
      "Epoch 100 \t Batch 460 \t Training Loss: 45.33345018469769\n",
      "Epoch 100 \t Batch 480 \t Training Loss: 45.37624614238739\n",
      "Epoch 100 \t Batch 500 \t Training Loss: 45.334620262146\n",
      "Epoch 100 \t Batch 520 \t Training Loss: 45.33167090782752\n",
      "Epoch 100 \t Batch 540 \t Training Loss: 45.34103507995606\n",
      "Epoch 100 \t Batch 560 \t Training Loss: 45.326522615977694\n",
      "Epoch 100 \t Batch 580 \t Training Loss: 45.304955949454474\n",
      "Epoch 100 \t Batch 600 \t Training Loss: 45.30785758972168\n",
      "Epoch 100 \t Batch 620 \t Training Loss: 45.318331373891525\n",
      "Epoch 100 \t Batch 640 \t Training Loss: 45.329917550086975\n",
      "Epoch 100 \t Batch 660 \t Training Loss: 45.31444259990345\n",
      "Epoch 100 \t Batch 680 \t Training Loss: 45.32748795677634\n",
      "Epoch 100 \t Batch 700 \t Training Loss: 45.30085076468332\n",
      "Epoch 100 \t Batch 720 \t Training Loss: 45.30599414507548\n",
      "Epoch 100 \t Batch 740 \t Training Loss: 45.324661321897764\n",
      "Epoch 100 \t Batch 760 \t Training Loss: 45.29697309795179\n",
      "Epoch 100 \t Batch 780 \t Training Loss: 45.27131050794552\n",
      "Epoch 100 \t Batch 800 \t Training Loss: 45.27288969516754\n",
      "Epoch 100 \t Batch 820 \t Training Loss: 45.25114800988174\n",
      "Epoch 100 \t Batch 840 \t Training Loss: 45.27358222234817\n",
      "Epoch 100 \t Batch 860 \t Training Loss: 45.314485070871754\n",
      "Epoch 100 \t Batch 880 \t Training Loss: 45.316604796322906\n",
      "Epoch 100 \t Batch 900 \t Training Loss: 45.31716094122993\n",
      "Epoch 100 \t Batch 20 \t Validation Loss: 12.765260648727416\n",
      "Epoch 100 \t Batch 40 \t Validation Loss: 15.971656823158265\n",
      "Epoch 100 \t Batch 60 \t Validation Loss: 15.691496006647746\n",
      "Epoch 100 \t Batch 80 \t Validation Loss: 16.704123699665068\n",
      "Epoch 100 \t Batch 100 \t Validation Loss: 19.08163010597229\n",
      "Epoch 100 \t Batch 120 \t Validation Loss: 20.86041378180186\n",
      "Epoch 100 \t Batch 140 \t Validation Loss: 22.025705657686505\n",
      "Epoch 100 \t Batch 160 \t Validation Loss: 24.635983604192734\n",
      "Epoch 100 \t Batch 180 \t Validation Loss: 28.849590031305947\n",
      "Epoch 100 \t Batch 200 \t Validation Loss: 30.70655424594879\n",
      "Epoch 100 \t Batch 220 \t Validation Loss: 32.216604540564795\n",
      "Epoch 100 \t Batch 240 \t Validation Loss: 32.943346639474235\n",
      "Epoch 100 \t Batch 260 \t Validation Loss: 35.26464644211989\n",
      "Epoch 100 \t Batch 280 \t Validation Loss: 36.544309210777286\n",
      "Epoch 100 \t Batch 300 \t Validation Loss: 37.92835594495138\n",
      "Epoch 100 \t Batch 320 \t Validation Loss: 38.578776648640634\n",
      "Epoch 100 \t Batch 340 \t Validation Loss: 38.609434259639066\n",
      "Epoch 100 \t Batch 360 \t Validation Loss: 38.58804609775543\n",
      "Epoch 100 \t Batch 380 \t Validation Loss: 38.90743462662948\n",
      "Epoch 100 \t Batch 400 \t Validation Loss: 38.573793127536774\n",
      "Epoch 100 \t Batch 420 \t Validation Loss: 38.60901573953174\n",
      "Epoch 100 \t Batch 440 \t Validation Loss: 38.356108385866335\n",
      "Epoch 100 \t Batch 460 \t Validation Loss: 38.7202880838643\n",
      "Epoch 100 \t Batch 480 \t Validation Loss: 39.24115874171257\n",
      "Epoch 100 \t Batch 500 \t Validation Loss: 38.997436361312865\n",
      "Epoch 100 \t Batch 520 \t Validation Loss: 38.90190118826353\n",
      "Epoch 100 \t Batch 540 \t Validation Loss: 38.7509228653378\n",
      "Epoch 100 \t Batch 560 \t Validation Loss: 38.60445252997535\n",
      "Epoch 100 \t Batch 580 \t Validation Loss: 38.49049097751749\n",
      "Epoch 100 \t Batch 600 \t Validation Loss: 38.74516950130462\n",
      "Epoch 100 Training Loss: 45.296406486952186 Validation Loss: 39.48174183554464\n",
      "Epoch 100 completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from dataloader import *\n",
    "\n",
    "model = SimpleFCN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "class RMSE(nn.Module):\n",
    "    \"\"\" \n",
    "        Weighted RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        self.mse = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    def __call__(self, prediction, target, weights = 1):\n",
    "        # prediction = prediction[:, 0]\n",
    "        return torch.sqrt(torch.mean(weights * self.mse(prediction,target)))\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.latlon = True\n",
    "        self.bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "        self.bm = True\n",
    "        self.patch_size = [15,15]\n",
    "        self.norm_strat = 'pct'\n",
    "        self.norm = False\n",
    "\n",
    "args = Args()\n",
    "fnames = ['data_nonan_0-5.h5', 'data_nonan_1-5.h5', 'data_nonan_2-5.h5', 'data_nonan_3-5.h5', 'data_nonan_4-5.h5']\n",
    "mode = 'train'\n",
    "ds_training = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "mode = 'val'\n",
    "ds_validation = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(\"inputs.shape: \", inputs.shape)\n",
    "        # print(\"targets.shape: \", targets.shape)\n",
    "        # # # print(outputs)\n",
    "        # print(\"outputs.shape: \", outputs.shape)\n",
    "        # loss1 = criterion(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    i=0\n",
    "    model.eval()\n",
    "    for inputs, targets in validloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        valid_loss += loss.item()\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    " \n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model2.pth')\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n",
      "Epoch 1 \t Batch 20 \t Training Loss: 109.57520904541016\n",
      "Epoch 1 \t Batch 40 \t Training Loss: 98.65093517303467\n",
      "Epoch 1 \t Batch 60 \t Training Loss: 87.56623617808025\n",
      "Epoch 1 \t Batch 80 \t Training Loss: 79.36099495887757\n",
      "Epoch 1 \t Batch 100 \t Training Loss: 74.23469905853271\n",
      "Epoch 1 \t Batch 120 \t Training Loss: 70.86601209640503\n",
      "Epoch 1 \t Batch 140 \t Training Loss: 68.3813807623727\n",
      "Epoch 1 \t Batch 160 \t Training Loss: 66.60697207450866\n",
      "Epoch 1 \t Batch 180 \t Training Loss: 65.04681466420492\n",
      "Epoch 1 \t Batch 200 \t Training Loss: 63.97086687088013\n",
      "Epoch 1 \t Batch 220 \t Training Loss: 63.11709398789839\n",
      "Epoch 1 \t Batch 240 \t Training Loss: 62.223071082433066\n",
      "Epoch 1 \t Batch 260 \t Training Loss: 61.63299022087684\n",
      "Epoch 1 \t Batch 280 \t Training Loss: 60.87944984436035\n",
      "Epoch 1 \t Batch 300 \t Training Loss: 60.303937962849936\n",
      "Epoch 1 \t Batch 320 \t Training Loss: 59.74504283666611\n",
      "Epoch 1 \t Batch 340 \t Training Loss: 59.41683048921473\n",
      "Epoch 1 \t Batch 360 \t Training Loss: 59.0484724468655\n",
      "Epoch 1 \t Batch 380 \t Training Loss: 58.61995270377711\n",
      "Epoch 1 \t Batch 400 \t Training Loss: 58.247486667633055\n",
      "Epoch 1 \t Batch 420 \t Training Loss: 57.84358367011661\n",
      "Epoch 1 \t Batch 440 \t Training Loss: 57.64971233714711\n",
      "Epoch 1 \t Batch 460 \t Training Loss: 57.42951106195864\n",
      "Epoch 1 \t Batch 480 \t Training Loss: 57.20460251967112\n",
      "Epoch 1 \t Batch 500 \t Training Loss: 57.003876945495605\n",
      "Epoch 1 \t Batch 520 \t Training Loss: 56.801849937438966\n",
      "Epoch 1 \t Batch 540 \t Training Loss: 56.598744540744356\n",
      "Epoch 1 \t Batch 560 \t Training Loss: 56.407788794381275\n",
      "Epoch 1 \t Batch 580 \t Training Loss: 56.19825177685968\n",
      "Epoch 1 \t Batch 600 \t Training Loss: 56.029460741678875\n",
      "Epoch 1 \t Batch 620 \t Training Loss: 55.834162416765764\n",
      "Epoch 1 \t Batch 640 \t Training Loss: 55.771957302093504\n",
      "Epoch 1 \t Batch 660 \t Training Loss: 55.6474941369259\n",
      "Epoch 1 \t Batch 680 \t Training Loss: 55.51822517058429\n",
      "Epoch 1 \t Batch 700 \t Training Loss: 55.43650466373989\n",
      "Epoch 1 \t Batch 720 \t Training Loss: 55.36187665197584\n",
      "Epoch 1 \t Batch 740 \t Training Loss: 55.19933681488037\n",
      "Epoch 1 \t Batch 760 \t Training Loss: 55.12880717829654\n",
      "Epoch 1 \t Batch 780 \t Training Loss: 55.04160992059952\n",
      "Epoch 1 \t Batch 800 \t Training Loss: 54.9794099521637\n",
      "Epoch 1 \t Batch 820 \t Training Loss: 54.92809210056212\n",
      "Epoch 1 \t Batch 840 \t Training Loss: 54.85022682916551\n",
      "Epoch 1 \t Batch 860 \t Training Loss: 54.75754444654598\n",
      "Epoch 1 \t Batch 880 \t Training Loss: 54.691535065390845\n",
      "Epoch 1 \t Batch 900 \t Training Loss: 54.61909531487359\n",
      "Epoch 1 \t Batch 20 \t Validation Loss: 18.061865234375\n",
      "Epoch 1 \t Batch 40 \t Validation Loss: 21.796929121017456\n",
      "Epoch 1 \t Batch 60 \t Validation Loss: 20.911891078948976\n",
      "Epoch 1 \t Batch 80 \t Validation Loss: 21.685929977893828\n",
      "Epoch 1 \t Batch 100 \t Validation Loss: 23.084184885025024\n",
      "Epoch 1 \t Batch 120 \t Validation Loss: 24.280981826782227\n",
      "Epoch 1 \t Batch 140 \t Validation Loss: 24.97482376098633\n",
      "Epoch 1 \t Batch 160 \t Validation Loss: 27.32661578655243\n",
      "Epoch 1 \t Batch 180 \t Validation Loss: 31.002242845959135\n",
      "Epoch 1 \t Batch 200 \t Validation Loss: 32.682817063331605\n",
      "Epoch 1 \t Batch 220 \t Validation Loss: 34.17231857559898\n",
      "Epoch 1 \t Batch 240 \t Validation Loss: 34.735348220666246\n",
      "Epoch 1 \t Batch 260 \t Validation Loss: 36.873254104760974\n",
      "Epoch 1 \t Batch 280 \t Validation Loss: 38.101703139713834\n",
      "Epoch 1 \t Batch 300 \t Validation Loss: 39.25011612574259\n",
      "Epoch 1 \t Batch 320 \t Validation Loss: 39.78404904007912\n",
      "Epoch 1 \t Batch 340 \t Validation Loss: 39.82742430743049\n",
      "Epoch 1 \t Batch 360 \t Validation Loss: 39.868631760279335\n",
      "Epoch 1 \t Batch 380 \t Validation Loss: 40.238424376437536\n",
      "Epoch 1 \t Batch 400 \t Validation Loss: 40.14512995243072\n",
      "Epoch 1 \t Batch 420 \t Validation Loss: 40.26741263979957\n",
      "Epoch 1 \t Batch 440 \t Validation Loss: 40.23571091565219\n",
      "Epoch 1 \t Batch 460 \t Validation Loss: 40.578593320431914\n",
      "Epoch 1 \t Batch 480 \t Validation Loss: 41.116674002011614\n",
      "Epoch 1 \t Batch 500 \t Validation Loss: 40.873489501953124\n",
      "Epoch 1 \t Batch 520 \t Validation Loss: 40.91490913537832\n",
      "Epoch 1 \t Batch 540 \t Validation Loss: 40.719004249572755\n",
      "Epoch 1 \t Batch 560 \t Validation Loss: 40.53223176343101\n",
      "Epoch 1 \t Batch 580 \t Validation Loss: 40.41516174119094\n",
      "Epoch 1 \t Batch 600 \t Validation Loss: 40.588112179438276\n",
      "Epoch 1 Training Loss: 54.51791303212406 Validation Loss: 41.26226294195497\n",
      "Validation Loss Decreased(inf--->25417.553972244263) Saving The Model\n",
      "Epoch 1 completed\n",
      "Epoch 2 \t Batch 20 \t Training Loss: 51.01539897918701\n",
      "Epoch 2 \t Batch 40 \t Training Loss: 50.55330514907837\n",
      "Epoch 2 \t Batch 60 \t Training Loss: 50.57772185007731\n",
      "Epoch 2 \t Batch 80 \t Training Loss: 50.76990280151367\n",
      "Epoch 2 \t Batch 100 \t Training Loss: 50.986930809021\n",
      "Epoch 2 \t Batch 120 \t Training Loss: 51.00486167271932\n",
      "Epoch 2 \t Batch 140 \t Training Loss: 51.06539944240025\n",
      "Epoch 2 \t Batch 160 \t Training Loss: 51.0060877084732\n",
      "Epoch 2 \t Batch 180 \t Training Loss: 50.94574794769287\n",
      "Epoch 2 \t Batch 200 \t Training Loss: 50.88050706863403\n",
      "Epoch 2 \t Batch 220 \t Training Loss: 50.901433459195225\n",
      "Epoch 2 \t Batch 240 \t Training Loss: 50.88896112442016\n",
      "Epoch 2 \t Batch 260 \t Training Loss: 50.98375354179969\n",
      "Epoch 2 \t Batch 280 \t Training Loss: 50.917529351370675\n",
      "Epoch 2 \t Batch 300 \t Training Loss: 50.9890798441569\n",
      "Epoch 2 \t Batch 320 \t Training Loss: 50.96009111404419\n",
      "Epoch 2 \t Batch 340 \t Training Loss: 50.879039472692156\n",
      "Epoch 2 \t Batch 360 \t Training Loss: 50.85832824707031\n",
      "Epoch 2 \t Batch 380 \t Training Loss: 50.834742977744654\n",
      "Epoch 2 \t Batch 400 \t Training Loss: 50.84794155120849\n",
      "Epoch 2 \t Batch 420 \t Training Loss: 50.83037015824091\n",
      "Epoch 2 \t Batch 440 \t Training Loss: 50.82929789803245\n",
      "Epoch 2 \t Batch 460 \t Training Loss: 50.82639649432638\n",
      "Epoch 2 \t Batch 480 \t Training Loss: 50.75446339448293\n",
      "Epoch 2 \t Batch 500 \t Training Loss: 50.827914215087894\n",
      "Epoch 2 \t Batch 520 \t Training Loss: 50.81452476794903\n",
      "Epoch 2 \t Batch 540 \t Training Loss: 50.85632203420003\n",
      "Epoch 2 \t Batch 560 \t Training Loss: 50.84048295702253\n",
      "Epoch 2 \t Batch 580 \t Training Loss: 50.81743258772225\n",
      "Epoch 2 \t Batch 600 \t Training Loss: 50.81923540751139\n",
      "Epoch 2 \t Batch 620 \t Training Loss: 50.81405492598011\n",
      "Epoch 2 \t Batch 640 \t Training Loss: 50.773215126991275\n",
      "Epoch 2 \t Batch 660 \t Training Loss: 50.736773346409656\n",
      "Epoch 2 \t Batch 680 \t Training Loss: 50.70466481938082\n",
      "Epoch 2 \t Batch 700 \t Training Loss: 50.71245365687779\n",
      "Epoch 2 \t Batch 720 \t Training Loss: 50.682223632600575\n",
      "Epoch 2 \t Batch 740 \t Training Loss: 50.71193779610299\n",
      "Epoch 2 \t Batch 760 \t Training Loss: 50.68893636904265\n",
      "Epoch 2 \t Batch 780 \t Training Loss: 50.66335409115522\n",
      "Epoch 2 \t Batch 800 \t Training Loss: 50.63344349861145\n",
      "Epoch 2 \t Batch 820 \t Training Loss: 50.57880994750232\n",
      "Epoch 2 \t Batch 840 \t Training Loss: 50.58429362887428\n",
      "Epoch 2 \t Batch 860 \t Training Loss: 50.570483300852224\n",
      "Epoch 2 \t Batch 880 \t Training Loss: 50.562782408974385\n",
      "Epoch 2 \t Batch 900 \t Training Loss: 50.54102403004964\n",
      "Epoch 2 \t Batch 20 \t Validation Loss: 15.705199003219604\n",
      "Epoch 2 \t Batch 40 \t Validation Loss: 19.030930519104004\n",
      "Epoch 2 \t Batch 60 \t Validation Loss: 18.75674165089925\n",
      "Epoch 2 \t Batch 80 \t Validation Loss: 20.00480818748474\n",
      "Epoch 2 \t Batch 100 \t Validation Loss: 21.294930877685548\n",
      "Epoch 2 \t Batch 120 \t Validation Loss: 22.693019088109335\n",
      "Epoch 2 \t Batch 140 \t Validation Loss: 23.322762959344047\n",
      "Epoch 2 \t Batch 160 \t Validation Loss: 25.515137082338335\n",
      "Epoch 2 \t Batch 180 \t Validation Loss: 28.755476114485\n",
      "Epoch 2 \t Batch 200 \t Validation Loss: 30.017433958053587\n",
      "Epoch 2 \t Batch 220 \t Validation Loss: 31.322794463417747\n",
      "Epoch 2 \t Batch 240 \t Validation Loss: 31.815972407658894\n",
      "Epoch 2 \t Batch 260 \t Validation Loss: 33.78443249188937\n",
      "Epoch 2 \t Batch 280 \t Validation Loss: 34.83988759177072\n",
      "Epoch 2 \t Batch 300 \t Validation Loss: 35.92829226175944\n",
      "Epoch 2 \t Batch 320 \t Validation Loss: 36.465224117040634\n",
      "Epoch 2 \t Batch 340 \t Validation Loss: 36.51470769433414\n",
      "Epoch 2 \t Batch 360 \t Validation Loss: 36.500353044933746\n",
      "Epoch 2 \t Batch 380 \t Validation Loss: 36.8592807067068\n",
      "Epoch 2 \t Batch 400 \t Validation Loss: 36.60003620147705\n",
      "Epoch 2 \t Batch 420 \t Validation Loss: 36.755443332308815\n",
      "Epoch 2 \t Batch 440 \t Validation Loss: 36.593007278442386\n",
      "Epoch 2 \t Batch 460 \t Validation Loss: 36.875042591924256\n",
      "Epoch 2 \t Batch 480 \t Validation Loss: 37.439397331078844\n",
      "Epoch 2 \t Batch 500 \t Validation Loss: 37.221586555480954\n",
      "Epoch 2 \t Batch 520 \t Validation Loss: 37.06378234899961\n",
      "Epoch 2 \t Batch 540 \t Validation Loss: 36.79990945922004\n",
      "Epoch 2 \t Batch 560 \t Validation Loss: 36.590107306412285\n",
      "Epoch 2 \t Batch 580 \t Validation Loss: 36.33903666693589\n",
      "Epoch 2 \t Batch 600 \t Validation Loss: 36.555276087125144\n",
      "Epoch 2 Training Loss: 50.54404423255047 Validation Loss: 37.18388895400159\n",
      "Validation Loss Decreased(25417.553972244263--->22905.275595664978) Saving The Model\n",
      "Epoch 2 completed\n",
      "Epoch 3 \t Batch 20 \t Training Loss: 49.53908901214599\n",
      "Epoch 3 \t Batch 40 \t Training Loss: 49.17517004013062\n",
      "Epoch 3 \t Batch 60 \t Training Loss: 49.323702557881674\n",
      "Epoch 3 \t Batch 80 \t Training Loss: 49.20522723197937\n",
      "Epoch 3 \t Batch 100 \t Training Loss: 49.39608798980713\n",
      "Epoch 3 \t Batch 120 \t Training Loss: 49.61047910054525\n",
      "Epoch 3 \t Batch 140 \t Training Loss: 49.561306953430176\n",
      "Epoch 3 \t Batch 160 \t Training Loss: 49.70105996131897\n",
      "Epoch 3 \t Batch 180 \t Training Loss: 49.89092239803738\n",
      "Epoch 3 \t Batch 200 \t Training Loss: 49.92436405181885\n",
      "Epoch 3 \t Batch 220 \t Training Loss: 49.83977588306774\n",
      "Epoch 3 \t Batch 240 \t Training Loss: 49.82702059745789\n",
      "Epoch 3 \t Batch 260 \t Training Loss: 49.69244229243352\n",
      "Epoch 3 \t Batch 280 \t Training Loss: 49.792218003954204\n",
      "Epoch 3 \t Batch 300 \t Training Loss: 49.76724798838298\n",
      "Epoch 3 \t Batch 320 \t Training Loss: 49.79983098506928\n",
      "Epoch 3 \t Batch 340 \t Training Loss: 49.855209036434395\n",
      "Epoch 3 \t Batch 360 \t Training Loss: 49.80837917327881\n",
      "Epoch 3 \t Batch 380 \t Training Loss: 49.88291897020842\n",
      "Epoch 3 \t Batch 400 \t Training Loss: 49.83047795295715\n",
      "Epoch 3 \t Batch 420 \t Training Loss: 49.91988308316186\n",
      "Epoch 3 \t Batch 440 \t Training Loss: 49.904021349820226\n",
      "Epoch 3 \t Batch 460 \t Training Loss: 49.92546534330948\n",
      "Epoch 3 \t Batch 480 \t Training Loss: 49.94215605258942\n",
      "Epoch 3 \t Batch 500 \t Training Loss: 49.899780548095706\n",
      "Epoch 3 \t Batch 520 \t Training Loss: 49.95398464936476\n",
      "Epoch 3 \t Batch 540 \t Training Loss: 49.94558610562925\n",
      "Epoch 3 \t Batch 560 \t Training Loss: 50.02690275056022\n",
      "Epoch 3 \t Batch 580 \t Training Loss: 50.02589939380514\n",
      "Epoch 3 \t Batch 600 \t Training Loss: 49.94630326588948\n",
      "Epoch 3 \t Batch 620 \t Training Loss: 49.94017179550663\n",
      "Epoch 3 \t Batch 640 \t Training Loss: 49.93672273755074\n",
      "Epoch 3 \t Batch 660 \t Training Loss: 49.95885029417096\n",
      "Epoch 3 \t Batch 680 \t Training Loss: 49.90917382520788\n",
      "Epoch 3 \t Batch 700 \t Training Loss: 49.93955093383789\n",
      "Epoch 3 \t Batch 720 \t Training Loss: 49.94915910826789\n",
      "Epoch 3 \t Batch 740 \t Training Loss: 49.97873486183785\n",
      "Epoch 3 \t Batch 760 \t Training Loss: 49.985204531017104\n",
      "Epoch 3 \t Batch 780 \t Training Loss: 49.98682809487367\n",
      "Epoch 3 \t Batch 800 \t Training Loss: 49.99403676509857\n",
      "Epoch 3 \t Batch 820 \t Training Loss: 49.976605178088676\n",
      "Epoch 3 \t Batch 840 \t Training Loss: 49.96802978969755\n",
      "Epoch 3 \t Batch 860 \t Training Loss: 49.92831863580748\n",
      "Epoch 3 \t Batch 880 \t Training Loss: 49.94380499666387\n",
      "Epoch 3 \t Batch 900 \t Training Loss: 49.895801845126684\n",
      "Epoch 3 \t Batch 20 \t Validation Loss: 13.861097860336304\n",
      "Epoch 3 \t Batch 40 \t Validation Loss: 18.169169306755066\n",
      "Epoch 3 \t Batch 60 \t Validation Loss: 17.489597098032633\n",
      "Epoch 3 \t Batch 80 \t Validation Loss: 18.40579310655594\n",
      "Epoch 3 \t Batch 100 \t Validation Loss: 20.296888751983644\n",
      "Epoch 3 \t Batch 120 \t Validation Loss: 21.93411139647166\n",
      "Epoch 3 \t Batch 140 \t Validation Loss: 22.856242758887156\n",
      "Epoch 3 \t Batch 160 \t Validation Loss: 25.450980800390244\n",
      "Epoch 3 \t Batch 180 \t Validation Loss: 29.608741394678752\n",
      "Epoch 3 \t Batch 200 \t Validation Loss: 31.500421233177185\n",
      "Epoch 3 \t Batch 220 \t Validation Loss: 33.335279659791425\n",
      "Epoch 3 \t Batch 240 \t Validation Loss: 34.09942732254664\n",
      "Epoch 3 \t Batch 260 \t Validation Loss: 36.49892021325918\n",
      "Epoch 3 \t Batch 280 \t Validation Loss: 37.8164224760873\n",
      "Epoch 3 \t Batch 300 \t Validation Loss: 39.11903874715169\n",
      "Epoch 3 \t Batch 320 \t Validation Loss: 39.72321600317955\n",
      "Epoch 3 \t Batch 340 \t Validation Loss: 39.72685361189001\n",
      "Epoch 3 \t Batch 360 \t Validation Loss: 39.69570353031158\n",
      "Epoch 3 \t Batch 380 \t Validation Loss: 40.09769151336268\n",
      "Epoch 3 \t Batch 400 \t Validation Loss: 39.81334233522415\n",
      "Epoch 3 \t Batch 420 \t Validation Loss: 39.83459920201983\n",
      "Epoch 3 \t Batch 440 \t Validation Loss: 39.64624058766798\n",
      "Epoch 3 \t Batch 460 \t Validation Loss: 39.86032697221507\n",
      "Epoch 3 \t Batch 480 \t Validation Loss: 40.37778056263924\n",
      "Epoch 3 \t Batch 500 \t Validation Loss: 40.07775405311585\n",
      "Epoch 3 \t Batch 520 \t Validation Loss: 39.99650965287135\n",
      "Epoch 3 \t Batch 540 \t Validation Loss: 39.69515063497755\n",
      "Epoch 3 \t Batch 560 \t Validation Loss: 39.423458138534\n",
      "Epoch 3 \t Batch 580 \t Validation Loss: 39.188966469929134\n",
      "Epoch 3 \t Batch 600 \t Validation Loss: 39.329656624794005\n",
      "Epoch 3 Training Loss: 49.901587366667805 Validation Loss: 39.92194351437804\n",
      "Epoch 3 completed\n",
      "Epoch 4 \t Batch 20 \t Training Loss: 48.925818252563474\n",
      "Epoch 4 \t Batch 40 \t Training Loss: 49.47777452468872\n",
      "Epoch 4 \t Batch 60 \t Training Loss: 49.45300013224284\n",
      "Epoch 4 \t Batch 80 \t Training Loss: 49.604903650283816\n",
      "Epoch 4 \t Batch 100 \t Training Loss: 49.2898261642456\n",
      "Epoch 4 \t Batch 120 \t Training Loss: 49.29211813608806\n",
      "Epoch 4 \t Batch 140 \t Training Loss: 49.14243937901088\n",
      "Epoch 4 \t Batch 160 \t Training Loss: 49.21960232257843\n",
      "Epoch 4 \t Batch 180 \t Training Loss: 49.056498273213705\n",
      "Epoch 4 \t Batch 200 \t Training Loss: 49.103015995025636\n",
      "Epoch 4 \t Batch 220 \t Training Loss: 49.304539819197224\n",
      "Epoch 4 \t Batch 240 \t Training Loss: 49.337588993708295\n",
      "Epoch 4 \t Batch 260 \t Training Loss: 49.40361407353328\n",
      "Epoch 4 \t Batch 280 \t Training Loss: 49.36529688153948\n",
      "Epoch 4 \t Batch 300 \t Training Loss: 49.475777537027994\n",
      "Epoch 4 \t Batch 320 \t Training Loss: 49.47120704650879\n",
      "Epoch 4 \t Batch 340 \t Training Loss: 49.50997593823601\n",
      "Epoch 4 \t Batch 360 \t Training Loss: 49.46904645495945\n",
      "Epoch 4 \t Batch 380 \t Training Loss: 49.523624831751775\n",
      "Epoch 4 \t Batch 400 \t Training Loss: 49.52764269828796\n",
      "Epoch 4 \t Batch 420 \t Training Loss: 49.49912828717913\n",
      "Epoch 4 \t Batch 440 \t Training Loss: 49.493258120796895\n",
      "Epoch 4 \t Batch 460 \t Training Loss: 49.447873820429265\n",
      "Epoch 4 \t Batch 480 \t Training Loss: 49.49132806460063\n",
      "Epoch 4 \t Batch 500 \t Training Loss: 49.48434564971924\n",
      "Epoch 4 \t Batch 520 \t Training Loss: 49.558372813004716\n",
      "Epoch 4 \t Batch 540 \t Training Loss: 49.54342591321027\n",
      "Epoch 4 \t Batch 560 \t Training Loss: 49.53159704208374\n",
      "Epoch 4 \t Batch 580 \t Training Loss: 49.52744078142889\n",
      "Epoch 4 \t Batch 600 \t Training Loss: 49.53114475250244\n",
      "Epoch 4 \t Batch 620 \t Training Loss: 49.503403983577606\n",
      "Epoch 4 \t Batch 640 \t Training Loss: 49.50575121045112\n",
      "Epoch 4 \t Batch 660 \t Training Loss: 49.499673987879895\n",
      "Epoch 4 \t Batch 680 \t Training Loss: 49.516217972250544\n",
      "Epoch 4 \t Batch 700 \t Training Loss: 49.54354869842529\n",
      "Epoch 4 \t Batch 720 \t Training Loss: 49.54281838734945\n",
      "Epoch 4 \t Batch 740 \t Training Loss: 49.55809758160565\n",
      "Epoch 4 \t Batch 760 \t Training Loss: 49.532116518522564\n",
      "Epoch 4 \t Batch 780 \t Training Loss: 49.57847435780061\n",
      "Epoch 4 \t Batch 800 \t Training Loss: 49.60441335678101\n",
      "Epoch 4 \t Batch 820 \t Training Loss: 49.59086968491717\n",
      "Epoch 4 \t Batch 840 \t Training Loss: 49.57989611398606\n",
      "Epoch 4 \t Batch 860 \t Training Loss: 49.585584449768064\n",
      "Epoch 4 \t Batch 880 \t Training Loss: 49.58325923139399\n",
      "Epoch 4 \t Batch 900 \t Training Loss: 49.5613343556722\n",
      "Epoch 4 \t Batch 20 \t Validation Loss: 15.704887390136719\n",
      "Epoch 4 \t Batch 40 \t Validation Loss: 20.320668601989745\n",
      "Epoch 4 \t Batch 60 \t Validation Loss: 19.308760945002238\n",
      "Epoch 4 \t Batch 80 \t Validation Loss: 20.163615143299104\n",
      "Epoch 4 \t Batch 100 \t Validation Loss: 21.69123243331909\n",
      "Epoch 4 \t Batch 120 \t Validation Loss: 23.07687493165334\n",
      "Epoch 4 \t Batch 140 \t Validation Loss: 23.93024127823966\n",
      "Epoch 4 \t Batch 160 \t Validation Loss: 26.486195546388625\n",
      "Epoch 4 \t Batch 180 \t Validation Loss: 30.14807178179423\n",
      "Epoch 4 \t Batch 200 \t Validation Loss: 31.7096279668808\n",
      "Epoch 4 \t Batch 220 \t Validation Loss: 33.208305397900666\n",
      "Epoch 4 \t Batch 240 \t Validation Loss: 33.867570372422534\n",
      "Epoch 4 \t Batch 260 \t Validation Loss: 35.93611192703247\n",
      "Epoch 4 \t Batch 280 \t Validation Loss: 37.00533266408103\n",
      "Epoch 4 \t Batch 300 \t Validation Loss: 38.30635038057963\n",
      "Epoch 4 \t Batch 320 \t Validation Loss: 38.95565157234669\n",
      "Epoch 4 \t Batch 340 \t Validation Loss: 39.06110355994281\n",
      "Epoch 4 \t Batch 360 \t Validation Loss: 39.16668006314172\n",
      "Epoch 4 \t Batch 380 \t Validation Loss: 39.566942423268365\n",
      "Epoch 4 \t Batch 400 \t Validation Loss: 39.49365170240402\n",
      "Epoch 4 \t Batch 420 \t Validation Loss: 39.645090182622276\n",
      "Epoch 4 \t Batch 440 \t Validation Loss: 39.658147159489715\n",
      "Epoch 4 \t Batch 460 \t Validation Loss: 39.988163141582326\n",
      "Epoch 4 \t Batch 480 \t Validation Loss: 40.57060652772586\n",
      "Epoch 4 \t Batch 500 \t Validation Loss: 40.36090088462829\n",
      "Epoch 4 \t Batch 520 \t Validation Loss: 40.45447924137115\n",
      "Epoch 4 \t Batch 540 \t Validation Loss: 40.42767750245554\n",
      "Epoch 4 \t Batch 560 \t Validation Loss: 40.47760288204466\n",
      "Epoch 4 \t Batch 580 \t Validation Loss: 40.62922694272009\n",
      "Epoch 4 \t Batch 600 \t Validation Loss: 40.94123093763987\n",
      "Epoch 4 Training Loss: 49.542282728488466 Validation Loss: 41.672774239019915\n",
      "Epoch 4 completed\n",
      "Epoch 5 \t Batch 20 \t Training Loss: 49.90452766418457\n",
      "Epoch 5 \t Batch 40 \t Training Loss: 49.596871089935306\n",
      "Epoch 5 \t Batch 60 \t Training Loss: 49.4450039545695\n",
      "Epoch 5 \t Batch 80 \t Training Loss: 49.380281162261966\n",
      "Epoch 5 \t Batch 100 \t Training Loss: 49.48950065612793\n",
      "Epoch 5 \t Batch 120 \t Training Loss: 49.488877550760904\n",
      "Epoch 5 \t Batch 140 \t Training Loss: 49.39611535753522\n",
      "Epoch 5 \t Batch 160 \t Training Loss: 49.46680474281311\n",
      "Epoch 5 \t Batch 180 \t Training Loss: 49.35483057234023\n",
      "Epoch 5 \t Batch 200 \t Training Loss: 49.4165353012085\n",
      "Epoch 5 \t Batch 220 \t Training Loss: 49.292882000316276\n",
      "Epoch 5 \t Batch 240 \t Training Loss: 49.39653353691101\n",
      "Epoch 5 \t Batch 260 \t Training Loss: 49.381885763315054\n",
      "Epoch 5 \t Batch 280 \t Training Loss: 49.39257149015154\n",
      "Epoch 5 \t Batch 300 \t Training Loss: 49.502496045430505\n",
      "Epoch 5 \t Batch 320 \t Training Loss: 49.52272762060166\n",
      "Epoch 5 \t Batch 340 \t Training Loss: 49.59076540329877\n",
      "Epoch 5 \t Batch 360 \t Training Loss: 49.45669322543674\n",
      "Epoch 5 \t Batch 380 \t Training Loss: 49.53951545514558\n",
      "Epoch 5 \t Batch 400 \t Training Loss: 49.60310910224914\n",
      "Epoch 5 \t Batch 420 \t Training Loss: 49.54513014838809\n",
      "Epoch 5 \t Batch 440 \t Training Loss: 49.53071250915527\n",
      "Epoch 5 \t Batch 460 \t Training Loss: 49.53422657510509\n",
      "Epoch 5 \t Batch 480 \t Training Loss: 49.4904435078303\n",
      "Epoch 5 \t Batch 500 \t Training Loss: 49.41361767578125\n",
      "Epoch 5 \t Batch 520 \t Training Loss: 49.34139841519869\n",
      "Epoch 5 \t Batch 540 \t Training Loss: 49.36887659849944\n",
      "Epoch 5 \t Batch 560 \t Training Loss: 49.333782938548495\n",
      "Epoch 5 \t Batch 580 \t Training Loss: 49.34170185615277\n",
      "Epoch 5 \t Batch 600 \t Training Loss: 49.359134171803795\n",
      "Epoch 5 \t Batch 620 \t Training Loss: 49.33813093862226\n",
      "Epoch 5 \t Batch 640 \t Training Loss: 49.34217373132706\n",
      "Epoch 5 \t Batch 660 \t Training Loss: 49.2990426381429\n",
      "Epoch 5 \t Batch 680 \t Training Loss: 49.30766560610603\n",
      "Epoch 5 \t Batch 700 \t Training Loss: 49.26926769256592\n",
      "Epoch 5 \t Batch 720 \t Training Loss: 49.23254255188836\n",
      "Epoch 5 \t Batch 740 \t Training Loss: 49.28707155279211\n",
      "Epoch 5 \t Batch 760 \t Training Loss: 49.25562010313335\n",
      "Epoch 5 \t Batch 780 \t Training Loss: 49.235752081259704\n",
      "Epoch 5 \t Batch 800 \t Training Loss: 49.21732879638672\n",
      "Epoch 5 \t Batch 820 \t Training Loss: 49.237783511092026\n",
      "Epoch 5 \t Batch 840 \t Training Loss: 49.250676681881856\n",
      "Epoch 5 \t Batch 860 \t Training Loss: 49.25773709319358\n",
      "Epoch 5 \t Batch 880 \t Training Loss: 49.26310435641896\n",
      "Epoch 5 \t Batch 900 \t Training Loss: 49.29437833150228\n",
      "Epoch 5 \t Batch 20 \t Validation Loss: 14.586094903945924\n",
      "Epoch 5 \t Batch 40 \t Validation Loss: 17.46895213127136\n",
      "Epoch 5 \t Batch 60 \t Validation Loss: 17.16124890645345\n",
      "Epoch 5 \t Batch 80 \t Validation Loss: 17.91831679344177\n",
      "Epoch 5 \t Batch 100 \t Validation Loss: 19.775057277679444\n",
      "Epoch 5 \t Batch 120 \t Validation Loss: 21.496180764834087\n",
      "Epoch 5 \t Batch 140 \t Validation Loss: 22.343637377875194\n",
      "Epoch 5 \t Batch 160 \t Validation Loss: 24.85813825726509\n",
      "Epoch 5 \t Batch 180 \t Validation Loss: 29.074422619077893\n",
      "Epoch 5 \t Batch 200 \t Validation Loss: 30.953202929496765\n",
      "Epoch 5 \t Batch 220 \t Validation Loss: 32.798772391405976\n",
      "Epoch 5 \t Batch 240 \t Validation Loss: 33.598759444554645\n",
      "Epoch 5 \t Batch 260 \t Validation Loss: 35.95363763662485\n",
      "Epoch 5 \t Batch 280 \t Validation Loss: 37.29270780767713\n",
      "Epoch 5 \t Batch 300 \t Validation Loss: 38.62198145230611\n",
      "Epoch 5 \t Batch 320 \t Validation Loss: 39.275498488545416\n",
      "Epoch 5 \t Batch 340 \t Validation Loss: 39.30756136669832\n",
      "Epoch 5 \t Batch 360 \t Validation Loss: 39.26833926041921\n",
      "Epoch 5 \t Batch 380 \t Validation Loss: 39.62374147364968\n",
      "Epoch 5 \t Batch 400 \t Validation Loss: 39.21674352407455\n",
      "Epoch 5 \t Batch 420 \t Validation Loss: 39.287240807215376\n",
      "Epoch 5 \t Batch 440 \t Validation Loss: 39.01756594181061\n",
      "Epoch 5 \t Batch 460 \t Validation Loss: 39.22288552367169\n",
      "Epoch 5 \t Batch 480 \t Validation Loss: 39.71552379727363\n",
      "Epoch 5 \t Batch 500 \t Validation Loss: 39.436513521194456\n",
      "Epoch 5 \t Batch 520 \t Validation Loss: 39.14302210991199\n",
      "Epoch 5 \t Batch 540 \t Validation Loss: 38.97446320675038\n",
      "Epoch 5 \t Batch 560 \t Validation Loss: 38.83305846282414\n",
      "Epoch 5 \t Batch 580 \t Validation Loss: 38.55117160205183\n",
      "Epoch 5 \t Batch 600 \t Validation Loss: 38.89987193743388\n",
      "Epoch 5 Training Loss: 49.302888514561374 Validation Loss: 39.49777384547444\n",
      "Epoch 5 completed\n",
      "Epoch 6 \t Batch 20 \t Training Loss: 48.76146335601807\n",
      "Epoch 6 \t Batch 40 \t Training Loss: 48.82686700820923\n",
      "Epoch 6 \t Batch 60 \t Training Loss: 49.37419001261393\n",
      "Epoch 6 \t Batch 80 \t Training Loss: 49.111399269104005\n",
      "Epoch 6 \t Batch 100 \t Training Loss: 49.45539768218994\n",
      "Epoch 6 \t Batch 120 \t Training Loss: 49.426237201690675\n",
      "Epoch 6 \t Batch 140 \t Training Loss: 49.494939940316335\n",
      "Epoch 6 \t Batch 160 \t Training Loss: 49.340368318557736\n",
      "Epoch 6 \t Batch 180 \t Training Loss: 49.36675900353326\n",
      "Epoch 6 \t Batch 200 \t Training Loss: 49.3008731842041\n",
      "Epoch 6 \t Batch 220 \t Training Loss: 49.25841721621427\n",
      "Epoch 6 \t Batch 240 \t Training Loss: 49.185163434346514\n",
      "Epoch 6 \t Batch 260 \t Training Loss: 49.255251943148096\n",
      "Epoch 6 \t Batch 280 \t Training Loss: 49.26088173730033\n",
      "Epoch 6 \t Batch 300 \t Training Loss: 49.31789309183757\n",
      "Epoch 6 \t Batch 320 \t Training Loss: 49.274262118339536\n",
      "Epoch 6 \t Batch 340 \t Training Loss: 49.28061768026913\n",
      "Epoch 6 \t Batch 360 \t Training Loss: 49.25078558391995\n",
      "Epoch 6 \t Batch 380 \t Training Loss: 49.27087614159835\n",
      "Epoch 6 \t Batch 400 \t Training Loss: 49.317627029418944\n",
      "Epoch 6 \t Batch 420 \t Training Loss: 49.26797462645031\n",
      "Epoch 6 \t Batch 440 \t Training Loss: 49.25343490947377\n",
      "Epoch 6 \t Batch 460 \t Training Loss: 49.16442586650019\n",
      "Epoch 6 \t Batch 480 \t Training Loss: 49.14815098444621\n",
      "Epoch 6 \t Batch 500 \t Training Loss: 49.17050912475586\n",
      "Epoch 6 \t Batch 520 \t Training Loss: 49.17573334620549\n",
      "Epoch 6 \t Batch 540 \t Training Loss: 49.22298309184887\n",
      "Epoch 6 \t Batch 560 \t Training Loss: 49.2024485043117\n",
      "Epoch 6 \t Batch 580 \t Training Loss: 49.23839503978861\n",
      "Epoch 6 \t Batch 600 \t Training Loss: 49.23953296025594\n",
      "Epoch 6 \t Batch 620 \t Training Loss: 49.19601618243802\n",
      "Epoch 6 \t Batch 640 \t Training Loss: 49.174783545732495\n",
      "Epoch 6 \t Batch 660 \t Training Loss: 49.14816688190807\n",
      "Epoch 6 \t Batch 680 \t Training Loss: 49.14919648450964\n",
      "Epoch 6 \t Batch 700 \t Training Loss: 49.189629358564105\n",
      "Epoch 6 \t Batch 720 \t Training Loss: 49.16769585079617\n",
      "Epoch 6 \t Batch 740 \t Training Loss: 49.15956441518423\n",
      "Epoch 6 \t Batch 760 \t Training Loss: 49.12256291540046\n",
      "Epoch 6 \t Batch 780 \t Training Loss: 49.08751634940123\n",
      "Epoch 6 \t Batch 800 \t Training Loss: 49.12517897605896\n",
      "Epoch 6 \t Batch 820 \t Training Loss: 49.13551391508521\n",
      "Epoch 6 \t Batch 840 \t Training Loss: 49.12920604887463\n",
      "Epoch 6 \t Batch 860 \t Training Loss: 49.124497985839845\n",
      "Epoch 6 \t Batch 880 \t Training Loss: 49.09877439412204\n",
      "Epoch 6 \t Batch 900 \t Training Loss: 49.08388478597005\n",
      "Epoch 6 \t Batch 20 \t Validation Loss: 22.695227575302123\n",
      "Epoch 6 \t Batch 40 \t Validation Loss: 25.249354314804076\n",
      "Epoch 6 \t Batch 60 \t Validation Loss: 25.22849504152934\n",
      "Epoch 6 \t Batch 80 \t Validation Loss: 25.98984067440033\n",
      "Epoch 6 \t Batch 100 \t Validation Loss: 26.128678169250488\n",
      "Epoch 6 \t Batch 120 \t Validation Loss: 26.619071801503498\n",
      "Epoch 6 \t Batch 140 \t Validation Loss: 26.730911663600377\n",
      "Epoch 6 \t Batch 160 \t Validation Loss: 28.419649040699007\n",
      "Epoch 6 \t Batch 180 \t Validation Loss: 31.26150411499871\n",
      "Epoch 6 \t Batch 200 \t Validation Loss: 32.15484133243561\n",
      "Epoch 6 \t Batch 220 \t Validation Loss: 33.110147107731215\n",
      "Epoch 6 \t Batch 240 \t Validation Loss: 33.398700972398125\n",
      "Epoch 6 \t Batch 260 \t Validation Loss: 35.0145607141348\n",
      "Epoch 6 \t Batch 280 \t Validation Loss: 35.79139699935913\n",
      "Epoch 6 \t Batch 300 \t Validation Loss: 36.85507397969564\n",
      "Epoch 6 \t Batch 320 \t Validation Loss: 37.29882305264473\n",
      "Epoch 6 \t Batch 340 \t Validation Loss: 37.28873676973231\n",
      "Epoch 6 \t Batch 360 \t Validation Loss: 37.26436971028646\n",
      "Epoch 6 \t Batch 380 \t Validation Loss: 37.482162109174226\n",
      "Epoch 6 \t Batch 400 \t Validation Loss: 37.19732346057892\n",
      "Epoch 6 \t Batch 420 \t Validation Loss: 37.290330905006044\n",
      "Epoch 6 \t Batch 440 \t Validation Loss: 37.07150190960277\n",
      "Epoch 6 \t Batch 460 \t Validation Loss: 37.33361455253933\n",
      "Epoch 6 \t Batch 480 \t Validation Loss: 37.863893671830496\n",
      "Epoch 6 \t Batch 500 \t Validation Loss: 37.615181392669676\n",
      "Epoch 6 \t Batch 520 \t Validation Loss: 37.43364114027757\n",
      "Epoch 6 \t Batch 540 \t Validation Loss: 37.21788001943518\n",
      "Epoch 6 \t Batch 560 \t Validation Loss: 37.03721811090197\n",
      "Epoch 6 \t Batch 580 \t Validation Loss: 36.87076251917872\n",
      "Epoch 6 \t Batch 600 \t Validation Loss: 37.0961323928833\n",
      "Epoch 6 Training Loss: 49.09301550645735 Validation Loss: 37.72261577147942\n",
      "Epoch 6 completed\n",
      "Epoch 7 \t Batch 20 \t Training Loss: 49.161536026000974\n",
      "Epoch 7 \t Batch 40 \t Training Loss: 49.288998985290526\n",
      "Epoch 7 \t Batch 60 \t Training Loss: 49.366676266988115\n",
      "Epoch 7 \t Batch 80 \t Training Loss: 49.932912302017215\n",
      "Epoch 7 \t Batch 100 \t Training Loss: 49.523484497070314\n",
      "Epoch 7 \t Batch 120 \t Training Loss: 49.55913966496785\n",
      "Epoch 7 \t Batch 140 \t Training Loss: 49.32124737330845\n",
      "Epoch 7 \t Batch 160 \t Training Loss: 49.35235652923584\n",
      "Epoch 7 \t Batch 180 \t Training Loss: 49.2224689271715\n",
      "Epoch 7 \t Batch 200 \t Training Loss: 49.20715148925781\n",
      "Epoch 7 \t Batch 220 \t Training Loss: 49.15994769009677\n",
      "Epoch 7 \t Batch 240 \t Training Loss: 48.97844864527384\n",
      "Epoch 7 \t Batch 260 \t Training Loss: 48.9173487003033\n",
      "Epoch 7 \t Batch 280 \t Training Loss: 48.960337557111465\n",
      "Epoch 7 \t Batch 300 \t Training Loss: 48.93817192077637\n",
      "Epoch 7 \t Batch 320 \t Training Loss: 48.86074075698853\n",
      "Epoch 7 \t Batch 340 \t Training Loss: 48.859727949254655\n",
      "Epoch 7 \t Batch 360 \t Training Loss: 48.80995648701985\n",
      "Epoch 7 \t Batch 380 \t Training Loss: 48.822011104382966\n",
      "Epoch 7 \t Batch 400 \t Training Loss: 48.80209981918335\n",
      "Epoch 7 \t Batch 420 \t Training Loss: 48.83975911821638\n",
      "Epoch 7 \t Batch 440 \t Training Loss: 48.86751646562056\n",
      "Epoch 7 \t Batch 460 \t Training Loss: 48.863082280366314\n",
      "Epoch 7 \t Batch 480 \t Training Loss: 48.87015868028005\n",
      "Epoch 7 \t Batch 500 \t Training Loss: 48.87934065246582\n",
      "Epoch 7 \t Batch 520 \t Training Loss: 48.86888055067796\n",
      "Epoch 7 \t Batch 540 \t Training Loss: 48.89156465883608\n",
      "Epoch 7 \t Batch 560 \t Training Loss: 48.8613183089665\n",
      "Epoch 7 \t Batch 580 \t Training Loss: 48.82724714608028\n",
      "Epoch 7 \t Batch 600 \t Training Loss: 48.85106461207072\n",
      "Epoch 7 \t Batch 620 \t Training Loss: 48.88509344900808\n",
      "Epoch 7 \t Batch 640 \t Training Loss: 48.908313220739366\n",
      "Epoch 7 \t Batch 660 \t Training Loss: 48.93205260652484\n",
      "Epoch 7 \t Batch 680 \t Training Loss: 48.91527483323041\n",
      "Epoch 7 \t Batch 700 \t Training Loss: 48.93062970297677\n",
      "Epoch 7 \t Batch 720 \t Training Loss: 48.93842027982076\n",
      "Epoch 7 \t Batch 740 \t Training Loss: 48.94473597036826\n",
      "Epoch 7 \t Batch 760 \t Training Loss: 48.92689567867078\n",
      "Epoch 7 \t Batch 780 \t Training Loss: 48.90745380108173\n",
      "Epoch 7 \t Batch 800 \t Training Loss: 48.957515816688534\n",
      "Epoch 7 \t Batch 820 \t Training Loss: 48.944136042711214\n",
      "Epoch 7 \t Batch 840 \t Training Loss: 48.961834557851155\n",
      "Epoch 7 \t Batch 860 \t Training Loss: 48.97283122484074\n",
      "Epoch 7 \t Batch 880 \t Training Loss: 48.95194037177346\n",
      "Epoch 7 \t Batch 900 \t Training Loss: 48.932063289218476\n",
      "Epoch 7 \t Batch 20 \t Validation Loss: 15.579801106452942\n",
      "Epoch 7 \t Batch 40 \t Validation Loss: 19.052806866168975\n",
      "Epoch 7 \t Batch 60 \t Validation Loss: 18.59974755446116\n",
      "Epoch 7 \t Batch 80 \t Validation Loss: 19.053006702661513\n",
      "Epoch 7 \t Batch 100 \t Validation Loss: 20.630166811943056\n",
      "Epoch 7 \t Batch 120 \t Validation Loss: 21.993036448955536\n",
      "Epoch 7 \t Batch 140 \t Validation Loss: 22.773661712237768\n",
      "Epoch 7 \t Batch 160 \t Validation Loss: 25.117404249310493\n",
      "Epoch 7 \t Batch 180 \t Validation Loss: 28.892702603340148\n",
      "Epoch 7 \t Batch 200 \t Validation Loss: 30.414699375629425\n",
      "Epoch 7 \t Batch 220 \t Validation Loss: 31.937065126679162\n",
      "Epoch 7 \t Batch 240 \t Validation Loss: 32.57401188413302\n",
      "Epoch 7 \t Batch 260 \t Validation Loss: 34.72129577673399\n",
      "Epoch 7 \t Batch 280 \t Validation Loss: 35.86434312377657\n",
      "Epoch 7 \t Batch 300 \t Validation Loss: 37.17722511132558\n",
      "Epoch 7 \t Batch 320 \t Validation Loss: 37.77092852443457\n",
      "Epoch 7 \t Batch 340 \t Validation Loss: 37.78178736602559\n",
      "Epoch 7 \t Batch 360 \t Validation Loss: 37.77016561428706\n",
      "Epoch 7 \t Batch 380 \t Validation Loss: 38.08443971056687\n",
      "Epoch 7 \t Batch 400 \t Validation Loss: 37.705637928247455\n",
      "Epoch 7 \t Batch 420 \t Validation Loss: 37.772001880691164\n",
      "Epoch 7 \t Batch 440 \t Validation Loss: 37.48903232162649\n",
      "Epoch 7 \t Batch 460 \t Validation Loss: 37.72498705179795\n",
      "Epoch 7 \t Batch 480 \t Validation Loss: 38.23210531175137\n",
      "Epoch 7 \t Batch 500 \t Validation Loss: 37.96367360401153\n",
      "Epoch 7 \t Batch 520 \t Validation Loss: 37.74640104312163\n",
      "Epoch 7 \t Batch 540 \t Validation Loss: 37.4897179612407\n",
      "Epoch 7 \t Batch 560 \t Validation Loss: 37.26762389540672\n",
      "Epoch 7 \t Batch 580 \t Validation Loss: 37.031356259872176\n",
      "Epoch 7 \t Batch 600 \t Validation Loss: 37.23752606153488\n",
      "Epoch 7 Training Loss: 48.942068272614556 Validation Loss: 37.85310911900037\n",
      "Epoch 7 completed\n",
      "Epoch 8 \t Batch 20 \t Training Loss: 50.065420150756836\n",
      "Epoch 8 \t Batch 40 \t Training Loss: 49.356886291503905\n",
      "Epoch 8 \t Batch 60 \t Training Loss: 49.33021462758382\n",
      "Epoch 8 \t Batch 80 \t Training Loss: 49.278537082672116\n",
      "Epoch 8 \t Batch 100 \t Training Loss: 49.29903102874756\n",
      "Epoch 8 \t Batch 120 \t Training Loss: 49.21819836298625\n",
      "Epoch 8 \t Batch 140 \t Training Loss: 49.090554755074635\n",
      "Epoch 8 \t Batch 160 \t Training Loss: 48.98767623901367\n",
      "Epoch 8 \t Batch 180 \t Training Loss: 49.115991062588165\n",
      "Epoch 8 \t Batch 200 \t Training Loss: 48.941779193878176\n",
      "Epoch 8 \t Batch 220 \t Training Loss: 48.940199089050296\n",
      "Epoch 8 \t Batch 240 \t Training Loss: 48.97524240811666\n",
      "Epoch 8 \t Batch 260 \t Training Loss: 49.02541279425988\n",
      "Epoch 8 \t Batch 280 \t Training Loss: 49.05478042875018\n",
      "Epoch 8 \t Batch 300 \t Training Loss: 49.058746096293135\n",
      "Epoch 8 \t Batch 320 \t Training Loss: 49.05262768268585\n",
      "Epoch 8 \t Batch 340 \t Training Loss: 49.064222358254824\n",
      "Epoch 8 \t Batch 360 \t Training Loss: 48.99827869203356\n",
      "Epoch 8 \t Batch 380 \t Training Loss: 48.9825688412315\n",
      "Epoch 8 \t Batch 400 \t Training Loss: 49.00193043708801\n",
      "Epoch 8 \t Batch 420 \t Training Loss: 49.01290613810222\n",
      "Epoch 8 \t Batch 440 \t Training Loss: 48.971333200281315\n",
      "Epoch 8 \t Batch 460 \t Training Loss: 48.99004364013672\n",
      "Epoch 8 \t Batch 480 \t Training Loss: 48.981905261675514\n",
      "Epoch 8 \t Batch 500 \t Training Loss: 49.002041038513184\n",
      "Epoch 8 \t Batch 520 \t Training Loss: 48.923997226128215\n",
      "Epoch 8 \t Batch 540 \t Training Loss: 48.94897742094817\n",
      "Epoch 8 \t Batch 560 \t Training Loss: 48.9555775301797\n",
      "Epoch 8 \t Batch 580 \t Training Loss: 48.88884305625126\n",
      "Epoch 8 \t Batch 600 \t Training Loss: 48.88326271692912\n",
      "Epoch 8 \t Batch 620 \t Training Loss: 48.86037990200904\n",
      "Epoch 8 \t Batch 640 \t Training Loss: 48.86561394929886\n",
      "Epoch 8 \t Batch 660 \t Training Loss: 48.898911695769335\n",
      "Epoch 8 \t Batch 680 \t Training Loss: 48.874327973758476\n",
      "Epoch 8 \t Batch 700 \t Training Loss: 48.87011615208217\n",
      "Epoch 8 \t Batch 720 \t Training Loss: 48.87694239086575\n",
      "Epoch 8 \t Batch 740 \t Training Loss: 48.867571990554396\n",
      "Epoch 8 \t Batch 760 \t Training Loss: 48.8863705534684\n",
      "Epoch 8 \t Batch 780 \t Training Loss: 48.85210371261988\n",
      "Epoch 8 \t Batch 800 \t Training Loss: 48.863873596191404\n",
      "Epoch 8 \t Batch 820 \t Training Loss: 48.822249575359066\n",
      "Epoch 8 \t Batch 840 \t Training Loss: 48.80016173408145\n",
      "Epoch 8 \t Batch 860 \t Training Loss: 48.78945442465849\n",
      "Epoch 8 \t Batch 880 \t Training Loss: 48.74564586119218\n",
      "Epoch 8 \t Batch 900 \t Training Loss: 48.77989159901937\n",
      "Epoch 8 \t Batch 20 \t Validation Loss: 17.84438419342041\n",
      "Epoch 8 \t Batch 40 \t Validation Loss: 20.680405378341675\n",
      "Epoch 8 \t Batch 60 \t Validation Loss: 20.597529141108193\n",
      "Epoch 8 \t Batch 80 \t Validation Loss: 21.170524501800536\n",
      "Epoch 8 \t Batch 100 \t Validation Loss: 22.18671977996826\n",
      "Epoch 8 \t Batch 120 \t Validation Loss: 23.30583377679189\n",
      "Epoch 8 \t Batch 140 \t Validation Loss: 23.815303209849766\n",
      "Epoch 8 \t Batch 160 \t Validation Loss: 25.97275783419609\n",
      "Epoch 8 \t Batch 180 \t Validation Loss: 29.770546139611138\n",
      "Epoch 8 \t Batch 200 \t Validation Loss: 31.333292331695556\n",
      "Epoch 8 \t Batch 220 \t Validation Loss: 32.82137920206243\n",
      "Epoch 8 \t Batch 240 \t Validation Loss: 33.47806007464727\n",
      "Epoch 8 \t Batch 260 \t Validation Loss: 35.578431859383215\n",
      "Epoch 8 \t Batch 280 \t Validation Loss: 36.64128984042576\n",
      "Epoch 8 \t Batch 300 \t Validation Loss: 37.95654697418213\n",
      "Epoch 8 \t Batch 320 \t Validation Loss: 38.55963243842125\n",
      "Epoch 8 \t Batch 340 \t Validation Loss: 38.55954995435827\n",
      "Epoch 8 \t Batch 360 \t Validation Loss: 38.48329751756456\n",
      "Epoch 8 \t Batch 380 \t Validation Loss: 38.76039618441933\n",
      "Epoch 8 \t Batch 400 \t Validation Loss: 38.38342029094696\n",
      "Epoch 8 \t Batch 420 \t Validation Loss: 38.4548684324537\n",
      "Epoch 8 \t Batch 440 \t Validation Loss: 38.17632327296517\n",
      "Epoch 8 \t Batch 460 \t Validation Loss: 38.36882642870364\n",
      "Epoch 8 \t Batch 480 \t Validation Loss: 38.8619376162688\n",
      "Epoch 8 \t Batch 500 \t Validation Loss: 38.57967847251892\n",
      "Epoch 8 \t Batch 520 \t Validation Loss: 38.328848459170416\n",
      "Epoch 8 \t Batch 540 \t Validation Loss: 38.032887541806254\n",
      "Epoch 8 \t Batch 560 \t Validation Loss: 37.79481000559671\n",
      "Epoch 8 \t Batch 580 \t Validation Loss: 37.56449499952382\n",
      "Epoch 8 \t Batch 600 \t Validation Loss: 37.74214030742645\n",
      "Epoch 8 Training Loss: 48.78811101799053 Validation Loss: 38.364017927801456\n",
      "Epoch 8 completed\n",
      "Epoch 9 \t Batch 20 \t Training Loss: 47.764369583129884\n",
      "Epoch 9 \t Batch 40 \t Training Loss: 47.75233869552612\n",
      "Epoch 9 \t Batch 60 \t Training Loss: 48.25230032602946\n",
      "Epoch 9 \t Batch 80 \t Training Loss: 48.17262191772461\n",
      "Epoch 9 \t Batch 100 \t Training Loss: 48.12176349639893\n",
      "Epoch 9 \t Batch 120 \t Training Loss: 47.953072738647464\n",
      "Epoch 9 \t Batch 140 \t Training Loss: 48.15922388349261\n",
      "Epoch 9 \t Batch 160 \t Training Loss: 48.33203887939453\n",
      "Epoch 9 \t Batch 180 \t Training Loss: 48.317018021477594\n",
      "Epoch 9 \t Batch 200 \t Training Loss: 48.32910106658935\n",
      "Epoch 9 \t Batch 220 \t Training Loss: 48.339602730490945\n",
      "Epoch 9 \t Batch 240 \t Training Loss: 48.43939723968506\n",
      "Epoch 9 \t Batch 260 \t Training Loss: 48.49260459313026\n",
      "Epoch 9 \t Batch 280 \t Training Loss: 48.432280608585906\n",
      "Epoch 9 \t Batch 300 \t Training Loss: 48.39651269276937\n",
      "Epoch 9 \t Batch 320 \t Training Loss: 48.40891119241714\n",
      "Epoch 9 \t Batch 340 \t Training Loss: 48.45255852867575\n",
      "Epoch 9 \t Batch 360 \t Training Loss: 48.41057307985094\n",
      "Epoch 9 \t Batch 380 \t Training Loss: 48.53013387981214\n",
      "Epoch 9 \t Batch 400 \t Training Loss: 48.53266719818115\n",
      "Epoch 9 \t Batch 420 \t Training Loss: 48.55680670057024\n",
      "Epoch 9 \t Batch 440 \t Training Loss: 48.540873778950086\n",
      "Epoch 9 \t Batch 460 \t Training Loss: 48.56949640356976\n",
      "Epoch 9 \t Batch 480 \t Training Loss: 48.6052837451299\n",
      "Epoch 9 \t Batch 500 \t Training Loss: 48.64329531860351\n",
      "Epoch 9 \t Batch 520 \t Training Loss: 48.634156740628754\n",
      "Epoch 9 \t Batch 540 \t Training Loss: 48.64337037404378\n",
      "Epoch 9 \t Batch 560 \t Training Loss: 48.67013795716422\n",
      "Epoch 9 \t Batch 580 \t Training Loss: 48.70662425468708\n",
      "Epoch 9 \t Batch 600 \t Training Loss: 48.69942358652751\n",
      "Epoch 9 \t Batch 620 \t Training Loss: 48.71708340798655\n",
      "Epoch 9 \t Batch 640 \t Training Loss: 48.7179091155529\n",
      "Epoch 9 \t Batch 660 \t Training Loss: 48.691744370894\n",
      "Epoch 9 \t Batch 680 \t Training Loss: 48.63907449946684\n",
      "Epoch 9 \t Batch 700 \t Training Loss: 48.66277475629534\n",
      "Epoch 9 \t Batch 720 \t Training Loss: 48.657667271296184\n",
      "Epoch 9 \t Batch 740 \t Training Loss: 48.704943770331305\n",
      "Epoch 9 \t Batch 760 \t Training Loss: 48.701725809197676\n",
      "Epoch 9 \t Batch 780 \t Training Loss: 48.65567653362567\n",
      "Epoch 9 \t Batch 800 \t Training Loss: 48.649324893951416\n",
      "Epoch 9 \t Batch 820 \t Training Loss: 48.66798078955674\n",
      "Epoch 9 \t Batch 840 \t Training Loss: 48.634003711882094\n",
      "Epoch 9 \t Batch 860 \t Training Loss: 48.661086605870445\n",
      "Epoch 9 \t Batch 880 \t Training Loss: 48.67267275723544\n",
      "Epoch 9 \t Batch 900 \t Training Loss: 48.644245876736115\n",
      "Epoch 9 \t Batch 20 \t Validation Loss: 14.596839570999146\n",
      "Epoch 9 \t Batch 40 \t Validation Loss: 19.147961354255678\n",
      "Epoch 9 \t Batch 60 \t Validation Loss: 18.300120226542155\n",
      "Epoch 9 \t Batch 80 \t Validation Loss: 18.725975382328034\n",
      "Epoch 9 \t Batch 100 \t Validation Loss: 20.518569660186767\n",
      "Epoch 9 \t Batch 120 \t Validation Loss: 22.086801632245383\n",
      "Epoch 9 \t Batch 140 \t Validation Loss: 23.02394050189427\n",
      "Epoch 9 \t Batch 160 \t Validation Loss: 25.51690519452095\n",
      "Epoch 9 \t Batch 180 \t Validation Loss: 29.88329986996121\n",
      "Epoch 9 \t Batch 200 \t Validation Loss: 31.76412362098694\n",
      "Epoch 9 \t Batch 220 \t Validation Loss: 33.558197324926205\n",
      "Epoch 9 \t Batch 240 \t Validation Loss: 34.37631827195485\n",
      "Epoch 9 \t Batch 260 \t Validation Loss: 36.71825259648836\n",
      "Epoch 9 \t Batch 280 \t Validation Loss: 37.98463338102613\n",
      "Epoch 9 \t Batch 300 \t Validation Loss: 39.361714111963906\n",
      "Epoch 9 \t Batch 320 \t Validation Loss: 39.993740662932396\n",
      "Epoch 9 \t Batch 340 \t Validation Loss: 39.980376134199254\n",
      "Epoch 9 \t Batch 360 \t Validation Loss: 39.93698827160729\n",
      "Epoch 9 \t Batch 380 \t Validation Loss: 40.20986132119831\n",
      "Epoch 9 \t Batch 400 \t Validation Loss: 39.7707618021965\n",
      "Epoch 9 \t Batch 420 \t Validation Loss: 39.76300285657247\n",
      "Epoch 9 \t Batch 440 \t Validation Loss: 39.39748424833471\n",
      "Epoch 9 \t Batch 460 \t Validation Loss: 39.567608163667764\n",
      "Epoch 9 \t Batch 480 \t Validation Loss: 40.02099735935529\n",
      "Epoch 9 \t Batch 500 \t Validation Loss: 39.73037079429626\n",
      "Epoch 9 \t Batch 520 \t Validation Loss: 39.45500683601086\n",
      "Epoch 9 \t Batch 540 \t Validation Loss: 39.16602896054586\n",
      "Epoch 9 \t Batch 560 \t Validation Loss: 38.911302850927626\n",
      "Epoch 9 \t Batch 580 \t Validation Loss: 38.62272771473589\n",
      "Epoch 9 \t Batch 600 \t Validation Loss: 38.805753156344096\n",
      "Epoch 9 Training Loss: 48.67040352940949 Validation Loss: 39.42196473053524\n",
      "Epoch 9 completed\n",
      "Epoch 10 \t Batch 20 \t Training Loss: 48.30882015228271\n",
      "Epoch 10 \t Batch 40 \t Training Loss: 48.61640558242798\n",
      "Epoch 10 \t Batch 60 \t Training Loss: 48.683493550618486\n",
      "Epoch 10 \t Batch 80 \t Training Loss: 48.47433967590332\n",
      "Epoch 10 \t Batch 100 \t Training Loss: 48.4025675201416\n",
      "Epoch 10 \t Batch 120 \t Training Loss: 48.42679951985677\n",
      "Epoch 10 \t Batch 140 \t Training Loss: 48.46324225834438\n",
      "Epoch 10 \t Batch 160 \t Training Loss: 48.28634521961212\n",
      "Epoch 10 \t Batch 180 \t Training Loss: 48.31888190375434\n",
      "Epoch 10 \t Batch 200 \t Training Loss: 48.31044208526611\n",
      "Epoch 10 \t Batch 220 \t Training Loss: 48.37834354747425\n",
      "Epoch 10 \t Batch 240 \t Training Loss: 48.30144707361857\n",
      "Epoch 10 \t Batch 260 \t Training Loss: 48.398018704927885\n",
      "Epoch 10 \t Batch 280 \t Training Loss: 48.37213550295149\n",
      "Epoch 10 \t Batch 300 \t Training Loss: 48.38202213287354\n",
      "Epoch 10 \t Batch 320 \t Training Loss: 48.49079122543335\n",
      "Epoch 10 \t Batch 340 \t Training Loss: 48.404741713579966\n",
      "Epoch 10 \t Batch 360 \t Training Loss: 48.53496363957723\n",
      "Epoch 10 \t Batch 380 \t Training Loss: 48.497083202161285\n",
      "Epoch 10 \t Batch 400 \t Training Loss: 48.52330192565918\n",
      "Epoch 10 \t Batch 420 \t Training Loss: 48.50473296755836\n",
      "Epoch 10 \t Batch 440 \t Training Loss: 48.56471787366\n",
      "Epoch 10 \t Batch 460 \t Training Loss: 48.53675419351329\n",
      "Epoch 10 \t Batch 480 \t Training Loss: 48.57460219860077\n",
      "Epoch 10 \t Batch 500 \t Training Loss: 48.538893005371094\n",
      "Epoch 10 \t Batch 520 \t Training Loss: 48.51945210970365\n",
      "Epoch 10 \t Batch 540 \t Training Loss: 48.4965152811121\n",
      "Epoch 10 \t Batch 560 \t Training Loss: 48.50513366290501\n",
      "Epoch 10 \t Batch 580 \t Training Loss: 48.47502156619368\n",
      "Epoch 10 \t Batch 600 \t Training Loss: 48.43185180028279\n",
      "Epoch 10 \t Batch 620 \t Training Loss: 48.43603086163921\n",
      "Epoch 10 \t Batch 640 \t Training Loss: 48.452337723970416\n",
      "Epoch 10 \t Batch 660 \t Training Loss: 48.479894973292495\n",
      "Epoch 10 \t Batch 680 \t Training Loss: 48.50708778044757\n",
      "Epoch 10 \t Batch 700 \t Training Loss: 48.454796033586774\n",
      "Epoch 10 \t Batch 720 \t Training Loss: 48.442910242080686\n",
      "Epoch 10 \t Batch 740 \t Training Loss: 48.47300947550181\n",
      "Epoch 10 \t Batch 760 \t Training Loss: 48.49107556092112\n",
      "Epoch 10 \t Batch 780 \t Training Loss: 48.50264388353397\n",
      "Epoch 10 \t Batch 800 \t Training Loss: 48.562747616767886\n",
      "Epoch 10 \t Batch 820 \t Training Loss: 48.56747878470072\n",
      "Epoch 10 \t Batch 840 \t Training Loss: 48.5775400797526\n",
      "Epoch 10 \t Batch 860 \t Training Loss: 48.578855403634\n",
      "Epoch 10 \t Batch 880 \t Training Loss: 48.54872582175515\n",
      "Epoch 10 \t Batch 900 \t Training Loss: 48.57457455529107\n",
      "Epoch 10 \t Batch 20 \t Validation Loss: 18.152754497528075\n",
      "Epoch 10 \t Batch 40 \t Validation Loss: 20.832267928123475\n",
      "Epoch 10 \t Batch 60 \t Validation Loss: 20.741328636805218\n",
      "Epoch 10 \t Batch 80 \t Validation Loss: 21.401031720638276\n",
      "Epoch 10 \t Batch 100 \t Validation Loss: 22.498493185043333\n",
      "Epoch 10 \t Batch 120 \t Validation Loss: 23.59250821272532\n",
      "Epoch 10 \t Batch 140 \t Validation Loss: 24.14548215866089\n",
      "Epoch 10 \t Batch 160 \t Validation Loss: 26.2964009642601\n",
      "Epoch 10 \t Batch 180 \t Validation Loss: 30.08036578496297\n",
      "Epoch 10 \t Batch 200 \t Validation Loss: 31.537931060791017\n",
      "Epoch 10 \t Batch 220 \t Validation Loss: 32.99890304045244\n",
      "Epoch 10 \t Batch 240 \t Validation Loss: 33.63890740076701\n",
      "Epoch 10 \t Batch 260 \t Validation Loss: 35.749970568143404\n",
      "Epoch 10 \t Batch 280 \t Validation Loss: 36.854065145765034\n",
      "Epoch 10 \t Batch 300 \t Validation Loss: 38.15191388448079\n",
      "Epoch 10 \t Batch 320 \t Validation Loss: 38.72836492657662\n",
      "Epoch 10 \t Batch 340 \t Validation Loss: 38.69958147722132\n",
      "Epoch 10 \t Batch 360 \t Validation Loss: 38.64451006783379\n",
      "Epoch 10 \t Batch 380 \t Validation Loss: 38.89519308491757\n",
      "Epoch 10 \t Batch 400 \t Validation Loss: 38.46529179096222\n",
      "Epoch 10 \t Batch 420 \t Validation Loss: 38.5038121405102\n",
      "Epoch 10 \t Batch 440 \t Validation Loss: 38.18954861380837\n",
      "Epoch 10 \t Batch 460 \t Validation Loss: 38.371928866013235\n",
      "Epoch 10 \t Batch 480 \t Validation Loss: 38.863831742604575\n",
      "Epoch 10 \t Batch 500 \t Validation Loss: 38.58248992919922\n",
      "Epoch 10 \t Batch 520 \t Validation Loss: 38.31460904341478\n",
      "Epoch 10 \t Batch 540 \t Validation Loss: 38.03835202323066\n",
      "Epoch 10 \t Batch 560 \t Validation Loss: 37.804528968674795\n",
      "Epoch 10 \t Batch 580 \t Validation Loss: 37.50802920111295\n",
      "Epoch 10 \t Batch 600 \t Validation Loss: 37.72058523178101\n",
      "Epoch 10 Training Loss: 48.5696130500885 Validation Loss: 38.354424761487294\n",
      "Epoch 10 completed\n",
      "Epoch 11 \t Batch 20 \t Training Loss: 47.47440128326416\n",
      "Epoch 11 \t Batch 40 \t Training Loss: 47.638087558746335\n",
      "Epoch 11 \t Batch 60 \t Training Loss: 47.36649233500163\n",
      "Epoch 11 \t Batch 80 \t Training Loss: 47.857253837585446\n",
      "Epoch 11 \t Batch 100 \t Training Loss: 48.13323783874512\n",
      "Epoch 11 \t Batch 120 \t Training Loss: 48.07066059112549\n",
      "Epoch 11 \t Batch 140 \t Training Loss: 48.40173906598773\n",
      "Epoch 11 \t Batch 160 \t Training Loss: 48.38626339435577\n",
      "Epoch 11 \t Batch 180 \t Training Loss: 48.25694461398655\n",
      "Epoch 11 \t Batch 200 \t Training Loss: 48.40096004486084\n",
      "Epoch 11 \t Batch 220 \t Training Loss: 48.32693587216464\n",
      "Epoch 11 \t Batch 240 \t Training Loss: 48.411868778864545\n",
      "Epoch 11 \t Batch 260 \t Training Loss: 48.41983853853666\n",
      "Epoch 11 \t Batch 280 \t Training Loss: 48.56055338723319\n",
      "Epoch 11 \t Batch 300 \t Training Loss: 48.59168025970459\n",
      "Epoch 11 \t Batch 320 \t Training Loss: 48.564004349708554\n",
      "Epoch 11 \t Batch 340 \t Training Loss: 48.465482162026795\n",
      "Epoch 11 \t Batch 360 \t Training Loss: 48.479427888658314\n",
      "Epoch 11 \t Batch 380 \t Training Loss: 48.54225338383725\n",
      "Epoch 11 \t Batch 400 \t Training Loss: 48.57764352798462\n",
      "Epoch 11 \t Batch 420 \t Training Loss: 48.598180480230425\n",
      "Epoch 11 \t Batch 440 \t Training Loss: 48.54133860848167\n",
      "Epoch 11 \t Batch 460 \t Training Loss: 48.56192363241445\n",
      "Epoch 11 \t Batch 480 \t Training Loss: 48.591564957300825\n",
      "Epoch 11 \t Batch 500 \t Training Loss: 48.58730495452881\n",
      "Epoch 11 \t Batch 520 \t Training Loss: 48.60412932175856\n",
      "Epoch 11 \t Batch 540 \t Training Loss: 48.567984333744754\n",
      "Epoch 11 \t Batch 560 \t Training Loss: 48.575960227421355\n",
      "Epoch 11 \t Batch 580 \t Training Loss: 48.51087993095661\n",
      "Epoch 11 \t Batch 600 \t Training Loss: 48.473631318410234\n",
      "Epoch 11 \t Batch 620 \t Training Loss: 48.44464717988045\n",
      "Epoch 11 \t Batch 640 \t Training Loss: 48.457147645950315\n",
      "Epoch 11 \t Batch 660 \t Training Loss: 48.43121566772461\n",
      "Epoch 11 \t Batch 680 \t Training Loss: 48.409752318438365\n",
      "Epoch 11 \t Batch 700 \t Training Loss: 48.39318673815046\n",
      "Epoch 11 \t Batch 720 \t Training Loss: 48.39046474562751\n",
      "Epoch 11 \t Batch 740 \t Training Loss: 48.42211343404409\n",
      "Epoch 11 \t Batch 760 \t Training Loss: 48.37233064551103\n",
      "Epoch 11 \t Batch 780 \t Training Loss: 48.38128307049091\n",
      "Epoch 11 \t Batch 800 \t Training Loss: 48.39652740001679\n",
      "Epoch 11 \t Batch 820 \t Training Loss: 48.3792676181328\n",
      "Epoch 11 \t Batch 840 \t Training Loss: 48.38437827428182\n",
      "Epoch 11 \t Batch 860 \t Training Loss: 48.40430144376533\n",
      "Epoch 11 \t Batch 880 \t Training Loss: 48.42946907823736\n",
      "Epoch 11 \t Batch 900 \t Training Loss: 48.43416119045681\n",
      "Epoch 11 \t Batch 20 \t Validation Loss: 22.79461030960083\n",
      "Epoch 11 \t Batch 40 \t Validation Loss: 24.29023320674896\n",
      "Epoch 11 \t Batch 60 \t Validation Loss: 24.426650190353392\n",
      "Epoch 11 \t Batch 80 \t Validation Loss: 24.291016280651093\n",
      "Epoch 11 \t Batch 100 \t Validation Loss: 24.83406765937805\n",
      "Epoch 11 \t Batch 120 \t Validation Loss: 25.576480929056803\n",
      "Epoch 11 \t Batch 140 \t Validation Loss: 25.79362497329712\n",
      "Epoch 11 \t Batch 160 \t Validation Loss: 27.696506643295287\n",
      "Epoch 11 \t Batch 180 \t Validation Loss: 31.443185726801556\n",
      "Epoch 11 \t Batch 200 \t Validation Loss: 32.971648297309876\n",
      "Epoch 11 \t Batch 220 \t Validation Loss: 34.42454693967646\n",
      "Epoch 11 \t Batch 240 \t Validation Loss: 34.94856951236725\n",
      "Epoch 11 \t Batch 260 \t Validation Loss: 37.11900235689603\n",
      "Epoch 11 \t Batch 280 \t Validation Loss: 38.288214036396575\n",
      "Epoch 11 \t Batch 300 \t Validation Loss: 39.46176928838094\n",
      "Epoch 11 \t Batch 320 \t Validation Loss: 39.94037102460861\n",
      "Epoch 11 \t Batch 340 \t Validation Loss: 39.853639883153576\n",
      "Epoch 11 \t Batch 360 \t Validation Loss: 39.684134931034514\n",
      "Epoch 11 \t Batch 380 \t Validation Loss: 39.911893460625095\n",
      "Epoch 11 \t Batch 400 \t Validation Loss: 39.43833158254623\n",
      "Epoch 11 \t Batch 420 \t Validation Loss: 39.41484484445481\n",
      "Epoch 11 \t Batch 440 \t Validation Loss: 39.05135629827326\n",
      "Epoch 11 \t Batch 460 \t Validation Loss: 39.21050779508508\n",
      "Epoch 11 \t Batch 480 \t Validation Loss: 39.65510424375534\n",
      "Epoch 11 \t Batch 500 \t Validation Loss: 39.33179188156128\n",
      "Epoch 11 \t Batch 520 \t Validation Loss: 39.041164917212264\n",
      "Epoch 11 \t Batch 540 \t Validation Loss: 38.7548390088258\n",
      "Epoch 11 \t Batch 560 \t Validation Loss: 38.49476927178247\n",
      "Epoch 11 \t Batch 580 \t Validation Loss: 38.15717975188946\n",
      "Epoch 11 \t Batch 600 \t Validation Loss: 38.35924312750498\n",
      "Epoch 11 Training Loss: 48.44781179230502 Validation Loss: 38.95219550968765\n",
      "Epoch 11 completed\n",
      "Epoch 12 \t Batch 20 \t Training Loss: 48.480526542663576\n",
      "Epoch 12 \t Batch 40 \t Training Loss: 48.431771469116214\n",
      "Epoch 12 \t Batch 60 \t Training Loss: 48.07377001444499\n",
      "Epoch 12 \t Batch 80 \t Training Loss: 48.715998315811156\n",
      "Epoch 12 \t Batch 100 \t Training Loss: 48.32647689819336\n",
      "Epoch 12 \t Batch 120 \t Training Loss: 48.04005661010742\n",
      "Epoch 12 \t Batch 140 \t Training Loss: 48.246248490469796\n",
      "Epoch 12 \t Batch 160 \t Training Loss: 48.4617681980133\n",
      "Epoch 12 \t Batch 180 \t Training Loss: 48.27389411926269\n",
      "Epoch 12 \t Batch 200 \t Training Loss: 48.1845682144165\n",
      "Epoch 12 \t Batch 220 \t Training Loss: 48.19290344931863\n",
      "Epoch 12 \t Batch 240 \t Training Loss: 48.27087348302205\n",
      "Epoch 12 \t Batch 260 \t Training Loss: 48.236113592294544\n",
      "Epoch 12 \t Batch 280 \t Training Loss: 48.200787339891704\n",
      "Epoch 12 \t Batch 300 \t Training Loss: 48.39390771230062\n",
      "Epoch 12 \t Batch 320 \t Training Loss: 48.49650603532791\n",
      "Epoch 12 \t Batch 340 \t Training Loss: 48.43019480985754\n",
      "Epoch 12 \t Batch 360 \t Training Loss: 48.409310266706676\n",
      "Epoch 12 \t Batch 380 \t Training Loss: 48.43639945983887\n",
      "Epoch 12 \t Batch 400 \t Training Loss: 48.35164384841919\n",
      "Epoch 12 \t Batch 420 \t Training Loss: 48.312092935471306\n",
      "Epoch 12 \t Batch 440 \t Training Loss: 48.27553925947709\n",
      "Epoch 12 \t Batch 460 \t Training Loss: 48.275819786735205\n",
      "Epoch 12 \t Batch 480 \t Training Loss: 48.31769696076711\n",
      "Epoch 12 \t Batch 500 \t Training Loss: 48.34066660308838\n",
      "Epoch 12 \t Batch 520 \t Training Loss: 48.31074441029475\n",
      "Epoch 12 \t Batch 540 \t Training Loss: 48.384713632089124\n",
      "Epoch 12 \t Batch 560 \t Training Loss: 48.35384914534433\n",
      "Epoch 12 \t Batch 580 \t Training Loss: 48.40781472962478\n",
      "Epoch 12 \t Batch 600 \t Training Loss: 48.41251328150431\n",
      "Epoch 12 \t Batch 620 \t Training Loss: 48.390402320123485\n",
      "Epoch 12 \t Batch 640 \t Training Loss: 48.38182485103607\n",
      "Epoch 12 \t Batch 660 \t Training Loss: 48.34962227561257\n",
      "Epoch 12 \t Batch 680 \t Training Loss: 48.33763013727525\n",
      "Epoch 12 \t Batch 700 \t Training Loss: 48.328969704764226\n",
      "Epoch 12 \t Batch 720 \t Training Loss: 48.32590803040399\n",
      "Epoch 12 \t Batch 740 \t Training Loss: 48.31371224377607\n",
      "Epoch 12 \t Batch 760 \t Training Loss: 48.335560211382415\n",
      "Epoch 12 \t Batch 780 \t Training Loss: 48.367232063489084\n",
      "Epoch 12 \t Batch 800 \t Training Loss: 48.34157493591309\n",
      "Epoch 12 \t Batch 820 \t Training Loss: 48.361731719970706\n",
      "Epoch 12 \t Batch 840 \t Training Loss: 48.37480047316778\n",
      "Epoch 12 \t Batch 860 \t Training Loss: 48.36805576501891\n",
      "Epoch 12 \t Batch 880 \t Training Loss: 48.37245734821666\n",
      "Epoch 12 \t Batch 900 \t Training Loss: 48.38437159220378\n",
      "Epoch 12 \t Batch 20 \t Validation Loss: 13.221112918853759\n",
      "Epoch 12 \t Batch 40 \t Validation Loss: 16.56211508512497\n",
      "Epoch 12 \t Batch 60 \t Validation Loss: 16.172555756568908\n",
      "Epoch 12 \t Batch 80 \t Validation Loss: 16.774841076135637\n",
      "Epoch 12 \t Batch 100 \t Validation Loss: 18.726605820655823\n",
      "Epoch 12 \t Batch 120 \t Validation Loss: 20.551767488320667\n",
      "Epoch 12 \t Batch 140 \t Validation Loss: 21.51711665221623\n",
      "Epoch 12 \t Batch 160 \t Validation Loss: 24.016322907805442\n",
      "Epoch 12 \t Batch 180 \t Validation Loss: 28.07934993373023\n",
      "Epoch 12 \t Batch 200 \t Validation Loss: 29.901447298526765\n",
      "Epoch 12 \t Batch 220 \t Validation Loss: 31.561909530379555\n",
      "Epoch 12 \t Batch 240 \t Validation Loss: 32.31778606375058\n",
      "Epoch 12 \t Batch 260 \t Validation Loss: 34.64444160644825\n",
      "Epoch 12 \t Batch 280 \t Validation Loss: 35.904536357947755\n",
      "Epoch 12 \t Batch 300 \t Validation Loss: 37.211726768811545\n",
      "Epoch 12 \t Batch 320 \t Validation Loss: 37.838729770481585\n",
      "Epoch 12 \t Batch 340 \t Validation Loss: 37.867718613848965\n",
      "Epoch 12 \t Batch 360 \t Validation Loss: 37.82226031223933\n",
      "Epoch 12 \t Batch 380 \t Validation Loss: 38.12853991483387\n",
      "Epoch 12 \t Batch 400 \t Validation Loss: 37.74571119427681\n",
      "Epoch 12 \t Batch 420 \t Validation Loss: 37.81829065708887\n",
      "Epoch 12 \t Batch 440 \t Validation Loss: 37.53481873707338\n",
      "Epoch 12 \t Batch 460 \t Validation Loss: 37.74978573944258\n",
      "Epoch 12 \t Batch 480 \t Validation Loss: 38.26814967095852\n",
      "Epoch 12 \t Batch 500 \t Validation Loss: 38.01726345539093\n",
      "Epoch 12 \t Batch 520 \t Validation Loss: 37.767338540920846\n",
      "Epoch 12 \t Batch 540 \t Validation Loss: 37.466818496033\n",
      "Epoch 12 \t Batch 560 \t Validation Loss: 37.24746808750289\n",
      "Epoch 12 \t Batch 580 \t Validation Loss: 36.98567991010074\n",
      "Epoch 12 \t Batch 600 \t Validation Loss: 37.16864361524582\n",
      "Epoch 12 Training Loss: 48.368161591474724 Validation Loss: 37.80517639510043\n",
      "Epoch 12 completed\n",
      "Epoch 13 \t Batch 20 \t Training Loss: 47.572980880737305\n",
      "Epoch 13 \t Batch 40 \t Training Loss: 48.016459941864014\n",
      "Epoch 13 \t Batch 60 \t Training Loss: 48.10441888173421\n",
      "Epoch 13 \t Batch 80 \t Training Loss: 48.454751586914064\n",
      "Epoch 13 \t Batch 100 \t Training Loss: 48.3298987197876\n",
      "Epoch 13 \t Batch 120 \t Training Loss: 48.31472953160604\n",
      "Epoch 13 \t Batch 140 \t Training Loss: 48.405169296264646\n",
      "Epoch 13 \t Batch 160 \t Training Loss: 48.494148468971254\n",
      "Epoch 13 \t Batch 180 \t Training Loss: 48.41140028635661\n",
      "Epoch 13 \t Batch 200 \t Training Loss: 48.428675193786624\n",
      "Epoch 13 \t Batch 220 \t Training Loss: 48.435221689397636\n",
      "Epoch 13 \t Batch 240 \t Training Loss: 48.27892295519511\n",
      "Epoch 13 \t Batch 260 \t Training Loss: 48.230817339970514\n",
      "Epoch 13 \t Batch 280 \t Training Loss: 48.27022909436907\n",
      "Epoch 13 \t Batch 300 \t Training Loss: 48.28884113311768\n",
      "Epoch 13 \t Batch 320 \t Training Loss: 48.321486341953275\n",
      "Epoch 13 \t Batch 340 \t Training Loss: 48.28060915329877\n",
      "Epoch 13 \t Batch 360 \t Training Loss: 48.19650100072225\n",
      "Epoch 13 \t Batch 380 \t Training Loss: 48.1925460213109\n",
      "Epoch 13 \t Batch 400 \t Training Loss: 48.127865352630614\n",
      "Epoch 13 \t Batch 420 \t Training Loss: 48.17852577936082\n",
      "Epoch 13 \t Batch 440 \t Training Loss: 48.09355333501642\n",
      "Epoch 13 \t Batch 460 \t Training Loss: 48.1585007874862\n",
      "Epoch 13 \t Batch 480 \t Training Loss: 48.22553356488546\n",
      "Epoch 13 \t Batch 500 \t Training Loss: 48.2732280960083\n",
      "Epoch 13 \t Batch 520 \t Training Loss: 48.31286733333881\n",
      "Epoch 13 \t Batch 540 \t Training Loss: 48.29711391307689\n",
      "Epoch 13 \t Batch 560 \t Training Loss: 48.29158124923706\n",
      "Epoch 13 \t Batch 580 \t Training Loss: 48.275810603437755\n",
      "Epoch 13 \t Batch 600 \t Training Loss: 48.23236841201782\n",
      "Epoch 13 \t Batch 620 \t Training Loss: 48.18407240836851\n",
      "Epoch 13 \t Batch 640 \t Training Loss: 48.26936515569687\n",
      "Epoch 13 \t Batch 660 \t Training Loss: 48.23710500543768\n",
      "Epoch 13 \t Batch 680 \t Training Loss: 48.23424237756168\n",
      "Epoch 13 \t Batch 700 \t Training Loss: 48.252816783360075\n",
      "Epoch 13 \t Batch 720 \t Training Loss: 48.234740278455945\n",
      "Epoch 13 \t Batch 740 \t Training Loss: 48.250001458863956\n",
      "Epoch 13 \t Batch 760 \t Training Loss: 48.27170238494873\n",
      "Epoch 13 \t Batch 780 \t Training Loss: 48.288776519971016\n",
      "Epoch 13 \t Batch 800 \t Training Loss: 48.312895808219906\n",
      "Epoch 13 \t Batch 820 \t Training Loss: 48.29741969224883\n",
      "Epoch 13 \t Batch 840 \t Training Loss: 48.31528344835554\n",
      "Epoch 13 \t Batch 860 \t Training Loss: 48.27296297383863\n",
      "Epoch 13 \t Batch 880 \t Training Loss: 48.27231474356218\n",
      "Epoch 13 \t Batch 900 \t Training Loss: 48.26239386664496\n",
      "Epoch 13 \t Batch 20 \t Validation Loss: 16.213587045669556\n",
      "Epoch 13 \t Batch 40 \t Validation Loss: 19.656278908252716\n",
      "Epoch 13 \t Batch 60 \t Validation Loss: 19.09059062798818\n",
      "Epoch 13 \t Batch 80 \t Validation Loss: 19.495848840475084\n",
      "Epoch 13 \t Batch 100 \t Validation Loss: 21.081193175315857\n",
      "Epoch 13 \t Batch 120 \t Validation Loss: 22.44139111439387\n",
      "Epoch 13 \t Batch 140 \t Validation Loss: 23.130024640900746\n",
      "Epoch 13 \t Batch 160 \t Validation Loss: 25.365171805024147\n",
      "Epoch 13 \t Batch 180 \t Validation Loss: 29.53791361120012\n",
      "Epoch 13 \t Batch 200 \t Validation Loss: 31.30188731431961\n",
      "Epoch 13 \t Batch 220 \t Validation Loss: 32.93108582279899\n",
      "Epoch 13 \t Batch 240 \t Validation Loss: 33.64036870598793\n",
      "Epoch 13 \t Batch 260 \t Validation Loss: 35.90212302758143\n",
      "Epoch 13 \t Batch 280 \t Validation Loss: 37.12715233904975\n",
      "Epoch 13 \t Batch 300 \t Validation Loss: 38.47230322360993\n",
      "Epoch 13 \t Batch 320 \t Validation Loss: 39.0717338129878\n",
      "Epoch 13 \t Batch 340 \t Validation Loss: 39.03487495394314\n",
      "Epoch 13 \t Batch 360 \t Validation Loss: 38.93892432186339\n",
      "Epoch 13 \t Batch 380 \t Validation Loss: 39.1821609886069\n",
      "Epoch 13 \t Batch 400 \t Validation Loss: 38.719859470129016\n",
      "Epoch 13 \t Batch 420 \t Validation Loss: 38.72209150564103\n",
      "Epoch 13 \t Batch 440 \t Validation Loss: 38.377778724106875\n",
      "Epoch 13 \t Batch 460 \t Validation Loss: 38.554344217673595\n",
      "Epoch 13 \t Batch 480 \t Validation Loss: 39.0199336240689\n",
      "Epoch 13 \t Batch 500 \t Validation Loss: 38.71496536922455\n",
      "Epoch 13 \t Batch 520 \t Validation Loss: 38.420076717780184\n",
      "Epoch 13 \t Batch 540 \t Validation Loss: 38.130228260711384\n",
      "Epoch 13 \t Batch 560 \t Validation Loss: 37.88357247199331\n",
      "Epoch 13 \t Batch 580 \t Validation Loss: 37.55553939506925\n",
      "Epoch 13 \t Batch 600 \t Validation Loss: 37.75557850599289\n",
      "Epoch 13 Training Loss: 48.27809341913221 Validation Loss: 38.342426759082\n",
      "Epoch 13 completed\n",
      "Epoch 14 \t Batch 20 \t Training Loss: 46.900823974609374\n",
      "Epoch 14 \t Batch 40 \t Training Loss: 47.855240154266355\n",
      "Epoch 14 \t Batch 60 \t Training Loss: 48.229880777994794\n",
      "Epoch 14 \t Batch 80 \t Training Loss: 47.91172204017639\n",
      "Epoch 14 \t Batch 100 \t Training Loss: 48.32074073791504\n",
      "Epoch 14 \t Batch 120 \t Training Loss: 48.26704800923665\n",
      "Epoch 14 \t Batch 140 \t Training Loss: 48.29084655216762\n",
      "Epoch 14 \t Batch 160 \t Training Loss: 48.332127809524536\n",
      "Epoch 14 \t Batch 180 \t Training Loss: 48.38979969024658\n",
      "Epoch 14 \t Batch 200 \t Training Loss: 48.40496723175049\n",
      "Epoch 14 \t Batch 220 \t Training Loss: 48.45414435646751\n",
      "Epoch 14 \t Batch 240 \t Training Loss: 48.35749991734823\n",
      "Epoch 14 \t Batch 260 \t Training Loss: 48.52559261322021\n",
      "Epoch 14 \t Batch 280 \t Training Loss: 48.60374134608677\n",
      "Epoch 14 \t Batch 300 \t Training Loss: 48.463969434102374\n",
      "Epoch 14 \t Batch 320 \t Training Loss: 48.45756837129593\n",
      "Epoch 14 \t Batch 340 \t Training Loss: 48.503622694576485\n",
      "Epoch 14 \t Batch 360 \t Training Loss: 48.446839565700955\n",
      "Epoch 14 \t Batch 380 \t Training Loss: 48.36856132306551\n",
      "Epoch 14 \t Batch 400 \t Training Loss: 48.37752935409546\n",
      "Epoch 14 \t Batch 420 \t Training Loss: 48.36926703680129\n",
      "Epoch 14 \t Batch 440 \t Training Loss: 48.303857352516864\n",
      "Epoch 14 \t Batch 460 \t Training Loss: 48.334513000819996\n",
      "Epoch 14 \t Batch 480 \t Training Loss: 48.345529961585996\n",
      "Epoch 14 \t Batch 500 \t Training Loss: 48.301285690307616\n",
      "Epoch 14 \t Batch 520 \t Training Loss: 48.22693848976722\n",
      "Epoch 14 \t Batch 540 \t Training Loss: 48.18666613543475\n",
      "Epoch 14 \t Batch 560 \t Training Loss: 48.17999397005354\n",
      "Epoch 14 \t Batch 580 \t Training Loss: 48.181900333536085\n",
      "Epoch 14 \t Batch 600 \t Training Loss: 48.226604563395185\n",
      "Epoch 14 \t Batch 620 \t Training Loss: 48.20560222748787\n",
      "Epoch 14 \t Batch 640 \t Training Loss: 48.21664627194404\n",
      "Epoch 14 \t Batch 660 \t Training Loss: 48.22646528301817\n",
      "Epoch 14 \t Batch 680 \t Training Loss: 48.25888013839722\n",
      "Epoch 14 \t Batch 700 \t Training Loss: 48.26667895725795\n",
      "Epoch 14 \t Batch 720 \t Training Loss: 48.29133491516113\n",
      "Epoch 14 \t Batch 740 \t Training Loss: 48.28009317243421\n",
      "Epoch 14 \t Batch 760 \t Training Loss: 48.256549820147065\n",
      "Epoch 14 \t Batch 780 \t Training Loss: 48.251155867943396\n",
      "Epoch 14 \t Batch 800 \t Training Loss: 48.268447723388675\n",
      "Epoch 14 \t Batch 820 \t Training Loss: 48.281884109683155\n",
      "Epoch 14 \t Batch 840 \t Training Loss: 48.25547393617176\n",
      "Epoch 14 \t Batch 860 \t Training Loss: 48.221370022795924\n",
      "Epoch 14 \t Batch 880 \t Training Loss: 48.212687223607844\n",
      "Epoch 14 \t Batch 900 \t Training Loss: 48.206861686706546\n",
      "Epoch 14 \t Batch 20 \t Validation Loss: 19.912670707702638\n",
      "Epoch 14 \t Batch 40 \t Validation Loss: 22.99764738082886\n",
      "Epoch 14 \t Batch 60 \t Validation Loss: 22.523303683598836\n",
      "Epoch 14 \t Batch 80 \t Validation Loss: 22.918306159973145\n",
      "Epoch 14 \t Batch 100 \t Validation Loss: 23.77160343170166\n",
      "Epoch 14 \t Batch 120 \t Validation Loss: 24.646891315778095\n",
      "Epoch 14 \t Batch 140 \t Validation Loss: 25.043420287540982\n",
      "Epoch 14 \t Batch 160 \t Validation Loss: 27.27812329530716\n",
      "Epoch 14 \t Batch 180 \t Validation Loss: 31.523674085405137\n",
      "Epoch 14 \t Batch 200 \t Validation Loss: 33.3018919467926\n",
      "Epoch 14 \t Batch 220 \t Validation Loss: 34.95584831237793\n",
      "Epoch 14 \t Batch 240 \t Validation Loss: 35.6640761256218\n",
      "Epoch 14 \t Batch 260 \t Validation Loss: 37.9395470729241\n",
      "Epoch 14 \t Batch 280 \t Validation Loss: 39.054615415845596\n",
      "Epoch 14 \t Batch 300 \t Validation Loss: 40.4699207051595\n",
      "Epoch 14 \t Batch 320 \t Validation Loss: 41.08511999845505\n",
      "Epoch 14 \t Batch 340 \t Validation Loss: 41.00483149921193\n",
      "Epoch 14 \t Batch 360 \t Validation Loss: 40.874482811821835\n",
      "Epoch 14 \t Batch 380 \t Validation Loss: 41.08471646559866\n",
      "Epoch 14 \t Batch 400 \t Validation Loss: 40.597012333869934\n",
      "Epoch 14 \t Batch 420 \t Validation Loss: 40.549804321924846\n",
      "Epoch 14 \t Batch 440 \t Validation Loss: 40.187400663982736\n",
      "Epoch 14 \t Batch 460 \t Validation Loss: 40.339270436245464\n",
      "Epoch 14 \t Batch 480 \t Validation Loss: 40.78242705066999\n",
      "Epoch 14 \t Batch 500 \t Validation Loss: 40.45909951972961\n",
      "Epoch 14 \t Batch 520 \t Validation Loss: 40.17904535256899\n",
      "Epoch 14 \t Batch 540 \t Validation Loss: 39.86137532658047\n",
      "Epoch 14 \t Batch 560 \t Validation Loss: 39.59425142662866\n",
      "Epoch 14 \t Batch 580 \t Validation Loss: 39.32010201585704\n",
      "Epoch 14 \t Batch 600 \t Validation Loss: 39.46188175360362\n",
      "Epoch 14 Training Loss: 48.181140550220285 Validation Loss: 40.089784406996394\n",
      "Epoch 14 completed\n",
      "Epoch 15 \t Batch 20 \t Training Loss: 47.58076400756836\n",
      "Epoch 15 \t Batch 40 \t Training Loss: 46.78455924987793\n",
      "Epoch 15 \t Batch 60 \t Training Loss: 47.25778071085612\n",
      "Epoch 15 \t Batch 80 \t Training Loss: 47.58682403564453\n",
      "Epoch 15 \t Batch 100 \t Training Loss: 47.25767524719238\n",
      "Epoch 15 \t Batch 120 \t Training Loss: 47.461699676513675\n",
      "Epoch 15 \t Batch 140 \t Training Loss: 47.378027370997835\n",
      "Epoch 15 \t Batch 160 \t Training Loss: 47.52762670516968\n",
      "Epoch 15 \t Batch 180 \t Training Loss: 47.573487514919705\n",
      "Epoch 15 \t Batch 200 \t Training Loss: 47.74308656692505\n",
      "Epoch 15 \t Batch 220 \t Training Loss: 47.72546827142889\n",
      "Epoch 15 \t Batch 240 \t Training Loss: 47.79904217720032\n",
      "Epoch 15 \t Batch 260 \t Training Loss: 47.95354204911452\n",
      "Epoch 15 \t Batch 280 \t Training Loss: 47.98184811728341\n",
      "Epoch 15 \t Batch 300 \t Training Loss: 47.910819155375165\n",
      "Epoch 15 \t Batch 320 \t Training Loss: 47.95305480957031\n",
      "Epoch 15 \t Batch 340 \t Training Loss: 47.975261564815746\n",
      "Epoch 15 \t Batch 360 \t Training Loss: 48.02646928363376\n",
      "Epoch 15 \t Batch 380 \t Training Loss: 47.9236857464439\n",
      "Epoch 15 \t Batch 400 \t Training Loss: 47.87086453437805\n",
      "Epoch 15 \t Batch 420 \t Training Loss: 47.85598864782424\n",
      "Epoch 15 \t Batch 440 \t Training Loss: 47.86957562186501\n",
      "Epoch 15 \t Batch 460 \t Training Loss: 47.83411171954611\n",
      "Epoch 15 \t Batch 480 \t Training Loss: 47.82569092114766\n",
      "Epoch 15 \t Batch 500 \t Training Loss: 47.874219345092776\n",
      "Epoch 15 \t Batch 520 \t Training Loss: 47.919139289855956\n",
      "Epoch 15 \t Batch 540 \t Training Loss: 47.91058445683232\n",
      "Epoch 15 \t Batch 560 \t Training Loss: 47.96899996485029\n",
      "Epoch 15 \t Batch 580 \t Training Loss: 47.96093815770642\n",
      "Epoch 15 \t Batch 600 \t Training Loss: 47.9804368464152\n",
      "Epoch 15 \t Batch 620 \t Training Loss: 47.9815220309842\n",
      "Epoch 15 \t Batch 640 \t Training Loss: 47.991655844449994\n",
      "Epoch 15 \t Batch 660 \t Training Loss: 47.97834514271129\n",
      "Epoch 15 \t Batch 680 \t Training Loss: 48.036405125786274\n",
      "Epoch 15 \t Batch 700 \t Training Loss: 47.98833255767822\n",
      "Epoch 15 \t Batch 720 \t Training Loss: 48.013697693083024\n",
      "Epoch 15 \t Batch 740 \t Training Loss: 48.034572632248334\n",
      "Epoch 15 \t Batch 760 \t Training Loss: 48.03639326095581\n",
      "Epoch 15 \t Batch 780 \t Training Loss: 48.062004250746504\n",
      "Epoch 15 \t Batch 800 \t Training Loss: 48.06255609035492\n",
      "Epoch 15 \t Batch 820 \t Training Loss: 48.09917843051073\n",
      "Epoch 15 \t Batch 840 \t Training Loss: 48.10085314796085\n",
      "Epoch 15 \t Batch 860 \t Training Loss: 48.11882858719937\n",
      "Epoch 15 \t Batch 880 \t Training Loss: 48.1320736321536\n",
      "Epoch 15 \t Batch 900 \t Training Loss: 48.10434150271946\n",
      "Epoch 15 \t Batch 20 \t Validation Loss: 15.55880880355835\n",
      "Epoch 15 \t Batch 40 \t Validation Loss: 19.77278107404709\n",
      "Epoch 15 \t Batch 60 \t Validation Loss: 19.030643232663472\n",
      "Epoch 15 \t Batch 80 \t Validation Loss: 19.27765459418297\n",
      "Epoch 15 \t Batch 100 \t Validation Loss: 20.900170969963074\n",
      "Epoch 15 \t Batch 120 \t Validation Loss: 22.307677718003593\n",
      "Epoch 15 \t Batch 140 \t Validation Loss: 23.095477298327854\n",
      "Epoch 15 \t Batch 160 \t Validation Loss: 25.370184674859047\n",
      "Epoch 15 \t Batch 180 \t Validation Loss: 29.461600677172342\n",
      "Epoch 15 \t Batch 200 \t Validation Loss: 31.216462767124177\n",
      "Epoch 15 \t Batch 220 \t Validation Loss: 32.78525326685472\n",
      "Epoch 15 \t Batch 240 \t Validation Loss: 33.48898692329725\n",
      "Epoch 15 \t Batch 260 \t Validation Loss: 35.736121832407434\n",
      "Epoch 15 \t Batch 280 \t Validation Loss: 36.95086574384144\n",
      "Epoch 15 \t Batch 300 \t Validation Loss: 38.262680630683896\n",
      "Epoch 15 \t Batch 320 \t Validation Loss: 38.867034529149535\n",
      "Epoch 15 \t Batch 340 \t Validation Loss: 38.847138094902036\n",
      "Epoch 15 \t Batch 360 \t Validation Loss: 38.76666951047049\n",
      "Epoch 15 \t Batch 380 \t Validation Loss: 39.032743442685984\n",
      "Epoch 15 \t Batch 400 \t Validation Loss: 38.63904105067253\n",
      "Epoch 15 \t Batch 420 \t Validation Loss: 38.65817182064056\n",
      "Epoch 15 \t Batch 440 \t Validation Loss: 38.36751946427605\n",
      "Epoch 15 \t Batch 460 \t Validation Loss: 38.586042063132574\n",
      "Epoch 15 \t Batch 480 \t Validation Loss: 39.06081426044305\n",
      "Epoch 15 \t Batch 500 \t Validation Loss: 38.77207853984833\n",
      "Epoch 15 \t Batch 520 \t Validation Loss: 38.545766306840456\n",
      "Epoch 15 \t Batch 540 \t Validation Loss: 38.30428333370774\n",
      "Epoch 15 \t Batch 560 \t Validation Loss: 38.08844841974122\n",
      "Epoch 15 \t Batch 580 \t Validation Loss: 37.871062960295845\n",
      "Epoch 15 \t Batch 600 \t Validation Loss: 38.07084855794906\n",
      "Epoch 15 Training Loss: 48.10052028477127 Validation Loss: 38.71677372440115\n",
      "Epoch 15 completed\n",
      "Epoch 16 \t Batch 20 \t Training Loss: 47.288803291320804\n",
      "Epoch 16 \t Batch 40 \t Training Loss: 46.96865386962891\n",
      "Epoch 16 \t Batch 60 \t Training Loss: 46.98094730377197\n",
      "Epoch 16 \t Batch 80 \t Training Loss: 47.04463782310486\n",
      "Epoch 16 \t Batch 100 \t Training Loss: 47.2679923248291\n",
      "Epoch 16 \t Batch 120 \t Training Loss: 47.40980342229207\n",
      "Epoch 16 \t Batch 140 \t Training Loss: 47.61103030613491\n",
      "Epoch 16 \t Batch 160 \t Training Loss: 47.657113790512085\n",
      "Epoch 16 \t Batch 180 \t Training Loss: 47.744700029161244\n",
      "Epoch 16 \t Batch 200 \t Training Loss: 47.800326633453366\n",
      "Epoch 16 \t Batch 220 \t Training Loss: 47.88762533014471\n",
      "Epoch 16 \t Batch 240 \t Training Loss: 47.88103391329447\n",
      "Epoch 16 \t Batch 260 \t Training Loss: 47.992567135737495\n",
      "Epoch 16 \t Batch 280 \t Training Loss: 48.04436276299613\n",
      "Epoch 16 \t Batch 300 \t Training Loss: 48.05772872924805\n",
      "Epoch 16 \t Batch 320 \t Training Loss: 48.05171662569046\n",
      "Epoch 16 \t Batch 340 \t Training Loss: 48.023594497231876\n",
      "Epoch 16 \t Batch 360 \t Training Loss: 48.08019193013509\n",
      "Epoch 16 \t Batch 380 \t Training Loss: 48.03038857108668\n",
      "Epoch 16 \t Batch 400 \t Training Loss: 48.071308336257935\n",
      "Epoch 16 \t Batch 420 \t Training Loss: 48.13816876184373\n",
      "Epoch 16 \t Batch 440 \t Training Loss: 48.180840917067094\n",
      "Epoch 16 \t Batch 460 \t Training Loss: 48.1514728711999\n",
      "Epoch 16 \t Batch 480 \t Training Loss: 48.136790760358174\n",
      "Epoch 16 \t Batch 500 \t Training Loss: 48.12161904144287\n",
      "Epoch 16 \t Batch 520 \t Training Loss: 48.138822804964505\n",
      "Epoch 16 \t Batch 540 \t Training Loss: 48.110821900544344\n",
      "Epoch 16 \t Batch 560 \t Training Loss: 48.119272913251606\n",
      "Epoch 16 \t Batch 580 \t Training Loss: 48.10623560280636\n",
      "Epoch 16 \t Batch 600 \t Training Loss: 48.07488384246826\n",
      "Epoch 16 \t Batch 620 \t Training Loss: 48.09721691377701\n",
      "Epoch 16 \t Batch 640 \t Training Loss: 48.069110721349716\n",
      "Epoch 16 \t Batch 660 \t Training Loss: 48.07597190394546\n",
      "Epoch 16 \t Batch 680 \t Training Loss: 48.06108221166274\n",
      "Epoch 16 \t Batch 700 \t Training Loss: 48.12722194671631\n",
      "Epoch 16 \t Batch 720 \t Training Loss: 48.124139383104115\n",
      "Epoch 16 \t Batch 740 \t Training Loss: 48.0824150446299\n",
      "Epoch 16 \t Batch 760 \t Training Loss: 48.04926145453202\n",
      "Epoch 16 \t Batch 780 \t Training Loss: 48.04527920453976\n",
      "Epoch 16 \t Batch 800 \t Training Loss: 48.0690568113327\n",
      "Epoch 16 \t Batch 820 \t Training Loss: 48.09961758125119\n",
      "Epoch 16 \t Batch 840 \t Training Loss: 48.05726786113921\n",
      "Epoch 16 \t Batch 860 \t Training Loss: 48.06353871101557\n",
      "Epoch 16 \t Batch 880 \t Training Loss: 48.03367957201871\n",
      "Epoch 16 \t Batch 900 \t Training Loss: 48.00615042368571\n",
      "Epoch 16 \t Batch 20 \t Validation Loss: 16.165450239181517\n",
      "Epoch 16 \t Batch 40 \t Validation Loss: 20.26899070739746\n",
      "Epoch 16 \t Batch 60 \t Validation Loss: 19.43787077267965\n",
      "Epoch 16 \t Batch 80 \t Validation Loss: 20.15525723695755\n",
      "Epoch 16 \t Batch 100 \t Validation Loss: 21.54570580482483\n",
      "Epoch 16 \t Batch 120 \t Validation Loss: 22.751726341247558\n",
      "Epoch 16 \t Batch 140 \t Validation Loss: 23.450089318411692\n",
      "Epoch 16 \t Batch 160 \t Validation Loss: 25.787584793567657\n",
      "Epoch 16 \t Batch 180 \t Validation Loss: 29.972438102298312\n",
      "Epoch 16 \t Batch 200 \t Validation Loss: 31.770213594436644\n",
      "Epoch 16 \t Batch 220 \t Validation Loss: 33.40135224082253\n",
      "Epoch 16 \t Batch 240 \t Validation Loss: 34.14459119240443\n",
      "Epoch 16 \t Batch 260 \t Validation Loss: 36.47281829393827\n",
      "Epoch 16 \t Batch 280 \t Validation Loss: 37.68933852059501\n",
      "Epoch 16 \t Batch 300 \t Validation Loss: 39.03092912038167\n",
      "Epoch 16 \t Batch 320 \t Validation Loss: 39.63415230512619\n",
      "Epoch 16 \t Batch 340 \t Validation Loss: 39.58849334716797\n",
      "Epoch 16 \t Batch 360 \t Validation Loss: 39.51378985775842\n",
      "Epoch 16 \t Batch 380 \t Validation Loss: 39.78061796238548\n",
      "Epoch 16 \t Batch 400 \t Validation Loss: 39.3567887377739\n",
      "Epoch 16 \t Batch 420 \t Validation Loss: 39.31881350562686\n",
      "Epoch 16 \t Batch 440 \t Validation Loss: 38.98774555813183\n",
      "Epoch 16 \t Batch 460 \t Validation Loss: 39.15539380778437\n",
      "Epoch 16 \t Batch 480 \t Validation Loss: 39.63201415538788\n",
      "Epoch 16 \t Batch 500 \t Validation Loss: 39.320781414031984\n",
      "Epoch 16 \t Batch 520 \t Validation Loss: 39.094214430222145\n",
      "Epoch 16 \t Batch 540 \t Validation Loss: 38.868943205586184\n",
      "Epoch 16 \t Batch 560 \t Validation Loss: 38.699585436071665\n",
      "Epoch 16 \t Batch 580 \t Validation Loss: 38.58693707071502\n",
      "Epoch 16 \t Batch 600 \t Validation Loss: 38.79892077922821\n",
      "Epoch 16 Training Loss: 48.038308970410796 Validation Loss: 39.468822694444036\n",
      "Epoch 16 completed\n",
      "Epoch 17 \t Batch 20 \t Training Loss: 47.849956321716306\n",
      "Epoch 17 \t Batch 40 \t Training Loss: 47.766967964172366\n",
      "Epoch 17 \t Batch 60 \t Training Loss: 47.74569396972656\n",
      "Epoch 17 \t Batch 80 \t Training Loss: 47.889486122131345\n",
      "Epoch 17 \t Batch 100 \t Training Loss: 47.97831687927246\n",
      "Epoch 17 \t Batch 120 \t Training Loss: 47.89075921376546\n",
      "Epoch 17 \t Batch 140 \t Training Loss: 48.05707152230399\n",
      "Epoch 17 \t Batch 160 \t Training Loss: 47.99459636211395\n",
      "Epoch 17 \t Batch 180 \t Training Loss: 48.05770123799642\n",
      "Epoch 17 \t Batch 200 \t Training Loss: 48.15685241699219\n",
      "Epoch 17 \t Batch 220 \t Training Loss: 48.147471219843084\n",
      "Epoch 17 \t Batch 240 \t Training Loss: 48.22072679201762\n",
      "Epoch 17 \t Batch 260 \t Training Loss: 48.12403180049016\n",
      "Epoch 17 \t Batch 280 \t Training Loss: 48.11291555677141\n",
      "Epoch 17 \t Batch 300 \t Training Loss: 48.00219332377116\n",
      "Epoch 17 \t Batch 320 \t Training Loss: 47.96229919195175\n",
      "Epoch 17 \t Batch 340 \t Training Loss: 47.8958854563096\n",
      "Epoch 17 \t Batch 360 \t Training Loss: 47.86551107830471\n",
      "Epoch 17 \t Batch 380 \t Training Loss: 47.922862765663545\n",
      "Epoch 17 \t Batch 400 \t Training Loss: 47.87724104881286\n",
      "Epoch 17 \t Batch 420 \t Training Loss: 47.87478086380732\n",
      "Epoch 17 \t Batch 440 \t Training Loss: 47.88159411170266\n",
      "Epoch 17 \t Batch 460 \t Training Loss: 47.81078271451204\n",
      "Epoch 17 \t Batch 480 \t Training Loss: 47.81426083246867\n",
      "Epoch 17 \t Batch 500 \t Training Loss: 47.77481881713867\n",
      "Epoch 17 \t Batch 520 \t Training Loss: 47.803985984508806\n",
      "Epoch 17 \t Batch 540 \t Training Loss: 47.80118842654758\n",
      "Epoch 17 \t Batch 560 \t Training Loss: 47.823377050672256\n",
      "Epoch 17 \t Batch 580 \t Training Loss: 47.86586832506903\n",
      "Epoch 17 \t Batch 600 \t Training Loss: 47.85801700592041\n",
      "Epoch 17 \t Batch 620 \t Training Loss: 47.89255248654273\n",
      "Epoch 17 \t Batch 640 \t Training Loss: 47.960515832901\n",
      "Epoch 17 \t Batch 660 \t Training Loss: 47.90990776293206\n",
      "Epoch 17 \t Batch 680 \t Training Loss: 47.94940725214341\n",
      "Epoch 17 \t Batch 700 \t Training Loss: 47.948700784955705\n",
      "Epoch 17 \t Batch 720 \t Training Loss: 47.9546173148685\n",
      "Epoch 17 \t Batch 740 \t Training Loss: 47.958713485099175\n",
      "Epoch 17 \t Batch 760 \t Training Loss: 47.907651514756054\n",
      "Epoch 17 \t Batch 780 \t Training Loss: 47.89384558751033\n",
      "Epoch 17 \t Batch 800 \t Training Loss: 47.880963468551634\n",
      "Epoch 17 \t Batch 820 \t Training Loss: 47.892530124943434\n",
      "Epoch 17 \t Batch 840 \t Training Loss: 47.89253535951887\n",
      "Epoch 17 \t Batch 860 \t Training Loss: 47.93694078312364\n",
      "Epoch 17 \t Batch 880 \t Training Loss: 47.97765606966885\n",
      "Epoch 17 \t Batch 900 \t Training Loss: 47.96183132595486\n",
      "Epoch 17 \t Batch 20 \t Validation Loss: 18.115143823623658\n",
      "Epoch 17 \t Batch 40 \t Validation Loss: 21.297126626968385\n",
      "Epoch 17 \t Batch 60 \t Validation Loss: 20.866172393163044\n",
      "Epoch 17 \t Batch 80 \t Validation Loss: 21.14830837249756\n",
      "Epoch 17 \t Batch 100 \t Validation Loss: 22.471067848205568\n",
      "Epoch 17 \t Batch 120 \t Validation Loss: 23.63141485850016\n",
      "Epoch 17 \t Batch 140 \t Validation Loss: 24.20583144596645\n",
      "Epoch 17 \t Batch 160 \t Validation Loss: 26.434824550151824\n",
      "Epoch 17 \t Batch 180 \t Validation Loss: 30.438973675833807\n",
      "Epoch 17 \t Batch 200 \t Validation Loss: 32.1478218793869\n",
      "Epoch 17 \t Batch 220 \t Validation Loss: 33.794677660681984\n",
      "Epoch 17 \t Batch 240 \t Validation Loss: 34.491066034634905\n",
      "Epoch 17 \t Batch 260 \t Validation Loss: 36.82457248981182\n",
      "Epoch 17 \t Batch 280 \t Validation Loss: 38.10275875840868\n",
      "Epoch 17 \t Batch 300 \t Validation Loss: 39.2730273660024\n",
      "Epoch 17 \t Batch 320 \t Validation Loss: 39.81785407364369\n",
      "Epoch 17 \t Batch 340 \t Validation Loss: 39.75991187095642\n",
      "Epoch 17 \t Batch 360 \t Validation Loss: 39.62587885326809\n",
      "Epoch 17 \t Batch 380 \t Validation Loss: 39.8928357827036\n",
      "Epoch 17 \t Batch 400 \t Validation Loss: 39.42555709838867\n",
      "Epoch 17 \t Batch 420 \t Validation Loss: 39.40871740522839\n",
      "Epoch 17 \t Batch 440 \t Validation Loss: 39.08199625882236\n",
      "Epoch 17 \t Batch 460 \t Validation Loss: 39.26612519388613\n",
      "Epoch 17 \t Batch 480 \t Validation Loss: 39.711999678611754\n",
      "Epoch 17 \t Batch 500 \t Validation Loss: 39.40597204208374\n",
      "Epoch 17 \t Batch 520 \t Validation Loss: 39.11900151692904\n",
      "Epoch 17 \t Batch 540 \t Validation Loss: 38.846157232920326\n",
      "Epoch 17 \t Batch 560 \t Validation Loss: 38.61698387690953\n",
      "Epoch 17 \t Batch 580 \t Validation Loss: 38.32410883081371\n",
      "Epoch 17 \t Batch 600 \t Validation Loss: 38.509164079030356\n",
      "Epoch 17 Training Loss: 47.977622565682346 Validation Loss: 39.10677278815926\n",
      "Epoch 17 completed\n",
      "Epoch 18 \t Batch 20 \t Training Loss: 49.155300331115725\n",
      "Epoch 18 \t Batch 40 \t Training Loss: 48.281379318237306\n",
      "Epoch 18 \t Batch 60 \t Training Loss: 48.476268196105956\n",
      "Epoch 18 \t Batch 80 \t Training Loss: 48.35196666717529\n",
      "Epoch 18 \t Batch 100 \t Training Loss: 48.1548747253418\n",
      "Epoch 18 \t Batch 120 \t Training Loss: 48.27903385162354\n",
      "Epoch 18 \t Batch 140 \t Training Loss: 48.391453688485285\n",
      "Epoch 18 \t Batch 160 \t Training Loss: 48.3727703332901\n",
      "Epoch 18 \t Batch 180 \t Training Loss: 48.16569324069553\n",
      "Epoch 18 \t Batch 200 \t Training Loss: 48.040006275177\n",
      "Epoch 18 \t Batch 220 \t Training Loss: 48.06321783932773\n",
      "Epoch 18 \t Batch 240 \t Training Loss: 48.10960442225139\n",
      "Epoch 18 \t Batch 260 \t Training Loss: 47.95189701960637\n",
      "Epoch 18 \t Batch 280 \t Training Loss: 47.98761555807931\n",
      "Epoch 18 \t Batch 300 \t Training Loss: 48.185715738932295\n",
      "Epoch 18 \t Batch 320 \t Training Loss: 48.23754036426544\n",
      "Epoch 18 \t Batch 340 \t Training Loss: 48.205750644908235\n",
      "Epoch 18 \t Batch 360 \t Training Loss: 48.16494160758124\n",
      "Epoch 18 \t Batch 380 \t Training Loss: 48.07183699356882\n",
      "Epoch 18 \t Batch 400 \t Training Loss: 48.03387315750122\n",
      "Epoch 18 \t Batch 420 \t Training Loss: 47.990194656735376\n",
      "Epoch 18 \t Batch 440 \t Training Loss: 47.948415184020995\n",
      "Epoch 18 \t Batch 460 \t Training Loss: 47.87585332289986\n",
      "Epoch 18 \t Batch 480 \t Training Loss: 47.822549724578856\n",
      "Epoch 18 \t Batch 500 \t Training Loss: 47.76590166473389\n",
      "Epoch 18 \t Batch 520 \t Training Loss: 47.754519015092114\n",
      "Epoch 18 \t Batch 540 \t Training Loss: 47.79663402416088\n",
      "Epoch 18 \t Batch 560 \t Training Loss: 47.83571934700012\n",
      "Epoch 18 \t Batch 580 \t Training Loss: 47.86187754663928\n",
      "Epoch 18 \t Batch 600 \t Training Loss: 47.83470948537191\n",
      "Epoch 18 \t Batch 620 \t Training Loss: 47.84707794804727\n",
      "Epoch 18 \t Batch 640 \t Training Loss: 47.84011477828026\n",
      "Epoch 18 \t Batch 660 \t Training Loss: 47.857784594911514\n",
      "Epoch 18 \t Batch 680 \t Training Loss: 47.85577632118674\n",
      "Epoch 18 \t Batch 700 \t Training Loss: 47.795471856253485\n",
      "Epoch 18 \t Batch 720 \t Training Loss: 47.76903377109104\n",
      "Epoch 18 \t Batch 740 \t Training Loss: 47.767535926200246\n",
      "Epoch 18 \t Batch 760 \t Training Loss: 47.767551141036186\n",
      "Epoch 18 \t Batch 780 \t Training Loss: 47.78228852687738\n",
      "Epoch 18 \t Batch 800 \t Training Loss: 47.82110888004303\n",
      "Epoch 18 \t Batch 820 \t Training Loss: 47.82434084822492\n",
      "Epoch 18 \t Batch 840 \t Training Loss: 47.827316838219055\n",
      "Epoch 18 \t Batch 860 \t Training Loss: 47.83783766280773\n",
      "Epoch 18 \t Batch 880 \t Training Loss: 47.86151478073814\n",
      "Epoch 18 \t Batch 900 \t Training Loss: 47.877930590311685\n",
      "Epoch 18 \t Batch 20 \t Validation Loss: 26.984204149246217\n",
      "Epoch 18 \t Batch 40 \t Validation Loss: 29.36853082180023\n",
      "Epoch 18 \t Batch 60 \t Validation Loss: 29.22356530825297\n",
      "Epoch 18 \t Batch 80 \t Validation Loss: 29.754844033718108\n",
      "Epoch 18 \t Batch 100 \t Validation Loss: 29.37006127357483\n",
      "Epoch 18 \t Batch 120 \t Validation Loss: 29.329754082361855\n",
      "Epoch 18 \t Batch 140 \t Validation Loss: 29.119047233036586\n",
      "Epoch 18 \t Batch 160 \t Validation Loss: 30.61798859834671\n",
      "Epoch 18 \t Batch 180 \t Validation Loss: 33.471193393071495\n",
      "Epoch 18 \t Batch 200 \t Validation Loss: 34.392878613471986\n",
      "Epoch 18 \t Batch 220 \t Validation Loss: 35.263281445069744\n",
      "Epoch 18 \t Batch 240 \t Validation Loss: 35.41430083910624\n",
      "Epoch 18 \t Batch 260 \t Validation Loss: 37.071073950254004\n",
      "Epoch 18 \t Batch 280 \t Validation Loss: 37.767569582802906\n",
      "Epoch 18 \t Batch 300 \t Validation Loss: 38.824154179890954\n",
      "Epoch 18 \t Batch 320 \t Validation Loss: 39.15808790326118\n",
      "Epoch 18 \t Batch 340 \t Validation Loss: 39.0476651023416\n",
      "Epoch 18 \t Batch 360 \t Validation Loss: 38.944684982299805\n",
      "Epoch 18 \t Batch 380 \t Validation Loss: 39.0819077843114\n",
      "Epoch 18 \t Batch 400 \t Validation Loss: 38.68335029602051\n",
      "Epoch 18 \t Batch 420 \t Validation Loss: 38.72674443835304\n",
      "Epoch 18 \t Batch 440 \t Validation Loss: 38.457486967606975\n",
      "Epoch 18 \t Batch 460 \t Validation Loss: 38.697351447395654\n",
      "Epoch 18 \t Batch 480 \t Validation Loss: 39.17248279651006\n",
      "Epoch 18 \t Batch 500 \t Validation Loss: 38.89905326461792\n",
      "Epoch 18 \t Batch 520 \t Validation Loss: 38.66831383705139\n",
      "Epoch 18 \t Batch 540 \t Validation Loss: 38.38857781798751\n",
      "Epoch 18 \t Batch 560 \t Validation Loss: 38.220429754257204\n",
      "Epoch 18 \t Batch 580 \t Validation Loss: 38.070463183830526\n",
      "Epoch 18 \t Batch 600 \t Validation Loss: 38.25152988751729\n",
      "Epoch 18 Training Loss: 47.89580015059776 Validation Loss: 38.893465035921565\n",
      "Epoch 18 completed\n",
      "Epoch 19 \t Batch 20 \t Training Loss: 48.99064903259277\n",
      "Epoch 19 \t Batch 40 \t Training Loss: 48.83125820159912\n",
      "Epoch 19 \t Batch 60 \t Training Loss: 48.58120905558268\n",
      "Epoch 19 \t Batch 80 \t Training Loss: 48.54236574172974\n",
      "Epoch 19 \t Batch 100 \t Training Loss: 48.18381004333496\n",
      "Epoch 19 \t Batch 120 \t Training Loss: 48.26423848470052\n",
      "Epoch 19 \t Batch 140 \t Training Loss: 48.30466703687395\n",
      "Epoch 19 \t Batch 160 \t Training Loss: 48.21001362800598\n",
      "Epoch 19 \t Batch 180 \t Training Loss: 48.03895899454753\n",
      "Epoch 19 \t Batch 200 \t Training Loss: 47.87509696960449\n",
      "Epoch 19 \t Batch 220 \t Training Loss: 47.901951841874556\n",
      "Epoch 19 \t Batch 240 \t Training Loss: 47.8283626238505\n",
      "Epoch 19 \t Batch 260 \t Training Loss: 47.80448873960055\n",
      "Epoch 19 \t Batch 280 \t Training Loss: 47.721480996268134\n",
      "Epoch 19 \t Batch 300 \t Training Loss: 47.722210273742675\n",
      "Epoch 19 \t Batch 320 \t Training Loss: 47.72119717597961\n",
      "Epoch 19 \t Batch 340 \t Training Loss: 47.75848112667308\n",
      "Epoch 19 \t Batch 360 \t Training Loss: 47.7486944410536\n",
      "Epoch 19 \t Batch 380 \t Training Loss: 47.69554411235609\n",
      "Epoch 19 \t Batch 400 \t Training Loss: 47.750476360321045\n",
      "Epoch 19 \t Batch 420 \t Training Loss: 47.753258432660786\n",
      "Epoch 19 \t Batch 440 \t Training Loss: 47.775447472659025\n",
      "Epoch 19 \t Batch 460 \t Training Loss: 47.709032390428625\n",
      "Epoch 19 \t Batch 480 \t Training Loss: 47.745332622528075\n",
      "Epoch 19 \t Batch 500 \t Training Loss: 47.67095570373535\n",
      "Epoch 19 \t Batch 520 \t Training Loss: 47.71766609778771\n",
      "Epoch 19 \t Batch 540 \t Training Loss: 47.72338942068595\n",
      "Epoch 19 \t Batch 560 \t Training Loss: 47.72592590195792\n",
      "Epoch 19 \t Batch 580 \t Training Loss: 47.67351496466275\n",
      "Epoch 19 \t Batch 600 \t Training Loss: 47.72806500116984\n",
      "Epoch 19 \t Batch 620 \t Training Loss: 47.702016092115834\n",
      "Epoch 19 \t Batch 640 \t Training Loss: 47.699760645627975\n",
      "Epoch 19 \t Batch 660 \t Training Loss: 47.66269111055316\n",
      "Epoch 19 \t Batch 680 \t Training Loss: 47.69601926242604\n",
      "Epoch 19 \t Batch 700 \t Training Loss: 47.67141514914376\n",
      "Epoch 19 \t Batch 720 \t Training Loss: 47.73298271497091\n",
      "Epoch 19 \t Batch 740 \t Training Loss: 47.73570067431476\n",
      "Epoch 19 \t Batch 760 \t Training Loss: 47.72969859775744\n",
      "Epoch 19 \t Batch 780 \t Training Loss: 47.70231416653364\n",
      "Epoch 19 \t Batch 800 \t Training Loss: 47.735779695510864\n",
      "Epoch 19 \t Batch 820 \t Training Loss: 47.73985616172232\n",
      "Epoch 19 \t Batch 840 \t Training Loss: 47.7749233563741\n",
      "Epoch 19 \t Batch 860 \t Training Loss: 47.8022366634635\n",
      "Epoch 19 \t Batch 880 \t Training Loss: 47.828705003044824\n",
      "Epoch 19 \t Batch 900 \t Training Loss: 47.82768388960096\n",
      "Epoch 19 \t Batch 20 \t Validation Loss: 14.153640699386596\n",
      "Epoch 19 \t Batch 40 \t Validation Loss: 17.911617708206176\n",
      "Epoch 19 \t Batch 60 \t Validation Loss: 17.327198266983032\n",
      "Epoch 19 \t Batch 80 \t Validation Loss: 18.063108205795288\n",
      "Epoch 19 \t Batch 100 \t Validation Loss: 19.93517333984375\n",
      "Epoch 19 \t Batch 120 \t Validation Loss: 21.57645632425944\n",
      "Epoch 19 \t Batch 140 \t Validation Loss: 22.408874654769896\n",
      "Epoch 19 \t Batch 160 \t Validation Loss: 24.716533464193343\n",
      "Epoch 19 \t Batch 180 \t Validation Loss: 28.947240326139664\n",
      "Epoch 19 \t Batch 200 \t Validation Loss: 30.792016711235046\n",
      "Epoch 19 \t Batch 220 \t Validation Loss: 32.423503359881316\n",
      "Epoch 19 \t Batch 240 \t Validation Loss: 33.193323985735574\n",
      "Epoch 19 \t Batch 260 \t Validation Loss: 35.548003779924834\n",
      "Epoch 19 \t Batch 280 \t Validation Loss: 36.83612564291273\n",
      "Epoch 19 \t Batch 300 \t Validation Loss: 38.172019001642866\n",
      "Epoch 19 \t Batch 320 \t Validation Loss: 38.80086461007595\n",
      "Epoch 19 \t Batch 340 \t Validation Loss: 38.788324151319614\n",
      "Epoch 19 \t Batch 360 \t Validation Loss: 38.66494272814857\n",
      "Epoch 19 \t Batch 380 \t Validation Loss: 38.95066223897432\n",
      "Epoch 19 \t Batch 400 \t Validation Loss: 38.52928439855575\n",
      "Epoch 19 \t Batch 420 \t Validation Loss: 38.55806298710051\n",
      "Epoch 19 \t Batch 440 \t Validation Loss: 38.26133199821819\n",
      "Epoch 19 \t Batch 460 \t Validation Loss: 38.477951176270196\n",
      "Epoch 19 \t Batch 480 \t Validation Loss: 38.96308019359906\n",
      "Epoch 19 \t Batch 500 \t Validation Loss: 38.66909961128235\n",
      "Epoch 19 \t Batch 520 \t Validation Loss: 38.40175624260536\n",
      "Epoch 19 \t Batch 540 \t Validation Loss: 38.15850882706819\n",
      "Epoch 19 \t Batch 560 \t Validation Loss: 37.94720400401524\n",
      "Epoch 19 \t Batch 580 \t Validation Loss: 37.635200727397\n",
      "Epoch 19 \t Batch 600 \t Validation Loss: 37.87767186482748\n",
      "Epoch 19 Training Loss: 47.84989673776741 Validation Loss: 38.50794947921456\n",
      "Epoch 19 completed\n",
      "Epoch 20 \t Batch 20 \t Training Loss: 46.59144039154053\n",
      "Epoch 20 \t Batch 40 \t Training Loss: 47.80842685699463\n",
      "Epoch 20 \t Batch 60 \t Training Loss: 47.69592157999674\n",
      "Epoch 20 \t Batch 80 \t Training Loss: 47.68462138175964\n",
      "Epoch 20 \t Batch 100 \t Training Loss: 47.46545970916748\n",
      "Epoch 20 \t Batch 120 \t Training Loss: 47.48736941019694\n",
      "Epoch 20 \t Batch 140 \t Training Loss: 47.52802132197789\n",
      "Epoch 20 \t Batch 160 \t Training Loss: 47.555831384658816\n",
      "Epoch 20 \t Batch 180 \t Training Loss: 47.5599616156684\n",
      "Epoch 20 \t Batch 200 \t Training Loss: 47.67568260192871\n",
      "Epoch 20 \t Batch 220 \t Training Loss: 47.712155636874115\n",
      "Epoch 20 \t Batch 240 \t Training Loss: 47.81822009086609\n",
      "Epoch 20 \t Batch 260 \t Training Loss: 47.883126229506274\n",
      "Epoch 20 \t Batch 280 \t Training Loss: 47.91300548825945\n",
      "Epoch 20 \t Batch 300 \t Training Loss: 47.81264663696289\n",
      "Epoch 20 \t Batch 320 \t Training Loss: 47.802318561077115\n",
      "Epoch 20 \t Batch 340 \t Training Loss: 47.7661111943862\n",
      "Epoch 20 \t Batch 360 \t Training Loss: 47.815794987148706\n",
      "Epoch 20 \t Batch 380 \t Training Loss: 47.79917572423032\n",
      "Epoch 20 \t Batch 400 \t Training Loss: 47.740238666534424\n",
      "Epoch 20 \t Batch 420 \t Training Loss: 47.733368537539526\n",
      "Epoch 20 \t Batch 440 \t Training Loss: 47.728797695853494\n",
      "Epoch 20 \t Batch 460 \t Training Loss: 47.72287891222083\n",
      "Epoch 20 \t Batch 480 \t Training Loss: 47.71076072851817\n",
      "Epoch 20 \t Batch 500 \t Training Loss: 47.71382219696045\n",
      "Epoch 20 \t Batch 520 \t Training Loss: 47.729065440251276\n",
      "Epoch 20 \t Batch 540 \t Training Loss: 47.65988897394251\n",
      "Epoch 20 \t Batch 560 \t Training Loss: 47.69142724445888\n",
      "Epoch 20 \t Batch 580 \t Training Loss: 47.71525386284138\n",
      "Epoch 20 \t Batch 600 \t Training Loss: 47.70086429595947\n",
      "Epoch 20 \t Batch 620 \t Training Loss: 47.66737584760112\n",
      "Epoch 20 \t Batch 640 \t Training Loss: 47.67179357409477\n",
      "Epoch 20 \t Batch 660 \t Training Loss: 47.68487747076786\n",
      "Epoch 20 \t Batch 680 \t Training Loss: 47.69866336373722\n",
      "Epoch 20 \t Batch 700 \t Training Loss: 47.704060467311315\n",
      "Epoch 20 \t Batch 720 \t Training Loss: 47.71008617613051\n",
      "Epoch 20 \t Batch 740 \t Training Loss: 47.7121474652677\n",
      "Epoch 20 \t Batch 760 \t Training Loss: 47.73672428633037\n",
      "Epoch 20 \t Batch 780 \t Training Loss: 47.73959042964837\n",
      "Epoch 20 \t Batch 800 \t Training Loss: 47.72979060173034\n",
      "Epoch 20 \t Batch 820 \t Training Loss: 47.739565430617915\n",
      "Epoch 20 \t Batch 840 \t Training Loss: 47.76048887343634\n",
      "Epoch 20 \t Batch 860 \t Training Loss: 47.75861943710682\n",
      "Epoch 20 \t Batch 880 \t Training Loss: 47.75875287055969\n",
      "Epoch 20 \t Batch 900 \t Training Loss: 47.76807829538981\n",
      "Epoch 20 \t Batch 20 \t Validation Loss: 22.22731533050537\n",
      "Epoch 20 \t Batch 40 \t Validation Loss: 24.911827182769777\n",
      "Epoch 20 \t Batch 60 \t Validation Loss: 24.52631834348043\n",
      "Epoch 20 \t Batch 80 \t Validation Loss: 24.61825387477875\n",
      "Epoch 20 \t Batch 100 \t Validation Loss: 25.28828329086304\n",
      "Epoch 20 \t Batch 120 \t Validation Loss: 26.072120269139607\n",
      "Epoch 20 \t Batch 140 \t Validation Loss: 26.357171685355052\n",
      "Epoch 20 \t Batch 160 \t Validation Loss: 28.165859615802766\n",
      "Epoch 20 \t Batch 180 \t Validation Loss: 31.6977462026808\n",
      "Epoch 20 \t Batch 200 \t Validation Loss: 33.16390379905701\n",
      "Epoch 20 \t Batch 220 \t Validation Loss: 34.3792537429116\n",
      "Epoch 20 \t Batch 240 \t Validation Loss: 34.825672030448914\n",
      "Epoch 20 \t Batch 260 \t Validation Loss: 36.84248878405644\n",
      "Epoch 20 \t Batch 280 \t Validation Loss: 37.87110179833004\n",
      "Epoch 20 \t Batch 300 \t Validation Loss: 38.98920838991801\n",
      "Epoch 20 \t Batch 320 \t Validation Loss: 39.468298360705376\n",
      "Epoch 20 \t Batch 340 \t Validation Loss: 39.38525494126713\n",
      "Epoch 20 \t Batch 360 \t Validation Loss: 39.23083079655965\n",
      "Epoch 20 \t Batch 380 \t Validation Loss: 39.42635486502397\n",
      "Epoch 20 \t Batch 400 \t Validation Loss: 39.000687742233275\n",
      "Epoch 20 \t Batch 420 \t Validation Loss: 39.006363001323884\n",
      "Epoch 20 \t Batch 440 \t Validation Loss: 38.68902121890675\n",
      "Epoch 20 \t Batch 460 \t Validation Loss: 38.87166974855506\n",
      "Epoch 20 \t Batch 480 \t Validation Loss: 39.3357130686442\n",
      "Epoch 20 \t Batch 500 \t Validation Loss: 39.03992413330078\n",
      "Epoch 20 \t Batch 520 \t Validation Loss: 38.80133217481466\n",
      "Epoch 20 \t Batch 540 \t Validation Loss: 38.57107013596429\n",
      "Epoch 20 \t Batch 560 \t Validation Loss: 38.382107136930735\n",
      "Epoch 20 \t Batch 580 \t Validation Loss: 38.218892151733925\n",
      "Epoch 20 \t Batch 600 \t Validation Loss: 38.42847563902537\n",
      "Epoch 20 Training Loss: 47.78893746778515 Validation Loss: 39.059608656090575\n",
      "Epoch 20 completed\n",
      "Epoch 21 \t Batch 20 \t Training Loss: 47.446671867370604\n",
      "Epoch 21 \t Batch 40 \t Training Loss: 48.17950086593628\n",
      "Epoch 21 \t Batch 60 \t Training Loss: 48.3891404469808\n",
      "Epoch 21 \t Batch 80 \t Training Loss: 47.97493290901184\n",
      "Epoch 21 \t Batch 100 \t Training Loss: 47.96140087127686\n",
      "Epoch 21 \t Batch 120 \t Training Loss: 47.68042672475179\n",
      "Epoch 21 \t Batch 140 \t Training Loss: 47.76640123639788\n",
      "Epoch 21 \t Batch 160 \t Training Loss: 47.68128898143768\n",
      "Epoch 21 \t Batch 180 \t Training Loss: 47.69754761589898\n",
      "Epoch 21 \t Batch 200 \t Training Loss: 47.614683837890624\n",
      "Epoch 21 \t Batch 220 \t Training Loss: 47.72370742451061\n",
      "Epoch 21 \t Batch 240 \t Training Loss: 47.77726240158081\n",
      "Epoch 21 \t Batch 260 \t Training Loss: 47.65901724008413\n",
      "Epoch 21 \t Batch 280 \t Training Loss: 47.67856258664813\n",
      "Epoch 21 \t Batch 300 \t Training Loss: 47.722971229553224\n",
      "Epoch 21 \t Batch 320 \t Training Loss: 47.70079439878464\n",
      "Epoch 21 \t Batch 340 \t Training Loss: 47.78711471557617\n",
      "Epoch 21 \t Batch 360 \t Training Loss: 47.771882979075116\n",
      "Epoch 21 \t Batch 380 \t Training Loss: 47.762520157663445\n",
      "Epoch 21 \t Batch 400 \t Training Loss: 47.80652710914612\n",
      "Epoch 21 \t Batch 420 \t Training Loss: 47.79517280941918\n",
      "Epoch 21 \t Batch 440 \t Training Loss: 47.83856133547696\n",
      "Epoch 21 \t Batch 460 \t Training Loss: 47.81369570856509\n",
      "Epoch 21 \t Batch 480 \t Training Loss: 47.804406785964964\n",
      "Epoch 21 \t Batch 500 \t Training Loss: 47.78136853027344\n",
      "Epoch 21 \t Batch 520 \t Training Loss: 47.76580998347356\n",
      "Epoch 21 \t Batch 540 \t Training Loss: 47.83861526913113\n",
      "Epoch 21 \t Batch 560 \t Training Loss: 47.8001960005079\n",
      "Epoch 21 \t Batch 580 \t Training Loss: 47.797964964241814\n",
      "Epoch 21 \t Batch 600 \t Training Loss: 47.81494857152303\n",
      "Epoch 21 \t Batch 620 \t Training Loss: 47.761952880121044\n",
      "Epoch 21 \t Batch 640 \t Training Loss: 47.729143005609515\n",
      "Epoch 21 \t Batch 660 \t Training Loss: 47.74075145143451\n",
      "Epoch 21 \t Batch 680 \t Training Loss: 47.72381085788503\n",
      "Epoch 21 \t Batch 700 \t Training Loss: 47.720220794677736\n",
      "Epoch 21 \t Batch 720 \t Training Loss: 47.72981399430169\n",
      "Epoch 21 \t Batch 740 \t Training Loss: 47.74246574092556\n",
      "Epoch 21 \t Batch 760 \t Training Loss: 47.75666954140914\n",
      "Epoch 21 \t Batch 780 \t Training Loss: 47.800124222193006\n",
      "Epoch 21 \t Batch 800 \t Training Loss: 47.842162199020386\n",
      "Epoch 21 \t Batch 820 \t Training Loss: 47.813351817247344\n",
      "Epoch 21 \t Batch 840 \t Training Loss: 47.80110663459415\n",
      "Epoch 21 \t Batch 860 \t Training Loss: 47.79183940443882\n",
      "Epoch 21 \t Batch 880 \t Training Loss: 47.77669522545555\n",
      "Epoch 21 \t Batch 900 \t Training Loss: 47.77138768513998\n",
      "Epoch 21 \t Batch 20 \t Validation Loss: 23.649476051330566\n",
      "Epoch 21 \t Batch 40 \t Validation Loss: 26.171230363845826\n",
      "Epoch 21 \t Batch 60 \t Validation Loss: 25.549786218007405\n",
      "Epoch 21 \t Batch 80 \t Validation Loss: 25.52621877193451\n",
      "Epoch 21 \t Batch 100 \t Validation Loss: 25.968993263244627\n",
      "Epoch 21 \t Batch 120 \t Validation Loss: 26.544833135604858\n",
      "Epoch 21 \t Batch 140 \t Validation Loss: 26.797589193071637\n",
      "Epoch 21 \t Batch 160 \t Validation Loss: 28.633026909828185\n",
      "Epoch 21 \t Batch 180 \t Validation Loss: 32.26859039200677\n",
      "Epoch 21 \t Batch 200 \t Validation Loss: 33.71617558956146\n",
      "Epoch 21 \t Batch 220 \t Validation Loss: 34.94031941240484\n",
      "Epoch 21 \t Batch 240 \t Validation Loss: 35.41469722191493\n",
      "Epoch 21 \t Batch 260 \t Validation Loss: 37.46069996173565\n",
      "Epoch 21 \t Batch 280 \t Validation Loss: 38.55181945051466\n",
      "Epoch 21 \t Batch 300 \t Validation Loss: 39.71432503064474\n",
      "Epoch 21 \t Batch 320 \t Validation Loss: 40.195993623137475\n",
      "Epoch 21 \t Batch 340 \t Validation Loss: 40.12699229016024\n",
      "Epoch 21 \t Batch 360 \t Validation Loss: 39.98874523639679\n",
      "Epoch 21 \t Batch 380 \t Validation Loss: 40.193534291417976\n",
      "Epoch 21 \t Batch 400 \t Validation Loss: 39.79631604909897\n",
      "Epoch 21 \t Batch 420 \t Validation Loss: 39.75826943261283\n",
      "Epoch 21 \t Batch 440 \t Validation Loss: 39.446093394539574\n",
      "Epoch 21 \t Batch 460 \t Validation Loss: 39.65824313785719\n",
      "Epoch 21 \t Batch 480 \t Validation Loss: 40.10726484060287\n",
      "Epoch 21 \t Batch 500 \t Validation Loss: 39.78953660583496\n",
      "Epoch 21 \t Batch 520 \t Validation Loss: 39.577170001543486\n",
      "Epoch 21 \t Batch 540 \t Validation Loss: 39.36637017285382\n",
      "Epoch 21 \t Batch 560 \t Validation Loss: 39.185144543647766\n",
      "Epoch 21 \t Batch 580 \t Validation Loss: 39.042323099333665\n",
      "Epoch 21 \t Batch 600 \t Validation Loss: 39.22764331181844\n",
      "Epoch 21 Training Loss: 47.739145601328744 Validation Loss: 39.878668518809526\n",
      "Epoch 21 completed\n",
      "Epoch 22 \t Batch 20 \t Training Loss: 48.517020797729494\n",
      "Epoch 22 \t Batch 40 \t Training Loss: 48.069917869567874\n",
      "Epoch 22 \t Batch 60 \t Training Loss: 47.48287080128988\n",
      "Epoch 22 \t Batch 80 \t Training Loss: 47.69648809432984\n",
      "Epoch 22 \t Batch 100 \t Training Loss: 47.49796558380127\n",
      "Epoch 22 \t Batch 120 \t Training Loss: 47.61937615076701\n",
      "Epoch 22 \t Batch 140 \t Training Loss: 47.90432423182896\n",
      "Epoch 22 \t Batch 160 \t Training Loss: 47.98770318031311\n",
      "Epoch 22 \t Batch 180 \t Training Loss: 48.056121253967284\n",
      "Epoch 22 \t Batch 200 \t Training Loss: 47.89589595794678\n",
      "Epoch 22 \t Batch 220 \t Training Loss: 47.84565322182395\n",
      "Epoch 22 \t Batch 240 \t Training Loss: 47.81872485478719\n",
      "Epoch 22 \t Batch 260 \t Training Loss: 47.80681910881629\n",
      "Epoch 22 \t Batch 280 \t Training Loss: 47.80747737884521\n",
      "Epoch 22 \t Batch 300 \t Training Loss: 47.875066655476886\n",
      "Epoch 22 \t Batch 320 \t Training Loss: 47.841331934928895\n",
      "Epoch 22 \t Batch 340 \t Training Loss: 47.84827314264634\n",
      "Epoch 22 \t Batch 360 \t Training Loss: 47.87541624704997\n",
      "Epoch 22 \t Batch 380 \t Training Loss: 47.918917615790114\n",
      "Epoch 22 \t Batch 400 \t Training Loss: 47.858721532821654\n",
      "Epoch 22 \t Batch 420 \t Training Loss: 47.86501632872082\n",
      "Epoch 22 \t Batch 440 \t Training Loss: 47.864936585859816\n",
      "Epoch 22 \t Batch 460 \t Training Loss: 47.93629557568094\n",
      "Epoch 22 \t Batch 480 \t Training Loss: 47.88694563706716\n",
      "Epoch 22 \t Batch 500 \t Training Loss: 47.91035494995117\n",
      "Epoch 22 \t Batch 520 \t Training Loss: 47.91529362751887\n",
      "Epoch 22 \t Batch 540 \t Training Loss: 47.89155665503608\n",
      "Epoch 22 \t Batch 560 \t Training Loss: 47.85171822820391\n",
      "Epoch 22 \t Batch 580 \t Training Loss: 47.842433646629594\n",
      "Epoch 22 \t Batch 600 \t Training Loss: 47.81534636179606\n",
      "Epoch 22 \t Batch 620 \t Training Loss: 47.82369577346309\n",
      "Epoch 22 \t Batch 640 \t Training Loss: 47.81110615730286\n",
      "Epoch 22 \t Batch 660 \t Training Loss: 47.76926639441288\n",
      "Epoch 22 \t Batch 680 \t Training Loss: 47.76069383060231\n",
      "Epoch 22 \t Batch 700 \t Training Loss: 47.745784334455216\n",
      "Epoch 22 \t Batch 720 \t Training Loss: 47.75938870112101\n",
      "Epoch 22 \t Batch 740 \t Training Loss: 47.736519566097776\n",
      "Epoch 22 \t Batch 760 \t Training Loss: 47.76365031694111\n",
      "Epoch 22 \t Batch 780 \t Training Loss: 47.73839040902945\n",
      "Epoch 22 \t Batch 800 \t Training Loss: 47.721045727729795\n",
      "Epoch 22 \t Batch 820 \t Training Loss: 47.752323178547186\n",
      "Epoch 22 \t Batch 840 \t Training Loss: 47.74433068774995\n",
      "Epoch 22 \t Batch 860 \t Training Loss: 47.74370919604634\n",
      "Epoch 22 \t Batch 880 \t Training Loss: 47.709914125095715\n",
      "Epoch 22 \t Batch 900 \t Training Loss: 47.688993788825144\n",
      "Epoch 22 \t Batch 20 \t Validation Loss: 13.661030507087707\n",
      "Epoch 22 \t Batch 40 \t Validation Loss: 17.07075390815735\n",
      "Epoch 22 \t Batch 60 \t Validation Loss: 16.88136795361837\n",
      "Epoch 22 \t Batch 80 \t Validation Loss: 17.430274665355682\n",
      "Epoch 22 \t Batch 100 \t Validation Loss: 19.388716220855713\n",
      "Epoch 22 \t Batch 120 \t Validation Loss: 21.019935115178427\n",
      "Epoch 22 \t Batch 140 \t Validation Loss: 21.885412134443012\n",
      "Epoch 22 \t Batch 160 \t Validation Loss: 24.000355195999145\n",
      "Epoch 22 \t Batch 180 \t Validation Loss: 27.727517721388075\n",
      "Epoch 22 \t Batch 200 \t Validation Loss: 29.38841570854187\n",
      "Epoch 22 \t Batch 220 \t Validation Loss: 30.748811184276235\n",
      "Epoch 22 \t Batch 240 \t Validation Loss: 31.35142582654953\n",
      "Epoch 22 \t Batch 260 \t Validation Loss: 33.49288725119371\n",
      "Epoch 22 \t Batch 280 \t Validation Loss: 34.70733416761671\n",
      "Epoch 22 \t Batch 300 \t Validation Loss: 35.86772053082784\n",
      "Epoch 22 \t Batch 320 \t Validation Loss: 36.41981941461563\n",
      "Epoch 22 \t Batch 340 \t Validation Loss: 36.459757737552415\n",
      "Epoch 22 \t Batch 360 \t Validation Loss: 36.364439347055225\n",
      "Epoch 22 \t Batch 380 \t Validation Loss: 36.64268649000871\n",
      "Epoch 22 \t Batch 400 \t Validation Loss: 36.31479208230972\n",
      "Epoch 22 \t Batch 420 \t Validation Loss: 36.42878252211071\n",
      "Epoch 22 \t Batch 440 \t Validation Loss: 36.20581470836293\n",
      "Epoch 22 \t Batch 460 \t Validation Loss: 36.479549296005914\n",
      "Epoch 22 \t Batch 480 \t Validation Loss: 37.0257670879364\n",
      "Epoch 22 \t Batch 500 \t Validation Loss: 36.801587284088136\n",
      "Epoch 22 \t Batch 520 \t Validation Loss: 36.58875587903536\n",
      "Epoch 22 \t Batch 540 \t Validation Loss: 36.36544057704784\n",
      "Epoch 22 \t Batch 560 \t Validation Loss: 36.189972509656634\n",
      "Epoch 22 \t Batch 580 \t Validation Loss: 35.96750133777487\n",
      "Epoch 22 \t Batch 600 \t Validation Loss: 36.212396246592206\n",
      "Epoch 22 Training Loss: 47.67691115750611 Validation Loss: 36.85930092613418\n",
      "Validation Loss Decreased(22905.275595664978--->22705.329370498657) Saving The Model\n",
      "Epoch 22 completed\n",
      "Epoch 23 \t Batch 20 \t Training Loss: 47.75817470550537\n",
      "Epoch 23 \t Batch 40 \t Training Loss: 48.31409816741943\n",
      "Epoch 23 \t Batch 60 \t Training Loss: 47.75846951802571\n",
      "Epoch 23 \t Batch 80 \t Training Loss: 47.62005114555359\n",
      "Epoch 23 \t Batch 100 \t Training Loss: 47.66894290924072\n",
      "Epoch 23 \t Batch 120 \t Training Loss: 47.87786750793457\n",
      "Epoch 23 \t Batch 140 \t Training Loss: 47.802562141418456\n",
      "Epoch 23 \t Batch 160 \t Training Loss: 47.808177256584166\n",
      "Epoch 23 \t Batch 180 \t Training Loss: 47.77980766296387\n",
      "Epoch 23 \t Batch 200 \t Training Loss: 47.70706451416016\n",
      "Epoch 23 \t Batch 220 \t Training Loss: 47.881320675936614\n",
      "Epoch 23 \t Batch 240 \t Training Loss: 47.818378671010336\n",
      "Epoch 23 \t Batch 260 \t Training Loss: 47.678390605633076\n",
      "Epoch 23 \t Batch 280 \t Training Loss: 47.714544895717076\n",
      "Epoch 23 \t Batch 300 \t Training Loss: 47.68487617492676\n",
      "Epoch 23 \t Batch 320 \t Training Loss: 47.761674070358275\n",
      "Epoch 23 \t Batch 340 \t Training Loss: 47.73772661545697\n",
      "Epoch 23 \t Batch 360 \t Training Loss: 47.72102837032742\n",
      "Epoch 23 \t Batch 380 \t Training Loss: 47.74978818391499\n",
      "Epoch 23 \t Batch 400 \t Training Loss: 47.699610223770144\n",
      "Epoch 23 \t Batch 420 \t Training Loss: 47.686604454403835\n",
      "Epoch 23 \t Batch 440 \t Training Loss: 47.7264480937611\n",
      "Epoch 23 \t Batch 460 \t Training Loss: 47.72030261495839\n",
      "Epoch 23 \t Batch 480 \t Training Loss: 47.74461270173391\n",
      "Epoch 23 \t Batch 500 \t Training Loss: 47.71589443206787\n",
      "Epoch 23 \t Batch 520 \t Training Loss: 47.67453413743239\n",
      "Epoch 23 \t Batch 540 \t Training Loss: 47.59685427347819\n",
      "Epoch 23 \t Batch 560 \t Training Loss: 47.56164039884295\n",
      "Epoch 23 \t Batch 580 \t Training Loss: 47.58374402934107\n",
      "Epoch 23 \t Batch 600 \t Training Loss: 47.61339172999064\n",
      "Epoch 23 \t Batch 620 \t Training Loss: 47.70248982214159\n",
      "Epoch 23 \t Batch 640 \t Training Loss: 47.72147373557091\n",
      "Epoch 23 \t Batch 660 \t Training Loss: 47.72091024572199\n",
      "Epoch 23 \t Batch 680 \t Training Loss: 47.73879815830904\n",
      "Epoch 23 \t Batch 700 \t Training Loss: 47.72024010249547\n",
      "Epoch 23 \t Batch 720 \t Training Loss: 47.70609458817376\n",
      "Epoch 23 \t Batch 740 \t Training Loss: 47.69014436360952\n",
      "Epoch 23 \t Batch 760 \t Training Loss: 47.701888365494575\n",
      "Epoch 23 \t Batch 780 \t Training Loss: 47.67507162338648\n",
      "Epoch 23 \t Batch 800 \t Training Loss: 47.66062221050262\n",
      "Epoch 23 \t Batch 820 \t Training Loss: 47.66360365704792\n",
      "Epoch 23 \t Batch 840 \t Training Loss: 47.63888095674061\n",
      "Epoch 23 \t Batch 860 \t Training Loss: 47.63433863617653\n",
      "Epoch 23 \t Batch 880 \t Training Loss: 47.65718163576993\n",
      "Epoch 23 \t Batch 900 \t Training Loss: 47.66675077650282\n",
      "Epoch 23 \t Batch 20 \t Validation Loss: 21.053193664550783\n",
      "Epoch 23 \t Batch 40 \t Validation Loss: 24.224856066703797\n",
      "Epoch 23 \t Batch 60 \t Validation Loss: 23.775803661346437\n",
      "Epoch 23 \t Batch 80 \t Validation Loss: 24.39670512676239\n",
      "Epoch 23 \t Batch 100 \t Validation Loss: 24.974646549224854\n",
      "Epoch 23 \t Batch 120 \t Validation Loss: 25.74636579354604\n",
      "Epoch 23 \t Batch 140 \t Validation Loss: 26.024541875294275\n",
      "Epoch 23 \t Batch 160 \t Validation Loss: 27.784412294626236\n",
      "Epoch 23 \t Batch 180 \t Validation Loss: 31.293349255455865\n",
      "Epoch 23 \t Batch 200 \t Validation Loss: 32.707517261505124\n",
      "Epoch 23 \t Batch 220 \t Validation Loss: 33.87491147301414\n",
      "Epoch 23 \t Batch 240 \t Validation Loss: 34.33819699684779\n",
      "Epoch 23 \t Batch 260 \t Validation Loss: 36.318406339792105\n",
      "Epoch 23 \t Batch 280 \t Validation Loss: 37.32504358291626\n",
      "Epoch 23 \t Batch 300 \t Validation Loss: 38.46886681874593\n",
      "Epoch 23 \t Batch 320 \t Validation Loss: 38.97122743427754\n",
      "Epoch 23 \t Batch 340 \t Validation Loss: 38.93408663413104\n",
      "Epoch 23 \t Batch 360 \t Validation Loss: 38.79094785319434\n",
      "Epoch 23 \t Batch 380 \t Validation Loss: 38.98217375152989\n",
      "Epoch 23 \t Batch 400 \t Validation Loss: 38.59299649477005\n",
      "Epoch 23 \t Batch 420 \t Validation Loss: 38.671043489092874\n",
      "Epoch 23 \t Batch 440 \t Validation Loss: 38.38644257242029\n",
      "Epoch 23 \t Batch 460 \t Validation Loss: 38.61878214919049\n",
      "Epoch 23 \t Batch 480 \t Validation Loss: 39.13963837424914\n",
      "Epoch 23 \t Batch 500 \t Validation Loss: 38.88500345802307\n",
      "Epoch 23 \t Batch 520 \t Validation Loss: 38.61869047054878\n",
      "Epoch 23 \t Batch 540 \t Validation Loss: 38.35435441511649\n",
      "Epoch 23 \t Batch 560 \t Validation Loss: 38.136329567432405\n",
      "Epoch 23 \t Batch 580 \t Validation Loss: 37.870211990948384\n",
      "Epoch 23 \t Batch 600 \t Validation Loss: 38.09177553017934\n",
      "Epoch 23 Training Loss: 47.63317160569984 Validation Loss: 38.72394898495117\n",
      "Epoch 23 completed\n",
      "Epoch 24 \t Batch 20 \t Training Loss: 47.532568740844724\n",
      "Epoch 24 \t Batch 40 \t Training Loss: 47.6506160736084\n",
      "Epoch 24 \t Batch 60 \t Training Loss: 47.19285360972086\n",
      "Epoch 24 \t Batch 80 \t Training Loss: 47.5747549533844\n",
      "Epoch 24 \t Batch 100 \t Training Loss: 47.48728351593017\n",
      "Epoch 24 \t Batch 120 \t Training Loss: 47.46979360580444\n",
      "Epoch 24 \t Batch 140 \t Training Loss: 47.282670211791995\n",
      "Epoch 24 \t Batch 160 \t Training Loss: 47.41561181545258\n",
      "Epoch 24 \t Batch 180 \t Training Loss: 47.48052043914795\n",
      "Epoch 24 \t Batch 200 \t Training Loss: 47.49147560119629\n",
      "Epoch 24 \t Batch 220 \t Training Loss: 47.39031162261963\n",
      "Epoch 24 \t Batch 240 \t Training Loss: 47.42084182103475\n",
      "Epoch 24 \t Batch 260 \t Training Loss: 47.595156713632434\n",
      "Epoch 24 \t Batch 280 \t Training Loss: 47.55697892052787\n",
      "Epoch 24 \t Batch 300 \t Training Loss: 47.51204989115397\n",
      "Epoch 24 \t Batch 320 \t Training Loss: 47.497552597522734\n",
      "Epoch 24 \t Batch 340 \t Training Loss: 47.459631067163805\n",
      "Epoch 24 \t Batch 360 \t Training Loss: 47.42275235917833\n",
      "Epoch 24 \t Batch 380 \t Training Loss: 47.42590669330798\n",
      "Epoch 24 \t Batch 400 \t Training Loss: 47.442619199752805\n",
      "Epoch 24 \t Batch 420 \t Training Loss: 47.44048180353074\n",
      "Epoch 24 \t Batch 440 \t Training Loss: 47.460979947176845\n",
      "Epoch 24 \t Batch 460 \t Training Loss: 47.43923045448635\n",
      "Epoch 24 \t Batch 480 \t Training Loss: 47.50289901097616\n",
      "Epoch 24 \t Batch 500 \t Training Loss: 47.593587020874025\n",
      "Epoch 24 \t Batch 520 \t Training Loss: 47.58093043840849\n",
      "Epoch 24 \t Batch 540 \t Training Loss: 47.529850239223904\n",
      "Epoch 24 \t Batch 560 \t Training Loss: 47.528112983703615\n",
      "Epoch 24 \t Batch 580 \t Training Loss: 47.58316455051817\n",
      "Epoch 24 \t Batch 600 \t Training Loss: 47.58218108495077\n",
      "Epoch 24 \t Batch 620 \t Training Loss: 47.5522211874685\n",
      "Epoch 24 \t Batch 640 \t Training Loss: 47.59662547111511\n",
      "Epoch 24 \t Batch 660 \t Training Loss: 47.633654513503565\n",
      "Epoch 24 \t Batch 680 \t Training Loss: 47.62947140300975\n",
      "Epoch 24 \t Batch 700 \t Training Loss: 47.61890320369176\n",
      "Epoch 24 \t Batch 720 \t Training Loss: 47.61675729221768\n",
      "Epoch 24 \t Batch 740 \t Training Loss: 47.6391602954349\n",
      "Epoch 24 \t Batch 760 \t Training Loss: 47.66190658368562\n",
      "Epoch 24 \t Batch 780 \t Training Loss: 47.658652956057814\n",
      "Epoch 24 \t Batch 800 \t Training Loss: 47.61759732723236\n",
      "Epoch 24 \t Batch 820 \t Training Loss: 47.58371472242402\n",
      "Epoch 24 \t Batch 840 \t Training Loss: 47.6151022320702\n",
      "Epoch 24 \t Batch 860 \t Training Loss: 47.61466288455697\n",
      "Epoch 24 \t Batch 880 \t Training Loss: 47.594286818937825\n",
      "Epoch 24 \t Batch 900 \t Training Loss: 47.597337574428984\n",
      "Epoch 24 \t Batch 20 \t Validation Loss: 26.73742208480835\n",
      "Epoch 24 \t Batch 40 \t Validation Loss: 28.239254212379457\n",
      "Epoch 24 \t Batch 60 \t Validation Loss: 28.231270265579223\n",
      "Epoch 24 \t Batch 80 \t Validation Loss: 27.96171499490738\n",
      "Epoch 24 \t Batch 100 \t Validation Loss: 28.03613450050354\n",
      "Epoch 24 \t Batch 120 \t Validation Loss: 28.53004841009776\n",
      "Epoch 24 \t Batch 140 \t Validation Loss: 28.53954870360238\n",
      "Epoch 24 \t Batch 160 \t Validation Loss: 29.884820598363877\n",
      "Epoch 24 \t Batch 180 \t Validation Loss: 32.86529619428847\n",
      "Epoch 24 \t Batch 200 \t Validation Loss: 33.96515542030335\n",
      "Epoch 24 \t Batch 220 \t Validation Loss: 34.86610695232044\n",
      "Epoch 24 \t Batch 240 \t Validation Loss: 35.11750147740046\n",
      "Epoch 24 \t Batch 260 \t Validation Loss: 36.92839011412401\n",
      "Epoch 24 \t Batch 280 \t Validation Loss: 37.869942736625674\n",
      "Epoch 24 \t Batch 300 \t Validation Loss: 38.747589133580526\n",
      "Epoch 24 \t Batch 320 \t Validation Loss: 39.1208616733551\n",
      "Epoch 24 \t Batch 340 \t Validation Loss: 39.00480268141803\n",
      "Epoch 24 \t Batch 360 \t Validation Loss: 38.81513311068217\n",
      "Epoch 24 \t Batch 380 \t Validation Loss: 38.959421770196215\n",
      "Epoch 24 \t Batch 400 \t Validation Loss: 38.548567390441896\n",
      "Epoch 24 \t Batch 420 \t Validation Loss: 38.55368938446045\n",
      "Epoch 24 \t Batch 440 \t Validation Loss: 38.24403562979265\n",
      "Epoch 24 \t Batch 460 \t Validation Loss: 38.445966820094895\n",
      "Epoch 24 \t Batch 480 \t Validation Loss: 38.91741053263346\n",
      "Epoch 24 \t Batch 500 \t Validation Loss: 38.62388151168823\n",
      "Epoch 24 \t Batch 520 \t Validation Loss: 38.37491816373972\n",
      "Epoch 24 \t Batch 540 \t Validation Loss: 38.14737660090129\n",
      "Epoch 24 \t Batch 560 \t Validation Loss: 37.97920960358211\n",
      "Epoch 24 \t Batch 580 \t Validation Loss: 37.78113810769443\n",
      "Epoch 24 \t Batch 600 \t Validation Loss: 38.00651048342387\n",
      "Epoch 24 Training Loss: 47.57652675000506 Validation Loss: 38.71218321540139\n",
      "Epoch 24 completed\n",
      "Epoch 25 \t Batch 20 \t Training Loss: 46.92275562286377\n",
      "Epoch 25 \t Batch 40 \t Training Loss: 47.29990768432617\n",
      "Epoch 25 \t Batch 60 \t Training Loss: 47.64889430999756\n",
      "Epoch 25 \t Batch 80 \t Training Loss: 47.68826513290405\n",
      "Epoch 25 \t Batch 100 \t Training Loss: 48.013503875732425\n",
      "Epoch 25 \t Batch 120 \t Training Loss: 47.68508011500041\n",
      "Epoch 25 \t Batch 140 \t Training Loss: 47.709793962751114\n",
      "Epoch 25 \t Batch 160 \t Training Loss: 47.595240759849545\n",
      "Epoch 25 \t Batch 180 \t Training Loss: 47.565546332465274\n",
      "Epoch 25 \t Batch 200 \t Training Loss: 47.583278770446775\n",
      "Epoch 25 \t Batch 220 \t Training Loss: 47.55730984427712\n",
      "Epoch 25 \t Batch 240 \t Training Loss: 47.48829992612203\n",
      "Epoch 25 \t Batch 260 \t Training Loss: 47.575968214181756\n",
      "Epoch 25 \t Batch 280 \t Training Loss: 47.60064462934221\n",
      "Epoch 25 \t Batch 300 \t Training Loss: 47.61793956756592\n",
      "Epoch 25 \t Batch 320 \t Training Loss: 47.68659118413925\n",
      "Epoch 25 \t Batch 340 \t Training Loss: 47.62181813857135\n",
      "Epoch 25 \t Batch 360 \t Training Loss: 47.698225593566896\n",
      "Epoch 25 \t Batch 380 \t Training Loss: 47.717855734574165\n",
      "Epoch 25 \t Batch 400 \t Training Loss: 47.65088541030884\n",
      "Epoch 25 \t Batch 420 \t Training Loss: 47.63417623610724\n",
      "Epoch 25 \t Batch 440 \t Training Loss: 47.595321759310636\n",
      "Epoch 25 \t Batch 460 \t Training Loss: 47.510646040543264\n",
      "Epoch 25 \t Batch 480 \t Training Loss: 47.47317320505778\n",
      "Epoch 25 \t Batch 500 \t Training Loss: 47.48884886932373\n",
      "Epoch 25 \t Batch 520 \t Training Loss: 47.47509133999164\n",
      "Epoch 25 \t Batch 540 \t Training Loss: 47.43789691925049\n",
      "Epoch 25 \t Batch 560 \t Training Loss: 47.43778581619263\n",
      "Epoch 25 \t Batch 580 \t Training Loss: 47.417853092325146\n",
      "Epoch 25 \t Batch 600 \t Training Loss: 47.4202286974589\n",
      "Epoch 25 \t Batch 620 \t Training Loss: 47.392008873724166\n",
      "Epoch 25 \t Batch 640 \t Training Loss: 47.40657439827919\n",
      "Epoch 25 \t Batch 660 \t Training Loss: 47.38568587447658\n",
      "Epoch 25 \t Batch 680 \t Training Loss: 47.40865226633409\n",
      "Epoch 25 \t Batch 700 \t Training Loss: 47.395561888558525\n",
      "Epoch 25 \t Batch 720 \t Training Loss: 47.40618504948086\n",
      "Epoch 25 \t Batch 740 \t Training Loss: 47.40300350704709\n",
      "Epoch 25 \t Batch 760 \t Training Loss: 47.393755907761424\n",
      "Epoch 25 \t Batch 780 \t Training Loss: 47.420560797666894\n",
      "Epoch 25 \t Batch 800 \t Training Loss: 47.4304622888565\n",
      "Epoch 25 \t Batch 820 \t Training Loss: 47.46898214758896\n",
      "Epoch 25 \t Batch 840 \t Training Loss: 47.469462626320976\n",
      "Epoch 25 \t Batch 860 \t Training Loss: 47.484669663185294\n",
      "Epoch 25 \t Batch 880 \t Training Loss: 47.49298774112355\n",
      "Epoch 25 \t Batch 900 \t Training Loss: 47.5047764884101\n",
      "Epoch 25 \t Batch 20 \t Validation Loss: 22.228020858764648\n",
      "Epoch 25 \t Batch 40 \t Validation Loss: 24.42227931022644\n",
      "Epoch 25 \t Batch 60 \t Validation Loss: 24.249752378463747\n",
      "Epoch 25 \t Batch 80 \t Validation Loss: 24.2882248044014\n",
      "Epoch 25 \t Batch 100 \t Validation Loss: 25.81845088005066\n",
      "Epoch 25 \t Batch 120 \t Validation Loss: 27.106355404853822\n",
      "Epoch 25 \t Batch 140 \t Validation Loss: 27.558096483775547\n",
      "Epoch 25 \t Batch 160 \t Validation Loss: 29.147388249635696\n",
      "Epoch 25 \t Batch 180 \t Validation Loss: 32.472679646809894\n",
      "Epoch 25 \t Batch 200 \t Validation Loss: 33.70279120445252\n",
      "Epoch 25 \t Batch 220 \t Validation Loss: 34.809032526883215\n",
      "Epoch 25 \t Batch 240 \t Validation Loss: 35.19138280948003\n",
      "Epoch 25 \t Batch 260 \t Validation Loss: 37.142812574826756\n",
      "Epoch 25 \t Batch 280 \t Validation Loss: 38.18747347763607\n",
      "Epoch 25 \t Batch 300 \t Validation Loss: 39.15953273455302\n",
      "Epoch 25 \t Batch 320 \t Validation Loss: 39.579233440756795\n",
      "Epoch 25 \t Batch 340 \t Validation Loss: 39.47319418682772\n",
      "Epoch 25 \t Batch 360 \t Validation Loss: 39.2716551038954\n",
      "Epoch 25 \t Batch 380 \t Validation Loss: 39.43597183729473\n",
      "Epoch 25 \t Batch 400 \t Validation Loss: 38.99329246520996\n",
      "Epoch 25 \t Batch 420 \t Validation Loss: 38.967519928160165\n",
      "Epoch 25 \t Batch 440 \t Validation Loss: 38.64792467030612\n",
      "Epoch 25 \t Batch 460 \t Validation Loss: 38.84385088630344\n",
      "Epoch 25 \t Batch 480 \t Validation Loss: 39.28597745895386\n",
      "Epoch 25 \t Batch 500 \t Validation Loss: 38.95772748565674\n",
      "Epoch 25 \t Batch 520 \t Validation Loss: 38.674145779242885\n",
      "Epoch 25 \t Batch 540 \t Validation Loss: 38.467800532446965\n",
      "Epoch 25 \t Batch 560 \t Validation Loss: 38.29988742555891\n",
      "Epoch 25 \t Batch 580 \t Validation Loss: 38.033101361373376\n",
      "Epoch 25 \t Batch 600 \t Validation Loss: 38.308630253473915\n",
      "Epoch 25 Training Loss: 47.52217746430941 Validation Loss: 38.9394107329381\n",
      "Epoch 25 completed\n",
      "Epoch 26 \t Batch 20 \t Training Loss: 47.060173416137694\n",
      "Epoch 26 \t Batch 40 \t Training Loss: 47.168496227264406\n",
      "Epoch 26 \t Batch 60 \t Training Loss: 47.33676662445068\n",
      "Epoch 26 \t Batch 80 \t Training Loss: 47.307852029800415\n",
      "Epoch 26 \t Batch 100 \t Training Loss: 47.106409530639645\n",
      "Epoch 26 \t Batch 120 \t Training Loss: 47.11404848098755\n",
      "Epoch 26 \t Batch 140 \t Training Loss: 47.0480007989066\n",
      "Epoch 26 \t Batch 160 \t Training Loss: 47.16905145645141\n",
      "Epoch 26 \t Batch 180 \t Training Loss: 47.30334765116374\n",
      "Epoch 26 \t Batch 200 \t Training Loss: 47.22558902740479\n",
      "Epoch 26 \t Batch 220 \t Training Loss: 47.263259679620916\n",
      "Epoch 26 \t Batch 240 \t Training Loss: 47.4283127784729\n",
      "Epoch 26 \t Batch 260 \t Training Loss: 47.544756463857794\n",
      "Epoch 26 \t Batch 280 \t Training Loss: 47.50406773430961\n",
      "Epoch 26 \t Batch 300 \t Training Loss: 47.48612631479899\n",
      "Epoch 26 \t Batch 320 \t Training Loss: 47.50595138072968\n",
      "Epoch 26 \t Batch 340 \t Training Loss: 47.56476365257712\n",
      "Epoch 26 \t Batch 360 \t Training Loss: 47.550885020362\n",
      "Epoch 26 \t Batch 380 \t Training Loss: 47.615947592885874\n",
      "Epoch 26 \t Batch 400 \t Training Loss: 47.57213970184326\n",
      "Epoch 26 \t Batch 420 \t Training Loss: 47.59739082881382\n",
      "Epoch 26 \t Batch 440 \t Training Loss: 47.591628161343664\n",
      "Epoch 26 \t Batch 460 \t Training Loss: 47.54623705822488\n",
      "Epoch 26 \t Batch 480 \t Training Loss: 47.483012890815736\n",
      "Epoch 26 \t Batch 500 \t Training Loss: 47.46315056610108\n",
      "Epoch 26 \t Batch 520 \t Training Loss: 47.434613132476805\n",
      "Epoch 26 \t Batch 540 \t Training Loss: 47.414779592443395\n",
      "Epoch 26 \t Batch 560 \t Training Loss: 47.438061271395\n",
      "Epoch 26 \t Batch 580 \t Training Loss: 47.43401325488912\n",
      "Epoch 26 \t Batch 600 \t Training Loss: 47.45229278564453\n",
      "Epoch 26 \t Batch 620 \t Training Loss: 47.4421858449136\n",
      "Epoch 26 \t Batch 640 \t Training Loss: 47.46437497138977\n",
      "Epoch 26 \t Batch 660 \t Training Loss: 47.421508089701334\n",
      "Epoch 26 \t Batch 680 \t Training Loss: 47.47019034553976\n",
      "Epoch 26 \t Batch 700 \t Training Loss: 47.506935457502095\n",
      "Epoch 26 \t Batch 720 \t Training Loss: 47.52626119189792\n",
      "Epoch 26 \t Batch 740 \t Training Loss: 47.4748874148807\n",
      "Epoch 26 \t Batch 760 \t Training Loss: 47.47318515777588\n",
      "Epoch 26 \t Batch 780 \t Training Loss: 47.453267743037294\n",
      "Epoch 26 \t Batch 800 \t Training Loss: 47.406973261833194\n",
      "Epoch 26 \t Batch 820 \t Training Loss: 47.41681149180342\n",
      "Epoch 26 \t Batch 840 \t Training Loss: 47.44028108233497\n",
      "Epoch 26 \t Batch 860 \t Training Loss: 47.46825615195341\n",
      "Epoch 26 \t Batch 880 \t Training Loss: 47.431355255300346\n",
      "Epoch 26 \t Batch 900 \t Training Loss: 47.443864080641006\n",
      "Epoch 26 \t Batch 20 \t Validation Loss: 15.511997318267822\n",
      "Epoch 26 \t Batch 40 \t Validation Loss: 19.847421860694887\n",
      "Epoch 26 \t Batch 60 \t Validation Loss: 19.2873898824056\n",
      "Epoch 26 \t Batch 80 \t Validation Loss: 20.08547019958496\n",
      "Epoch 26 \t Batch 100 \t Validation Loss: 21.45187795639038\n",
      "Epoch 26 \t Batch 120 \t Validation Loss: 22.747796519597372\n",
      "Epoch 26 \t Batch 140 \t Validation Loss: 23.383727850232805\n",
      "Epoch 26 \t Batch 160 \t Validation Loss: 25.40057256221771\n",
      "Epoch 26 \t Batch 180 \t Validation Loss: 28.857395712534586\n",
      "Epoch 26 \t Batch 200 \t Validation Loss: 30.350283718109132\n",
      "Epoch 26 \t Batch 220 \t Validation Loss: 31.62458404194225\n",
      "Epoch 26 \t Batch 240 \t Validation Loss: 32.19210763374964\n",
      "Epoch 26 \t Batch 260 \t Validation Loss: 34.24215018932636\n",
      "Epoch 26 \t Batch 280 \t Validation Loss: 35.366523783547535\n",
      "Epoch 26 \t Batch 300 \t Validation Loss: 36.398377176920576\n",
      "Epoch 26 \t Batch 320 \t Validation Loss: 36.945718365907666\n",
      "Epoch 26 \t Batch 340 \t Validation Loss: 36.987842481276566\n",
      "Epoch 26 \t Batch 360 \t Validation Loss: 36.88915982776218\n",
      "Epoch 26 \t Batch 380 \t Validation Loss: 37.16113002174779\n",
      "Epoch 26 \t Batch 400 \t Validation Loss: 36.872494940757754\n",
      "Epoch 26 \t Batch 420 \t Validation Loss: 37.00974188305083\n",
      "Epoch 26 \t Batch 440 \t Validation Loss: 36.8054183179682\n",
      "Epoch 26 \t Batch 460 \t Validation Loss: 37.078821285911225\n",
      "Epoch 26 \t Batch 480 \t Validation Loss: 37.61084028482437\n",
      "Epoch 26 \t Batch 500 \t Validation Loss: 37.39091614913941\n",
      "Epoch 26 \t Batch 520 \t Validation Loss: 37.18449840178857\n",
      "Epoch 26 \t Batch 540 \t Validation Loss: 36.984247832828096\n",
      "Epoch 26 \t Batch 560 \t Validation Loss: 36.82780984469822\n",
      "Epoch 26 \t Batch 580 \t Validation Loss: 36.580222734911686\n",
      "Epoch 26 \t Batch 600 \t Validation Loss: 36.837889496485396\n",
      "Epoch 26 Training Loss: 47.45419559041962 Validation Loss: 37.48983633363402\n",
      "Epoch 26 completed\n",
      "Epoch 27 \t Batch 20 \t Training Loss: 47.015487098693846\n",
      "Epoch 27 \t Batch 40 \t Training Loss: 46.96387166976929\n",
      "Epoch 27 \t Batch 60 \t Training Loss: 47.573686536153154\n",
      "Epoch 27 \t Batch 80 \t Training Loss: 47.70586247444153\n",
      "Epoch 27 \t Batch 100 \t Training Loss: 47.7549649810791\n",
      "Epoch 27 \t Batch 120 \t Training Loss: 47.71310749053955\n",
      "Epoch 27 \t Batch 140 \t Training Loss: 47.38582314082554\n",
      "Epoch 27 \t Batch 160 \t Training Loss: 47.55048768520355\n",
      "Epoch 27 \t Batch 180 \t Training Loss: 47.745363299051924\n",
      "Epoch 27 \t Batch 200 \t Training Loss: 47.75368547439575\n",
      "Epoch 27 \t Batch 220 \t Training Loss: 47.74627474004572\n",
      "Epoch 27 \t Batch 240 \t Training Loss: 47.78505851427714\n",
      "Epoch 27 \t Batch 260 \t Training Loss: 47.76005514585055\n",
      "Epoch 27 \t Batch 280 \t Training Loss: 47.819566113608225\n",
      "Epoch 27 \t Batch 300 \t Training Loss: 47.68491095225016\n",
      "Epoch 27 \t Batch 320 \t Training Loss: 47.6318475484848\n",
      "Epoch 27 \t Batch 340 \t Training Loss: 47.61663955239688\n",
      "Epoch 27 \t Batch 360 \t Training Loss: 47.66910830603705\n",
      "Epoch 27 \t Batch 380 \t Training Loss: 47.594048871492085\n",
      "Epoch 27 \t Batch 400 \t Training Loss: 47.59768904685974\n",
      "Epoch 27 \t Batch 420 \t Training Loss: 47.530593063717795\n",
      "Epoch 27 \t Batch 440 \t Training Loss: 47.46493532874367\n",
      "Epoch 27 \t Batch 460 \t Training Loss: 47.39499998507292\n",
      "Epoch 27 \t Batch 480 \t Training Loss: 47.37457753022512\n",
      "Epoch 27 \t Batch 500 \t Training Loss: 47.332123214721676\n",
      "Epoch 27 \t Batch 520 \t Training Loss: 47.350608789003815\n",
      "Epoch 27 \t Batch 540 \t Training Loss: 47.29755694777877\n",
      "Epoch 27 \t Batch 560 \t Training Loss: 47.293253782817295\n",
      "Epoch 27 \t Batch 580 \t Training Loss: 47.2599125697695\n",
      "Epoch 27 \t Batch 600 \t Training Loss: 47.24825310389201\n",
      "Epoch 27 \t Batch 620 \t Training Loss: 47.31631448191981\n",
      "Epoch 27 \t Batch 640 \t Training Loss: 47.34604551196098\n",
      "Epoch 27 \t Batch 660 \t Training Loss: 47.37449427517978\n",
      "Epoch 27 \t Batch 680 \t Training Loss: 47.372820523205924\n",
      "Epoch 27 \t Batch 700 \t Training Loss: 47.37484011513846\n",
      "Epoch 27 \t Batch 720 \t Training Loss: 47.41013257768419\n",
      "Epoch 27 \t Batch 740 \t Training Loss: 47.45890215280894\n",
      "Epoch 27 \t Batch 760 \t Training Loss: 47.42817013389186\n",
      "Epoch 27 \t Batch 780 \t Training Loss: 47.40154110835149\n",
      "Epoch 27 \t Batch 800 \t Training Loss: 47.408260016441346\n",
      "Epoch 27 \t Batch 820 \t Training Loss: 47.36562893565108\n",
      "Epoch 27 \t Batch 840 \t Training Loss: 47.37126687367757\n",
      "Epoch 27 \t Batch 860 \t Training Loss: 47.35519363048465\n",
      "Epoch 27 \t Batch 880 \t Training Loss: 47.36793898235668\n",
      "Epoch 27 \t Batch 900 \t Training Loss: 47.392591116163466\n",
      "Epoch 27 \t Batch 20 \t Validation Loss: 18.325043153762817\n",
      "Epoch 27 \t Batch 40 \t Validation Loss: 22.0988849401474\n",
      "Epoch 27 \t Batch 60 \t Validation Loss: 21.233305803934734\n",
      "Epoch 27 \t Batch 80 \t Validation Loss: 21.584884691238404\n",
      "Epoch 27 \t Batch 100 \t Validation Loss: 22.80306499481201\n",
      "Epoch 27 \t Batch 120 \t Validation Loss: 23.998536912600198\n",
      "Epoch 27 \t Batch 140 \t Validation Loss: 24.614312723704746\n",
      "Epoch 27 \t Batch 160 \t Validation Loss: 26.652032965421675\n",
      "Epoch 27 \t Batch 180 \t Validation Loss: 30.367888270484077\n",
      "Epoch 27 \t Batch 200 \t Validation Loss: 31.910821828842163\n",
      "Epoch 27 \t Batch 220 \t Validation Loss: 33.13899552605369\n",
      "Epoch 27 \t Batch 240 \t Validation Loss: 33.65241667032242\n",
      "Epoch 27 \t Batch 260 \t Validation Loss: 35.74125276345473\n",
      "Epoch 27 \t Batch 280 \t Validation Loss: 36.86156962939671\n",
      "Epoch 27 \t Batch 300 \t Validation Loss: 38.05275661468506\n",
      "Epoch 27 \t Batch 320 \t Validation Loss: 38.603567057847975\n",
      "Epoch 27 \t Batch 340 \t Validation Loss: 38.57329060610603\n",
      "Epoch 27 \t Batch 360 \t Validation Loss: 38.494663657082455\n",
      "Epoch 27 \t Batch 380 \t Validation Loss: 38.72231452339574\n",
      "Epoch 27 \t Batch 400 \t Validation Loss: 38.34396478176117\n",
      "Epoch 27 \t Batch 420 \t Validation Loss: 38.361975983210975\n",
      "Epoch 27 \t Batch 440 \t Validation Loss: 38.088360604372895\n",
      "Epoch 27 \t Batch 460 \t Validation Loss: 38.38558702883513\n",
      "Epoch 27 \t Batch 480 \t Validation Loss: 38.90088904301326\n",
      "Epoch 27 \t Batch 500 \t Validation Loss: 38.647276958465575\n",
      "Epoch 27 \t Batch 520 \t Validation Loss: 38.47680435180664\n",
      "Epoch 27 \t Batch 540 \t Validation Loss: 38.26150072592276\n",
      "Epoch 27 \t Batch 560 \t Validation Loss: 38.07904611655644\n",
      "Epoch 27 \t Batch 580 \t Validation Loss: 37.9138849685932\n",
      "Epoch 27 \t Batch 600 \t Validation Loss: 38.13108302434286\n",
      "Epoch 27 Training Loss: 47.41741033572797 Validation Loss: 38.79704301388233\n",
      "Epoch 27 completed\n",
      "Epoch 28 \t Batch 20 \t Training Loss: 48.37343215942383\n",
      "Epoch 28 \t Batch 40 \t Training Loss: 47.56594648361206\n",
      "Epoch 28 \t Batch 60 \t Training Loss: 47.106685129801434\n",
      "Epoch 28 \t Batch 80 \t Training Loss: 47.3130078792572\n",
      "Epoch 28 \t Batch 100 \t Training Loss: 46.98946720123291\n",
      "Epoch 28 \t Batch 120 \t Training Loss: 47.11891066233317\n",
      "Epoch 28 \t Batch 140 \t Training Loss: 47.03010035923549\n",
      "Epoch 28 \t Batch 160 \t Training Loss: 47.173100757598874\n",
      "Epoch 28 \t Batch 180 \t Training Loss: 47.31934356689453\n",
      "Epoch 28 \t Batch 200 \t Training Loss: 47.33663198471069\n",
      "Epoch 28 \t Batch 220 \t Training Loss: 47.335466488924894\n",
      "Epoch 28 \t Batch 240 \t Training Loss: 47.4822364171346\n",
      "Epoch 28 \t Batch 260 \t Training Loss: 47.479549642709586\n",
      "Epoch 28 \t Batch 280 \t Training Loss: 47.56316303525652\n",
      "Epoch 28 \t Batch 300 \t Training Loss: 47.561043930053714\n",
      "Epoch 28 \t Batch 320 \t Training Loss: 47.56983579397202\n",
      "Epoch 28 \t Batch 340 \t Training Loss: 47.624665282754336\n",
      "Epoch 28 \t Batch 360 \t Training Loss: 47.51621444490221\n",
      "Epoch 28 \t Batch 380 \t Training Loss: 47.5774344193308\n",
      "Epoch 28 \t Batch 400 \t Training Loss: 47.58626546859741\n",
      "Epoch 28 \t Batch 420 \t Training Loss: 47.55880059741792\n",
      "Epoch 28 \t Batch 440 \t Training Loss: 47.56799995248968\n",
      "Epoch 28 \t Batch 460 \t Training Loss: 47.64366348100745\n",
      "Epoch 28 \t Batch 480 \t Training Loss: 47.608104689915976\n",
      "Epoch 28 \t Batch 500 \t Training Loss: 47.627830871582034\n",
      "Epoch 28 \t Batch 520 \t Training Loss: 47.56400741430429\n",
      "Epoch 28 \t Batch 540 \t Training Loss: 47.581713379753964\n",
      "Epoch 28 \t Batch 560 \t Training Loss: 47.535091284343174\n",
      "Epoch 28 \t Batch 580 \t Training Loss: 47.54519692124992\n",
      "Epoch 28 \t Batch 600 \t Training Loss: 47.538085301717125\n",
      "Epoch 28 \t Batch 620 \t Training Loss: 47.466344230405745\n",
      "Epoch 28 \t Batch 640 \t Training Loss: 47.45231067538261\n",
      "Epoch 28 \t Batch 660 \t Training Loss: 47.46678830927068\n",
      "Epoch 28 \t Batch 680 \t Training Loss: 47.47284792170805\n",
      "Epoch 28 \t Batch 700 \t Training Loss: 47.472421182904924\n",
      "Epoch 28 \t Batch 720 \t Training Loss: 47.45170573658413\n",
      "Epoch 28 \t Batch 740 \t Training Loss: 47.388834886293154\n",
      "Epoch 28 \t Batch 760 \t Training Loss: 47.36138791034096\n",
      "Epoch 28 \t Batch 780 \t Training Loss: 47.36410332215138\n",
      "Epoch 28 \t Batch 800 \t Training Loss: 47.33738691329956\n",
      "Epoch 28 \t Batch 820 \t Training Loss: 47.35456228488829\n",
      "Epoch 28 \t Batch 840 \t Training Loss: 47.352046380724225\n",
      "Epoch 28 \t Batch 860 \t Training Loss: 47.36679330426593\n",
      "Epoch 28 \t Batch 880 \t Training Loss: 47.41403361667286\n",
      "Epoch 28 \t Batch 900 \t Training Loss: 47.39204995473226\n",
      "Epoch 28 \t Batch 20 \t Validation Loss: 16.122427701950073\n",
      "Epoch 28 \t Batch 40 \t Validation Loss: 21.079489278793336\n",
      "Epoch 28 \t Batch 60 \t Validation Loss: 20.49715929031372\n",
      "Epoch 28 \t Batch 80 \t Validation Loss: 21.19933182001114\n",
      "Epoch 28 \t Batch 100 \t Validation Loss: 22.589463605880738\n",
      "Epoch 28 \t Batch 120 \t Validation Loss: 23.791390363375346\n",
      "Epoch 28 \t Batch 140 \t Validation Loss: 24.392457696369718\n",
      "Epoch 28 \t Batch 160 \t Validation Loss: 26.510843116045\n",
      "Epoch 28 \t Batch 180 \t Validation Loss: 30.224751785066392\n",
      "Epoch 28 \t Batch 200 \t Validation Loss: 31.78591185092926\n",
      "Epoch 28 \t Batch 220 \t Validation Loss: 33.21355380578475\n",
      "Epoch 28 \t Batch 240 \t Validation Loss: 33.7881929119428\n",
      "Epoch 28 \t Batch 260 \t Validation Loss: 35.966589109714214\n",
      "Epoch 28 \t Batch 280 \t Validation Loss: 37.17023060321808\n",
      "Epoch 28 \t Batch 300 \t Validation Loss: 38.30705189069112\n",
      "Epoch 28 \t Batch 320 \t Validation Loss: 38.82467792928219\n",
      "Epoch 28 \t Batch 340 \t Validation Loss: 38.77904932639178\n",
      "Epoch 28 \t Batch 360 \t Validation Loss: 38.69354809919993\n",
      "Epoch 28 \t Batch 380 \t Validation Loss: 38.955020194304616\n",
      "Epoch 28 \t Batch 400 \t Validation Loss: 38.550771491527556\n",
      "Epoch 28 \t Batch 420 \t Validation Loss: 38.5602625642504\n",
      "Epoch 28 \t Batch 440 \t Validation Loss: 38.26806728839874\n",
      "Epoch 28 \t Batch 460 \t Validation Loss: 38.47202474552652\n",
      "Epoch 28 \t Batch 480 \t Validation Loss: 38.94525455037753\n",
      "Epoch 28 \t Batch 500 \t Validation Loss: 38.67328032875061\n",
      "Epoch 28 \t Batch 520 \t Validation Loss: 38.476542054689844\n",
      "Epoch 28 \t Batch 540 \t Validation Loss: 38.27194229761759\n",
      "Epoch 28 \t Batch 560 \t Validation Loss: 38.10948866435459\n",
      "Epoch 28 \t Batch 580 \t Validation Loss: 37.94424520361012\n",
      "Epoch 28 \t Batch 600 \t Validation Loss: 38.17194364547729\n",
      "Epoch 28 Training Loss: 47.37475869231812 Validation Loss: 38.81005219050816\n",
      "Epoch 28 completed\n",
      "Epoch 29 \t Batch 20 \t Training Loss: 47.357429122924806\n",
      "Epoch 29 \t Batch 40 \t Training Loss: 47.86747360229492\n",
      "Epoch 29 \t Batch 60 \t Training Loss: 47.87647482554118\n",
      "Epoch 29 \t Batch 80 \t Training Loss: 47.77244319915771\n",
      "Epoch 29 \t Batch 100 \t Training Loss: 47.85155132293701\n",
      "Epoch 29 \t Batch 120 \t Training Loss: 47.718217913309736\n",
      "Epoch 29 \t Batch 140 \t Training Loss: 47.519850676400324\n",
      "Epoch 29 \t Batch 160 \t Training Loss: 47.506527757644655\n",
      "Epoch 29 \t Batch 180 \t Training Loss: 47.59278269873725\n",
      "Epoch 29 \t Batch 200 \t Training Loss: 47.377705116271976\n",
      "Epoch 29 \t Batch 220 \t Training Loss: 47.32417167316783\n",
      "Epoch 29 \t Batch 240 \t Training Loss: 47.42245906194051\n",
      "Epoch 29 \t Batch 260 \t Training Loss: 47.22993111243615\n",
      "Epoch 29 \t Batch 280 \t Training Loss: 47.31514746802194\n",
      "Epoch 29 \t Batch 300 \t Training Loss: 47.3268498357137\n",
      "Epoch 29 \t Batch 320 \t Training Loss: 47.307831740379335\n",
      "Epoch 29 \t Batch 340 \t Training Loss: 47.276002401464126\n",
      "Epoch 29 \t Batch 360 \t Training Loss: 47.26074196497599\n",
      "Epoch 29 \t Batch 380 \t Training Loss: 47.335165756627134\n",
      "Epoch 29 \t Batch 400 \t Training Loss: 47.397516918182376\n",
      "Epoch 29 \t Batch 420 \t Training Loss: 47.31279480343773\n",
      "Epoch 29 \t Batch 440 \t Training Loss: 47.30913972854614\n",
      "Epoch 29 \t Batch 460 \t Training Loss: 47.28720545561417\n",
      "Epoch 29 \t Batch 480 \t Training Loss: 47.29080085754394\n",
      "Epoch 29 \t Batch 500 \t Training Loss: 47.299223251342774\n",
      "Epoch 29 \t Batch 520 \t Training Loss: 47.35118719981267\n",
      "Epoch 29 \t Batch 540 \t Training Loss: 47.36442364586724\n",
      "Epoch 29 \t Batch 560 \t Training Loss: 47.462265505109514\n",
      "Epoch 29 \t Batch 580 \t Training Loss: 47.44169967256743\n",
      "Epoch 29 \t Batch 600 \t Training Loss: 47.39746128082275\n",
      "Epoch 29 \t Batch 620 \t Training Loss: 47.37661882215931\n",
      "Epoch 29 \t Batch 640 \t Training Loss: 47.32282964587212\n",
      "Epoch 29 \t Batch 660 \t Training Loss: 47.32260279799953\n",
      "Epoch 29 \t Batch 680 \t Training Loss: 47.30942865820492\n",
      "Epoch 29 \t Batch 700 \t Training Loss: 47.305631501334055\n",
      "Epoch 29 \t Batch 720 \t Training Loss: 47.32982521586948\n",
      "Epoch 29 \t Batch 740 \t Training Loss: 47.341251512475914\n",
      "Epoch 29 \t Batch 760 \t Training Loss: 47.35456990693745\n",
      "Epoch 29 \t Batch 780 \t Training Loss: 47.35611029893924\n",
      "Epoch 29 \t Batch 800 \t Training Loss: 47.33661256790161\n",
      "Epoch 29 \t Batch 820 \t Training Loss: 47.33731346130371\n",
      "Epoch 29 \t Batch 840 \t Training Loss: 47.33679770969209\n",
      "Epoch 29 \t Batch 860 \t Training Loss: 47.32287969367449\n",
      "Epoch 29 \t Batch 880 \t Training Loss: 47.337790034034036\n",
      "Epoch 29 \t Batch 900 \t Training Loss: 47.333758705986874\n",
      "Epoch 29 \t Batch 20 \t Validation Loss: 23.478685522079466\n",
      "Epoch 29 \t Batch 40 \t Validation Loss: 25.607304430007936\n",
      "Epoch 29 \t Batch 60 \t Validation Loss: 25.39790770212809\n",
      "Epoch 29 \t Batch 80 \t Validation Loss: 25.860645401477814\n",
      "Epoch 29 \t Batch 100 \t Validation Loss: 26.29352255821228\n",
      "Epoch 29 \t Batch 120 \t Validation Loss: 26.999254250526427\n",
      "Epoch 29 \t Batch 140 \t Validation Loss: 27.164373786108836\n",
      "Epoch 29 \t Batch 160 \t Validation Loss: 29.072920745611192\n",
      "Epoch 29 \t Batch 180 \t Validation Loss: 32.60616948339674\n",
      "Epoch 29 \t Batch 200 \t Validation Loss: 34.05638565540314\n",
      "Epoch 29 \t Batch 220 \t Validation Loss: 35.44343023733659\n",
      "Epoch 29 \t Batch 240 \t Validation Loss: 35.94012772639592\n",
      "Epoch 29 \t Batch 260 \t Validation Loss: 38.056434279221754\n",
      "Epoch 29 \t Batch 280 \t Validation Loss: 39.19923026561737\n",
      "Epoch 29 \t Batch 300 \t Validation Loss: 40.247574179967245\n",
      "Epoch 29 \t Batch 320 \t Validation Loss: 40.72784282267094\n",
      "Epoch 29 \t Batch 340 \t Validation Loss: 40.6353608047261\n",
      "Epoch 29 \t Batch 360 \t Validation Loss: 40.483605927891205\n",
      "Epoch 29 \t Batch 380 \t Validation Loss: 40.72730714647393\n",
      "Epoch 29 \t Batch 400 \t Validation Loss: 40.303971478939054\n",
      "Epoch 29 \t Batch 420 \t Validation Loss: 40.3281449613117\n",
      "Epoch 29 \t Batch 440 \t Validation Loss: 40.022973747686905\n",
      "Epoch 29 \t Batch 460 \t Validation Loss: 40.2492087882498\n",
      "Epoch 29 \t Batch 480 \t Validation Loss: 40.715478005011875\n",
      "Epoch 29 \t Batch 500 \t Validation Loss: 40.44442758750915\n",
      "Epoch 29 \t Batch 520 \t Validation Loss: 40.18224179011125\n",
      "Epoch 29 \t Batch 540 \t Validation Loss: 39.88697777677466\n",
      "Epoch 29 \t Batch 560 \t Validation Loss: 39.6272735783032\n",
      "Epoch 29 \t Batch 580 \t Validation Loss: 39.29908793054778\n",
      "Epoch 29 \t Batch 600 \t Validation Loss: 39.451526303291324\n",
      "Epoch 29 Training Loss: 47.33251026698521 Validation Loss: 40.03190755379664\n",
      "Epoch 29 completed\n",
      "Epoch 30 \t Batch 20 \t Training Loss: 46.37544555664063\n",
      "Epoch 30 \t Batch 40 \t Training Loss: 46.384036254882815\n",
      "Epoch 30 \t Batch 60 \t Training Loss: 46.77581418355306\n",
      "Epoch 30 \t Batch 80 \t Training Loss: 46.570323848724364\n",
      "Epoch 30 \t Batch 100 \t Training Loss: 46.75962657928467\n",
      "Epoch 30 \t Batch 120 \t Training Loss: 46.94067977269491\n",
      "Epoch 30 \t Batch 140 \t Training Loss: 47.1037094116211\n",
      "Epoch 30 \t Batch 160 \t Training Loss: 47.22743818759918\n",
      "Epoch 30 \t Batch 180 \t Training Loss: 47.273545286390515\n",
      "Epoch 30 \t Batch 200 \t Training Loss: 47.284412155151365\n",
      "Epoch 30 \t Batch 220 \t Training Loss: 47.34421788995916\n",
      "Epoch 30 \t Batch 240 \t Training Loss: 47.25460569063822\n",
      "Epoch 30 \t Batch 260 \t Training Loss: 47.1530098254864\n",
      "Epoch 30 \t Batch 280 \t Training Loss: 47.21865753446306\n",
      "Epoch 30 \t Batch 300 \t Training Loss: 47.29381586710612\n",
      "Epoch 30 \t Batch 320 \t Training Loss: 47.29462380409241\n",
      "Epoch 30 \t Batch 340 \t Training Loss: 47.28512796514175\n",
      "Epoch 30 \t Batch 360 \t Training Loss: 47.30899556477865\n",
      "Epoch 30 \t Batch 380 \t Training Loss: 47.27249348289088\n",
      "Epoch 30 \t Batch 400 \t Training Loss: 47.29333294868469\n",
      "Epoch 30 \t Batch 420 \t Training Loss: 47.32062940143403\n",
      "Epoch 30 \t Batch 440 \t Training Loss: 47.344204954667525\n",
      "Epoch 30 \t Batch 460 \t Training Loss: 47.3798514407614\n",
      "Epoch 30 \t Batch 480 \t Training Loss: 47.39221143722534\n",
      "Epoch 30 \t Batch 500 \t Training Loss: 47.367405029296876\n",
      "Epoch 30 \t Batch 520 \t Training Loss: 47.32504011300894\n",
      "Epoch 30 \t Batch 540 \t Training Loss: 47.351312269987886\n",
      "Epoch 30 \t Batch 560 \t Training Loss: 47.314319474356516\n",
      "Epoch 30 \t Batch 580 \t Training Loss: 47.35290370809621\n",
      "Epoch 30 \t Batch 600 \t Training Loss: 47.32604700724284\n",
      "Epoch 30 \t Batch 620 \t Training Loss: 47.277862548828125\n",
      "Epoch 30 \t Batch 640 \t Training Loss: 47.263829082250595\n",
      "Epoch 30 \t Batch 660 \t Training Loss: 47.273826344807944\n",
      "Epoch 30 \t Batch 680 \t Training Loss: 47.31952178057502\n",
      "Epoch 30 \t Batch 700 \t Training Loss: 47.31253235408238\n",
      "Epoch 30 \t Batch 720 \t Training Loss: 47.287049537234836\n",
      "Epoch 30 \t Batch 740 \t Training Loss: 47.30950605547106\n",
      "Epoch 30 \t Batch 760 \t Training Loss: 47.29443813625135\n",
      "Epoch 30 \t Batch 780 \t Training Loss: 47.27332118107722\n",
      "Epoch 30 \t Batch 800 \t Training Loss: 47.22285801887512\n",
      "Epoch 30 \t Batch 820 \t Training Loss: 47.20790530879323\n",
      "Epoch 30 \t Batch 840 \t Training Loss: 47.19788699831281\n",
      "Epoch 30 \t Batch 860 \t Training Loss: 47.23616079286087\n",
      "Epoch 30 \t Batch 880 \t Training Loss: 47.25274853272872\n",
      "Epoch 30 \t Batch 900 \t Training Loss: 47.26374117533366\n",
      "Epoch 30 \t Batch 20 \t Validation Loss: 21.16033320426941\n",
      "Epoch 30 \t Batch 40 \t Validation Loss: 23.858460998535158\n",
      "Epoch 30 \t Batch 60 \t Validation Loss: 23.44106763203939\n",
      "Epoch 30 \t Batch 80 \t Validation Loss: 23.940977108478545\n",
      "Epoch 30 \t Batch 100 \t Validation Loss: 24.79454300880432\n",
      "Epoch 30 \t Batch 120 \t Validation Loss: 25.72203696568807\n",
      "Epoch 30 \t Batch 140 \t Validation Loss: 26.03494088309152\n",
      "Epoch 30 \t Batch 160 \t Validation Loss: 27.791571617126465\n",
      "Epoch 30 \t Batch 180 \t Validation Loss: 31.10832297007243\n",
      "Epoch 30 \t Batch 200 \t Validation Loss: 32.40512485504151\n",
      "Epoch 30 \t Batch 220 \t Validation Loss: 33.59700995358554\n",
      "Epoch 30 \t Batch 240 \t Validation Loss: 34.04313395023346\n",
      "Epoch 30 \t Batch 260 \t Validation Loss: 36.0286145063547\n",
      "Epoch 30 \t Batch 280 \t Validation Loss: 37.081303926876615\n",
      "Epoch 30 \t Batch 300 \t Validation Loss: 38.03548973401387\n",
      "Epoch 30 \t Batch 320 \t Validation Loss: 38.49411675035954\n",
      "Epoch 30 \t Batch 340 \t Validation Loss: 38.41551565001993\n",
      "Epoch 30 \t Batch 360 \t Validation Loss: 38.23778660297394\n",
      "Epoch 30 \t Batch 380 \t Validation Loss: 38.474071409827786\n",
      "Epoch 30 \t Batch 400 \t Validation Loss: 38.083208515644074\n",
      "Epoch 30 \t Batch 420 \t Validation Loss: 38.12525619552249\n",
      "Epoch 30 \t Batch 440 \t Validation Loss: 37.868461311947215\n",
      "Epoch 30 \t Batch 460 \t Validation Loss: 38.08033072222834\n",
      "Epoch 30 \t Batch 480 \t Validation Loss: 38.55660238862038\n",
      "Epoch 30 \t Batch 500 \t Validation Loss: 38.248297082901004\n",
      "Epoch 30 \t Batch 520 \t Validation Loss: 37.994698900442856\n",
      "Epoch 30 \t Batch 540 \t Validation Loss: 37.78461454709371\n",
      "Epoch 30 \t Batch 560 \t Validation Loss: 37.64332550423486\n",
      "Epoch 30 \t Batch 580 \t Validation Loss: 37.402324997145556\n",
      "Epoch 30 \t Batch 600 \t Validation Loss: 37.68594180901845\n",
      "Epoch 30 Training Loss: 47.28292969814954 Validation Loss: 38.345248750277925\n",
      "Epoch 30 completed\n",
      "Epoch 31 \t Batch 20 \t Training Loss: 46.54959354400635\n",
      "Epoch 31 \t Batch 40 \t Training Loss: 47.37401275634765\n",
      "Epoch 31 \t Batch 60 \t Training Loss: 47.51202646891276\n",
      "Epoch 31 \t Batch 80 \t Training Loss: 47.57982802391052\n",
      "Epoch 31 \t Batch 100 \t Training Loss: 47.75454132080078\n",
      "Epoch 31 \t Batch 120 \t Training Loss: 47.62680559158325\n",
      "Epoch 31 \t Batch 140 \t Training Loss: 47.67515313284738\n",
      "Epoch 31 \t Batch 160 \t Training Loss: 47.61491184234619\n",
      "Epoch 31 \t Batch 180 \t Training Loss: 47.4760799407959\n",
      "Epoch 31 \t Batch 200 \t Training Loss: 47.51885368347168\n",
      "Epoch 31 \t Batch 220 \t Training Loss: 47.35250963731246\n",
      "Epoch 31 \t Batch 240 \t Training Loss: 47.36336714426677\n",
      "Epoch 31 \t Batch 260 \t Training Loss: 47.39156005565937\n",
      "Epoch 31 \t Batch 280 \t Training Loss: 47.4123816217695\n",
      "Epoch 31 \t Batch 300 \t Training Loss: 47.34103905995687\n",
      "Epoch 31 \t Batch 320 \t Training Loss: 47.383304476737976\n",
      "Epoch 31 \t Batch 340 \t Training Loss: 47.435628004635085\n",
      "Epoch 31 \t Batch 360 \t Training Loss: 47.45112547344632\n",
      "Epoch 31 \t Batch 380 \t Training Loss: 47.47575500889828\n",
      "Epoch 31 \t Batch 400 \t Training Loss: 47.46314569473267\n",
      "Epoch 31 \t Batch 420 \t Training Loss: 47.45838942754836\n",
      "Epoch 31 \t Batch 440 \t Training Loss: 47.47064254934138\n",
      "Epoch 31 \t Batch 460 \t Training Loss: 47.48638274151346\n",
      "Epoch 31 \t Batch 480 \t Training Loss: 47.46199812889099\n",
      "Epoch 31 \t Batch 500 \t Training Loss: 47.48115128326416\n",
      "Epoch 31 \t Batch 520 \t Training Loss: 47.50520798609807\n",
      "Epoch 31 \t Batch 540 \t Training Loss: 47.48859936043068\n",
      "Epoch 31 \t Batch 560 \t Training Loss: 47.50764950343541\n",
      "Epoch 31 \t Batch 580 \t Training Loss: 47.45907918338118\n",
      "Epoch 31 \t Batch 600 \t Training Loss: 47.47156050999959\n",
      "Epoch 31 \t Batch 620 \t Training Loss: 47.44051833614226\n",
      "Epoch 31 \t Batch 640 \t Training Loss: 47.44900558590889\n",
      "Epoch 31 \t Batch 660 \t Training Loss: 47.439773114522296\n",
      "Epoch 31 \t Batch 680 \t Training Loss: 47.45519382252413\n",
      "Epoch 31 \t Batch 700 \t Training Loss: 47.38053001403809\n",
      "Epoch 31 \t Batch 720 \t Training Loss: 47.35087543593512\n",
      "Epoch 31 \t Batch 740 \t Training Loss: 47.34473580025338\n",
      "Epoch 31 \t Batch 760 \t Training Loss: 47.32869315900301\n",
      "Epoch 31 \t Batch 780 \t Training Loss: 47.343662843948756\n",
      "Epoch 31 \t Batch 800 \t Training Loss: 47.32920025825501\n",
      "Epoch 31 \t Batch 820 \t Training Loss: 47.338945063149055\n",
      "Epoch 31 \t Batch 840 \t Training Loss: 47.34855195454189\n",
      "Epoch 31 \t Batch 860 \t Training Loss: 47.34301974274391\n",
      "Epoch 31 \t Batch 880 \t Training Loss: 47.28769109465859\n",
      "Epoch 31 \t Batch 900 \t Training Loss: 47.26463387383355\n",
      "Epoch 31 \t Batch 20 \t Validation Loss: 14.339942264556885\n",
      "Epoch 31 \t Batch 40 \t Validation Loss: 17.697092151641847\n",
      "Epoch 31 \t Batch 60 \t Validation Loss: 17.464017470677693\n",
      "Epoch 31 \t Batch 80 \t Validation Loss: 18.306121397018433\n",
      "Epoch 31 \t Batch 100 \t Validation Loss: 20.10952224731445\n",
      "Epoch 31 \t Batch 120 \t Validation Loss: 21.868007667859395\n",
      "Epoch 31 \t Batch 140 \t Validation Loss: 22.722402885981968\n",
      "Epoch 31 \t Batch 160 \t Validation Loss: 25.06372126340866\n",
      "Epoch 31 \t Batch 180 \t Validation Loss: 28.999579524993898\n",
      "Epoch 31 \t Batch 200 \t Validation Loss: 30.73972319602966\n",
      "Epoch 31 \t Batch 220 \t Validation Loss: 32.3462837826122\n",
      "Epoch 31 \t Batch 240 \t Validation Loss: 33.04273424943288\n",
      "Epoch 31 \t Batch 260 \t Validation Loss: 35.391577999408426\n",
      "Epoch 31 \t Batch 280 \t Validation Loss: 36.70021963800703\n",
      "Epoch 31 \t Batch 300 \t Validation Loss: 37.8299037361145\n",
      "Epoch 31 \t Batch 320 \t Validation Loss: 38.4066427052021\n",
      "Epoch 31 \t Batch 340 \t Validation Loss: 38.419906223521515\n",
      "Epoch 31 \t Batch 360 \t Validation Loss: 38.315654163890414\n",
      "Epoch 31 \t Batch 380 \t Validation Loss: 38.62473254203796\n",
      "Epoch 31 \t Batch 400 \t Validation Loss: 38.25529500246048\n",
      "Epoch 31 \t Batch 420 \t Validation Loss: 38.30193213281177\n",
      "Epoch 31 \t Batch 440 \t Validation Loss: 38.04310857382688\n",
      "Epoch 31 \t Batch 460 \t Validation Loss: 38.29449793981469\n",
      "Epoch 31 \t Batch 480 \t Validation Loss: 38.8302704513073\n",
      "Epoch 31 \t Batch 500 \t Validation Loss: 38.59092828178406\n",
      "Epoch 31 \t Batch 520 \t Validation Loss: 38.340204787254336\n",
      "Epoch 31 \t Batch 540 \t Validation Loss: 38.127227993364684\n",
      "Epoch 31 \t Batch 560 \t Validation Loss: 37.95990938629423\n",
      "Epoch 31 \t Batch 580 \t Validation Loss: 37.70136956346446\n",
      "Epoch 31 \t Batch 600 \t Validation Loss: 37.95213964303335\n",
      "Epoch 31 Training Loss: 47.258329239640396 Validation Loss: 38.59905379933196\n",
      "Epoch 31 completed\n",
      "Epoch 32 \t Batch 20 \t Training Loss: 47.16324710845947\n",
      "Epoch 32 \t Batch 40 \t Training Loss: 47.74479265213013\n",
      "Epoch 32 \t Batch 60 \t Training Loss: 47.38990421295166\n",
      "Epoch 32 \t Batch 80 \t Training Loss: 47.38033380508423\n",
      "Epoch 32 \t Batch 100 \t Training Loss: 47.189057693481445\n",
      "Epoch 32 \t Batch 120 \t Training Loss: 47.30619093577067\n",
      "Epoch 32 \t Batch 140 \t Training Loss: 47.14332823072161\n",
      "Epoch 32 \t Batch 160 \t Training Loss: 47.133036971092224\n",
      "Epoch 32 \t Batch 180 \t Training Loss: 47.270505078633626\n",
      "Epoch 32 \t Batch 200 \t Training Loss: 47.46377016067505\n",
      "Epoch 32 \t Batch 220 \t Training Loss: 47.424017281965774\n",
      "Epoch 32 \t Batch 240 \t Training Loss: 47.32327419916789\n",
      "Epoch 32 \t Batch 260 \t Training Loss: 47.29864336160513\n",
      "Epoch 32 \t Batch 280 \t Training Loss: 47.24636810847691\n",
      "Epoch 32 \t Batch 300 \t Training Loss: 47.205374565124515\n",
      "Epoch 32 \t Batch 320 \t Training Loss: 47.15041185617447\n",
      "Epoch 32 \t Batch 340 \t Training Loss: 47.21973461824305\n",
      "Epoch 32 \t Batch 360 \t Training Loss: 47.2812667104933\n",
      "Epoch 32 \t Batch 380 \t Training Loss: 47.19378752457468\n",
      "Epoch 32 \t Batch 400 \t Training Loss: 47.25401997566223\n",
      "Epoch 32 \t Batch 420 \t Training Loss: 47.273180443899975\n",
      "Epoch 32 \t Batch 440 \t Training Loss: 47.20481628071178\n",
      "Epoch 32 \t Batch 460 \t Training Loss: 47.15668443596881\n",
      "Epoch 32 \t Batch 480 \t Training Loss: 47.19168190161387\n",
      "Epoch 32 \t Batch 500 \t Training Loss: 47.16390514373779\n",
      "Epoch 32 \t Batch 520 \t Training Loss: 47.18246787144587\n",
      "Epoch 32 \t Batch 540 \t Training Loss: 47.158336342705624\n",
      "Epoch 32 \t Batch 560 \t Training Loss: 47.155129180635726\n",
      "Epoch 32 \t Batch 580 \t Training Loss: 47.183534194683205\n",
      "Epoch 32 \t Batch 600 \t Training Loss: 47.15859230677287\n",
      "Epoch 32 \t Batch 620 \t Training Loss: 47.19927133129489\n",
      "Epoch 32 \t Batch 640 \t Training Loss: 47.25656526684761\n",
      "Epoch 32 \t Batch 660 \t Training Loss: 47.257217806035825\n",
      "Epoch 32 \t Batch 680 \t Training Loss: 47.23146929460413\n",
      "Epoch 32 \t Batch 700 \t Training Loss: 47.27005638122559\n",
      "Epoch 32 \t Batch 720 \t Training Loss: 47.24180188708835\n",
      "Epoch 32 \t Batch 740 \t Training Loss: 47.24895611840326\n",
      "Epoch 32 \t Batch 760 \t Training Loss: 47.26614457180626\n",
      "Epoch 32 \t Batch 780 \t Training Loss: 47.2401296664507\n",
      "Epoch 32 \t Batch 800 \t Training Loss: 47.22617357730866\n",
      "Epoch 32 \t Batch 820 \t Training Loss: 47.239214445904985\n",
      "Epoch 32 \t Batch 840 \t Training Loss: 47.21966892878215\n",
      "Epoch 32 \t Batch 860 \t Training Loss: 47.22901735527571\n",
      "Epoch 32 \t Batch 880 \t Training Loss: 47.237127273732966\n",
      "Epoch 32 \t Batch 900 \t Training Loss: 47.23811230977376\n",
      "Epoch 32 \t Batch 20 \t Validation Loss: 20.87603669166565\n",
      "Epoch 32 \t Batch 40 \t Validation Loss: 22.698112344741823\n",
      "Epoch 32 \t Batch 60 \t Validation Loss: 22.476588106155397\n",
      "Epoch 32 \t Batch 80 \t Validation Loss: 22.86763949394226\n",
      "Epoch 32 \t Batch 100 \t Validation Loss: 23.8031329536438\n",
      "Epoch 32 \t Batch 120 \t Validation Loss: 24.71664691766103\n",
      "Epoch 32 \t Batch 140 \t Validation Loss: 25.178201280321392\n",
      "Epoch 32 \t Batch 160 \t Validation Loss: 27.055631864070893\n",
      "Epoch 32 \t Batch 180 \t Validation Loss: 30.597151221169366\n",
      "Epoch 32 \t Batch 200 \t Validation Loss: 32.05692341327667\n",
      "Epoch 32 \t Batch 220 \t Validation Loss: 33.272262473539875\n",
      "Epoch 32 \t Batch 240 \t Validation Loss: 33.74637823104858\n",
      "Epoch 32 \t Batch 260 \t Validation Loss: 35.79503926864037\n",
      "Epoch 32 \t Batch 280 \t Validation Loss: 36.87322452408927\n",
      "Epoch 32 \t Batch 300 \t Validation Loss: 37.965921554565426\n",
      "Epoch 32 \t Batch 320 \t Validation Loss: 38.44332764446735\n",
      "Epoch 32 \t Batch 340 \t Validation Loss: 38.37853031999924\n",
      "Epoch 32 \t Batch 360 \t Validation Loss: 38.24152534537845\n",
      "Epoch 32 \t Batch 380 \t Validation Loss: 38.452165018884756\n",
      "Epoch 32 \t Batch 400 \t Validation Loss: 38.06242818117142\n",
      "Epoch 32 \t Batch 420 \t Validation Loss: 38.07016686711992\n",
      "Epoch 32 \t Batch 440 \t Validation Loss: 37.78615623604168\n",
      "Epoch 32 \t Batch 460 \t Validation Loss: 38.007456400083456\n",
      "Epoch 32 \t Batch 480 \t Validation Loss: 38.49121497670809\n",
      "Epoch 32 \t Batch 500 \t Validation Loss: 38.196950803756714\n",
      "Epoch 32 \t Batch 520 \t Validation Loss: 38.02747287200047\n",
      "Epoch 32 \t Batch 540 \t Validation Loss: 37.825948819407714\n",
      "Epoch 32 \t Batch 560 \t Validation Loss: 37.65208686930793\n",
      "Epoch 32 \t Batch 580 \t Validation Loss: 37.48586776503201\n",
      "Epoch 32 \t Batch 600 \t Validation Loss: 37.70948403199514\n",
      "Epoch 32 Training Loss: 47.238438900580455 Validation Loss: 38.412046012940344\n",
      "Epoch 32 completed\n",
      "Epoch 33 \t Batch 20 \t Training Loss: 46.04521560668945\n",
      "Epoch 33 \t Batch 40 \t Training Loss: 46.301618576049805\n",
      "Epoch 33 \t Batch 60 \t Training Loss: 46.01997585296631\n",
      "Epoch 33 \t Batch 80 \t Training Loss: 46.29834661483765\n",
      "Epoch 33 \t Batch 100 \t Training Loss: 46.224685974121094\n",
      "Epoch 33 \t Batch 120 \t Training Loss: 46.28889195124308\n",
      "Epoch 33 \t Batch 140 \t Training Loss: 46.38254844120571\n",
      "Epoch 33 \t Batch 160 \t Training Loss: 46.30993821620941\n",
      "Epoch 33 \t Batch 180 \t Training Loss: 46.5153923034668\n",
      "Epoch 33 \t Batch 200 \t Training Loss: 46.48271995544434\n",
      "Epoch 33 \t Batch 220 \t Training Loss: 46.58805770874024\n",
      "Epoch 33 \t Batch 240 \t Training Loss: 46.70023409525553\n",
      "Epoch 33 \t Batch 260 \t Training Loss: 46.6244999225323\n",
      "Epoch 33 \t Batch 280 \t Training Loss: 46.78728358404977\n",
      "Epoch 33 \t Batch 300 \t Training Loss: 46.84622155507406\n",
      "Epoch 33 \t Batch 320 \t Training Loss: 46.97425276041031\n",
      "Epoch 33 \t Batch 340 \t Training Loss: 46.99075416116153\n",
      "Epoch 33 \t Batch 360 \t Training Loss: 47.049508825937906\n",
      "Epoch 33 \t Batch 380 \t Training Loss: 46.94906827023155\n",
      "Epoch 33 \t Batch 400 \t Training Loss: 47.04151019096375\n",
      "Epoch 33 \t Batch 420 \t Training Loss: 47.09483232044038\n",
      "Epoch 33 \t Batch 440 \t Training Loss: 47.16160443045876\n",
      "Epoch 33 \t Batch 460 \t Training Loss: 47.16068811831267\n",
      "Epoch 33 \t Batch 480 \t Training Loss: 47.19015996456146\n",
      "Epoch 33 \t Batch 500 \t Training Loss: 47.16695835113526\n",
      "Epoch 33 \t Batch 520 \t Training Loss: 47.160359162550705\n",
      "Epoch 33 \t Batch 540 \t Training Loss: 47.20018408740008\n",
      "Epoch 33 \t Batch 560 \t Training Loss: 47.181018822533744\n",
      "Epoch 33 \t Batch 580 \t Training Loss: 47.20207054532808\n",
      "Epoch 33 \t Batch 600 \t Training Loss: 47.2090474764506\n",
      "Epoch 33 \t Batch 620 \t Training Loss: 47.19449076498709\n",
      "Epoch 33 \t Batch 640 \t Training Loss: 47.18891511559487\n",
      "Epoch 33 \t Batch 660 \t Training Loss: 47.2267285491481\n",
      "Epoch 33 \t Batch 680 \t Training Loss: 47.22321528266458\n",
      "Epoch 33 \t Batch 700 \t Training Loss: 47.221285182407925\n",
      "Epoch 33 \t Batch 720 \t Training Loss: 47.21248271200392\n",
      "Epoch 33 \t Batch 740 \t Training Loss: 47.22333966332513\n",
      "Epoch 33 \t Batch 760 \t Training Loss: 47.24716732627467\n",
      "Epoch 33 \t Batch 780 \t Training Loss: 47.280359625205016\n",
      "Epoch 33 \t Batch 800 \t Training Loss: 47.26060406684876\n",
      "Epoch 33 \t Batch 820 \t Training Loss: 47.221123411597276\n",
      "Epoch 33 \t Batch 840 \t Training Loss: 47.22853073392596\n",
      "Epoch 33 \t Batch 860 \t Training Loss: 47.22472294208615\n",
      "Epoch 33 \t Batch 880 \t Training Loss: 47.18748545213179\n",
      "Epoch 33 \t Batch 900 \t Training Loss: 47.19283846537272\n",
      "Epoch 33 \t Batch 20 \t Validation Loss: 13.943245148658752\n",
      "Epoch 33 \t Batch 40 \t Validation Loss: 18.545339465141296\n",
      "Epoch 33 \t Batch 60 \t Validation Loss: 17.79760791460673\n",
      "Epoch 33 \t Batch 80 \t Validation Loss: 18.738871109485626\n",
      "Epoch 33 \t Batch 100 \t Validation Loss: 20.756176824569703\n",
      "Epoch 33 \t Batch 120 \t Validation Loss: 22.26631659666697\n",
      "Epoch 33 \t Batch 140 \t Validation Loss: 23.211716876711165\n",
      "Epoch 33 \t Batch 160 \t Validation Loss: 25.63221680521965\n",
      "Epoch 33 \t Batch 180 \t Validation Loss: 29.55268809000651\n",
      "Epoch 33 \t Batch 200 \t Validation Loss: 31.331706638336183\n",
      "Epoch 33 \t Batch 220 \t Validation Loss: 32.82574809681285\n",
      "Epoch 33 \t Batch 240 \t Validation Loss: 33.48637078603109\n",
      "Epoch 33 \t Batch 260 \t Validation Loss: 35.76954512229332\n",
      "Epoch 33 \t Batch 280 \t Validation Loss: 37.000975775718686\n",
      "Epoch 33 \t Batch 300 \t Validation Loss: 38.23424591382344\n",
      "Epoch 33 \t Batch 320 \t Validation Loss: 38.819731971621515\n",
      "Epoch 33 \t Batch 340 \t Validation Loss: 38.797296936371744\n",
      "Epoch 33 \t Batch 360 \t Validation Loss: 38.778999050458275\n",
      "Epoch 33 \t Batch 380 \t Validation Loss: 39.10903992150959\n",
      "Epoch 33 \t Batch 400 \t Validation Loss: 38.768344733715054\n",
      "Epoch 33 \t Batch 420 \t Validation Loss: 38.78070252055213\n",
      "Epoch 33 \t Batch 440 \t Validation Loss: 38.557271937890484\n",
      "Epoch 33 \t Batch 460 \t Validation Loss: 38.92405734476836\n",
      "Epoch 33 \t Batch 480 \t Validation Loss: 39.437407992283504\n",
      "Epoch 33 \t Batch 500 \t Validation Loss: 39.174736906051635\n",
      "Epoch 33 \t Batch 520 \t Validation Loss: 39.08961595755357\n",
      "Epoch 33 \t Batch 540 \t Validation Loss: 38.939760529553446\n",
      "Epoch 33 \t Batch 560 \t Validation Loss: 38.850285686765396\n",
      "Epoch 33 \t Batch 580 \t Validation Loss: 38.78131415926177\n",
      "Epoch 33 \t Batch 600 \t Validation Loss: 39.03928037643433\n",
      "Epoch 33 Training Loss: 47.17318691474141 Validation Loss: 39.756094861340216\n",
      "Epoch 33 completed\n",
      "Epoch 34 \t Batch 20 \t Training Loss: 48.62120304107666\n",
      "Epoch 34 \t Batch 40 \t Training Loss: 48.49942855834961\n",
      "Epoch 34 \t Batch 60 \t Training Loss: 48.680266571044925\n",
      "Epoch 34 \t Batch 80 \t Training Loss: 48.42004871368408\n",
      "Epoch 34 \t Batch 100 \t Training Loss: 48.02940277099609\n",
      "Epoch 34 \t Batch 120 \t Training Loss: 47.94292087554932\n",
      "Epoch 34 \t Batch 140 \t Training Loss: 47.72005378178188\n",
      "Epoch 34 \t Batch 160 \t Training Loss: 47.6104914188385\n",
      "Epoch 34 \t Batch 180 \t Training Loss: 47.724269994099934\n",
      "Epoch 34 \t Batch 200 \t Training Loss: 47.7250710105896\n",
      "Epoch 34 \t Batch 220 \t Training Loss: 47.66090100028298\n",
      "Epoch 34 \t Batch 240 \t Training Loss: 47.5868553797404\n",
      "Epoch 34 \t Batch 260 \t Training Loss: 47.51938976874718\n",
      "Epoch 34 \t Batch 280 \t Training Loss: 47.348048850468224\n",
      "Epoch 34 \t Batch 300 \t Training Loss: 47.30144865671794\n",
      "Epoch 34 \t Batch 320 \t Training Loss: 47.282901394367215\n",
      "Epoch 34 \t Batch 340 \t Training Loss: 47.28649546679328\n",
      "Epoch 34 \t Batch 360 \t Training Loss: 47.28364241917928\n",
      "Epoch 34 \t Batch 380 \t Training Loss: 47.25546200400905\n",
      "Epoch 34 \t Batch 400 \t Training Loss: 47.22221767425537\n",
      "Epoch 34 \t Batch 420 \t Training Loss: 47.17085699353899\n",
      "Epoch 34 \t Batch 440 \t Training Loss: 47.185941167311235\n",
      "Epoch 34 \t Batch 460 \t Training Loss: 47.205662743941595\n",
      "Epoch 34 \t Batch 480 \t Training Loss: 47.2188319683075\n",
      "Epoch 34 \t Batch 500 \t Training Loss: 47.198624618530275\n",
      "Epoch 34 \t Batch 520 \t Training Loss: 47.20882617510282\n",
      "Epoch 34 \t Batch 540 \t Training Loss: 47.18580098328767\n",
      "Epoch 34 \t Batch 560 \t Training Loss: 47.17580945151193\n",
      "Epoch 34 \t Batch 580 \t Training Loss: 47.200401615274366\n",
      "Epoch 34 \t Batch 600 \t Training Loss: 47.18386359532674\n",
      "Epoch 34 \t Batch 620 \t Training Loss: 47.215213861773094\n",
      "Epoch 34 \t Batch 640 \t Training Loss: 47.178140139579774\n",
      "Epoch 34 \t Batch 660 \t Training Loss: 47.1922942421653\n",
      "Epoch 34 \t Batch 680 \t Training Loss: 47.17876545962165\n",
      "Epoch 34 \t Batch 700 \t Training Loss: 47.196041477748324\n",
      "Epoch 34 \t Batch 720 \t Training Loss: 47.21809474627177\n",
      "Epoch 34 \t Batch 740 \t Training Loss: 47.21891691362536\n",
      "Epoch 34 \t Batch 760 \t Training Loss: 47.20743916160182\n",
      "Epoch 34 \t Batch 780 \t Training Loss: 47.152767161833935\n",
      "Epoch 34 \t Batch 800 \t Training Loss: 47.12075372695923\n",
      "Epoch 34 \t Batch 820 \t Training Loss: 47.11497050029475\n",
      "Epoch 34 \t Batch 840 \t Training Loss: 47.14701949982416\n",
      "Epoch 34 \t Batch 860 \t Training Loss: 47.15275142137394\n",
      "Epoch 34 \t Batch 880 \t Training Loss: 47.16938930858265\n",
      "Epoch 34 \t Batch 900 \t Training Loss: 47.18357042100695\n",
      "Epoch 34 \t Batch 20 \t Validation Loss: 18.805858182907105\n",
      "Epoch 34 \t Batch 40 \t Validation Loss: 21.03793046474457\n",
      "Epoch 34 \t Batch 60 \t Validation Loss: 21.024478594462078\n",
      "Epoch 34 \t Batch 80 \t Validation Loss: 21.591436088085175\n",
      "Epoch 34 \t Batch 100 \t Validation Loss: 22.79628664970398\n",
      "Epoch 34 \t Batch 120 \t Validation Loss: 23.8811953385671\n",
      "Epoch 34 \t Batch 140 \t Validation Loss: 24.33548413004194\n",
      "Epoch 34 \t Batch 160 \t Validation Loss: 26.003947669267653\n",
      "Epoch 34 \t Batch 180 \t Validation Loss: 29.081447511249117\n",
      "Epoch 34 \t Batch 200 \t Validation Loss: 30.298825240135194\n",
      "Epoch 34 \t Batch 220 \t Validation Loss: 31.36401677131653\n",
      "Epoch 34 \t Batch 240 \t Validation Loss: 31.73589752117793\n",
      "Epoch 34 \t Batch 260 \t Validation Loss: 33.64651773892916\n",
      "Epoch 34 \t Batch 280 \t Validation Loss: 34.689081362315584\n",
      "Epoch 34 \t Batch 300 \t Validation Loss: 35.59804056803385\n",
      "Epoch 34 \t Batch 320 \t Validation Loss: 36.07494803965092\n",
      "Epoch 34 \t Batch 340 \t Validation Loss: 36.10064365443061\n",
      "Epoch 34 \t Batch 360 \t Validation Loss: 35.94556966357761\n",
      "Epoch 34 \t Batch 380 \t Validation Loss: 36.207251001659195\n",
      "Epoch 34 \t Batch 400 \t Validation Loss: 35.88294605731964\n",
      "Epoch 34 \t Batch 420 \t Validation Loss: 36.003872701099944\n",
      "Epoch 34 \t Batch 440 \t Validation Loss: 35.791560972820626\n",
      "Epoch 34 \t Batch 460 \t Validation Loss: 36.089389006987865\n",
      "Epoch 34 \t Batch 480 \t Validation Loss: 36.64180897275607\n",
      "Epoch 34 \t Batch 500 \t Validation Loss: 36.40657820701599\n",
      "Epoch 34 \t Batch 520 \t Validation Loss: 36.19620384619786\n",
      "Epoch 34 \t Batch 540 \t Validation Loss: 36.031590686021026\n",
      "Epoch 34 \t Batch 560 \t Validation Loss: 35.90334272554943\n",
      "Epoch 34 \t Batch 580 \t Validation Loss: 35.649453404854086\n",
      "Epoch 34 \t Batch 600 \t Validation Loss: 35.96085482438405\n",
      "Epoch 34 Training Loss: 47.15323534916635 Validation Loss: 36.633244450990254\n",
      "Validation Loss Decreased(22705.329370498657--->22566.078581809998) Saving The Model\n",
      "Epoch 34 completed\n",
      "Epoch 35 \t Batch 20 \t Training Loss: 46.571598815917966\n",
      "Epoch 35 \t Batch 40 \t Training Loss: 47.2309811592102\n",
      "Epoch 35 \t Batch 60 \t Training Loss: 47.03533802032471\n",
      "Epoch 35 \t Batch 80 \t Training Loss: 47.389459466934206\n",
      "Epoch 35 \t Batch 100 \t Training Loss: 47.39729515075683\n",
      "Epoch 35 \t Batch 120 \t Training Loss: 47.049501991271974\n",
      "Epoch 35 \t Batch 140 \t Training Loss: 46.8555474962507\n",
      "Epoch 35 \t Batch 160 \t Training Loss: 46.95981268882751\n",
      "Epoch 35 \t Batch 180 \t Training Loss: 47.08042401207818\n",
      "Epoch 35 \t Batch 200 \t Training Loss: 47.07098934173584\n",
      "Epoch 35 \t Batch 220 \t Training Loss: 47.12258146459406\n",
      "Epoch 35 \t Batch 240 \t Training Loss: 47.113626607259114\n",
      "Epoch 35 \t Batch 260 \t Training Loss: 47.1134458395151\n",
      "Epoch 35 \t Batch 280 \t Training Loss: 47.12836164746966\n",
      "Epoch 35 \t Batch 300 \t Training Loss: 47.17012460072835\n",
      "Epoch 35 \t Batch 320 \t Training Loss: 47.135188376903535\n",
      "Epoch 35 \t Batch 340 \t Training Loss: 47.16189753588508\n",
      "Epoch 35 \t Batch 360 \t Training Loss: 47.13324237399631\n",
      "Epoch 35 \t Batch 380 \t Training Loss: 47.09639127630936\n",
      "Epoch 35 \t Batch 400 \t Training Loss: 47.02381521224976\n",
      "Epoch 35 \t Batch 420 \t Training Loss: 47.04329116457985\n",
      "Epoch 35 \t Batch 440 \t Training Loss: 47.165697175806216\n",
      "Epoch 35 \t Batch 460 \t Training Loss: 47.21781665138576\n",
      "Epoch 35 \t Batch 480 \t Training Loss: 47.2072732925415\n",
      "Epoch 35 \t Batch 500 \t Training Loss: 47.207830909729005\n",
      "Epoch 35 \t Batch 520 \t Training Loss: 47.23725129641019\n",
      "Epoch 35 \t Batch 540 \t Training Loss: 47.2679753197564\n",
      "Epoch 35 \t Batch 560 \t Training Loss: 47.252496889659334\n",
      "Epoch 35 \t Batch 580 \t Training Loss: 47.19794146439125\n",
      "Epoch 35 \t Batch 600 \t Training Loss: 47.23871894200643\n",
      "Epoch 35 \t Batch 620 \t Training Loss: 47.228636655499855\n",
      "Epoch 35 \t Batch 640 \t Training Loss: 47.247316187620164\n",
      "Epoch 35 \t Batch 660 \t Training Loss: 47.25497127301765\n",
      "Epoch 35 \t Batch 680 \t Training Loss: 47.26609609828276\n",
      "Epoch 35 \t Batch 700 \t Training Loss: 47.20943089076451\n",
      "Epoch 35 \t Batch 720 \t Training Loss: 47.2049377017551\n",
      "Epoch 35 \t Batch 740 \t Training Loss: 47.18037916647421\n",
      "Epoch 35 \t Batch 760 \t Training Loss: 47.18280729494597\n",
      "Epoch 35 \t Batch 780 \t Training Loss: 47.18622890863663\n",
      "Epoch 35 \t Batch 800 \t Training Loss: 47.19078777313232\n",
      "Epoch 35 \t Batch 820 \t Training Loss: 47.160159855354124\n",
      "Epoch 35 \t Batch 840 \t Training Loss: 47.18245204743885\n",
      "Epoch 35 \t Batch 860 \t Training Loss: 47.13990820064101\n",
      "Epoch 35 \t Batch 880 \t Training Loss: 47.1014424497431\n",
      "Epoch 35 \t Batch 900 \t Training Loss: 47.10889767540826\n",
      "Epoch 35 \t Batch 20 \t Validation Loss: 27.683884191513062\n",
      "Epoch 35 \t Batch 40 \t Validation Loss: 28.268236207962037\n",
      "Epoch 35 \t Batch 60 \t Validation Loss: 28.675656700134276\n",
      "Epoch 35 \t Batch 80 \t Validation Loss: 29.017143666744232\n",
      "Epoch 35 \t Batch 100 \t Validation Loss: 29.14410443305969\n",
      "Epoch 35 \t Batch 120 \t Validation Loss: 29.49901332060496\n",
      "Epoch 35 \t Batch 140 \t Validation Loss: 29.393360934938702\n",
      "Epoch 35 \t Batch 160 \t Validation Loss: 30.87650024294853\n",
      "Epoch 35 \t Batch 180 \t Validation Loss: 34.14453435473972\n",
      "Epoch 35 \t Batch 200 \t Validation Loss: 35.262995862960814\n",
      "Epoch 35 \t Batch 220 \t Validation Loss: 36.38522347536954\n",
      "Epoch 35 \t Batch 240 \t Validation Loss: 36.72592690785726\n",
      "Epoch 35 \t Batch 260 \t Validation Loss: 38.644411182403566\n",
      "Epoch 35 \t Batch 280 \t Validation Loss: 39.59690476485661\n",
      "Epoch 35 \t Batch 300 \t Validation Loss: 40.60447699546814\n",
      "Epoch 35 \t Batch 320 \t Validation Loss: 40.99821291267872\n",
      "Epoch 35 \t Batch 340 \t Validation Loss: 40.82859766623553\n",
      "Epoch 35 \t Batch 360 \t Validation Loss: 40.60803618166182\n",
      "Epoch 35 \t Batch 380 \t Validation Loss: 40.74080546780637\n",
      "Epoch 35 \t Batch 400 \t Validation Loss: 40.22103793859482\n",
      "Epoch 35 \t Batch 420 \t Validation Loss: 40.146542560486566\n",
      "Epoch 35 \t Batch 440 \t Validation Loss: 39.77490838440982\n",
      "Epoch 35 \t Batch 460 \t Validation Loss: 39.95286905247232\n",
      "Epoch 35 \t Batch 480 \t Validation Loss: 40.37113268574079\n",
      "Epoch 35 \t Batch 500 \t Validation Loss: 40.0491846408844\n",
      "Epoch 35 \t Batch 520 \t Validation Loss: 39.777000205333415\n",
      "Epoch 35 \t Batch 540 \t Validation Loss: 39.48155071293866\n",
      "Epoch 35 \t Batch 560 \t Validation Loss: 39.251005423069\n",
      "Epoch 35 \t Batch 580 \t Validation Loss: 38.97746606859668\n",
      "Epoch 35 \t Batch 600 \t Validation Loss: 39.15285545508067\n",
      "Epoch 35 Training Loss: 47.1005659196993 Validation Loss: 39.791679761626504\n",
      "Epoch 35 completed\n",
      "Epoch 36 \t Batch 20 \t Training Loss: 46.45032005310058\n",
      "Epoch 36 \t Batch 40 \t Training Loss: 46.51684379577637\n",
      "Epoch 36 \t Batch 60 \t Training Loss: 46.55026569366455\n",
      "Epoch 36 \t Batch 80 \t Training Loss: 46.571834325790405\n",
      "Epoch 36 \t Batch 100 \t Training Loss: 46.826362190246584\n",
      "Epoch 36 \t Batch 120 \t Training Loss: 46.95699116388957\n",
      "Epoch 36 \t Batch 140 \t Training Loss: 46.91908051627023\n",
      "Epoch 36 \t Batch 160 \t Training Loss: 46.83352482318878\n",
      "Epoch 36 \t Batch 180 \t Training Loss: 46.94492971632216\n",
      "Epoch 36 \t Batch 200 \t Training Loss: 46.986442985534666\n",
      "Epoch 36 \t Batch 220 \t Training Loss: 47.04015391956676\n",
      "Epoch 36 \t Batch 240 \t Training Loss: 47.01787357330322\n",
      "Epoch 36 \t Batch 260 \t Training Loss: 46.96819298083965\n",
      "Epoch 36 \t Batch 280 \t Training Loss: 46.91628243582589\n",
      "Epoch 36 \t Batch 300 \t Training Loss: 46.876563212076825\n",
      "Epoch 36 \t Batch 320 \t Training Loss: 46.84991809129715\n",
      "Epoch 36 \t Batch 340 \t Training Loss: 46.840166271434114\n",
      "Epoch 36 \t Batch 360 \t Training Loss: 46.78793687820435\n",
      "Epoch 36 \t Batch 380 \t Training Loss: 46.79406439128675\n",
      "Epoch 36 \t Batch 400 \t Training Loss: 46.771175937652586\n",
      "Epoch 36 \t Batch 420 \t Training Loss: 46.837282444181895\n",
      "Epoch 36 \t Batch 440 \t Training Loss: 46.834066625074904\n",
      "Epoch 36 \t Batch 460 \t Training Loss: 46.90944758705471\n",
      "Epoch 36 \t Batch 480 \t Training Loss: 46.918594733874\n",
      "Epoch 36 \t Batch 500 \t Training Loss: 46.9011528930664\n",
      "Epoch 36 \t Batch 520 \t Training Loss: 46.88298884171706\n",
      "Epoch 36 \t Batch 540 \t Training Loss: 46.882921586213286\n",
      "Epoch 36 \t Batch 560 \t Training Loss: 46.9236889907292\n",
      "Epoch 36 \t Batch 580 \t Training Loss: 46.93582743940682\n",
      "Epoch 36 \t Batch 600 \t Training Loss: 46.90816565831502\n",
      "Epoch 36 \t Batch 620 \t Training Loss: 46.88306744483209\n",
      "Epoch 36 \t Batch 640 \t Training Loss: 46.895346480607984\n",
      "Epoch 36 \t Batch 660 \t Training Loss: 46.915356595588456\n",
      "Epoch 36 \t Batch 680 \t Training Loss: 46.90243364782894\n",
      "Epoch 36 \t Batch 700 \t Training Loss: 46.877299003601074\n",
      "Epoch 36 \t Batch 720 \t Training Loss: 46.89703548219469\n",
      "Epoch 36 \t Batch 740 \t Training Loss: 46.92204083107613\n",
      "Epoch 36 \t Batch 760 \t Training Loss: 46.933494869031406\n",
      "Epoch 36 \t Batch 780 \t Training Loss: 46.89321961525159\n",
      "Epoch 36 \t Batch 800 \t Training Loss: 46.9110253572464\n",
      "Epoch 36 \t Batch 820 \t Training Loss: 46.947889723428865\n",
      "Epoch 36 \t Batch 840 \t Training Loss: 46.94446111406599\n",
      "Epoch 36 \t Batch 860 \t Training Loss: 46.99587216044581\n",
      "Epoch 36 \t Batch 880 \t Training Loss: 47.014488471638074\n",
      "Epoch 36 \t Batch 900 \t Training Loss: 47.03884642283122\n",
      "Epoch 36 \t Batch 20 \t Validation Loss: 24.366710329055785\n",
      "Epoch 36 \t Batch 40 \t Validation Loss: 26.150662970542907\n",
      "Epoch 36 \t Batch 60 \t Validation Loss: 25.98856890996297\n",
      "Epoch 36 \t Batch 80 \t Validation Loss: 26.484666669368742\n",
      "Epoch 36 \t Batch 100 \t Validation Loss: 26.910193128585817\n",
      "Epoch 36 \t Batch 120 \t Validation Loss: 27.381131235758463\n",
      "Epoch 36 \t Batch 140 \t Validation Loss: 27.547018909454344\n",
      "Epoch 36 \t Batch 160 \t Validation Loss: 29.35300507545471\n",
      "Epoch 36 \t Batch 180 \t Validation Loss: 33.16168154610528\n",
      "Epoch 36 \t Batch 200 \t Validation Loss: 34.628101139068605\n",
      "Epoch 36 \t Batch 220 \t Validation Loss: 35.87879424528642\n",
      "Epoch 36 \t Batch 240 \t Validation Loss: 36.36527219613393\n",
      "Epoch 36 \t Batch 260 \t Validation Loss: 38.44244029705341\n",
      "Epoch 36 \t Batch 280 \t Validation Loss: 39.476925826072694\n",
      "Epoch 36 \t Batch 300 \t Validation Loss: 40.70820081392924\n",
      "Epoch 36 \t Batch 320 \t Validation Loss: 41.192660990357396\n",
      "Epoch 36 \t Batch 340 \t Validation Loss: 41.060233304079844\n",
      "Epoch 36 \t Batch 360 \t Validation Loss: 40.88225574228499\n",
      "Epoch 36 \t Batch 380 \t Validation Loss: 41.03461478132951\n",
      "Epoch 36 \t Batch 400 \t Validation Loss: 40.56935560464859\n",
      "Epoch 36 \t Batch 420 \t Validation Loss: 40.494596274693805\n",
      "Epoch 36 \t Batch 440 \t Validation Loss: 40.135760773311965\n",
      "Epoch 36 \t Batch 460 \t Validation Loss: 40.32029991357223\n",
      "Epoch 36 \t Batch 480 \t Validation Loss: 40.740454159180324\n",
      "Epoch 36 \t Batch 500 \t Validation Loss: 40.41421922492981\n",
      "Epoch 36 \t Batch 520 \t Validation Loss: 40.18084691304427\n",
      "Epoch 36 \t Batch 540 \t Validation Loss: 39.93632977626942\n",
      "Epoch 36 \t Batch 560 \t Validation Loss: 39.72650334664753\n",
      "Epoch 36 \t Batch 580 \t Validation Loss: 39.521386174497934\n",
      "Epoch 36 \t Batch 600 \t Validation Loss: 39.686518702507016\n",
      "Epoch 36 Training Loss: 47.059126010507896 Validation Loss: 40.29698292465953\n",
      "Epoch 36 completed\n",
      "Epoch 37 \t Batch 20 \t Training Loss: 47.8786584854126\n",
      "Epoch 37 \t Batch 40 \t Training Loss: 46.82484617233276\n",
      "Epoch 37 \t Batch 60 \t Training Loss: 47.35740636189779\n",
      "Epoch 37 \t Batch 80 \t Training Loss: 47.13266448974609\n",
      "Epoch 37 \t Batch 100 \t Training Loss: 46.8549426651001\n",
      "Epoch 37 \t Batch 120 \t Training Loss: 47.01373157501221\n",
      "Epoch 37 \t Batch 140 \t Training Loss: 46.797604424612864\n",
      "Epoch 37 \t Batch 160 \t Training Loss: 46.80930242538452\n",
      "Epoch 37 \t Batch 180 \t Training Loss: 46.79851449330648\n",
      "Epoch 37 \t Batch 200 \t Training Loss: 47.06707151412964\n",
      "Epoch 37 \t Batch 220 \t Training Loss: 47.01810039173473\n",
      "Epoch 37 \t Batch 240 \t Training Loss: 47.09280142784119\n",
      "Epoch 37 \t Batch 260 \t Training Loss: 47.051087804941034\n",
      "Epoch 37 \t Batch 280 \t Training Loss: 46.942355673653736\n",
      "Epoch 37 \t Batch 300 \t Training Loss: 46.98376487731934\n",
      "Epoch 37 \t Batch 320 \t Training Loss: 46.8959415435791\n",
      "Epoch 37 \t Batch 340 \t Training Loss: 46.83013054342831\n",
      "Epoch 37 \t Batch 360 \t Training Loss: 46.78429882261488\n",
      "Epoch 37 \t Batch 380 \t Training Loss: 46.71781103234542\n",
      "Epoch 37 \t Batch 400 \t Training Loss: 46.73430292129517\n",
      "Epoch 37 \t Batch 420 \t Training Loss: 46.82903208051409\n",
      "Epoch 37 \t Batch 440 \t Training Loss: 46.93063331083818\n",
      "Epoch 37 \t Batch 460 \t Training Loss: 46.986905496016796\n",
      "Epoch 37 \t Batch 480 \t Training Loss: 47.0215743303299\n",
      "Epoch 37 \t Batch 500 \t Training Loss: 47.04758724975586\n",
      "Epoch 37 \t Batch 520 \t Training Loss: 47.026579541426436\n",
      "Epoch 37 \t Batch 540 \t Training Loss: 46.96145190486202\n",
      "Epoch 37 \t Batch 560 \t Training Loss: 46.95347818647112\n",
      "Epoch 37 \t Batch 580 \t Training Loss: 46.92243550399254\n",
      "Epoch 37 \t Batch 600 \t Training Loss: 46.88722270965576\n",
      "Epoch 37 \t Batch 620 \t Training Loss: 46.888312665877805\n",
      "Epoch 37 \t Batch 640 \t Training Loss: 46.90352384448052\n",
      "Epoch 37 \t Batch 660 \t Training Loss: 46.92190000360662\n",
      "Epoch 37 \t Batch 680 \t Training Loss: 46.88423925287583\n",
      "Epoch 37 \t Batch 700 \t Training Loss: 46.885401306152346\n",
      "Epoch 37 \t Batch 720 \t Training Loss: 46.9242449177636\n",
      "Epoch 37 \t Batch 740 \t Training Loss: 46.90925935796789\n",
      "Epoch 37 \t Batch 760 \t Training Loss: 46.909620997780245\n",
      "Epoch 37 \t Batch 780 \t Training Loss: 46.95316520104041\n",
      "Epoch 37 \t Batch 800 \t Training Loss: 46.98097856998444\n",
      "Epoch 37 \t Batch 820 \t Training Loss: 47.00486707454775\n",
      "Epoch 37 \t Batch 840 \t Training Loss: 47.0195316814241\n",
      "Epoch 37 \t Batch 860 \t Training Loss: 47.02265163244203\n",
      "Epoch 37 \t Batch 880 \t Training Loss: 47.02443700270219\n",
      "Epoch 37 \t Batch 900 \t Training Loss: 47.058250965542264\n",
      "Epoch 37 \t Batch 20 \t Validation Loss: 25.372597408294677\n",
      "Epoch 37 \t Batch 40 \t Validation Loss: 27.689544749259948\n",
      "Epoch 37 \t Batch 60 \t Validation Loss: 27.409863901138305\n",
      "Epoch 37 \t Batch 80 \t Validation Loss: 27.68526358604431\n",
      "Epoch 37 \t Batch 100 \t Validation Loss: 27.8742799949646\n",
      "Epoch 37 \t Batch 120 \t Validation Loss: 28.249009617169698\n",
      "Epoch 37 \t Batch 140 \t Validation Loss: 28.24577478000096\n",
      "Epoch 37 \t Batch 160 \t Validation Loss: 29.71091086268425\n",
      "Epoch 37 \t Batch 180 \t Validation Loss: 32.900831847720674\n",
      "Epoch 37 \t Batch 200 \t Validation Loss: 34.044194307327274\n",
      "Epoch 37 \t Batch 220 \t Validation Loss: 35.06548020622947\n",
      "Epoch 37 \t Batch 240 \t Validation Loss: 35.365712122122446\n",
      "Epoch 37 \t Batch 260 \t Validation Loss: 37.24760925219609\n",
      "Epoch 37 \t Batch 280 \t Validation Loss: 38.22076079504831\n",
      "Epoch 37 \t Batch 300 \t Validation Loss: 39.19652735392253\n",
      "Epoch 37 \t Batch 320 \t Validation Loss: 39.599648147821426\n",
      "Epoch 37 \t Batch 340 \t Validation Loss: 39.467653622346766\n",
      "Epoch 37 \t Batch 360 \t Validation Loss: 39.25819945865207\n",
      "Epoch 37 \t Batch 380 \t Validation Loss: 39.40839028107492\n",
      "Epoch 37 \t Batch 400 \t Validation Loss: 38.95077228069305\n",
      "Epoch 37 \t Batch 420 \t Validation Loss: 38.925804910205656\n",
      "Epoch 37 \t Batch 440 \t Validation Loss: 38.575101375579834\n",
      "Epoch 37 \t Batch 460 \t Validation Loss: 38.76148771203083\n",
      "Epoch 37 \t Batch 480 \t Validation Loss: 39.21526225010554\n",
      "Epoch 37 \t Batch 500 \t Validation Loss: 38.90612059020996\n",
      "Epoch 37 \t Batch 520 \t Validation Loss: 38.63024003689106\n",
      "Epoch 37 \t Batch 540 \t Validation Loss: 38.39454689379092\n",
      "Epoch 37 \t Batch 560 \t Validation Loss: 38.19970002174377\n",
      "Epoch 37 \t Batch 580 \t Validation Loss: 37.93316602378056\n",
      "Epoch 37 \t Batch 600 \t Validation Loss: 38.15967040379842\n",
      "Epoch 37 Training Loss: 47.02530678901964 Validation Loss: 38.79471961863629\n",
      "Epoch 37 completed\n",
      "Epoch 38 \t Batch 20 \t Training Loss: 47.23789901733399\n",
      "Epoch 38 \t Batch 40 \t Training Loss: 47.49524946212769\n",
      "Epoch 38 \t Batch 60 \t Training Loss: 47.10098648071289\n",
      "Epoch 38 \t Batch 80 \t Training Loss: 46.87623310089111\n",
      "Epoch 38 \t Batch 100 \t Training Loss: 46.690806503295896\n",
      "Epoch 38 \t Batch 120 \t Training Loss: 46.48903700510661\n",
      "Epoch 38 \t Batch 140 \t Training Loss: 46.5257774080549\n",
      "Epoch 38 \t Batch 160 \t Training Loss: 46.74410088062287\n",
      "Epoch 38 \t Batch 180 \t Training Loss: 46.817415640089244\n",
      "Epoch 38 \t Batch 200 \t Training Loss: 46.84988784790039\n",
      "Epoch 38 \t Batch 220 \t Training Loss: 46.87441602186723\n",
      "Epoch 38 \t Batch 240 \t Training Loss: 46.90060963630676\n",
      "Epoch 38 \t Batch 260 \t Training Loss: 46.97460121741662\n",
      "Epoch 38 \t Batch 280 \t Training Loss: 47.00076407023838\n",
      "Epoch 38 \t Batch 300 \t Training Loss: 47.010585708618166\n",
      "Epoch 38 \t Batch 320 \t Training Loss: 47.103527796268466\n",
      "Epoch 38 \t Batch 340 \t Training Loss: 47.09554387260886\n",
      "Epoch 38 \t Batch 360 \t Training Loss: 47.06207243601481\n",
      "Epoch 38 \t Batch 380 \t Training Loss: 47.08723378432425\n",
      "Epoch 38 \t Batch 400 \t Training Loss: 47.06097559928894\n",
      "Epoch 38 \t Batch 420 \t Training Loss: 47.011273574829104\n",
      "Epoch 38 \t Batch 440 \t Training Loss: 47.02773944681341\n",
      "Epoch 38 \t Batch 460 \t Training Loss: 46.9848715574845\n",
      "Epoch 38 \t Batch 480 \t Training Loss: 46.99571584860484\n",
      "Epoch 38 \t Batch 500 \t Training Loss: 46.92213436126709\n",
      "Epoch 38 \t Batch 520 \t Training Loss: 46.950264021066516\n",
      "Epoch 38 \t Batch 540 \t Training Loss: 46.90787850132695\n",
      "Epoch 38 \t Batch 560 \t Training Loss: 46.920959976741244\n",
      "Epoch 38 \t Batch 580 \t Training Loss: 46.93087001668996\n",
      "Epoch 38 \t Batch 600 \t Training Loss: 46.91825750986735\n",
      "Epoch 38 \t Batch 620 \t Training Loss: 46.90285469793504\n",
      "Epoch 38 \t Batch 640 \t Training Loss: 46.91557818651199\n",
      "Epoch 38 \t Batch 660 \t Training Loss: 46.94190373276219\n",
      "Epoch 38 \t Batch 680 \t Training Loss: 46.96079460031846\n",
      "Epoch 38 \t Batch 700 \t Training Loss: 46.95064796447754\n",
      "Epoch 38 \t Batch 720 \t Training Loss: 46.96888331307305\n",
      "Epoch 38 \t Batch 740 \t Training Loss: 46.95481163231102\n",
      "Epoch 38 \t Batch 760 \t Training Loss: 46.981344112597014\n",
      "Epoch 38 \t Batch 780 \t Training Loss: 46.98377417050875\n",
      "Epoch 38 \t Batch 800 \t Training Loss: 46.976708960533145\n",
      "Epoch 38 \t Batch 820 \t Training Loss: 47.02071493195324\n",
      "Epoch 38 \t Batch 840 \t Training Loss: 47.03880339577084\n",
      "Epoch 38 \t Batch 860 \t Training Loss: 47.028165622090185\n",
      "Epoch 38 \t Batch 880 \t Training Loss: 47.04010806083679\n",
      "Epoch 38 \t Batch 900 \t Training Loss: 47.04636761135525\n",
      "Epoch 38 \t Batch 20 \t Validation Loss: 22.63211283683777\n",
      "Epoch 38 \t Batch 40 \t Validation Loss: 25.733518958091736\n",
      "Epoch 38 \t Batch 60 \t Validation Loss: 24.968731451034547\n",
      "Epoch 38 \t Batch 80 \t Validation Loss: 25.2903701543808\n",
      "Epoch 38 \t Batch 100 \t Validation Loss: 26.235801639556886\n",
      "Epoch 38 \t Batch 120 \t Validation Loss: 27.02883348464966\n",
      "Epoch 38 \t Batch 140 \t Validation Loss: 27.289733151027136\n",
      "Epoch 38 \t Batch 160 \t Validation Loss: 29.081661641597748\n",
      "Epoch 38 \t Batch 180 \t Validation Loss: 32.69161523713006\n",
      "Epoch 38 \t Batch 200 \t Validation Loss: 34.06469032764435\n",
      "Epoch 38 \t Batch 220 \t Validation Loss: 35.37191322066567\n",
      "Epoch 38 \t Batch 240 \t Validation Loss: 35.83411133289337\n",
      "Epoch 38 \t Batch 260 \t Validation Loss: 37.91758516751803\n",
      "Epoch 38 \t Batch 280 \t Validation Loss: 39.01995891162327\n",
      "Epoch 38 \t Batch 300 \t Validation Loss: 40.10613286336263\n",
      "Epoch 38 \t Batch 320 \t Validation Loss: 40.58653470277786\n",
      "Epoch 38 \t Batch 340 \t Validation Loss: 40.45366061715519\n",
      "Epoch 38 \t Batch 360 \t Validation Loss: 40.267739624447294\n",
      "Epoch 38 \t Batch 380 \t Validation Loss: 40.454141195196854\n",
      "Epoch 38 \t Batch 400 \t Validation Loss: 39.9667714881897\n",
      "Epoch 38 \t Batch 420 \t Validation Loss: 39.907248047419955\n",
      "Epoch 38 \t Batch 440 \t Validation Loss: 39.5430815263228\n",
      "Epoch 38 \t Batch 460 \t Validation Loss: 39.70171982723733\n",
      "Epoch 38 \t Batch 480 \t Validation Loss: 40.121543697516124\n",
      "Epoch 38 \t Batch 500 \t Validation Loss: 39.8024919090271\n",
      "Epoch 38 \t Batch 520 \t Validation Loss: 39.55453292039724\n",
      "Epoch 38 \t Batch 540 \t Validation Loss: 39.248017671373155\n",
      "Epoch 38 \t Batch 560 \t Validation Loss: 39.01054402760097\n",
      "Epoch 38 \t Batch 580 \t Validation Loss: 38.73582301633111\n",
      "Epoch 38 \t Batch 600 \t Validation Loss: 38.899714403152466\n",
      "Epoch 38 Training Loss: 47.008962808024535 Validation Loss: 39.51871880308374\n",
      "Epoch 38 completed\n",
      "Epoch 39 \t Batch 20 \t Training Loss: 46.24899463653564\n",
      "Epoch 39 \t Batch 40 \t Training Loss: 46.63082618713379\n",
      "Epoch 39 \t Batch 60 \t Training Loss: 46.89318726857503\n",
      "Epoch 39 \t Batch 80 \t Training Loss: 47.18022994995117\n",
      "Epoch 39 \t Batch 100 \t Training Loss: 46.79823532104492\n",
      "Epoch 39 \t Batch 120 \t Training Loss: 46.711809698740645\n",
      "Epoch 39 \t Batch 140 \t Training Loss: 46.9078513281686\n",
      "Epoch 39 \t Batch 160 \t Training Loss: 46.78830437660217\n",
      "Epoch 39 \t Batch 180 \t Training Loss: 46.65391156938341\n",
      "Epoch 39 \t Batch 200 \t Training Loss: 46.69664041519165\n",
      "Epoch 39 \t Batch 220 \t Training Loss: 46.60246186689897\n",
      "Epoch 39 \t Batch 240 \t Training Loss: 46.64176007906596\n",
      "Epoch 39 \t Batch 260 \t Training Loss: 46.573649054307204\n",
      "Epoch 39 \t Batch 280 \t Training Loss: 46.5729100227356\n",
      "Epoch 39 \t Batch 300 \t Training Loss: 46.548051020304364\n",
      "Epoch 39 \t Batch 320 \t Training Loss: 46.66500461101532\n",
      "Epoch 39 \t Batch 340 \t Training Loss: 46.6600783179788\n",
      "Epoch 39 \t Batch 360 \t Training Loss: 46.698698700798886\n",
      "Epoch 39 \t Batch 380 \t Training Loss: 46.711672220732034\n",
      "Epoch 39 \t Batch 400 \t Training Loss: 46.81285984039307\n",
      "Epoch 39 \t Batch 420 \t Training Loss: 46.78038518088204\n",
      "Epoch 39 \t Batch 440 \t Training Loss: 46.77085345008157\n",
      "Epoch 39 \t Batch 460 \t Training Loss: 46.79786872034487\n",
      "Epoch 39 \t Batch 480 \t Training Loss: 46.81169752279917\n",
      "Epoch 39 \t Batch 500 \t Training Loss: 46.864017387390135\n",
      "Epoch 39 \t Batch 520 \t Training Loss: 46.900718688964844\n",
      "Epoch 39 \t Batch 540 \t Training Loss: 46.792744813142\n",
      "Epoch 39 \t Batch 560 \t Training Loss: 46.776018142700195\n",
      "Epoch 39 \t Batch 580 \t Training Loss: 46.75279637040763\n",
      "Epoch 39 \t Batch 600 \t Training Loss: 46.7231765238444\n",
      "Epoch 39 \t Batch 620 \t Training Loss: 46.74153065835276\n",
      "Epoch 39 \t Batch 640 \t Training Loss: 46.714896500110626\n",
      "Epoch 39 \t Batch 660 \t Training Loss: 46.76214723298044\n",
      "Epoch 39 \t Batch 680 \t Training Loss: 46.79450352612664\n",
      "Epoch 39 \t Batch 700 \t Training Loss: 46.77113535744803\n",
      "Epoch 39 \t Batch 720 \t Training Loss: 46.77864111264547\n",
      "Epoch 39 \t Batch 740 \t Training Loss: 46.78111854243923\n",
      "Epoch 39 \t Batch 760 \t Training Loss: 46.79973918011314\n",
      "Epoch 39 \t Batch 780 \t Training Loss: 46.822579251802885\n",
      "Epoch 39 \t Batch 800 \t Training Loss: 46.8371433877945\n",
      "Epoch 39 \t Batch 820 \t Training Loss: 46.87210262577708\n",
      "Epoch 39 \t Batch 840 \t Training Loss: 46.90306752522786\n",
      "Epoch 39 \t Batch 860 \t Training Loss: 46.94791459371877\n",
      "Epoch 39 \t Batch 880 \t Training Loss: 46.93032082644376\n",
      "Epoch 39 \t Batch 900 \t Training Loss: 46.95247030046251\n",
      "Epoch 39 \t Batch 20 \t Validation Loss: 26.030314254760743\n",
      "Epoch 39 \t Batch 40 \t Validation Loss: 27.02048318386078\n",
      "Epoch 39 \t Batch 60 \t Validation Loss: 26.964984941482545\n",
      "Epoch 39 \t Batch 80 \t Validation Loss: 27.264049661159515\n",
      "Epoch 39 \t Batch 100 \t Validation Loss: 28.040044260025024\n",
      "Epoch 39 \t Batch 120 \t Validation Loss: 28.978162328402203\n",
      "Epoch 39 \t Batch 140 \t Validation Loss: 29.222599049976893\n",
      "Epoch 39 \t Batch 160 \t Validation Loss: 30.765391474962236\n",
      "Epoch 39 \t Batch 180 \t Validation Loss: 33.9709543440077\n",
      "Epoch 39 \t Batch 200 \t Validation Loss: 35.196570320129396\n",
      "Epoch 39 \t Batch 220 \t Validation Loss: 36.25238236514005\n",
      "Epoch 39 \t Batch 240 \t Validation Loss: 36.55491162141164\n",
      "Epoch 39 \t Batch 260 \t Validation Loss: 38.48952933458182\n",
      "Epoch 39 \t Batch 280 \t Validation Loss: 39.48409747055599\n",
      "Epoch 39 \t Batch 300 \t Validation Loss: 40.434739713668826\n",
      "Epoch 39 \t Batch 320 \t Validation Loss: 40.84538556933403\n",
      "Epoch 39 \t Batch 340 \t Validation Loss: 40.69446997923009\n",
      "Epoch 39 \t Batch 360 \t Validation Loss: 40.493715355131364\n",
      "Epoch 39 \t Batch 380 \t Validation Loss: 40.66398276780781\n",
      "Epoch 39 \t Batch 400 \t Validation Loss: 40.192962398529055\n",
      "Epoch 39 \t Batch 420 \t Validation Loss: 40.11328013965062\n",
      "Epoch 39 \t Batch 440 \t Validation Loss: 39.768332249468024\n",
      "Epoch 39 \t Batch 460 \t Validation Loss: 40.016253444422844\n",
      "Epoch 39 \t Batch 480 \t Validation Loss: 40.41741345922152\n",
      "Epoch 39 \t Batch 500 \t Validation Loss: 40.1011932926178\n",
      "Epoch 39 \t Batch 520 \t Validation Loss: 39.92420904819782\n",
      "Epoch 39 \t Batch 540 \t Validation Loss: 39.65940608625059\n",
      "Epoch 39 \t Batch 560 \t Validation Loss: 39.433130151884896\n",
      "Epoch 39 \t Batch 580 \t Validation Loss: 39.16955826857994\n",
      "Epoch 39 \t Batch 600 \t Validation Loss: 39.34928064346313\n",
      "Epoch 39 Training Loss: 46.96701714152728 Validation Loss: 39.969919118014246\n",
      "Epoch 39 completed\n",
      "Epoch 40 \t Batch 20 \t Training Loss: 46.678564834594724\n",
      "Epoch 40 \t Batch 40 \t Training Loss: 46.90852270126343\n",
      "Epoch 40 \t Batch 60 \t Training Loss: 47.22443892161051\n",
      "Epoch 40 \t Batch 80 \t Training Loss: 47.061957120895386\n",
      "Epoch 40 \t Batch 100 \t Training Loss: 46.86796745300293\n",
      "Epoch 40 \t Batch 120 \t Training Loss: 46.65478642781576\n",
      "Epoch 40 \t Batch 140 \t Training Loss: 46.719380950927736\n",
      "Epoch 40 \t Batch 160 \t Training Loss: 46.52085208892822\n",
      "Epoch 40 \t Batch 180 \t Training Loss: 46.74094996982151\n",
      "Epoch 40 \t Batch 200 \t Training Loss: 46.74248529434204\n",
      "Epoch 40 \t Batch 220 \t Training Loss: 46.72466458407315\n",
      "Epoch 40 \t Batch 240 \t Training Loss: 46.70089880625407\n",
      "Epoch 40 \t Batch 260 \t Training Loss: 46.8109432220459\n",
      "Epoch 40 \t Batch 280 \t Training Loss: 46.78442715236119\n",
      "Epoch 40 \t Batch 300 \t Training Loss: 46.775792401631676\n",
      "Epoch 40 \t Batch 320 \t Training Loss: 46.78713548183441\n",
      "Epoch 40 \t Batch 340 \t Training Loss: 46.81930214377011\n",
      "Epoch 40 \t Batch 360 \t Training Loss: 46.85667787128025\n",
      "Epoch 40 \t Batch 380 \t Training Loss: 46.8946018720928\n",
      "Epoch 40 \t Batch 400 \t Training Loss: 46.878268575668336\n",
      "Epoch 40 \t Batch 420 \t Training Loss: 46.93176449366978\n",
      "Epoch 40 \t Batch 440 \t Training Loss: 46.9372974395752\n",
      "Epoch 40 \t Batch 460 \t Training Loss: 46.938822348221485\n",
      "Epoch 40 \t Batch 480 \t Training Loss: 46.85687844753265\n",
      "Epoch 40 \t Batch 500 \t Training Loss: 46.890872955322266\n",
      "Epoch 40 \t Batch 520 \t Training Loss: 46.91523346534142\n",
      "Epoch 40 \t Batch 540 \t Training Loss: 46.88370552769414\n",
      "Epoch 40 \t Batch 560 \t Training Loss: 46.90762905393328\n",
      "Epoch 40 \t Batch 580 \t Training Loss: 46.88884099105309\n",
      "Epoch 40 \t Batch 600 \t Training Loss: 46.87257807413737\n",
      "Epoch 40 \t Batch 620 \t Training Loss: 46.90661685697494\n",
      "Epoch 40 \t Batch 640 \t Training Loss: 46.903461277484894\n",
      "Epoch 40 \t Batch 660 \t Training Loss: 46.90291906992594\n",
      "Epoch 40 \t Batch 680 \t Training Loss: 46.86836832831888\n",
      "Epoch 40 \t Batch 700 \t Training Loss: 46.85073034014021\n",
      "Epoch 40 \t Batch 720 \t Training Loss: 46.85985619227092\n",
      "Epoch 40 \t Batch 740 \t Training Loss: 46.860567907384926\n",
      "Epoch 40 \t Batch 760 \t Training Loss: 46.900004778410256\n",
      "Epoch 40 \t Batch 780 \t Training Loss: 46.92697260929988\n",
      "Epoch 40 \t Batch 800 \t Training Loss: 46.906772084236145\n",
      "Epoch 40 \t Batch 820 \t Training Loss: 46.94557604906036\n",
      "Epoch 40 \t Batch 840 \t Training Loss: 46.948877148401166\n",
      "Epoch 40 \t Batch 860 \t Training Loss: 46.976421351765474\n",
      "Epoch 40 \t Batch 880 \t Training Loss: 46.96578923138705\n",
      "Epoch 40 \t Batch 900 \t Training Loss: 46.974699096679686\n",
      "Epoch 40 \t Batch 20 \t Validation Loss: 23.881889581680298\n",
      "Epoch 40 \t Batch 40 \t Validation Loss: 24.254200220108032\n",
      "Epoch 40 \t Batch 60 \t Validation Loss: 25.066539319356284\n",
      "Epoch 40 \t Batch 80 \t Validation Loss: 25.450368559360506\n",
      "Epoch 40 \t Batch 100 \t Validation Loss: 26.46483407020569\n",
      "Epoch 40 \t Batch 120 \t Validation Loss: 27.55200774669647\n",
      "Epoch 40 \t Batch 140 \t Validation Loss: 27.913816145488195\n",
      "Epoch 40 \t Batch 160 \t Validation Loss: 29.49984716773033\n",
      "Epoch 40 \t Batch 180 \t Validation Loss: 32.806509272257486\n",
      "Epoch 40 \t Batch 200 \t Validation Loss: 33.98346215248108\n",
      "Epoch 40 \t Batch 220 \t Validation Loss: 35.09096540971236\n",
      "Epoch 40 \t Batch 240 \t Validation Loss: 35.45005200703939\n",
      "Epoch 40 \t Batch 260 \t Validation Loss: 37.41507012660687\n",
      "Epoch 40 \t Batch 280 \t Validation Loss: 38.463469805036276\n",
      "Epoch 40 \t Batch 300 \t Validation Loss: 39.43732786178589\n",
      "Epoch 40 \t Batch 320 \t Validation Loss: 39.846142914891246\n",
      "Epoch 40 \t Batch 340 \t Validation Loss: 39.71137394344105\n",
      "Epoch 40 \t Batch 360 \t Validation Loss: 39.50555982854631\n",
      "Epoch 40 \t Batch 380 \t Validation Loss: 39.6670557900479\n",
      "Epoch 40 \t Batch 400 \t Validation Loss: 39.179357035160066\n",
      "Epoch 40 \t Batch 420 \t Validation Loss: 39.13637576557341\n",
      "Epoch 40 \t Batch 440 \t Validation Loss: 38.7643665638837\n",
      "Epoch 40 \t Batch 460 \t Validation Loss: 38.92515198873437\n",
      "Epoch 40 \t Batch 480 \t Validation Loss: 39.377626023689906\n",
      "Epoch 40 \t Batch 500 \t Validation Loss: 39.07226337242126\n",
      "Epoch 40 \t Batch 520 \t Validation Loss: 38.79303193275745\n",
      "Epoch 40 \t Batch 540 \t Validation Loss: 38.556526270619145\n",
      "Epoch 40 \t Batch 560 \t Validation Loss: 38.37107449769974\n",
      "Epoch 40 \t Batch 580 \t Validation Loss: 38.14076824681512\n",
      "Epoch 40 \t Batch 600 \t Validation Loss: 38.36341555754343\n",
      "Epoch 40 Training Loss: 46.93767741878264 Validation Loss: 39.03798318373693\n",
      "Epoch 40 completed\n",
      "Epoch 41 \t Batch 20 \t Training Loss: 48.270829391479495\n",
      "Epoch 41 \t Batch 40 \t Training Loss: 47.845941638946535\n",
      "Epoch 41 \t Batch 60 \t Training Loss: 47.58818302154541\n",
      "Epoch 41 \t Batch 80 \t Training Loss: 47.28030252456665\n",
      "Epoch 41 \t Batch 100 \t Training Loss: 47.48517135620117\n",
      "Epoch 41 \t Batch 120 \t Training Loss: 47.37020543416341\n",
      "Epoch 41 \t Batch 140 \t Training Loss: 47.22798385620117\n",
      "Epoch 41 \t Batch 160 \t Training Loss: 47.12476139068603\n",
      "Epoch 41 \t Batch 180 \t Training Loss: 47.1914022869534\n",
      "Epoch 41 \t Batch 200 \t Training Loss: 47.22177839279175\n",
      "Epoch 41 \t Batch 220 \t Training Loss: 47.3042762929743\n",
      "Epoch 41 \t Batch 240 \t Training Loss: 47.20706380208333\n",
      "Epoch 41 \t Batch 260 \t Training Loss: 47.201622126652644\n",
      "Epoch 41 \t Batch 280 \t Training Loss: 47.12902687617711\n",
      "Epoch 41 \t Batch 300 \t Training Loss: 47.13356266021729\n",
      "Epoch 41 \t Batch 320 \t Training Loss: 47.200369250774386\n",
      "Epoch 41 \t Batch 340 \t Training Loss: 47.14551529603846\n",
      "Epoch 41 \t Batch 360 \t Training Loss: 47.120554468366834\n",
      "Epoch 41 \t Batch 380 \t Training Loss: 47.1748107608996\n",
      "Epoch 41 \t Batch 400 \t Training Loss: 47.13989517211914\n",
      "Epoch 41 \t Batch 420 \t Training Loss: 47.072429012116935\n",
      "Epoch 41 \t Batch 440 \t Training Loss: 47.000430670651525\n",
      "Epoch 41 \t Batch 460 \t Training Loss: 46.93993055094843\n",
      "Epoch 41 \t Batch 480 \t Training Loss: 46.94136286576589\n",
      "Epoch 41 \t Batch 500 \t Training Loss: 46.89601531219483\n",
      "Epoch 41 \t Batch 520 \t Training Loss: 46.93948265222403\n",
      "Epoch 41 \t Batch 540 \t Training Loss: 46.92915025640417\n",
      "Epoch 41 \t Batch 560 \t Training Loss: 46.93786095210484\n",
      "Epoch 41 \t Batch 580 \t Training Loss: 46.92250527349012\n",
      "Epoch 41 \t Batch 600 \t Training Loss: 46.94705192565918\n",
      "Epoch 41 \t Batch 620 \t Training Loss: 46.9615266184653\n",
      "Epoch 41 \t Batch 640 \t Training Loss: 46.96594372391701\n",
      "Epoch 41 \t Batch 660 \t Training Loss: 46.976039331609556\n",
      "Epoch 41 \t Batch 680 \t Training Loss: 46.982202574786015\n",
      "Epoch 41 \t Batch 700 \t Training Loss: 46.96677637372698\n",
      "Epoch 41 \t Batch 720 \t Training Loss: 46.9252742184533\n",
      "Epoch 41 \t Batch 740 \t Training Loss: 46.920543773754225\n",
      "Epoch 41 \t Batch 760 \t Training Loss: 46.93333237296657\n",
      "Epoch 41 \t Batch 780 \t Training Loss: 46.91928847875351\n",
      "Epoch 41 \t Batch 800 \t Training Loss: 46.91089463710785\n",
      "Epoch 41 \t Batch 820 \t Training Loss: 46.93346064497785\n",
      "Epoch 41 \t Batch 840 \t Training Loss: 46.94467919213431\n",
      "Epoch 41 \t Batch 860 \t Training Loss: 46.934500840652824\n",
      "Epoch 41 \t Batch 880 \t Training Loss: 46.90730059363625\n",
      "Epoch 41 \t Batch 900 \t Training Loss: 46.89348843044705\n",
      "Epoch 41 \t Batch 20 \t Validation Loss: 24.35782747268677\n",
      "Epoch 41 \t Batch 40 \t Validation Loss: 26.05102319717407\n",
      "Epoch 41 \t Batch 60 \t Validation Loss: 25.75488986968994\n",
      "Epoch 41 \t Batch 80 \t Validation Loss: 26.352030479907988\n",
      "Epoch 41 \t Batch 100 \t Validation Loss: 27.090466451644897\n",
      "Epoch 41 \t Batch 120 \t Validation Loss: 27.724482973416645\n",
      "Epoch 41 \t Batch 140 \t Validation Loss: 27.832883855274744\n",
      "Epoch 41 \t Batch 160 \t Validation Loss: 29.557051008939744\n",
      "Epoch 41 \t Batch 180 \t Validation Loss: 33.06017631954617\n",
      "Epoch 41 \t Batch 200 \t Validation Loss: 34.36576539039612\n",
      "Epoch 41 \t Batch 220 \t Validation Loss: 35.619106622175735\n",
      "Epoch 41 \t Batch 240 \t Validation Loss: 36.033600405852\n",
      "Epoch 41 \t Batch 260 \t Validation Loss: 38.073085436454186\n",
      "Epoch 41 \t Batch 280 \t Validation Loss: 39.14964404446738\n",
      "Epoch 41 \t Batch 300 \t Validation Loss: 40.215002355575564\n",
      "Epoch 41 \t Batch 320 \t Validation Loss: 40.66352044045925\n",
      "Epoch 41 \t Batch 340 \t Validation Loss: 40.52630461244022\n",
      "Epoch 41 \t Batch 360 \t Validation Loss: 40.340634571181404\n",
      "Epoch 41 \t Batch 380 \t Validation Loss: 40.50404968512686\n",
      "Epoch 41 \t Batch 400 \t Validation Loss: 39.994535882472995\n",
      "Epoch 41 \t Batch 420 \t Validation Loss: 39.96084805216108\n",
      "Epoch 41 \t Batch 440 \t Validation Loss: 39.58722395246679\n",
      "Epoch 41 \t Batch 460 \t Validation Loss: 39.73941424203956\n",
      "Epoch 41 \t Batch 480 \t Validation Loss: 40.16336368918419\n",
      "Epoch 41 \t Batch 500 \t Validation Loss: 39.85179898262024\n",
      "Epoch 41 \t Batch 520 \t Validation Loss: 39.54101085112645\n",
      "Epoch 41 \t Batch 540 \t Validation Loss: 39.268023616296276\n",
      "Epoch 41 \t Batch 560 \t Validation Loss: 39.04671060528074\n",
      "Epoch 41 \t Batch 580 \t Validation Loss: 38.73545935565028\n",
      "Epoch 41 \t Batch 600 \t Validation Loss: 38.94062819004059\n",
      "Epoch 41 Training Loss: 46.910327848977325 Validation Loss: 39.56807157745609\n",
      "Epoch 41 completed\n",
      "Epoch 42 \t Batch 20 \t Training Loss: 45.08764915466308\n",
      "Epoch 42 \t Batch 40 \t Training Loss: 45.88587217330932\n",
      "Epoch 42 \t Batch 60 \t Training Loss: 46.15545069376628\n",
      "Epoch 42 \t Batch 80 \t Training Loss: 46.565794706344604\n",
      "Epoch 42 \t Batch 100 \t Training Loss: 46.60036979675293\n",
      "Epoch 42 \t Batch 120 \t Training Loss: 47.06710488001506\n",
      "Epoch 42 \t Batch 140 \t Training Loss: 46.9876059123448\n",
      "Epoch 42 \t Batch 160 \t Training Loss: 46.930363464355466\n",
      "Epoch 42 \t Batch 180 \t Training Loss: 47.047045368618434\n",
      "Epoch 42 \t Batch 200 \t Training Loss: 47.019784450531006\n",
      "Epoch 42 \t Batch 220 \t Training Loss: 47.11282707561146\n",
      "Epoch 42 \t Batch 240 \t Training Loss: 47.066729354858396\n",
      "Epoch 42 \t Batch 260 \t Training Loss: 46.99036772801326\n",
      "Epoch 42 \t Batch 280 \t Training Loss: 46.98894552503313\n",
      "Epoch 42 \t Batch 300 \t Training Loss: 46.8909423828125\n",
      "Epoch 42 \t Batch 320 \t Training Loss: 46.870474946498874\n",
      "Epoch 42 \t Batch 340 \t Training Loss: 46.82007587657255\n",
      "Epoch 42 \t Batch 360 \t Training Loss: 46.88116710450914\n",
      "Epoch 42 \t Batch 380 \t Training Loss: 46.889545450712504\n",
      "Epoch 42 \t Batch 400 \t Training Loss: 46.90687218666076\n",
      "Epoch 42 \t Batch 420 \t Training Loss: 46.86246098109654\n",
      "Epoch 42 \t Batch 440 \t Training Loss: 46.78314557508989\n",
      "Epoch 42 \t Batch 460 \t Training Loss: 46.80177478790283\n",
      "Epoch 42 \t Batch 480 \t Training Loss: 46.798472213745114\n",
      "Epoch 42 \t Batch 500 \t Training Loss: 46.78391381072998\n",
      "Epoch 42 \t Batch 520 \t Training Loss: 46.742044676267184\n",
      "Epoch 42 \t Batch 540 \t Training Loss: 46.74636775829174\n",
      "Epoch 42 \t Batch 560 \t Training Loss: 46.708276871272496\n",
      "Epoch 42 \t Batch 580 \t Training Loss: 46.7222179741695\n",
      "Epoch 42 \t Batch 600 \t Training Loss: 46.72083234786987\n",
      "Epoch 42 \t Batch 620 \t Training Loss: 46.76715241709063\n",
      "Epoch 42 \t Batch 640 \t Training Loss: 46.78178217411041\n",
      "Epoch 42 \t Batch 660 \t Training Loss: 46.738293237397166\n",
      "Epoch 42 \t Batch 680 \t Training Loss: 46.77032539143282\n",
      "Epoch 42 \t Batch 700 \t Training Loss: 46.785671457563126\n",
      "Epoch 42 \t Batch 720 \t Training Loss: 46.81202246877882\n",
      "Epoch 42 \t Batch 740 \t Training Loss: 46.855591877086745\n",
      "Epoch 42 \t Batch 760 \t Training Loss: 46.827010455884434\n",
      "Epoch 42 \t Batch 780 \t Training Loss: 46.82402383853228\n",
      "Epoch 42 \t Batch 800 \t Training Loss: 46.79497460365295\n",
      "Epoch 42 \t Batch 820 \t Training Loss: 46.82761030894954\n",
      "Epoch 42 \t Batch 840 \t Training Loss: 46.841483774639315\n",
      "Epoch 42 \t Batch 860 \t Training Loss: 46.854392907785815\n",
      "Epoch 42 \t Batch 880 \t Training Loss: 46.870949320359664\n",
      "Epoch 42 \t Batch 900 \t Training Loss: 46.859524455600315\n",
      "Epoch 42 \t Batch 20 \t Validation Loss: 22.65225167274475\n",
      "Epoch 42 \t Batch 40 \t Validation Loss: 23.601642656326295\n",
      "Epoch 42 \t Batch 60 \t Validation Loss: 23.713728125890096\n",
      "Epoch 42 \t Batch 80 \t Validation Loss: 24.067119550704955\n",
      "Epoch 42 \t Batch 100 \t Validation Loss: 24.90009504318237\n",
      "Epoch 42 \t Batch 120 \t Validation Loss: 25.775179227193195\n",
      "Epoch 42 \t Batch 140 \t Validation Loss: 26.08648338317871\n",
      "Epoch 42 \t Batch 160 \t Validation Loss: 27.94712747335434\n",
      "Epoch 42 \t Batch 180 \t Validation Loss: 31.517648855845135\n",
      "Epoch 42 \t Batch 200 \t Validation Loss: 32.88438342094421\n",
      "Epoch 42 \t Batch 220 \t Validation Loss: 34.104279058629814\n",
      "Epoch 42 \t Batch 240 \t Validation Loss: 34.55216287374496\n",
      "Epoch 42 \t Batch 260 \t Validation Loss: 36.59577247913067\n",
      "Epoch 42 \t Batch 280 \t Validation Loss: 37.68741858005524\n",
      "Epoch 42 \t Batch 300 \t Validation Loss: 38.82855767250061\n",
      "Epoch 42 \t Batch 320 \t Validation Loss: 39.355221810936925\n",
      "Epoch 42 \t Batch 340 \t Validation Loss: 39.281938000286324\n",
      "Epoch 42 \t Batch 360 \t Validation Loss: 39.13626799583435\n",
      "Epoch 42 \t Batch 380 \t Validation Loss: 39.32904090379414\n",
      "Epoch 42 \t Batch 400 \t Validation Loss: 38.877664165496824\n",
      "Epoch 42 \t Batch 420 \t Validation Loss: 38.8896692616599\n",
      "Epoch 42 \t Batch 440 \t Validation Loss: 38.559650614044884\n",
      "Epoch 42 \t Batch 460 \t Validation Loss: 38.78354247341985\n",
      "Epoch 42 \t Batch 480 \t Validation Loss: 39.27136221528053\n",
      "Epoch 42 \t Batch 500 \t Validation Loss: 39.00823691749573\n",
      "Epoch 42 \t Batch 520 \t Validation Loss: 38.76765497831198\n",
      "Epoch 42 \t Batch 540 \t Validation Loss: 38.507567153153595\n",
      "Epoch 42 \t Batch 560 \t Validation Loss: 38.2959062899862\n",
      "Epoch 42 \t Batch 580 \t Validation Loss: 38.0066651360742\n",
      "Epoch 42 \t Batch 600 \t Validation Loss: 38.22561720689138\n",
      "Epoch 42 Training Loss: 46.85894641439423 Validation Loss: 38.85542275224413\n",
      "Epoch 42 completed\n",
      "Epoch 43 \t Batch 20 \t Training Loss: 47.81547470092774\n",
      "Epoch 43 \t Batch 40 \t Training Loss: 47.46580114364624\n",
      "Epoch 43 \t Batch 60 \t Training Loss: 47.10278739929199\n",
      "Epoch 43 \t Batch 80 \t Training Loss: 47.324760389328006\n",
      "Epoch 43 \t Batch 100 \t Training Loss: 47.20976543426514\n",
      "Epoch 43 \t Batch 120 \t Training Loss: 47.01324927012126\n",
      "Epoch 43 \t Batch 140 \t Training Loss: 46.8056069782802\n",
      "Epoch 43 \t Batch 160 \t Training Loss: 46.79385068416595\n",
      "Epoch 43 \t Batch 180 \t Training Loss: 46.72275136311849\n",
      "Epoch 43 \t Batch 200 \t Training Loss: 46.710694522857665\n",
      "Epoch 43 \t Batch 220 \t Training Loss: 46.787849963795054\n",
      "Epoch 43 \t Batch 240 \t Training Loss: 46.7721931775411\n",
      "Epoch 43 \t Batch 260 \t Training Loss: 46.85460037818322\n",
      "Epoch 43 \t Batch 280 \t Training Loss: 46.79407250540597\n",
      "Epoch 43 \t Batch 300 \t Training Loss: 46.797200736999514\n",
      "Epoch 43 \t Batch 320 \t Training Loss: 46.786868917942044\n",
      "Epoch 43 \t Batch 340 \t Training Loss: 46.66054780623492\n",
      "Epoch 43 \t Batch 360 \t Training Loss: 46.65805912017822\n",
      "Epoch 43 \t Batch 380 \t Training Loss: 46.70360934608861\n",
      "Epoch 43 \t Batch 400 \t Training Loss: 46.69700103759766\n",
      "Epoch 43 \t Batch 420 \t Training Loss: 46.63423211233957\n",
      "Epoch 43 \t Batch 440 \t Training Loss: 46.60133007222956\n",
      "Epoch 43 \t Batch 460 \t Training Loss: 46.62609874891198\n",
      "Epoch 43 \t Batch 480 \t Training Loss: 46.63094782829285\n",
      "Epoch 43 \t Batch 500 \t Training Loss: 46.62246771240235\n",
      "Epoch 43 \t Batch 520 \t Training Loss: 46.62592740425697\n",
      "Epoch 43 \t Batch 540 \t Training Loss: 46.64642202589247\n",
      "Epoch 43 \t Batch 560 \t Training Loss: 46.6538745198931\n",
      "Epoch 43 \t Batch 580 \t Training Loss: 46.61544797173862\n",
      "Epoch 43 \t Batch 600 \t Training Loss: 46.67361310958862\n",
      "Epoch 43 \t Batch 620 \t Training Loss: 46.655092300907256\n",
      "Epoch 43 \t Batch 640 \t Training Loss: 46.646720266342165\n",
      "Epoch 43 \t Batch 660 \t Training Loss: 46.65862282839689\n",
      "Epoch 43 \t Batch 680 \t Training Loss: 46.66923009087058\n",
      "Epoch 43 \t Batch 700 \t Training Loss: 46.68049809047154\n",
      "Epoch 43 \t Batch 720 \t Training Loss: 46.6876568529341\n",
      "Epoch 43 \t Batch 740 \t Training Loss: 46.71707258997737\n",
      "Epoch 43 \t Batch 760 \t Training Loss: 46.72452799144544\n",
      "Epoch 43 \t Batch 780 \t Training Loss: 46.782119765648474\n",
      "Epoch 43 \t Batch 800 \t Training Loss: 46.76761264324188\n",
      "Epoch 43 \t Batch 820 \t Training Loss: 46.760817876676235\n",
      "Epoch 43 \t Batch 840 \t Training Loss: 46.783747023627875\n",
      "Epoch 43 \t Batch 860 \t Training Loss: 46.7810461487881\n",
      "Epoch 43 \t Batch 880 \t Training Loss: 46.78637867840854\n",
      "Epoch 43 \t Batch 900 \t Training Loss: 46.80725560506185\n",
      "Epoch 43 \t Batch 20 \t Validation Loss: 23.14600005149841\n",
      "Epoch 43 \t Batch 40 \t Validation Loss: 25.376705121994018\n",
      "Epoch 43 \t Batch 60 \t Validation Loss: 25.09662742614746\n",
      "Epoch 43 \t Batch 80 \t Validation Loss: 25.430525994300844\n",
      "Epoch 43 \t Batch 100 \t Validation Loss: 26.094969310760497\n",
      "Epoch 43 \t Batch 120 \t Validation Loss: 26.80409590403239\n",
      "Epoch 43 \t Batch 140 \t Validation Loss: 26.95862500326974\n",
      "Epoch 43 \t Batch 160 \t Validation Loss: 28.6670934677124\n",
      "Epoch 43 \t Batch 180 \t Validation Loss: 32.00031193627252\n",
      "Epoch 43 \t Batch 200 \t Validation Loss: 33.335650329589846\n",
      "Epoch 43 \t Batch 220 \t Validation Loss: 34.51398157639937\n",
      "Epoch 43 \t Batch 240 \t Validation Loss: 34.898955416679385\n",
      "Epoch 43 \t Batch 260 \t Validation Loss: 36.93877275540279\n",
      "Epoch 43 \t Batch 280 \t Validation Loss: 38.01447153091431\n",
      "Epoch 43 \t Batch 300 \t Validation Loss: 38.99620439529419\n",
      "Epoch 43 \t Batch 320 \t Validation Loss: 39.45741328597069\n",
      "Epoch 43 \t Batch 340 \t Validation Loss: 39.36015645195456\n",
      "Epoch 43 \t Batch 360 \t Validation Loss: 39.16896890004476\n",
      "Epoch 43 \t Batch 380 \t Validation Loss: 39.36844811188547\n",
      "Epoch 43 \t Batch 400 \t Validation Loss: 38.931888642311094\n",
      "Epoch 43 \t Batch 420 \t Validation Loss: 38.95716285478501\n",
      "Epoch 43 \t Batch 440 \t Validation Loss: 38.6295077085495\n",
      "Epoch 43 \t Batch 460 \t Validation Loss: 38.82869609542515\n",
      "Epoch 43 \t Batch 480 \t Validation Loss: 39.314078678687416\n",
      "Epoch 43 \t Batch 500 \t Validation Loss: 39.037700101852415\n",
      "Epoch 43 \t Batch 520 \t Validation Loss: 38.76364170404581\n",
      "Epoch 43 \t Batch 540 \t Validation Loss: 38.494057667696914\n",
      "Epoch 43 \t Batch 560 \t Validation Loss: 38.264472908633095\n",
      "Epoch 43 \t Batch 580 \t Validation Loss: 38.00157894759342\n",
      "Epoch 43 \t Batch 600 \t Validation Loss: 38.21118276119232\n",
      "Epoch 43 Training Loss: 46.81046508572744 Validation Loss: 38.85934177776436\n",
      "Epoch 43 completed\n",
      "Epoch 44 \t Batch 20 \t Training Loss: 49.18049945831299\n",
      "Epoch 44 \t Batch 40 \t Training Loss: 48.22367134094238\n",
      "Epoch 44 \t Batch 60 \t Training Loss: 47.3742592493693\n",
      "Epoch 44 \t Batch 80 \t Training Loss: 47.53636260032654\n",
      "Epoch 44 \t Batch 100 \t Training Loss: 47.394929542541504\n",
      "Epoch 44 \t Batch 120 \t Training Loss: 47.22705198923747\n",
      "Epoch 44 \t Batch 140 \t Training Loss: 47.28392500196185\n",
      "Epoch 44 \t Batch 160 \t Training Loss: 47.163571333885194\n",
      "Epoch 44 \t Batch 180 \t Training Loss: 47.11784265306261\n",
      "Epoch 44 \t Batch 200 \t Training Loss: 46.953175201416016\n",
      "Epoch 44 \t Batch 220 \t Training Loss: 46.90027479691939\n",
      "Epoch 44 \t Batch 240 \t Training Loss: 46.931736675898236\n",
      "Epoch 44 \t Batch 260 \t Training Loss: 46.94802103776198\n",
      "Epoch 44 \t Batch 280 \t Training Loss: 46.977518572126115\n",
      "Epoch 44 \t Batch 300 \t Training Loss: 47.0203891881307\n",
      "Epoch 44 \t Batch 320 \t Training Loss: 46.922361040115355\n",
      "Epoch 44 \t Batch 340 \t Training Loss: 46.88388250014361\n",
      "Epoch 44 \t Batch 360 \t Training Loss: 46.881540065341525\n",
      "Epoch 44 \t Batch 380 \t Training Loss: 46.95069130345395\n",
      "Epoch 44 \t Batch 400 \t Training Loss: 46.82637865066528\n",
      "Epoch 44 \t Batch 420 \t Training Loss: 46.7992799668085\n",
      "Epoch 44 \t Batch 440 \t Training Loss: 46.81672013889659\n",
      "Epoch 44 \t Batch 460 \t Training Loss: 46.90109173318614\n",
      "Epoch 44 \t Batch 480 \t Training Loss: 46.95407632191976\n",
      "Epoch 44 \t Batch 500 \t Training Loss: 47.020217460632324\n",
      "Epoch 44 \t Batch 520 \t Training Loss: 46.95958857169518\n",
      "Epoch 44 \t Batch 540 \t Training Loss: 46.91715848004377\n",
      "Epoch 44 \t Batch 560 \t Training Loss: 46.903296266283306\n",
      "Epoch 44 \t Batch 580 \t Training Loss: 46.89537936901224\n",
      "Epoch 44 \t Batch 600 \t Training Loss: 46.88597236633301\n",
      "Epoch 44 \t Batch 620 \t Training Loss: 46.88084946909258\n",
      "Epoch 44 \t Batch 640 \t Training Loss: 46.88030333518982\n",
      "Epoch 44 \t Batch 660 \t Training Loss: 46.89969029860063\n",
      "Epoch 44 \t Batch 680 \t Training Loss: 46.86196356380687\n",
      "Epoch 44 \t Batch 700 \t Training Loss: 46.833215408325195\n",
      "Epoch 44 \t Batch 720 \t Training Loss: 46.85174456702338\n",
      "Epoch 44 \t Batch 740 \t Training Loss: 46.874073420344175\n",
      "Epoch 44 \t Batch 760 \t Training Loss: 46.89571206946122\n",
      "Epoch 44 \t Batch 780 \t Training Loss: 46.89791148748153\n",
      "Epoch 44 \t Batch 800 \t Training Loss: 46.89730936527252\n",
      "Epoch 44 \t Batch 820 \t Training Loss: 46.893339366447634\n",
      "Epoch 44 \t Batch 840 \t Training Loss: 46.866989580790204\n",
      "Epoch 44 \t Batch 860 \t Training Loss: 46.84480355728504\n",
      "Epoch 44 \t Batch 880 \t Training Loss: 46.80503252202814\n",
      "Epoch 44 \t Batch 900 \t Training Loss: 46.83188740200467\n",
      "Epoch 44 \t Batch 20 \t Validation Loss: 20.589140367507934\n",
      "Epoch 44 \t Batch 40 \t Validation Loss: 23.632190418243407\n",
      "Epoch 44 \t Batch 60 \t Validation Loss: 23.20600035985311\n",
      "Epoch 44 \t Batch 80 \t Validation Loss: 23.883272433280943\n",
      "Epoch 44 \t Batch 100 \t Validation Loss: 24.76380910873413\n",
      "Epoch 44 \t Batch 120 \t Validation Loss: 25.693235182762145\n",
      "Epoch 44 \t Batch 140 \t Validation Loss: 26.044702659334455\n",
      "Epoch 44 \t Batch 160 \t Validation Loss: 27.890870863199233\n",
      "Epoch 44 \t Batch 180 \t Validation Loss: 31.574716499116686\n",
      "Epoch 44 \t Batch 200 \t Validation Loss: 32.99306771755219\n",
      "Epoch 44 \t Batch 220 \t Validation Loss: 34.230045184222135\n",
      "Epoch 44 \t Batch 240 \t Validation Loss: 34.7028214653333\n",
      "Epoch 44 \t Batch 260 \t Validation Loss: 36.77233566871056\n",
      "Epoch 44 \t Batch 280 \t Validation Loss: 37.88681382451739\n",
      "Epoch 44 \t Batch 300 \t Validation Loss: 39.04139778137207\n",
      "Epoch 44 \t Batch 320 \t Validation Loss: 39.56173260211945\n",
      "Epoch 44 \t Batch 340 \t Validation Loss: 39.46848690930535\n",
      "Epoch 44 \t Batch 360 \t Validation Loss: 39.30117327107324\n",
      "Epoch 44 \t Batch 380 \t Validation Loss: 39.50595964883503\n",
      "Epoch 44 \t Batch 400 \t Validation Loss: 39.06520475625992\n",
      "Epoch 44 \t Batch 420 \t Validation Loss: 39.05512663523356\n",
      "Epoch 44 \t Batch 440 \t Validation Loss: 38.742673995278096\n",
      "Epoch 44 \t Batch 460 \t Validation Loss: 38.98648390977279\n",
      "Epoch 44 \t Batch 480 \t Validation Loss: 39.44741268157959\n",
      "Epoch 44 \t Batch 500 \t Validation Loss: 39.14955175018311\n",
      "Epoch 44 \t Batch 520 \t Validation Loss: 38.92204559766329\n",
      "Epoch 44 \t Batch 540 \t Validation Loss: 38.653011586931015\n",
      "Epoch 44 \t Batch 560 \t Validation Loss: 38.43144089153835\n",
      "Epoch 44 \t Batch 580 \t Validation Loss: 38.148722306613266\n",
      "Epoch 44 \t Batch 600 \t Validation Loss: 38.346587324142455\n",
      "Epoch 44 Training Loss: 46.82561350232772 Validation Loss: 38.97959189600759\n",
      "Epoch 44 completed\n",
      "Epoch 45 \t Batch 20 \t Training Loss: 48.280795097351074\n",
      "Epoch 45 \t Batch 40 \t Training Loss: 46.59305582046509\n",
      "Epoch 45 \t Batch 60 \t Training Loss: 46.689389355977376\n",
      "Epoch 45 \t Batch 80 \t Training Loss: 46.73007154464722\n",
      "Epoch 45 \t Batch 100 \t Training Loss: 46.99252704620361\n",
      "Epoch 45 \t Batch 120 \t Training Loss: 46.93845911026001\n",
      "Epoch 45 \t Batch 140 \t Training Loss: 47.0436215536935\n",
      "Epoch 45 \t Batch 160 \t Training Loss: 47.18679602146149\n",
      "Epoch 45 \t Batch 180 \t Training Loss: 47.17874531216091\n",
      "Epoch 45 \t Batch 200 \t Training Loss: 47.11020067214966\n",
      "Epoch 45 \t Batch 220 \t Training Loss: 47.02335962815718\n",
      "Epoch 45 \t Batch 240 \t Training Loss: 47.02160660425822\n",
      "Epoch 45 \t Batch 260 \t Training Loss: 47.041522715641904\n",
      "Epoch 45 \t Batch 280 \t Training Loss: 47.00265686852591\n",
      "Epoch 45 \t Batch 300 \t Training Loss: 46.953140856424966\n",
      "Epoch 45 \t Batch 320 \t Training Loss: 46.95959132909775\n",
      "Epoch 45 \t Batch 340 \t Training Loss: 46.873584309746235\n",
      "Epoch 45 \t Batch 360 \t Training Loss: 46.889582580990265\n",
      "Epoch 45 \t Batch 380 \t Training Loss: 46.88438576146176\n",
      "Epoch 45 \t Batch 400 \t Training Loss: 46.923927011489866\n",
      "Epoch 45 \t Batch 420 \t Training Loss: 46.8892057237171\n",
      "Epoch 45 \t Batch 440 \t Training Loss: 46.84018845124678\n",
      "Epoch 45 \t Batch 460 \t Training Loss: 46.82844217549199\n",
      "Epoch 45 \t Batch 480 \t Training Loss: 46.74875450134277\n",
      "Epoch 45 \t Batch 500 \t Training Loss: 46.72293688964844\n",
      "Epoch 45 \t Batch 520 \t Training Loss: 46.74350957870483\n",
      "Epoch 45 \t Batch 540 \t Training Loss: 46.716439508508756\n",
      "Epoch 45 \t Batch 560 \t Training Loss: 46.705238819122314\n",
      "Epoch 45 \t Batch 580 \t Training Loss: 46.68115744097479\n",
      "Epoch 45 \t Batch 600 \t Training Loss: 46.66571407318115\n",
      "Epoch 45 \t Batch 620 \t Training Loss: 46.66017860289543\n",
      "Epoch 45 \t Batch 640 \t Training Loss: 46.660973846912384\n",
      "Epoch 45 \t Batch 660 \t Training Loss: 46.67731482765891\n",
      "Epoch 45 \t Batch 680 \t Training Loss: 46.70177373325124\n",
      "Epoch 45 \t Batch 700 \t Training Loss: 46.720005220685685\n",
      "Epoch 45 \t Batch 720 \t Training Loss: 46.703400701946684\n",
      "Epoch 45 \t Batch 740 \t Training Loss: 46.71447695654792\n",
      "Epoch 45 \t Batch 760 \t Training Loss: 46.731205769589074\n",
      "Epoch 45 \t Batch 780 \t Training Loss: 46.73765385945638\n",
      "Epoch 45 \t Batch 800 \t Training Loss: 46.79023950099945\n",
      "Epoch 45 \t Batch 820 \t Training Loss: 46.76810152937726\n",
      "Epoch 45 \t Batch 840 \t Training Loss: 46.74776401065645\n",
      "Epoch 45 \t Batch 860 \t Training Loss: 46.747408942289134\n",
      "Epoch 45 \t Batch 880 \t Training Loss: 46.77368233420632\n",
      "Epoch 45 \t Batch 900 \t Training Loss: 46.77512802971734\n",
      "Epoch 45 \t Batch 20 \t Validation Loss: 18.552016067504884\n",
      "Epoch 45 \t Batch 40 \t Validation Loss: 21.64874608516693\n",
      "Epoch 45 \t Batch 60 \t Validation Loss: 21.2054039478302\n",
      "Epoch 45 \t Batch 80 \t Validation Loss: 21.872207725048064\n",
      "Epoch 45 \t Batch 100 \t Validation Loss: 23.32839705467224\n",
      "Epoch 45 \t Batch 120 \t Validation Loss: 24.516795214017232\n",
      "Epoch 45 \t Batch 140 \t Validation Loss: 25.022946187428065\n",
      "Epoch 45 \t Batch 160 \t Validation Loss: 26.89970102906227\n",
      "Epoch 45 \t Batch 180 \t Validation Loss: 30.39840842352973\n",
      "Epoch 45 \t Batch 200 \t Validation Loss: 31.82656325340271\n",
      "Epoch 45 \t Batch 220 \t Validation Loss: 33.05390263470736\n",
      "Epoch 45 \t Batch 240 \t Validation Loss: 33.523893574873604\n",
      "Epoch 45 \t Batch 260 \t Validation Loss: 35.55252608519334\n",
      "Epoch 45 \t Batch 280 \t Validation Loss: 36.66547092710223\n",
      "Epoch 45 \t Batch 300 \t Validation Loss: 37.72413292566935\n",
      "Epoch 45 \t Batch 320 \t Validation Loss: 38.24412712156773\n",
      "Epoch 45 \t Batch 340 \t Validation Loss: 38.2037746850182\n",
      "Epoch 45 \t Batch 360 \t Validation Loss: 38.04512863953908\n",
      "Epoch 45 \t Batch 380 \t Validation Loss: 38.306067318665356\n",
      "Epoch 45 \t Batch 400 \t Validation Loss: 37.96343924283981\n",
      "Epoch 45 \t Batch 420 \t Validation Loss: 38.05345537321908\n",
      "Epoch 45 \t Batch 440 \t Validation Loss: 37.82748785235665\n",
      "Epoch 45 \t Batch 460 \t Validation Loss: 38.14910611899003\n",
      "Epoch 45 \t Batch 480 \t Validation Loss: 38.674390655756\n",
      "Epoch 45 \t Batch 500 \t Validation Loss: 38.424978372573854\n",
      "Epoch 45 \t Batch 520 \t Validation Loss: 38.21525572813474\n",
      "Epoch 45 \t Batch 540 \t Validation Loss: 37.98665926368148\n",
      "Epoch 45 \t Batch 560 \t Validation Loss: 37.779791615690506\n",
      "Epoch 45 \t Batch 580 \t Validation Loss: 37.47389785339092\n",
      "Epoch 45 \t Batch 600 \t Validation Loss: 37.716192072232566\n",
      "Epoch 45 Training Loss: 46.795898824377694 Validation Loss: 38.357820527894155\n",
      "Epoch 45 completed\n",
      "Epoch 46 \t Batch 20 \t Training Loss: 47.24376201629639\n",
      "Epoch 46 \t Batch 40 \t Training Loss: 47.03617763519287\n",
      "Epoch 46 \t Batch 60 \t Training Loss: 46.739397048950195\n",
      "Epoch 46 \t Batch 80 \t Training Loss: 46.70811576843262\n",
      "Epoch 46 \t Batch 100 \t Training Loss: 46.88158542633057\n",
      "Epoch 46 \t Batch 120 \t Training Loss: 46.78873793284098\n",
      "Epoch 46 \t Batch 140 \t Training Loss: 46.66591802324567\n",
      "Epoch 46 \t Batch 160 \t Training Loss: 46.72532939910889\n",
      "Epoch 46 \t Batch 180 \t Training Loss: 46.88969635433621\n",
      "Epoch 46 \t Batch 200 \t Training Loss: 46.779445018768314\n",
      "Epoch 46 \t Batch 220 \t Training Loss: 46.73550654324618\n",
      "Epoch 46 \t Batch 240 \t Training Loss: 46.71550186475118\n",
      "Epoch 46 \t Batch 260 \t Training Loss: 46.68510538247915\n",
      "Epoch 46 \t Batch 280 \t Training Loss: 46.73637067249843\n",
      "Epoch 46 \t Batch 300 \t Training Loss: 46.87056084950765\n",
      "Epoch 46 \t Batch 320 \t Training Loss: 46.75939965248108\n",
      "Epoch 46 \t Batch 340 \t Training Loss: 46.696221564797796\n",
      "Epoch 46 \t Batch 360 \t Training Loss: 46.726505247751874\n",
      "Epoch 46 \t Batch 380 \t Training Loss: 46.67643206746955\n",
      "Epoch 46 \t Batch 400 \t Training Loss: 46.664057884216305\n",
      "Epoch 46 \t Batch 420 \t Training Loss: 46.61342978704543\n",
      "Epoch 46 \t Batch 440 \t Training Loss: 46.6781832001426\n",
      "Epoch 46 \t Batch 460 \t Training Loss: 46.71728022202201\n",
      "Epoch 46 \t Batch 480 \t Training Loss: 46.73956178824107\n",
      "Epoch 46 \t Batch 500 \t Training Loss: 46.68338505554199\n",
      "Epoch 46 \t Batch 520 \t Training Loss: 46.685504597883956\n",
      "Epoch 46 \t Batch 540 \t Training Loss: 46.705015443872526\n",
      "Epoch 46 \t Batch 560 \t Training Loss: 46.6883241380964\n",
      "Epoch 46 \t Batch 580 \t Training Loss: 46.67848454179435\n",
      "Epoch 46 \t Batch 600 \t Training Loss: 46.69699190775553\n",
      "Epoch 46 \t Batch 620 \t Training Loss: 46.646602538324174\n",
      "Epoch 46 \t Batch 640 \t Training Loss: 46.65856045484543\n",
      "Epoch 46 \t Batch 660 \t Training Loss: 46.66430530548096\n",
      "Epoch 46 \t Batch 680 \t Training Loss: 46.65480131822474\n",
      "Epoch 46 \t Batch 700 \t Training Loss: 46.674725014822826\n",
      "Epoch 46 \t Batch 720 \t Training Loss: 46.694864474402536\n",
      "Epoch 46 \t Batch 740 \t Training Loss: 46.73886139843915\n",
      "Epoch 46 \t Batch 760 \t Training Loss: 46.79092102552715\n",
      "Epoch 46 \t Batch 780 \t Training Loss: 46.7785853214753\n",
      "Epoch 46 \t Batch 800 \t Training Loss: 46.80973096370697\n",
      "Epoch 46 \t Batch 820 \t Training Loss: 46.789622804595204\n",
      "Epoch 46 \t Batch 840 \t Training Loss: 46.781663744790215\n",
      "Epoch 46 \t Batch 860 \t Training Loss: 46.78744966373887\n",
      "Epoch 46 \t Batch 880 \t Training Loss: 46.77411596124823\n",
      "Epoch 46 \t Batch 900 \t Training Loss: 46.747037391662595\n",
      "Epoch 46 \t Batch 20 \t Validation Loss: 26.615356349945067\n",
      "Epoch 46 \t Batch 40 \t Validation Loss: 26.59970784187317\n",
      "Epoch 46 \t Batch 60 \t Validation Loss: 27.07505400975545\n",
      "Epoch 46 \t Batch 80 \t Validation Loss: 27.308977258205413\n",
      "Epoch 46 \t Batch 100 \t Validation Loss: 27.856975317001343\n",
      "Epoch 46 \t Batch 120 \t Validation Loss: 28.479950292905173\n",
      "Epoch 46 \t Batch 140 \t Validation Loss: 28.560900081907\n",
      "Epoch 46 \t Batch 160 \t Validation Loss: 30.083449214696884\n",
      "Epoch 46 \t Batch 180 \t Validation Loss: 33.41115875244141\n",
      "Epoch 46 \t Batch 200 \t Validation Loss: 34.62761371612549\n",
      "Epoch 46 \t Batch 220 \t Validation Loss: 35.68968830975619\n",
      "Epoch 46 \t Batch 240 \t Validation Loss: 35.98924305041631\n",
      "Epoch 46 \t Batch 260 \t Validation Loss: 37.935481056800256\n",
      "Epoch 46 \t Batch 280 \t Validation Loss: 38.91772543021611\n",
      "Epoch 46 \t Batch 300 \t Validation Loss: 39.921007261276245\n",
      "Epoch 46 \t Batch 320 \t Validation Loss: 40.34029842913151\n",
      "Epoch 46 \t Batch 340 \t Validation Loss: 40.190243942597334\n",
      "Epoch 46 \t Batch 360 \t Validation Loss: 39.9634337398741\n",
      "Epoch 46 \t Batch 380 \t Validation Loss: 40.10506775755631\n",
      "Epoch 46 \t Batch 400 \t Validation Loss: 39.62051204919815\n",
      "Epoch 46 \t Batch 420 \t Validation Loss: 39.579588867369154\n",
      "Epoch 46 \t Batch 440 \t Validation Loss: 39.2101377660578\n",
      "Epoch 46 \t Batch 460 \t Validation Loss: 39.376736591173255\n",
      "Epoch 46 \t Batch 480 \t Validation Loss: 39.82072096268336\n",
      "Epoch 46 \t Batch 500 \t Validation Loss: 39.5078616065979\n",
      "Epoch 46 \t Batch 520 \t Validation Loss: 39.220936325880196\n",
      "Epoch 46 \t Batch 540 \t Validation Loss: 38.928045772623136\n",
      "Epoch 46 \t Batch 560 \t Validation Loss: 38.6954306449209\n",
      "Epoch 46 \t Batch 580 \t Validation Loss: 38.42079680705893\n",
      "Epoch 46 \t Batch 600 \t Validation Loss: 38.60282359282176\n",
      "Epoch 46 Training Loss: 46.73375264274905 Validation Loss: 39.24095914116153\n",
      "Epoch 46 completed\n",
      "Epoch 47 \t Batch 20 \t Training Loss: 42.99777069091797\n",
      "Epoch 47 \t Batch 40 \t Training Loss: 44.561836338043214\n",
      "Epoch 47 \t Batch 60 \t Training Loss: 45.29446887969971\n",
      "Epoch 47 \t Batch 80 \t Training Loss: 45.45425238609314\n",
      "Epoch 47 \t Batch 100 \t Training Loss: 45.83446315765381\n",
      "Epoch 47 \t Batch 120 \t Training Loss: 45.7970547358195\n",
      "Epoch 47 \t Batch 140 \t Training Loss: 45.763624463762554\n",
      "Epoch 47 \t Batch 160 \t Training Loss: 45.87785186767578\n",
      "Epoch 47 \t Batch 180 \t Training Loss: 45.954990005493165\n",
      "Epoch 47 \t Batch 200 \t Training Loss: 46.07194492340088\n",
      "Epoch 47 \t Batch 220 \t Training Loss: 45.99254869981245\n",
      "Epoch 47 \t Batch 240 \t Training Loss: 46.21223724683126\n",
      "Epoch 47 \t Batch 260 \t Training Loss: 46.28804770249587\n",
      "Epoch 47 \t Batch 280 \t Training Loss: 46.3833199773516\n",
      "Epoch 47 \t Batch 300 \t Training Loss: 46.49294645945231\n",
      "Epoch 47 \t Batch 320 \t Training Loss: 46.48609701395035\n",
      "Epoch 47 \t Batch 340 \t Training Loss: 46.55009307861328\n",
      "Epoch 47 \t Batch 360 \t Training Loss: 46.56926623450385\n",
      "Epoch 47 \t Batch 380 \t Training Loss: 46.55719617542468\n",
      "Epoch 47 \t Batch 400 \t Training Loss: 46.58777017593384\n",
      "Epoch 47 \t Batch 420 \t Training Loss: 46.629348200843445\n",
      "Epoch 47 \t Batch 440 \t Training Loss: 46.593786525726316\n",
      "Epoch 47 \t Batch 460 \t Training Loss: 46.58745494510816\n",
      "Epoch 47 \t Batch 480 \t Training Loss: 46.55607773462931\n",
      "Epoch 47 \t Batch 500 \t Training Loss: 46.519803970336916\n",
      "Epoch 47 \t Batch 520 \t Training Loss: 46.55090408325195\n",
      "Epoch 47 \t Batch 540 \t Training Loss: 46.57740057486075\n",
      "Epoch 47 \t Batch 560 \t Training Loss: 46.59122110094343\n",
      "Epoch 47 \t Batch 580 \t Training Loss: 46.51405501530088\n",
      "Epoch 47 \t Batch 600 \t Training Loss: 46.548632265726724\n",
      "Epoch 47 \t Batch 620 \t Training Loss: 46.546313365813226\n",
      "Epoch 47 \t Batch 640 \t Training Loss: 46.5511118888855\n",
      "Epoch 47 \t Batch 660 \t Training Loss: 46.58299151333895\n",
      "Epoch 47 \t Batch 680 \t Training Loss: 46.61992479773129\n",
      "Epoch 47 \t Batch 700 \t Training Loss: 46.62670181819371\n",
      "Epoch 47 \t Batch 720 \t Training Loss: 46.649105066723294\n",
      "Epoch 47 \t Batch 740 \t Training Loss: 46.62847706562764\n",
      "Epoch 47 \t Batch 760 \t Training Loss: 46.59062859886571\n",
      "Epoch 47 \t Batch 780 \t Training Loss: 46.59865590609037\n",
      "Epoch 47 \t Batch 800 \t Training Loss: 46.60726449012756\n",
      "Epoch 47 \t Batch 820 \t Training Loss: 46.655116900002085\n",
      "Epoch 47 \t Batch 840 \t Training Loss: 46.6629025777181\n",
      "Epoch 47 \t Batch 860 \t Training Loss: 46.657642506444176\n",
      "Epoch 47 \t Batch 880 \t Training Loss: 46.69472896402532\n",
      "Epoch 47 \t Batch 900 \t Training Loss: 46.70712331559923\n",
      "Epoch 47 \t Batch 20 \t Validation Loss: 19.98214168548584\n",
      "Epoch 47 \t Batch 40 \t Validation Loss: 22.166286063194274\n",
      "Epoch 47 \t Batch 60 \t Validation Loss: 21.866629854838052\n",
      "Epoch 47 \t Batch 80 \t Validation Loss: 22.484451174736023\n",
      "Epoch 47 \t Batch 100 \t Validation Loss: 23.628099880218507\n",
      "Epoch 47 \t Batch 120 \t Validation Loss: 24.69045666853587\n",
      "Epoch 47 \t Batch 140 \t Validation Loss: 25.161645664487565\n",
      "Epoch 47 \t Batch 160 \t Validation Loss: 27.028639167547226\n",
      "Epoch 47 \t Batch 180 \t Validation Loss: 30.584448703130086\n",
      "Epoch 47 \t Batch 200 \t Validation Loss: 31.98442945957184\n",
      "Epoch 47 \t Batch 220 \t Validation Loss: 33.17031954418529\n",
      "Epoch 47 \t Batch 240 \t Validation Loss: 33.63484186728795\n",
      "Epoch 47 \t Batch 260 \t Validation Loss: 35.668070756472076\n",
      "Epoch 47 \t Batch 280 \t Validation Loss: 36.77366854463305\n",
      "Epoch 47 \t Batch 300 \t Validation Loss: 37.85374164263408\n",
      "Epoch 47 \t Batch 320 \t Validation Loss: 38.35815636515618\n",
      "Epoch 47 \t Batch 340 \t Validation Loss: 38.310192944021786\n",
      "Epoch 47 \t Batch 360 \t Validation Loss: 38.15727790196737\n",
      "Epoch 47 \t Batch 380 \t Validation Loss: 38.375954688222784\n",
      "Epoch 47 \t Batch 400 \t Validation Loss: 37.99619025707245\n",
      "Epoch 47 \t Batch 420 \t Validation Loss: 38.04501534643627\n",
      "Epoch 47 \t Batch 440 \t Validation Loss: 37.75137712088498\n",
      "Epoch 47 \t Batch 460 \t Validation Loss: 38.002379780230314\n",
      "Epoch 47 \t Batch 480 \t Validation Loss: 38.51282443801562\n",
      "Epoch 47 \t Batch 500 \t Validation Loss: 38.25963774299622\n",
      "Epoch 47 \t Batch 520 \t Validation Loss: 38.034605585611786\n",
      "Epoch 47 \t Batch 540 \t Validation Loss: 37.805826828214855\n",
      "Epoch 47 \t Batch 560 \t Validation Loss: 37.61161178009851\n",
      "Epoch 47 \t Batch 580 \t Validation Loss: 37.38841968240409\n",
      "Epoch 47 \t Batch 600 \t Validation Loss: 37.61354422410329\n",
      "Epoch 47 Training Loss: 46.71858022309181 Validation Loss: 38.26870638828773\n",
      "Epoch 47 completed\n",
      "Epoch 48 \t Batch 20 \t Training Loss: 47.56666641235351\n",
      "Epoch 48 \t Batch 40 \t Training Loss: 46.84981660842895\n",
      "Epoch 48 \t Batch 60 \t Training Loss: 46.76412321726481\n",
      "Epoch 48 \t Batch 80 \t Training Loss: 46.58155889511109\n",
      "Epoch 48 \t Batch 100 \t Training Loss: 46.445137939453126\n",
      "Epoch 48 \t Batch 120 \t Training Loss: 46.41772321065267\n",
      "Epoch 48 \t Batch 140 \t Training Loss: 46.602365030561174\n",
      "Epoch 48 \t Batch 160 \t Training Loss: 46.561255025863645\n",
      "Epoch 48 \t Batch 180 \t Training Loss: 46.61955892774794\n",
      "Epoch 48 \t Batch 200 \t Training Loss: 46.66329490661621\n",
      "Epoch 48 \t Batch 220 \t Training Loss: 46.692674827575686\n",
      "Epoch 48 \t Batch 240 \t Training Loss: 46.634365479151406\n",
      "Epoch 48 \t Batch 260 \t Training Loss: 46.67771132542537\n",
      "Epoch 48 \t Batch 280 \t Training Loss: 46.674259649004256\n",
      "Epoch 48 \t Batch 300 \t Training Loss: 46.71172828674317\n",
      "Epoch 48 \t Batch 320 \t Training Loss: 46.67665812969208\n",
      "Epoch 48 \t Batch 340 \t Training Loss: 46.70227329029756\n",
      "Epoch 48 \t Batch 360 \t Training Loss: 46.70282668007745\n",
      "Epoch 48 \t Batch 380 \t Training Loss: 46.72375684035452\n",
      "Epoch 48 \t Batch 400 \t Training Loss: 46.67310796737671\n",
      "Epoch 48 \t Batch 420 \t Training Loss: 46.66522773561024\n",
      "Epoch 48 \t Batch 440 \t Training Loss: 46.708851502158424\n",
      "Epoch 48 \t Batch 460 \t Training Loss: 46.72892022340194\n",
      "Epoch 48 \t Batch 480 \t Training Loss: 46.705082599322004\n",
      "Epoch 48 \t Batch 500 \t Training Loss: 46.7537974243164\n",
      "Epoch 48 \t Batch 520 \t Training Loss: 46.769905757904056\n",
      "Epoch 48 \t Batch 540 \t Training Loss: 46.775721323931656\n",
      "Epoch 48 \t Batch 560 \t Training Loss: 46.76246970721653\n",
      "Epoch 48 \t Batch 580 \t Training Loss: 46.77145959919897\n",
      "Epoch 48 \t Batch 600 \t Training Loss: 46.738406874338786\n",
      "Epoch 48 \t Batch 620 \t Training Loss: 46.72735424041748\n",
      "Epoch 48 \t Batch 640 \t Training Loss: 46.636365646123885\n",
      "Epoch 48 \t Batch 660 \t Training Loss: 46.61384891163219\n",
      "Epoch 48 \t Batch 680 \t Training Loss: 46.58990878497853\n",
      "Epoch 48 \t Batch 700 \t Training Loss: 46.57405343736921\n",
      "Epoch 48 \t Batch 720 \t Training Loss: 46.5847987651825\n",
      "Epoch 48 \t Batch 740 \t Training Loss: 46.617649279413996\n",
      "Epoch 48 \t Batch 760 \t Training Loss: 46.66227878269396\n",
      "Epoch 48 \t Batch 780 \t Training Loss: 46.660705082233136\n",
      "Epoch 48 \t Batch 800 \t Training Loss: 46.64742565631867\n",
      "Epoch 48 \t Batch 820 \t Training Loss: 46.59701783250018\n",
      "Epoch 48 \t Batch 840 \t Training Loss: 46.64223294939313\n",
      "Epoch 48 \t Batch 860 \t Training Loss: 46.64082609220993\n",
      "Epoch 48 \t Batch 880 \t Training Loss: 46.67948691628196\n",
      "Epoch 48 \t Batch 900 \t Training Loss: 46.67889557308621\n",
      "Epoch 48 \t Batch 20 \t Validation Loss: 17.677493619918824\n",
      "Epoch 48 \t Batch 40 \t Validation Loss: 20.54289665222168\n",
      "Epoch 48 \t Batch 60 \t Validation Loss: 20.083160622914633\n",
      "Epoch 48 \t Batch 80 \t Validation Loss: 20.87639936208725\n",
      "Epoch 48 \t Batch 100 \t Validation Loss: 22.622843046188354\n",
      "Epoch 48 \t Batch 120 \t Validation Loss: 23.860774326324464\n",
      "Epoch 48 \t Batch 140 \t Validation Loss: 24.523770114353724\n",
      "Epoch 48 \t Batch 160 \t Validation Loss: 26.609107542037965\n",
      "Epoch 48 \t Batch 180 \t Validation Loss: 30.448389938142565\n",
      "Epoch 48 \t Batch 200 \t Validation Loss: 31.987946925163268\n",
      "Epoch 48 \t Batch 220 \t Validation Loss: 33.298003287748855\n",
      "Epoch 48 \t Batch 240 \t Validation Loss: 33.85102844635646\n",
      "Epoch 48 \t Batch 260 \t Validation Loss: 35.994262941067035\n",
      "Epoch 48 \t Batch 280 \t Validation Loss: 37.14047460896628\n",
      "Epoch 48 \t Batch 300 \t Validation Loss: 38.36755007743835\n",
      "Epoch 48 \t Batch 320 \t Validation Loss: 38.92287303805351\n",
      "Epoch 48 \t Batch 340 \t Validation Loss: 38.863925344803754\n",
      "Epoch 48 \t Batch 360 \t Validation Loss: 38.75619503127204\n",
      "Epoch 48 \t Batch 380 \t Validation Loss: 38.97678256787752\n",
      "Epoch 48 \t Batch 400 \t Validation Loss: 38.56140800476074\n",
      "Epoch 48 \t Batch 420 \t Validation Loss: 38.540419678460985\n",
      "Epoch 48 \t Batch 440 \t Validation Loss: 38.22999620871111\n",
      "Epoch 48 \t Batch 460 \t Validation Loss: 38.465454918405285\n",
      "Epoch 48 \t Batch 480 \t Validation Loss: 38.957941289742784\n",
      "Epoch 48 \t Batch 500 \t Validation Loss: 38.67170822906494\n",
      "Epoch 48 \t Batch 520 \t Validation Loss: 38.498375210395224\n",
      "Epoch 48 \t Batch 540 \t Validation Loss: 38.27741839444196\n",
      "Epoch 48 \t Batch 560 \t Validation Loss: 38.10433147634779\n",
      "Epoch 48 \t Batch 580 \t Validation Loss: 37.94579463498346\n",
      "Epoch 48 \t Batch 600 \t Validation Loss: 38.15857511520386\n",
      "Epoch 48 Training Loss: 46.67005513980578 Validation Loss: 38.81517273110229\n",
      "Epoch 48 completed\n",
      "Epoch 49 \t Batch 20 \t Training Loss: 45.91601905822754\n",
      "Epoch 49 \t Batch 40 \t Training Loss: 46.70466613769531\n",
      "Epoch 49 \t Batch 60 \t Training Loss: 46.846298408508304\n",
      "Epoch 49 \t Batch 80 \t Training Loss: 46.31158652305603\n",
      "Epoch 49 \t Batch 100 \t Training Loss: 46.46557456970215\n",
      "Epoch 49 \t Batch 120 \t Training Loss: 46.61584870020548\n",
      "Epoch 49 \t Batch 140 \t Training Loss: 46.43630782536098\n",
      "Epoch 49 \t Batch 160 \t Training Loss: 46.63485772609711\n",
      "Epoch 49 \t Batch 180 \t Training Loss: 46.671966552734375\n",
      "Epoch 49 \t Batch 200 \t Training Loss: 46.759208812713624\n",
      "Epoch 49 \t Batch 220 \t Training Loss: 46.64847429448908\n",
      "Epoch 49 \t Batch 240 \t Training Loss: 46.741565752029416\n",
      "Epoch 49 \t Batch 260 \t Training Loss: 46.77535759852483\n",
      "Epoch 49 \t Batch 280 \t Training Loss: 46.8185416494097\n",
      "Epoch 49 \t Batch 300 \t Training Loss: 46.807272593180336\n",
      "Epoch 49 \t Batch 320 \t Training Loss: 46.80243390798569\n",
      "Epoch 49 \t Batch 340 \t Training Loss: 46.644898930717915\n",
      "Epoch 49 \t Batch 360 \t Training Loss: 46.66153195699056\n",
      "Epoch 49 \t Batch 380 \t Training Loss: 46.626702880859376\n",
      "Epoch 49 \t Batch 400 \t Training Loss: 46.60830966949463\n",
      "Epoch 49 \t Batch 420 \t Training Loss: 46.714045261201406\n",
      "Epoch 49 \t Batch 440 \t Training Loss: 46.715773565118965\n",
      "Epoch 49 \t Batch 460 \t Training Loss: 46.731680604685906\n",
      "Epoch 49 \t Batch 480 \t Training Loss: 46.71156236330668\n",
      "Epoch 49 \t Batch 500 \t Training Loss: 46.73494998931885\n",
      "Epoch 49 \t Batch 520 \t Training Loss: 46.756833289219784\n",
      "Epoch 49 \t Batch 540 \t Training Loss: 46.754561516090675\n",
      "Epoch 49 \t Batch 560 \t Training Loss: 46.743676342282974\n",
      "Epoch 49 \t Batch 580 \t Training Loss: 46.71081523237557\n",
      "Epoch 49 \t Batch 600 \t Training Loss: 46.669610563913984\n",
      "Epoch 49 \t Batch 620 \t Training Loss: 46.69012475782825\n",
      "Epoch 49 \t Batch 640 \t Training Loss: 46.6804652094841\n",
      "Epoch 49 \t Batch 660 \t Training Loss: 46.688996002890846\n",
      "Epoch 49 \t Batch 680 \t Training Loss: 46.65840872035307\n",
      "Epoch 49 \t Batch 700 \t Training Loss: 46.68318842751639\n",
      "Epoch 49 \t Batch 720 \t Training Loss: 46.65681013001336\n",
      "Epoch 49 \t Batch 740 \t Training Loss: 46.6469662331246\n",
      "Epoch 49 \t Batch 760 \t Training Loss: 46.63887732154445\n",
      "Epoch 49 \t Batch 780 \t Training Loss: 46.63213661389473\n",
      "Epoch 49 \t Batch 800 \t Training Loss: 46.62184763908386\n",
      "Epoch 49 \t Batch 820 \t Training Loss: 46.61578648730022\n",
      "Epoch 49 \t Batch 840 \t Training Loss: 46.63459481284732\n",
      "Epoch 49 \t Batch 860 \t Training Loss: 46.62796186402787\n",
      "Epoch 49 \t Batch 880 \t Training Loss: 46.63583473292264\n",
      "Epoch 49 \t Batch 900 \t Training Loss: 46.63887552049425\n",
      "Epoch 49 \t Batch 20 \t Validation Loss: 20.419879245758057\n",
      "Epoch 49 \t Batch 40 \t Validation Loss: 23.418496942520143\n",
      "Epoch 49 \t Batch 60 \t Validation Loss: 22.762157996495564\n",
      "Epoch 49 \t Batch 80 \t Validation Loss: 23.155158507823945\n",
      "Epoch 49 \t Batch 100 \t Validation Loss: 24.278619680404663\n",
      "Epoch 49 \t Batch 120 \t Validation Loss: 25.320845286051433\n",
      "Epoch 49 \t Batch 140 \t Validation Loss: 25.791585649762833\n",
      "Epoch 49 \t Batch 160 \t Validation Loss: 27.709561502933504\n",
      "Epoch 49 \t Batch 180 \t Validation Loss: 31.1613563908471\n",
      "Epoch 49 \t Batch 200 \t Validation Loss: 32.535508599281314\n",
      "Epoch 49 \t Batch 220 \t Validation Loss: 33.79096342867071\n",
      "Epoch 49 \t Batch 240 \t Validation Loss: 34.266243990262346\n",
      "Epoch 49 \t Batch 260 \t Validation Loss: 36.32344395197355\n",
      "Epoch 49 \t Batch 280 \t Validation Loss: 37.4536887713841\n",
      "Epoch 49 \t Batch 300 \t Validation Loss: 38.53123945871989\n",
      "Epoch 49 \t Batch 320 \t Validation Loss: 39.03560274243355\n",
      "Epoch 49 \t Batch 340 \t Validation Loss: 38.96459194071153\n",
      "Epoch 49 \t Batch 360 \t Validation Loss: 38.822244008382164\n",
      "Epoch 49 \t Batch 380 \t Validation Loss: 39.066754160429305\n",
      "Epoch 49 \t Batch 400 \t Validation Loss: 38.67666574954987\n",
      "Epoch 49 \t Batch 420 \t Validation Loss: 38.68318143799191\n",
      "Epoch 49 \t Batch 440 \t Validation Loss: 38.41183699477803\n",
      "Epoch 49 \t Batch 460 \t Validation Loss: 38.68993132840032\n",
      "Epoch 49 \t Batch 480 \t Validation Loss: 39.162737399339676\n",
      "Epoch 49 \t Batch 500 \t Validation Loss: 38.88990337181092\n",
      "Epoch 49 \t Batch 520 \t Validation Loss: 38.751741368954\n",
      "Epoch 49 \t Batch 540 \t Validation Loss: 38.53033652128997\n",
      "Epoch 49 \t Batch 560 \t Validation Loss: 38.38120327336448\n",
      "Epoch 49 \t Batch 580 \t Validation Loss: 38.20284509987667\n",
      "Epoch 49 \t Batch 600 \t Validation Loss: 38.418372042973836\n",
      "Epoch 49 Training Loss: 46.654452736791114 Validation Loss: 39.11247006329623\n",
      "Epoch 49 completed\n",
      "Epoch 50 \t Batch 20 \t Training Loss: 46.727013969421385\n",
      "Epoch 50 \t Batch 40 \t Training Loss: 46.72191495895386\n",
      "Epoch 50 \t Batch 60 \t Training Loss: 46.582969665527344\n",
      "Epoch 50 \t Batch 80 \t Training Loss: 46.872300910949704\n",
      "Epoch 50 \t Batch 100 \t Training Loss: 46.698958778381346\n",
      "Epoch 50 \t Batch 120 \t Training Loss: 46.482538572947185\n",
      "Epoch 50 \t Batch 140 \t Training Loss: 46.38230446406773\n",
      "Epoch 50 \t Batch 160 \t Training Loss: 46.449993467330934\n",
      "Epoch 50 \t Batch 180 \t Training Loss: 46.58441403706868\n",
      "Epoch 50 \t Batch 200 \t Training Loss: 46.73642183303833\n",
      "Epoch 50 \t Batch 220 \t Training Loss: 46.74471643621271\n",
      "Epoch 50 \t Batch 240 \t Training Loss: 46.6504207611084\n",
      "Epoch 50 \t Batch 260 \t Training Loss: 46.61164945455698\n",
      "Epoch 50 \t Batch 280 \t Training Loss: 46.666925988878525\n",
      "Epoch 50 \t Batch 300 \t Training Loss: 46.730147183736165\n",
      "Epoch 50 \t Batch 320 \t Training Loss: 46.714576435089114\n",
      "Epoch 50 \t Batch 340 \t Training Loss: 46.70120557897231\n",
      "Epoch 50 \t Batch 360 \t Training Loss: 46.69119281768799\n",
      "Epoch 50 \t Batch 380 \t Training Loss: 46.69720619603207\n",
      "Epoch 50 \t Batch 400 \t Training Loss: 46.71899991035461\n",
      "Epoch 50 \t Batch 420 \t Training Loss: 46.6681204477946\n",
      "Epoch 50 \t Batch 440 \t Training Loss: 46.715057815204965\n",
      "Epoch 50 \t Batch 460 \t Training Loss: 46.65926745870839\n",
      "Epoch 50 \t Batch 480 \t Training Loss: 46.6496924718221\n",
      "Epoch 50 \t Batch 500 \t Training Loss: 46.63652606201172\n",
      "Epoch 50 \t Batch 520 \t Training Loss: 46.62070447481596\n",
      "Epoch 50 \t Batch 540 \t Training Loss: 46.61257582770453\n",
      "Epoch 50 \t Batch 560 \t Training Loss: 46.66634246962411\n",
      "Epoch 50 \t Batch 580 \t Training Loss: 46.61024899976007\n",
      "Epoch 50 \t Batch 600 \t Training Loss: 46.62460741043091\n",
      "Epoch 50 \t Batch 620 \t Training Loss: 46.59351204902895\n",
      "Epoch 50 \t Batch 640 \t Training Loss: 46.59182057976723\n",
      "Epoch 50 \t Batch 660 \t Training Loss: 46.60946188839999\n",
      "Epoch 50 \t Batch 680 \t Training Loss: 46.57582394656013\n",
      "Epoch 50 \t Batch 700 \t Training Loss: 46.604420585632326\n",
      "Epoch 50 \t Batch 720 \t Training Loss: 46.59409419695536\n",
      "Epoch 50 \t Batch 740 \t Training Loss: 46.630210335190235\n",
      "Epoch 50 \t Batch 760 \t Training Loss: 46.67366015785619\n",
      "Epoch 50 \t Batch 780 \t Training Loss: 46.60626990000407\n",
      "Epoch 50 \t Batch 800 \t Training Loss: 46.5829595041275\n",
      "Epoch 50 \t Batch 820 \t Training Loss: 46.58244846157911\n",
      "Epoch 50 \t Batch 840 \t Training Loss: 46.59362627665202\n",
      "Epoch 50 \t Batch 860 \t Training Loss: 46.59995434339656\n",
      "Epoch 50 \t Batch 880 \t Training Loss: 46.62986345291138\n",
      "Epoch 50 \t Batch 900 \t Training Loss: 46.622630526224775\n",
      "Epoch 50 \t Batch 20 \t Validation Loss: 22.696275091171266\n",
      "Epoch 50 \t Batch 40 \t Validation Loss: 22.693975806236267\n",
      "Epoch 50 \t Batch 60 \t Validation Loss: 22.854882383346556\n",
      "Epoch 50 \t Batch 80 \t Validation Loss: 23.1587127327919\n",
      "Epoch 50 \t Batch 100 \t Validation Loss: 24.126933126449586\n",
      "Epoch 50 \t Batch 120 \t Validation Loss: 25.07842311064402\n",
      "Epoch 50 \t Batch 140 \t Validation Loss: 25.543183067866735\n",
      "Epoch 50 \t Batch 160 \t Validation Loss: 27.61677612066269\n",
      "Epoch 50 \t Batch 180 \t Validation Loss: 31.630142238405014\n",
      "Epoch 50 \t Batch 200 \t Validation Loss: 33.22701557636261\n",
      "Epoch 50 \t Batch 220 \t Validation Loss: 34.65615149844776\n",
      "Epoch 50 \t Batch 240 \t Validation Loss: 35.241632064183555\n",
      "Epoch 50 \t Batch 260 \t Validation Loss: 37.4547416650332\n",
      "Epoch 50 \t Batch 280 \t Validation Loss: 38.61424393313272\n",
      "Epoch 50 \t Batch 300 \t Validation Loss: 39.87250199953715\n",
      "Epoch 50 \t Batch 320 \t Validation Loss: 40.43715761601925\n",
      "Epoch 50 \t Batch 340 \t Validation Loss: 40.33430858780356\n",
      "Epoch 50 \t Batch 360 \t Validation Loss: 40.19451943238576\n",
      "Epoch 50 \t Batch 380 \t Validation Loss: 40.39209452428316\n",
      "Epoch 50 \t Batch 400 \t Validation Loss: 39.91147533178329\n",
      "Epoch 50 \t Batch 420 \t Validation Loss: 39.8297118414016\n",
      "Epoch 50 \t Batch 440 \t Validation Loss: 39.43496700200168\n",
      "Epoch 50 \t Batch 460 \t Validation Loss: 39.599901327879536\n",
      "Epoch 50 \t Batch 480 \t Validation Loss: 40.05981076161067\n",
      "Epoch 50 \t Batch 500 \t Validation Loss: 39.74829291534424\n",
      "Epoch 50 \t Batch 520 \t Validation Loss: 39.48023921159597\n",
      "Epoch 50 \t Batch 540 \t Validation Loss: 39.24184240765042\n",
      "Epoch 50 \t Batch 560 \t Validation Loss: 39.02737737042563\n",
      "Epoch 50 \t Batch 580 \t Validation Loss: 38.797835432249926\n",
      "Epoch 50 \t Batch 600 \t Validation Loss: 39.015779647827145\n",
      "Epoch 50 Training Loss: 46.61601309739906 Validation Loss: 39.66878223109555\n",
      "Epoch 50 completed\n",
      "Epoch 51 \t Batch 20 \t Training Loss: 48.43357906341553\n",
      "Epoch 51 \t Batch 40 \t Training Loss: 47.37748727798462\n",
      "Epoch 51 \t Batch 60 \t Training Loss: 46.958000055948894\n",
      "Epoch 51 \t Batch 80 \t Training Loss: 47.1345862865448\n",
      "Epoch 51 \t Batch 100 \t Training Loss: 47.04159069061279\n",
      "Epoch 51 \t Batch 120 \t Training Loss: 46.963529141743976\n",
      "Epoch 51 \t Batch 140 \t Training Loss: 46.8562591280256\n",
      "Epoch 51 \t Batch 160 \t Training Loss: 46.754844427108765\n",
      "Epoch 51 \t Batch 180 \t Training Loss: 46.68179155985514\n",
      "Epoch 51 \t Batch 200 \t Training Loss: 46.73003856658936\n",
      "Epoch 51 \t Batch 220 \t Training Loss: 46.665513732216574\n",
      "Epoch 51 \t Batch 240 \t Training Loss: 46.587997229894\n",
      "Epoch 51 \t Batch 260 \t Training Loss: 46.62041909144475\n",
      "Epoch 51 \t Batch 280 \t Training Loss: 46.6256701196943\n",
      "Epoch 51 \t Batch 300 \t Training Loss: 46.62006907145182\n",
      "Epoch 51 \t Batch 320 \t Training Loss: 46.689766430854796\n",
      "Epoch 51 \t Batch 340 \t Training Loss: 46.78933650746065\n",
      "Epoch 51 \t Batch 360 \t Training Loss: 46.77110579808553\n",
      "Epoch 51 \t Batch 380 \t Training Loss: 46.69596106880589\n",
      "Epoch 51 \t Batch 400 \t Training Loss: 46.69229893684387\n",
      "Epoch 51 \t Batch 420 \t Training Loss: 46.651731427510576\n",
      "Epoch 51 \t Batch 440 \t Training Loss: 46.57667914303866\n",
      "Epoch 51 \t Batch 460 \t Training Loss: 46.63627339238706\n",
      "Epoch 51 \t Batch 480 \t Training Loss: 46.578052314122516\n",
      "Epoch 51 \t Batch 500 \t Training Loss: 46.55488708496094\n",
      "Epoch 51 \t Batch 520 \t Training Loss: 46.574316699688254\n",
      "Epoch 51 \t Batch 540 \t Training Loss: 46.606450963903356\n",
      "Epoch 51 \t Batch 560 \t Training Loss: 46.590113271985736\n",
      "Epoch 51 \t Batch 580 \t Training Loss: 46.53734434719743\n",
      "Epoch 51 \t Batch 600 \t Training Loss: 46.501013634999595\n",
      "Epoch 51 \t Batch 620 \t Training Loss: 46.560310179187404\n",
      "Epoch 51 \t Batch 640 \t Training Loss: 46.61492267847061\n",
      "Epoch 51 \t Batch 660 \t Training Loss: 46.61091405695135\n",
      "Epoch 51 \t Batch 680 \t Training Loss: 46.61326725342695\n",
      "Epoch 51 \t Batch 700 \t Training Loss: 46.59736400059291\n",
      "Epoch 51 \t Batch 720 \t Training Loss: 46.61780092981127\n",
      "Epoch 51 \t Batch 740 \t Training Loss: 46.603723294026146\n",
      "Epoch 51 \t Batch 760 \t Training Loss: 46.58522169213546\n",
      "Epoch 51 \t Batch 780 \t Training Loss: 46.58096720866668\n",
      "Epoch 51 \t Batch 800 \t Training Loss: 46.54301261425018\n",
      "Epoch 51 \t Batch 820 \t Training Loss: 46.52696237331483\n",
      "Epoch 51 \t Batch 840 \t Training Loss: 46.550854646591915\n",
      "Epoch 51 \t Batch 860 \t Training Loss: 46.550018244011454\n",
      "Epoch 51 \t Batch 880 \t Training Loss: 46.54453180919994\n",
      "Epoch 51 \t Batch 900 \t Training Loss: 46.56453397538927\n",
      "Epoch 51 \t Batch 20 \t Validation Loss: 23.886378526687622\n",
      "Epoch 51 \t Batch 40 \t Validation Loss: 26.049198865890503\n",
      "Epoch 51 \t Batch 60 \t Validation Loss: 25.766035970052084\n",
      "Epoch 51 \t Batch 80 \t Validation Loss: 26.040073919296265\n",
      "Epoch 51 \t Batch 100 \t Validation Loss: 26.829592914581298\n",
      "Epoch 51 \t Batch 120 \t Validation Loss: 27.58093778292338\n",
      "Epoch 51 \t Batch 140 \t Validation Loss: 27.69926914487566\n",
      "Epoch 51 \t Batch 160 \t Validation Loss: 29.090611362457274\n",
      "Epoch 51 \t Batch 180 \t Validation Loss: 32.1009442753262\n",
      "Epoch 51 \t Batch 200 \t Validation Loss: 33.231101722717284\n",
      "Epoch 51 \t Batch 220 \t Validation Loss: 34.15129520242864\n",
      "Epoch 51 \t Batch 240 \t Validation Loss: 34.39356655279796\n",
      "Epoch 51 \t Batch 260 \t Validation Loss: 36.2586811872629\n",
      "Epoch 51 \t Batch 280 \t Validation Loss: 37.24920509542738\n",
      "Epoch 51 \t Batch 300 \t Validation Loss: 38.12640327135722\n",
      "Epoch 51 \t Batch 320 \t Validation Loss: 38.508029508590695\n",
      "Epoch 51 \t Batch 340 \t Validation Loss: 38.40428400039673\n",
      "Epoch 51 \t Batch 360 \t Validation Loss: 38.15783415105608\n",
      "Epoch 51 \t Batch 380 \t Validation Loss: 38.33198573714808\n",
      "Epoch 51 \t Batch 400 \t Validation Loss: 37.942072422504424\n",
      "Epoch 51 \t Batch 420 \t Validation Loss: 37.978525148119246\n",
      "Epoch 51 \t Batch 440 \t Validation Loss: 37.69507624452764\n",
      "Epoch 51 \t Batch 460 \t Validation Loss: 37.929885399859884\n",
      "Epoch 51 \t Batch 480 \t Validation Loss: 38.41749689976374\n",
      "Epoch 51 \t Batch 500 \t Validation Loss: 38.1322642288208\n",
      "Epoch 51 \t Batch 520 \t Validation Loss: 37.88693804374108\n",
      "Epoch 51 \t Batch 540 \t Validation Loss: 37.64476864779437\n",
      "Epoch 51 \t Batch 560 \t Validation Loss: 37.45492060525077\n",
      "Epoch 51 \t Batch 580 \t Validation Loss: 37.18222812455276\n",
      "Epoch 51 \t Batch 600 \t Validation Loss: 37.43037721633911\n",
      "Epoch 51 Training Loss: 46.57778925256095 Validation Loss: 38.10573543821062\n",
      "Epoch 51 completed\n",
      "Epoch 52 \t Batch 20 \t Training Loss: 45.40262088775635\n",
      "Epoch 52 \t Batch 40 \t Training Loss: 46.208899688720706\n",
      "Epoch 52 \t Batch 60 \t Training Loss: 46.3907896677653\n",
      "Epoch 52 \t Batch 80 \t Training Loss: 46.78020215034485\n",
      "Epoch 52 \t Batch 100 \t Training Loss: 46.787100563049314\n",
      "Epoch 52 \t Batch 120 \t Training Loss: 46.663661066691084\n",
      "Epoch 52 \t Batch 140 \t Training Loss: 46.71400506155832\n",
      "Epoch 52 \t Batch 160 \t Training Loss: 46.6394145488739\n",
      "Epoch 52 \t Batch 180 \t Training Loss: 46.74122827317979\n",
      "Epoch 52 \t Batch 200 \t Training Loss: 46.66451755523681\n",
      "Epoch 52 \t Batch 220 \t Training Loss: 46.632687256552956\n",
      "Epoch 52 \t Batch 240 \t Training Loss: 46.55357316335042\n",
      "Epoch 52 \t Batch 260 \t Training Loss: 46.589319860018215\n",
      "Epoch 52 \t Batch 280 \t Training Loss: 46.64428724561419\n",
      "Epoch 52 \t Batch 300 \t Training Loss: 46.721130447387694\n",
      "Epoch 52 \t Batch 320 \t Training Loss: 46.741295635700226\n",
      "Epoch 52 \t Batch 340 \t Training Loss: 46.77364187801585\n",
      "Epoch 52 \t Batch 360 \t Training Loss: 46.84089918136597\n",
      "Epoch 52 \t Batch 380 \t Training Loss: 46.804423563103924\n",
      "Epoch 52 \t Batch 400 \t Training Loss: 46.72749216079712\n",
      "Epoch 52 \t Batch 420 \t Training Loss: 46.70291666303362\n",
      "Epoch 52 \t Batch 440 \t Training Loss: 46.709974210912534\n",
      "Epoch 52 \t Batch 460 \t Training Loss: 46.74821562559708\n",
      "Epoch 52 \t Batch 480 \t Training Loss: 46.65407569408417\n",
      "Epoch 52 \t Batch 500 \t Training Loss: 46.676597885131834\n",
      "Epoch 52 \t Batch 520 \t Training Loss: 46.60949463477502\n",
      "Epoch 52 \t Batch 540 \t Training Loss: 46.65768499727602\n",
      "Epoch 52 \t Batch 560 \t Training Loss: 46.66435545512608\n",
      "Epoch 52 \t Batch 580 \t Training Loss: 46.66357805317846\n",
      "Epoch 52 \t Batch 600 \t Training Loss: 46.63574191411336\n",
      "Epoch 52 \t Batch 620 \t Training Loss: 46.64320124349287\n",
      "Epoch 52 \t Batch 640 \t Training Loss: 46.61307032704353\n",
      "Epoch 52 \t Batch 660 \t Training Loss: 46.610147938583836\n",
      "Epoch 52 \t Batch 680 \t Training Loss: 46.60227773329791\n",
      "Epoch 52 \t Batch 700 \t Training Loss: 46.62071702139718\n",
      "Epoch 52 \t Batch 720 \t Training Loss: 46.64360949728224\n",
      "Epoch 52 \t Batch 740 \t Training Loss: 46.64056207296011\n",
      "Epoch 52 \t Batch 760 \t Training Loss: 46.64700108578331\n",
      "Epoch 52 \t Batch 780 \t Training Loss: 46.60980725899721\n",
      "Epoch 52 \t Batch 800 \t Training Loss: 46.59376038074493\n",
      "Epoch 52 \t Batch 820 \t Training Loss: 46.541218204033086\n",
      "Epoch 52 \t Batch 840 \t Training Loss: 46.5511689776466\n",
      "Epoch 52 \t Batch 860 \t Training Loss: 46.550059695576515\n",
      "Epoch 52 \t Batch 880 \t Training Loss: 46.56997092420404\n",
      "Epoch 52 \t Batch 900 \t Training Loss: 46.584171795315214\n",
      "Epoch 52 \t Batch 20 \t Validation Loss: 20.788611078262328\n",
      "Epoch 52 \t Batch 40 \t Validation Loss: 24.281185698509216\n",
      "Epoch 52 \t Batch 60 \t Validation Loss: 23.312256161371867\n",
      "Epoch 52 \t Batch 80 \t Validation Loss: 23.680285680294038\n",
      "Epoch 52 \t Batch 100 \t Validation Loss: 24.803353872299194\n",
      "Epoch 52 \t Batch 120 \t Validation Loss: 25.90300648212433\n",
      "Epoch 52 \t Batch 140 \t Validation Loss: 26.310420179367064\n",
      "Epoch 52 \t Batch 160 \t Validation Loss: 27.99580745100975\n",
      "Epoch 52 \t Batch 180 \t Validation Loss: 31.46671765115526\n",
      "Epoch 52 \t Batch 200 \t Validation Loss: 32.78290516376495\n",
      "Epoch 52 \t Batch 220 \t Validation Loss: 33.84474427916787\n",
      "Epoch 52 \t Batch 240 \t Validation Loss: 34.23643493254979\n",
      "Epoch 52 \t Batch 260 \t Validation Loss: 36.18839565057021\n",
      "Epoch 52 \t Batch 280 \t Validation Loss: 37.2033497265407\n",
      "Epoch 52 \t Batch 300 \t Validation Loss: 38.27864756266276\n",
      "Epoch 52 \t Batch 320 \t Validation Loss: 38.74677491188049\n",
      "Epoch 52 \t Batch 340 \t Validation Loss: 38.66947926353006\n",
      "Epoch 52 \t Batch 360 \t Validation Loss: 38.49323715633816\n",
      "Epoch 52 \t Batch 380 \t Validation Loss: 38.65724639892578\n",
      "Epoch 52 \t Batch 400 \t Validation Loss: 38.268239846229555\n",
      "Epoch 52 \t Batch 420 \t Validation Loss: 38.28989198321388\n",
      "Epoch 52 \t Batch 440 \t Validation Loss: 37.98575288599188\n",
      "Epoch 52 \t Batch 460 \t Validation Loss: 38.1636668122333\n",
      "Epoch 52 \t Batch 480 \t Validation Loss: 38.65513143936793\n",
      "Epoch 52 \t Batch 500 \t Validation Loss: 38.366106834411625\n",
      "Epoch 52 \t Batch 520 \t Validation Loss: 38.12212041708139\n",
      "Epoch 52 \t Batch 540 \t Validation Loss: 37.89730663652773\n",
      "Epoch 52 \t Batch 560 \t Validation Loss: 37.7135511466435\n",
      "Epoch 52 \t Batch 580 \t Validation Loss: 37.47151064050609\n",
      "Epoch 52 \t Batch 600 \t Validation Loss: 37.711475655237834\n",
      "Epoch 52 Training Loss: 46.56848886670827 Validation Loss: 38.39320414716547\n",
      "Epoch 52 completed\n",
      "Epoch 53 \t Batch 20 \t Training Loss: 47.64127941131592\n",
      "Epoch 53 \t Batch 40 \t Training Loss: 46.77404308319092\n",
      "Epoch 53 \t Batch 60 \t Training Loss: 46.37952632904053\n",
      "Epoch 53 \t Batch 80 \t Training Loss: 46.13612403869629\n",
      "Epoch 53 \t Batch 100 \t Training Loss: 46.21079368591309\n",
      "Epoch 53 \t Batch 120 \t Training Loss: 46.09955304463704\n",
      "Epoch 53 \t Batch 140 \t Training Loss: 45.99694578988211\n",
      "Epoch 53 \t Batch 160 \t Training Loss: 45.922847056388854\n",
      "Epoch 53 \t Batch 180 \t Training Loss: 45.849578878614636\n",
      "Epoch 53 \t Batch 200 \t Training Loss: 46.009033584594725\n",
      "Epoch 53 \t Batch 220 \t Training Loss: 46.14712196696888\n",
      "Epoch 53 \t Batch 240 \t Training Loss: 46.216132465998335\n",
      "Epoch 53 \t Batch 260 \t Training Loss: 46.146571056659404\n",
      "Epoch 53 \t Batch 280 \t Training Loss: 46.140755912235804\n",
      "Epoch 53 \t Batch 300 \t Training Loss: 46.248630650838216\n",
      "Epoch 53 \t Batch 320 \t Training Loss: 46.263650715351105\n",
      "Epoch 53 \t Batch 340 \t Training Loss: 46.23256913353415\n",
      "Epoch 53 \t Batch 360 \t Training Loss: 46.23860922919379\n",
      "Epoch 53 \t Batch 380 \t Training Loss: 46.38346041629189\n",
      "Epoch 53 \t Batch 400 \t Training Loss: 46.386098642349246\n",
      "Epoch 53 \t Batch 420 \t Training Loss: 46.452072334289554\n",
      "Epoch 53 \t Batch 440 \t Training Loss: 46.42936528812755\n",
      "Epoch 53 \t Batch 460 \t Training Loss: 46.461895337312114\n",
      "Epoch 53 \t Batch 480 \t Training Loss: 46.46168315410614\n",
      "Epoch 53 \t Batch 500 \t Training Loss: 46.49215551757813\n",
      "Epoch 53 \t Batch 520 \t Training Loss: 46.49034959352934\n",
      "Epoch 53 \t Batch 540 \t Training Loss: 46.459938741613314\n",
      "Epoch 53 \t Batch 560 \t Training Loss: 46.49289566448757\n",
      "Epoch 53 \t Batch 580 \t Training Loss: 46.51796811202477\n",
      "Epoch 53 \t Batch 600 \t Training Loss: 46.54698209126791\n",
      "Epoch 53 \t Batch 620 \t Training Loss: 46.52819254475255\n",
      "Epoch 53 \t Batch 640 \t Training Loss: 46.5314720928669\n",
      "Epoch 53 \t Batch 660 \t Training Loss: 46.52310799107407\n",
      "Epoch 53 \t Batch 680 \t Training Loss: 46.5438824597527\n",
      "Epoch 53 \t Batch 700 \t Training Loss: 46.54193915230887\n",
      "Epoch 53 \t Batch 720 \t Training Loss: 46.575790357589725\n",
      "Epoch 53 \t Batch 740 \t Training Loss: 46.63277955441862\n",
      "Epoch 53 \t Batch 760 \t Training Loss: 46.61882466767964\n",
      "Epoch 53 \t Batch 780 \t Training Loss: 46.62586786319048\n",
      "Epoch 53 \t Batch 800 \t Training Loss: 46.62104789733887\n",
      "Epoch 53 \t Batch 820 \t Training Loss: 46.57385466505841\n",
      "Epoch 53 \t Batch 840 \t Training Loss: 46.574831658317926\n",
      "Epoch 53 \t Batch 860 \t Training Loss: 46.57254722062932\n",
      "Epoch 53 \t Batch 880 \t Training Loss: 46.55744589458812\n",
      "Epoch 53 \t Batch 900 \t Training Loss: 46.575127860175236\n",
      "Epoch 53 \t Batch 20 \t Validation Loss: 26.691840600967407\n",
      "Epoch 53 \t Batch 40 \t Validation Loss: 26.443907690048217\n",
      "Epoch 53 \t Batch 60 \t Validation Loss: 26.685507361094157\n",
      "Epoch 53 \t Batch 80 \t Validation Loss: 26.83793807029724\n",
      "Epoch 53 \t Batch 100 \t Validation Loss: 27.519690475463868\n",
      "Epoch 53 \t Batch 120 \t Validation Loss: 28.221689589818318\n",
      "Epoch 53 \t Batch 140 \t Validation Loss: 28.320814432416643\n",
      "Epoch 53 \t Batch 160 \t Validation Loss: 29.988115775585175\n",
      "Epoch 53 \t Batch 180 \t Validation Loss: 33.39953373803033\n",
      "Epoch 53 \t Batch 200 \t Validation Loss: 34.71753529548645\n",
      "Epoch 53 \t Batch 220 \t Validation Loss: 35.81824649464\n",
      "Epoch 53 \t Batch 240 \t Validation Loss: 36.17325710058212\n",
      "Epoch 53 \t Batch 260 \t Validation Loss: 38.13590934093182\n",
      "Epoch 53 \t Batch 280 \t Validation Loss: 39.12468509674072\n",
      "Epoch 53 \t Batch 300 \t Validation Loss: 40.185054175059\n",
      "Epoch 53 \t Batch 320 \t Validation Loss: 40.641201198101044\n",
      "Epoch 53 \t Batch 340 \t Validation Loss: 40.499871758853686\n",
      "Epoch 53 \t Batch 360 \t Validation Loss: 40.297848304112755\n",
      "Epoch 53 \t Batch 380 \t Validation Loss: 40.480023660157855\n",
      "Epoch 53 \t Batch 400 \t Validation Loss: 40.018011274337766\n",
      "Epoch 53 \t Batch 420 \t Validation Loss: 40.00671166238331\n",
      "Epoch 53 \t Batch 440 \t Validation Loss: 39.6642459934408\n",
      "Epoch 53 \t Batch 460 \t Validation Loss: 39.91885666017947\n",
      "Epoch 53 \t Batch 480 \t Validation Loss: 40.36793625156085\n",
      "Epoch 53 \t Batch 500 \t Validation Loss: 40.08706478691101\n",
      "Epoch 53 \t Batch 520 \t Validation Loss: 39.89495418438545\n",
      "Epoch 53 \t Batch 540 \t Validation Loss: 39.556854482933325\n",
      "Epoch 53 \t Batch 560 \t Validation Loss: 39.26152810539518\n",
      "Epoch 53 \t Batch 580 \t Validation Loss: 38.90350865495616\n",
      "Epoch 53 \t Batch 600 \t Validation Loss: 39.046994182268776\n",
      "Epoch 53 Training Loss: 46.56846750939555 Validation Loss: 39.6457574042407\n",
      "Epoch 53 completed\n",
      "Epoch 54 \t Batch 20 \t Training Loss: 47.667758560180665\n",
      "Epoch 54 \t Batch 40 \t Training Loss: 47.66159830093384\n",
      "Epoch 54 \t Batch 60 \t Training Loss: 46.99982846577962\n",
      "Epoch 54 \t Batch 80 \t Training Loss: 46.72148313522339\n",
      "Epoch 54 \t Batch 100 \t Training Loss: 46.89821361541748\n",
      "Epoch 54 \t Batch 120 \t Training Loss: 46.718360137939456\n",
      "Epoch 54 \t Batch 140 \t Training Loss: 46.61458653041294\n",
      "Epoch 54 \t Batch 160 \t Training Loss: 46.59121589660644\n",
      "Epoch 54 \t Batch 180 \t Training Loss: 46.405451435512965\n",
      "Epoch 54 \t Batch 200 \t Training Loss: 46.314462947845456\n",
      "Epoch 54 \t Batch 220 \t Training Loss: 46.31959134882147\n",
      "Epoch 54 \t Batch 240 \t Training Loss: 46.372849766413374\n",
      "Epoch 54 \t Batch 260 \t Training Loss: 46.45788663717417\n",
      "Epoch 54 \t Batch 280 \t Training Loss: 46.479161616734096\n",
      "Epoch 54 \t Batch 300 \t Training Loss: 46.36841899871826\n",
      "Epoch 54 \t Batch 320 \t Training Loss: 46.32357729673386\n",
      "Epoch 54 \t Batch 340 \t Training Loss: 46.373059665455536\n",
      "Epoch 54 \t Batch 360 \t Training Loss: 46.43617861005995\n",
      "Epoch 54 \t Batch 380 \t Training Loss: 46.41984758879009\n",
      "Epoch 54 \t Batch 400 \t Training Loss: 46.39131345748901\n",
      "Epoch 54 \t Batch 420 \t Training Loss: 46.31893536703927\n",
      "Epoch 54 \t Batch 440 \t Training Loss: 46.35794508673928\n",
      "Epoch 54 \t Batch 460 \t Training Loss: 46.36936447309411\n",
      "Epoch 54 \t Batch 480 \t Training Loss: 46.36070144971212\n",
      "Epoch 54 \t Batch 500 \t Training Loss: 46.36945290374756\n",
      "Epoch 54 \t Batch 520 \t Training Loss: 46.33552298912635\n",
      "Epoch 54 \t Batch 540 \t Training Loss: 46.316573503282335\n",
      "Epoch 54 \t Batch 560 \t Training Loss: 46.37481610434396\n",
      "Epoch 54 \t Batch 580 \t Training Loss: 46.3829820369852\n",
      "Epoch 54 \t Batch 600 \t Training Loss: 46.401163221995034\n",
      "Epoch 54 \t Batch 620 \t Training Loss: 46.40640404608942\n",
      "Epoch 54 \t Batch 640 \t Training Loss: 46.36586334109306\n",
      "Epoch 54 \t Batch 660 \t Training Loss: 46.34469117829294\n",
      "Epoch 54 \t Batch 680 \t Training Loss: 46.35369743459365\n",
      "Epoch 54 \t Batch 700 \t Training Loss: 46.37453396388462\n",
      "Epoch 54 \t Batch 720 \t Training Loss: 46.36188941531711\n",
      "Epoch 54 \t Batch 740 \t Training Loss: 46.35600099305849\n",
      "Epoch 54 \t Batch 760 \t Training Loss: 46.38333821045725\n",
      "Epoch 54 \t Batch 780 \t Training Loss: 46.42898858877329\n",
      "Epoch 54 \t Batch 800 \t Training Loss: 46.48896370887756\n",
      "Epoch 54 \t Batch 820 \t Training Loss: 46.484857149821956\n",
      "Epoch 54 \t Batch 840 \t Training Loss: 46.497678902035666\n",
      "Epoch 54 \t Batch 860 \t Training Loss: 46.4631426655969\n",
      "Epoch 54 \t Batch 880 \t Training Loss: 46.49808329668912\n",
      "Epoch 54 \t Batch 900 \t Training Loss: 46.530172466701934\n",
      "Epoch 54 \t Batch 20 \t Validation Loss: 21.546897315979002\n",
      "Epoch 54 \t Batch 40 \t Validation Loss: 23.799302434921266\n",
      "Epoch 54 \t Batch 60 \t Validation Loss: 23.492695077260336\n",
      "Epoch 54 \t Batch 80 \t Validation Loss: 23.876809763908387\n",
      "Epoch 54 \t Batch 100 \t Validation Loss: 24.909204177856445\n",
      "Epoch 54 \t Batch 120 \t Validation Loss: 25.98621629079183\n",
      "Epoch 54 \t Batch 140 \t Validation Loss: 26.369454710824147\n",
      "Epoch 54 \t Batch 160 \t Validation Loss: 28.17697055339813\n",
      "Epoch 54 \t Batch 180 \t Validation Loss: 31.68193811310662\n",
      "Epoch 54 \t Batch 200 \t Validation Loss: 33.06451491832733\n",
      "Epoch 54 \t Batch 220 \t Validation Loss: 34.270150457728995\n",
      "Epoch 54 \t Batch 240 \t Validation Loss: 34.70233551263809\n",
      "Epoch 54 \t Batch 260 \t Validation Loss: 36.74419820492084\n",
      "Epoch 54 \t Batch 280 \t Validation Loss: 37.847773276056564\n",
      "Epoch 54 \t Batch 300 \t Validation Loss: 38.90989661852519\n",
      "Epoch 54 \t Batch 320 \t Validation Loss: 39.38397531807423\n",
      "Epoch 54 \t Batch 340 \t Validation Loss: 39.29258857895346\n",
      "Epoch 54 \t Batch 360 \t Validation Loss: 39.121736545032924\n",
      "Epoch 54 \t Batch 380 \t Validation Loss: 39.34825557156613\n",
      "Epoch 54 \t Batch 400 \t Validation Loss: 38.941638553142546\n",
      "Epoch 54 \t Batch 420 \t Validation Loss: 38.96426386606126\n",
      "Epoch 54 \t Batch 440 \t Validation Loss: 38.659422397613525\n",
      "Epoch 54 \t Batch 460 \t Validation Loss: 38.89728645241779\n",
      "Epoch 54 \t Batch 480 \t Validation Loss: 39.37278960943222\n",
      "Epoch 54 \t Batch 500 \t Validation Loss: 39.093207069396975\n",
      "Epoch 54 \t Batch 520 \t Validation Loss: 38.87490670130803\n",
      "Epoch 54 \t Batch 540 \t Validation Loss: 38.607734542422826\n",
      "Epoch 54 \t Batch 560 \t Validation Loss: 38.3908004113606\n",
      "Epoch 54 \t Batch 580 \t Validation Loss: 38.15709546516682\n",
      "Epoch 54 \t Batch 600 \t Validation Loss: 38.36015513737996\n",
      "Epoch 54 Training Loss: 46.52015756338737 Validation Loss: 39.0160101388956\n",
      "Epoch 54 completed\n",
      "Epoch 55 \t Batch 20 \t Training Loss: 46.91788558959961\n",
      "Epoch 55 \t Batch 40 \t Training Loss: 46.978044509887695\n",
      "Epoch 55 \t Batch 60 \t Training Loss: 46.71119702657064\n",
      "Epoch 55 \t Batch 80 \t Training Loss: 46.64101552963257\n",
      "Epoch 55 \t Batch 100 \t Training Loss: 46.5855961227417\n",
      "Epoch 55 \t Batch 120 \t Training Loss: 46.546176719665525\n",
      "Epoch 55 \t Batch 140 \t Training Loss: 46.3235597882952\n",
      "Epoch 55 \t Batch 160 \t Training Loss: 46.16173872947693\n",
      "Epoch 55 \t Batch 180 \t Training Loss: 46.230422274271646\n",
      "Epoch 55 \t Batch 200 \t Training Loss: 46.41609321594238\n",
      "Epoch 55 \t Batch 220 \t Training Loss: 46.29671840667724\n",
      "Epoch 55 \t Batch 240 \t Training Loss: 46.203259547551475\n",
      "Epoch 55 \t Batch 260 \t Training Loss: 46.303149311359114\n",
      "Epoch 55 \t Batch 280 \t Training Loss: 46.27594902856009\n",
      "Epoch 55 \t Batch 300 \t Training Loss: 46.232815945943194\n",
      "Epoch 55 \t Batch 320 \t Training Loss: 46.38134799003601\n",
      "Epoch 55 \t Batch 340 \t Training Loss: 46.34797690896427\n",
      "Epoch 55 \t Batch 360 \t Training Loss: 46.311643271976045\n",
      "Epoch 55 \t Batch 380 \t Training Loss: 46.29657706210488\n",
      "Epoch 55 \t Batch 400 \t Training Loss: 46.297527141571045\n",
      "Epoch 55 \t Batch 420 \t Training Loss: 46.3653252919515\n",
      "Epoch 55 \t Batch 440 \t Training Loss: 46.45859337720004\n",
      "Epoch 55 \t Batch 460 \t Training Loss: 46.47557498268459\n",
      "Epoch 55 \t Batch 480 \t Training Loss: 46.512052512168886\n",
      "Epoch 55 \t Batch 500 \t Training Loss: 46.53112967681885\n",
      "Epoch 55 \t Batch 520 \t Training Loss: 46.59716241543109\n",
      "Epoch 55 \t Batch 540 \t Training Loss: 46.52580755021837\n",
      "Epoch 55 \t Batch 560 \t Training Loss: 46.574415063858034\n",
      "Epoch 55 \t Batch 580 \t Training Loss: 46.54341167581492\n",
      "Epoch 55 \t Batch 600 \t Training Loss: 46.57039164861043\n",
      "Epoch 55 \t Batch 620 \t Training Loss: 46.56155985555341\n",
      "Epoch 55 \t Batch 640 \t Training Loss: 46.55388171672821\n",
      "Epoch 55 \t Batch 660 \t Training Loss: 46.549251689332905\n",
      "Epoch 55 \t Batch 680 \t Training Loss: 46.50672623129452\n",
      "Epoch 55 \t Batch 700 \t Training Loss: 46.534008004324775\n",
      "Epoch 55 \t Batch 720 \t Training Loss: 46.538125069936115\n",
      "Epoch 55 \t Batch 740 \t Training Loss: 46.54664262823157\n",
      "Epoch 55 \t Batch 760 \t Training Loss: 46.54921291250932\n",
      "Epoch 55 \t Batch 780 \t Training Loss: 46.52392170245831\n",
      "Epoch 55 \t Batch 800 \t Training Loss: 46.517579774856564\n",
      "Epoch 55 \t Batch 820 \t Training Loss: 46.50661197755395\n",
      "Epoch 55 \t Batch 840 \t Training Loss: 46.464795630318775\n",
      "Epoch 55 \t Batch 860 \t Training Loss: 46.49611925080765\n",
      "Epoch 55 \t Batch 880 \t Training Loss: 46.47516006122936\n",
      "Epoch 55 \t Batch 900 \t Training Loss: 46.489276555379234\n",
      "Epoch 55 \t Batch 20 \t Validation Loss: 24.590807914733887\n",
      "Epoch 55 \t Batch 40 \t Validation Loss: 27.336683082580567\n",
      "Epoch 55 \t Batch 60 \t Validation Loss: 26.750344022115073\n",
      "Epoch 55 \t Batch 80 \t Validation Loss: 27.408206462860107\n",
      "Epoch 55 \t Batch 100 \t Validation Loss: 28.23732656478882\n",
      "Epoch 55 \t Batch 120 \t Validation Loss: 28.908177042007445\n",
      "Epoch 55 \t Batch 140 \t Validation Loss: 28.90013858250209\n",
      "Epoch 55 \t Batch 160 \t Validation Loss: 30.074763917922972\n",
      "Epoch 55 \t Batch 180 \t Validation Loss: 32.7906791528066\n",
      "Epoch 55 \t Batch 200 \t Validation Loss: 33.61805688381195\n",
      "Epoch 55 \t Batch 220 \t Validation Loss: 34.317747033726086\n",
      "Epoch 55 \t Batch 240 \t Validation Loss: 34.44538562297821\n",
      "Epoch 55 \t Batch 260 \t Validation Loss: 36.04767755728501\n",
      "Epoch 55 \t Batch 280 \t Validation Loss: 36.80309813703809\n",
      "Epoch 55 \t Batch 300 \t Validation Loss: 37.70100012143453\n",
      "Epoch 55 \t Batch 320 \t Validation Loss: 38.0562142431736\n",
      "Epoch 55 \t Batch 340 \t Validation Loss: 37.94650619170245\n",
      "Epoch 55 \t Batch 360 \t Validation Loss: 37.74362203015222\n",
      "Epoch 55 \t Batch 380 \t Validation Loss: 37.899570050992466\n",
      "Epoch 55 \t Batch 400 \t Validation Loss: 37.53075865507126\n",
      "Epoch 55 \t Batch 420 \t Validation Loss: 37.59053839274815\n",
      "Epoch 55 \t Batch 440 \t Validation Loss: 37.32200253660029\n",
      "Epoch 55 \t Batch 460 \t Validation Loss: 37.592636361329454\n",
      "Epoch 55 \t Batch 480 \t Validation Loss: 38.09224395751953\n",
      "Epoch 55 \t Batch 500 \t Validation Loss: 37.810129123687744\n",
      "Epoch 55 \t Batch 520 \t Validation Loss: 37.61453870259798\n",
      "Epoch 55 \t Batch 540 \t Validation Loss: 37.39605615403917\n",
      "Epoch 55 \t Batch 560 \t Validation Loss: 37.214380863734654\n",
      "Epoch 55 \t Batch 580 \t Validation Loss: 36.963947993311386\n",
      "Epoch 55 \t Batch 600 \t Validation Loss: 37.21163489023844\n",
      "Epoch 55 Training Loss: 46.47953406181044 Validation Loss: 37.90423962977025\n",
      "Epoch 55 completed\n",
      "Epoch 56 \t Batch 20 \t Training Loss: 46.25765056610108\n",
      "Epoch 56 \t Batch 40 \t Training Loss: 46.030233192443845\n",
      "Epoch 56 \t Batch 60 \t Training Loss: 46.051088396708174\n",
      "Epoch 56 \t Batch 80 \t Training Loss: 46.08020858764648\n",
      "Epoch 56 \t Batch 100 \t Training Loss: 46.32205898284912\n",
      "Epoch 56 \t Batch 120 \t Training Loss: 46.32961467107137\n",
      "Epoch 56 \t Batch 140 \t Training Loss: 46.29857918875558\n",
      "Epoch 56 \t Batch 160 \t Training Loss: 46.22489635944366\n",
      "Epoch 56 \t Batch 180 \t Training Loss: 46.40514229668511\n",
      "Epoch 56 \t Batch 200 \t Training Loss: 46.46077140808106\n",
      "Epoch 56 \t Batch 220 \t Training Loss: 46.41619798486883\n",
      "Epoch 56 \t Batch 240 \t Training Loss: 46.52614351908366\n",
      "Epoch 56 \t Batch 260 \t Training Loss: 46.45091696519118\n",
      "Epoch 56 \t Batch 280 \t Training Loss: 46.50082115445818\n",
      "Epoch 56 \t Batch 300 \t Training Loss: 46.47810540517171\n",
      "Epoch 56 \t Batch 320 \t Training Loss: 46.466915380954745\n",
      "Epoch 56 \t Batch 340 \t Training Loss: 46.5510555267334\n",
      "Epoch 56 \t Batch 360 \t Training Loss: 46.60775460137261\n",
      "Epoch 56 \t Batch 380 \t Training Loss: 46.56851877915232\n",
      "Epoch 56 \t Batch 400 \t Training Loss: 46.4934516620636\n",
      "Epoch 56 \t Batch 420 \t Training Loss: 46.47917602175758\n",
      "Epoch 56 \t Batch 440 \t Training Loss: 46.45518353202126\n",
      "Epoch 56 \t Batch 460 \t Training Loss: 46.422944939654805\n",
      "Epoch 56 \t Batch 480 \t Training Loss: 46.43789180914561\n",
      "Epoch 56 \t Batch 500 \t Training Loss: 46.44673206329346\n",
      "Epoch 56 \t Batch 520 \t Training Loss: 46.431204399695766\n",
      "Epoch 56 \t Batch 540 \t Training Loss: 46.4435209274292\n",
      "Epoch 56 \t Batch 560 \t Training Loss: 46.450005762917655\n",
      "Epoch 56 \t Batch 580 \t Training Loss: 46.45367292206863\n",
      "Epoch 56 \t Batch 600 \t Training Loss: 46.45271681467692\n",
      "Epoch 56 \t Batch 620 \t Training Loss: 46.43185724443005\n",
      "Epoch 56 \t Batch 640 \t Training Loss: 46.432801419496535\n",
      "Epoch 56 \t Batch 660 \t Training Loss: 46.50255610148112\n",
      "Epoch 56 \t Batch 680 \t Training Loss: 46.48399346295525\n",
      "Epoch 56 \t Batch 700 \t Training Loss: 46.48272571563721\n",
      "Epoch 56 \t Batch 720 \t Training Loss: 46.46646318435669\n",
      "Epoch 56 \t Batch 740 \t Training Loss: 46.507203421721584\n",
      "Epoch 56 \t Batch 760 \t Training Loss: 46.498727793442576\n",
      "Epoch 56 \t Batch 780 \t Training Loss: 46.49223163800362\n",
      "Epoch 56 \t Batch 800 \t Training Loss: 46.46441659450531\n",
      "Epoch 56 \t Batch 820 \t Training Loss: 46.46166768888148\n",
      "Epoch 56 \t Batch 840 \t Training Loss: 46.451365520840596\n",
      "Epoch 56 \t Batch 860 \t Training Loss: 46.470511667118515\n",
      "Epoch 56 \t Batch 880 \t Training Loss: 46.477551373568446\n",
      "Epoch 56 \t Batch 900 \t Training Loss: 46.451320843166776\n",
      "Epoch 56 \t Batch 20 \t Validation Loss: 20.986906957626342\n",
      "Epoch 56 \t Batch 40 \t Validation Loss: 23.766689348220826\n",
      "Epoch 56 \t Batch 60 \t Validation Loss: 23.332371044158936\n",
      "Epoch 56 \t Batch 80 \t Validation Loss: 23.86223566532135\n",
      "Epoch 56 \t Batch 100 \t Validation Loss: 25.073881187438964\n",
      "Epoch 56 \t Batch 120 \t Validation Loss: 26.177971839904785\n",
      "Epoch 56 \t Batch 140 \t Validation Loss: 26.56786012649536\n",
      "Epoch 56 \t Batch 160 \t Validation Loss: 28.20516619682312\n",
      "Epoch 56 \t Batch 180 \t Validation Loss: 31.538449329800077\n",
      "Epoch 56 \t Batch 200 \t Validation Loss: 32.77799700737\n",
      "Epoch 56 \t Batch 220 \t Validation Loss: 33.87881465391679\n",
      "Epoch 56 \t Batch 240 \t Validation Loss: 34.267324447631836\n",
      "Epoch 56 \t Batch 260 \t Validation Loss: 36.19809324924763\n",
      "Epoch 56 \t Batch 280 \t Validation Loss: 37.24242235251835\n",
      "Epoch 56 \t Batch 300 \t Validation Loss: 38.27390777269999\n",
      "Epoch 56 \t Batch 320 \t Validation Loss: 38.733773812651634\n",
      "Epoch 56 \t Batch 340 \t Validation Loss: 38.65567544768838\n",
      "Epoch 56 \t Batch 360 \t Validation Loss: 38.46461661921607\n",
      "Epoch 56 \t Batch 380 \t Validation Loss: 38.66021648959109\n",
      "Epoch 56 \t Batch 400 \t Validation Loss: 38.26443439245224\n",
      "Epoch 56 \t Batch 420 \t Validation Loss: 38.28908313342503\n",
      "Epoch 56 \t Batch 440 \t Validation Loss: 38.0048469435085\n",
      "Epoch 56 \t Batch 460 \t Validation Loss: 38.24114902123161\n",
      "Epoch 56 \t Batch 480 \t Validation Loss: 38.70722806652387\n",
      "Epoch 56 \t Batch 500 \t Validation Loss: 38.426158464431765\n",
      "Epoch 56 \t Batch 520 \t Validation Loss: 38.194714137224054\n",
      "Epoch 56 \t Batch 540 \t Validation Loss: 37.961640126616864\n",
      "Epoch 56 \t Batch 560 \t Validation Loss: 37.78217567546027\n",
      "Epoch 56 \t Batch 580 \t Validation Loss: 37.52263704661665\n",
      "Epoch 56 \t Batch 600 \t Validation Loss: 37.75875984668732\n",
      "Epoch 56 Training Loss: 46.46471714115455 Validation Loss: 38.419878156154184\n",
      "Epoch 56 completed\n",
      "Epoch 57 \t Batch 20 \t Training Loss: 46.442027473449706\n",
      "Epoch 57 \t Batch 40 \t Training Loss: 46.16105661392212\n",
      "Epoch 57 \t Batch 60 \t Training Loss: 46.31503410339356\n",
      "Epoch 57 \t Batch 80 \t Training Loss: 46.36052207946777\n",
      "Epoch 57 \t Batch 100 \t Training Loss: 46.17845329284668\n",
      "Epoch 57 \t Batch 120 \t Training Loss: 46.22155605951945\n",
      "Epoch 57 \t Batch 140 \t Training Loss: 46.34341294424875\n",
      "Epoch 57 \t Batch 160 \t Training Loss: 46.49665064811707\n",
      "Epoch 57 \t Batch 180 \t Training Loss: 46.58763126797146\n",
      "Epoch 57 \t Batch 200 \t Training Loss: 46.68916416168213\n",
      "Epoch 57 \t Batch 220 \t Training Loss: 46.77410465587269\n",
      "Epoch 57 \t Batch 240 \t Training Loss: 46.71705932617188\n",
      "Epoch 57 \t Batch 260 \t Training Loss: 46.65873245826134\n",
      "Epoch 57 \t Batch 280 \t Training Loss: 46.656158106667654\n",
      "Epoch 57 \t Batch 300 \t Training Loss: 46.56432159423828\n",
      "Epoch 57 \t Batch 320 \t Training Loss: 46.61424095630646\n",
      "Epoch 57 \t Batch 340 \t Training Loss: 46.5784132789163\n",
      "Epoch 57 \t Batch 360 \t Training Loss: 46.57206605275472\n",
      "Epoch 57 \t Batch 380 \t Training Loss: 46.496835768850225\n",
      "Epoch 57 \t Batch 400 \t Training Loss: 46.51668299674988\n",
      "Epoch 57 \t Batch 420 \t Training Loss: 46.49775763012114\n",
      "Epoch 57 \t Batch 440 \t Training Loss: 46.48455490632491\n",
      "Epoch 57 \t Batch 460 \t Training Loss: 46.43313664146092\n",
      "Epoch 57 \t Batch 480 \t Training Loss: 46.468121973673504\n",
      "Epoch 57 \t Batch 500 \t Training Loss: 46.46099171447754\n",
      "Epoch 57 \t Batch 520 \t Training Loss: 46.44348279512845\n",
      "Epoch 57 \t Batch 540 \t Training Loss: 46.420807661833585\n",
      "Epoch 57 \t Batch 560 \t Training Loss: 46.4239972455161\n",
      "Epoch 57 \t Batch 580 \t Training Loss: 46.43674094101478\n",
      "Epoch 57 \t Batch 600 \t Training Loss: 46.47964544932047\n",
      "Epoch 57 \t Batch 620 \t Training Loss: 46.4815456574963\n",
      "Epoch 57 \t Batch 640 \t Training Loss: 46.4106124162674\n",
      "Epoch 57 \t Batch 660 \t Training Loss: 46.45514481284402\n",
      "Epoch 57 \t Batch 680 \t Training Loss: 46.45025717230404\n",
      "Epoch 57 \t Batch 700 \t Training Loss: 46.43105359213693\n",
      "Epoch 57 \t Batch 720 \t Training Loss: 46.4378986676534\n",
      "Epoch 57 \t Batch 740 \t Training Loss: 46.42095685391813\n",
      "Epoch 57 \t Batch 760 \t Training Loss: 46.41326749701249\n",
      "Epoch 57 \t Batch 780 \t Training Loss: 46.401597712590146\n",
      "Epoch 57 \t Batch 800 \t Training Loss: 46.42010652542114\n",
      "Epoch 57 \t Batch 820 \t Training Loss: 46.43750139096888\n",
      "Epoch 57 \t Batch 840 \t Training Loss: 46.39115164620536\n",
      "Epoch 57 \t Batch 860 \t Training Loss: 46.38780917677769\n",
      "Epoch 57 \t Batch 880 \t Training Loss: 46.38298697905107\n",
      "Epoch 57 \t Batch 900 \t Training Loss: 46.42483800676134\n",
      "Epoch 57 \t Batch 20 \t Validation Loss: 17.183215188980103\n",
      "Epoch 57 \t Batch 40 \t Validation Loss: 20.56971800327301\n",
      "Epoch 57 \t Batch 60 \t Validation Loss: 20.201913944880168\n",
      "Epoch 57 \t Batch 80 \t Validation Loss: 21.036703443527223\n",
      "Epoch 57 \t Batch 100 \t Validation Loss: 22.719984302520754\n",
      "Epoch 57 \t Batch 120 \t Validation Loss: 24.0592720190684\n",
      "Epoch 57 \t Batch 140 \t Validation Loss: 24.61264509473528\n",
      "Epoch 57 \t Batch 160 \t Validation Loss: 26.41375414133072\n",
      "Epoch 57 \t Batch 180 \t Validation Loss: 29.665169996685453\n",
      "Epoch 57 \t Batch 200 \t Validation Loss: 30.917907128334047\n",
      "Epoch 57 \t Batch 220 \t Validation Loss: 32.0080631906336\n",
      "Epoch 57 \t Batch 240 \t Validation Loss: 32.40963747501373\n",
      "Epoch 57 \t Batch 260 \t Validation Loss: 34.33542624253493\n",
      "Epoch 57 \t Batch 280 \t Validation Loss: 35.39262113571167\n",
      "Epoch 57 \t Batch 300 \t Validation Loss: 36.41360193252564\n",
      "Epoch 57 \t Batch 320 \t Validation Loss: 36.89510422348976\n",
      "Epoch 57 \t Batch 340 \t Validation Loss: 36.88265895843506\n",
      "Epoch 57 \t Batch 360 \t Validation Loss: 36.73266149891747\n",
      "Epoch 57 \t Batch 380 \t Validation Loss: 36.972836020118315\n",
      "Epoch 57 \t Batch 400 \t Validation Loss: 36.64778930425644\n",
      "Epoch 57 \t Batch 420 \t Validation Loss: 36.730504814783735\n",
      "Epoch 57 \t Batch 440 \t Validation Loss: 36.505161109837616\n",
      "Epoch 57 \t Batch 460 \t Validation Loss: 36.80510509947072\n",
      "Epoch 57 \t Batch 480 \t Validation Loss: 37.329126971960065\n",
      "Epoch 57 \t Batch 500 \t Validation Loss: 37.087033708572385\n",
      "Epoch 57 \t Batch 520 \t Validation Loss: 36.897619362977835\n",
      "Epoch 57 \t Batch 540 \t Validation Loss: 36.69523830237212\n",
      "Epoch 57 \t Batch 560 \t Validation Loss: 36.52853760549\n",
      "Epoch 57 \t Batch 580 \t Validation Loss: 36.289463564445235\n",
      "Epoch 57 \t Batch 600 \t Validation Loss: 36.54969113826752\n",
      "Epoch 57 Training Loss: 46.45156435202096 Validation Loss: 37.22137459996459\n",
      "Epoch 57 completed\n",
      "Epoch 58 \t Batch 20 \t Training Loss: 47.58055934906006\n",
      "Epoch 58 \t Batch 40 \t Training Loss: 46.96147232055664\n",
      "Epoch 58 \t Batch 60 \t Training Loss: 47.21244500478109\n",
      "Epoch 58 \t Batch 80 \t Training Loss: 46.86720376014709\n",
      "Epoch 58 \t Batch 100 \t Training Loss: 46.69171977996826\n",
      "Epoch 58 \t Batch 120 \t Training Loss: 46.6187240600586\n",
      "Epoch 58 \t Batch 140 \t Training Loss: 46.51604987553188\n",
      "Epoch 58 \t Batch 160 \t Training Loss: 46.6093533039093\n",
      "Epoch 58 \t Batch 180 \t Training Loss: 46.71386142306858\n",
      "Epoch 58 \t Batch 200 \t Training Loss: 46.637792892456055\n",
      "Epoch 58 \t Batch 220 \t Training Loss: 46.58266459378329\n",
      "Epoch 58 \t Batch 240 \t Training Loss: 46.51052241325378\n",
      "Epoch 58 \t Batch 260 \t Training Loss: 46.45297073951134\n",
      "Epoch 58 \t Batch 280 \t Training Loss: 46.53708062853132\n",
      "Epoch 58 \t Batch 300 \t Training Loss: 46.611064427693684\n",
      "Epoch 58 \t Batch 320 \t Training Loss: 46.627510488033295\n",
      "Epoch 58 \t Batch 340 \t Training Loss: 46.53146284888773\n",
      "Epoch 58 \t Batch 360 \t Training Loss: 46.570018461015486\n",
      "Epoch 58 \t Batch 380 \t Training Loss: 46.50154370759663\n",
      "Epoch 58 \t Batch 400 \t Training Loss: 46.541968755722046\n",
      "Epoch 58 \t Batch 420 \t Training Loss: 46.59754585992722\n",
      "Epoch 58 \t Batch 440 \t Training Loss: 46.497553712671454\n",
      "Epoch 58 \t Batch 460 \t Training Loss: 46.47437069105065\n",
      "Epoch 58 \t Batch 480 \t Training Loss: 46.49794534842173\n",
      "Epoch 58 \t Batch 500 \t Training Loss: 46.47818259429932\n",
      "Epoch 58 \t Batch 520 \t Training Loss: 46.38481504733746\n",
      "Epoch 58 \t Batch 540 \t Training Loss: 46.41286222669813\n",
      "Epoch 58 \t Batch 560 \t Training Loss: 46.42837519645691\n",
      "Epoch 58 \t Batch 580 \t Training Loss: 46.48260478315682\n",
      "Epoch 58 \t Batch 600 \t Training Loss: 46.4791180229187\n",
      "Epoch 58 \t Batch 620 \t Training Loss: 46.42234386936311\n",
      "Epoch 58 \t Batch 640 \t Training Loss: 46.42931448817253\n",
      "Epoch 58 \t Batch 660 \t Training Loss: 46.385668534943555\n",
      "Epoch 58 \t Batch 680 \t Training Loss: 46.37412466161391\n",
      "Epoch 58 \t Batch 700 \t Training Loss: 46.40197174617222\n",
      "Epoch 58 \t Batch 720 \t Training Loss: 46.40053546163771\n",
      "Epoch 58 \t Batch 740 \t Training Loss: 46.38128582722432\n",
      "Epoch 58 \t Batch 760 \t Training Loss: 46.35358180999756\n",
      "Epoch 58 \t Batch 780 \t Training Loss: 46.359442016405936\n",
      "Epoch 58 \t Batch 800 \t Training Loss: 46.333615942001344\n",
      "Epoch 58 \t Batch 820 \t Training Loss: 46.36644689048209\n",
      "Epoch 58 \t Batch 840 \t Training Loss: 46.379671441941035\n",
      "Epoch 58 \t Batch 860 \t Training Loss: 46.37055820198946\n",
      "Epoch 58 \t Batch 880 \t Training Loss: 46.40762928615917\n",
      "Epoch 58 \t Batch 900 \t Training Loss: 46.414310150146484\n",
      "Epoch 58 \t Batch 20 \t Validation Loss: 24.37369637489319\n",
      "Epoch 58 \t Batch 40 \t Validation Loss: 25.536075091362\n",
      "Epoch 58 \t Batch 60 \t Validation Loss: 25.565055497487386\n",
      "Epoch 58 \t Batch 80 \t Validation Loss: 26.089318728446962\n",
      "Epoch 58 \t Batch 100 \t Validation Loss: 26.653897094726563\n",
      "Epoch 58 \t Batch 120 \t Validation Loss: 27.438046630223592\n",
      "Epoch 58 \t Batch 140 \t Validation Loss: 27.638830525534495\n",
      "Epoch 58 \t Batch 160 \t Validation Loss: 29.304746079444886\n",
      "Epoch 58 \t Batch 180 \t Validation Loss: 32.689586575826006\n",
      "Epoch 58 \t Batch 200 \t Validation Loss: 33.944670867919925\n",
      "Epoch 58 \t Batch 220 \t Validation Loss: 35.110844586112286\n",
      "Epoch 58 \t Batch 240 \t Validation Loss: 35.531477717558545\n",
      "Epoch 58 \t Batch 260 \t Validation Loss: 37.497191762924196\n",
      "Epoch 58 \t Batch 280 \t Validation Loss: 38.530385681561064\n",
      "Epoch 58 \t Batch 300 \t Validation Loss: 39.58773236910502\n",
      "Epoch 58 \t Batch 320 \t Validation Loss: 40.02315876185894\n",
      "Epoch 58 \t Batch 340 \t Validation Loss: 39.90195394123302\n",
      "Epoch 58 \t Batch 360 \t Validation Loss: 39.715877662764655\n",
      "Epoch 58 \t Batch 380 \t Validation Loss: 39.901113853956524\n",
      "Epoch 58 \t Batch 400 \t Validation Loss: 39.45057763814926\n",
      "Epoch 58 \t Batch 420 \t Validation Loss: 39.401180737359184\n",
      "Epoch 58 \t Batch 440 \t Validation Loss: 39.04635904702273\n",
      "Epoch 58 \t Batch 460 \t Validation Loss: 39.27420711724655\n",
      "Epoch 58 \t Batch 480 \t Validation Loss: 39.709717426697416\n",
      "Epoch 58 \t Batch 500 \t Validation Loss: 39.40531431388855\n",
      "Epoch 58 \t Batch 520 \t Validation Loss: 39.161897408045256\n",
      "Epoch 58 \t Batch 540 \t Validation Loss: 38.917832175007575\n",
      "Epoch 58 \t Batch 560 \t Validation Loss: 38.73468441792897\n",
      "Epoch 58 \t Batch 580 \t Validation Loss: 38.54064843901273\n",
      "Epoch 58 \t Batch 600 \t Validation Loss: 38.74371407667796\n",
      "Epoch 58 Training Loss: 46.404938742534675 Validation Loss: 39.440338671981515\n",
      "Epoch 58 completed\n",
      "Epoch 59 \t Batch 20 \t Training Loss: 45.66048946380615\n",
      "Epoch 59 \t Batch 40 \t Training Loss: 46.53673143386841\n",
      "Epoch 59 \t Batch 60 \t Training Loss: 46.451798057556154\n",
      "Epoch 59 \t Batch 80 \t Training Loss: 46.141370344161984\n",
      "Epoch 59 \t Batch 100 \t Training Loss: 46.28034286499023\n",
      "Epoch 59 \t Batch 120 \t Training Loss: 46.134983730316165\n",
      "Epoch 59 \t Batch 140 \t Training Loss: 46.215861320495605\n",
      "Epoch 59 \t Batch 160 \t Training Loss: 46.32805247306824\n",
      "Epoch 59 \t Batch 180 \t Training Loss: 46.38621139526367\n",
      "Epoch 59 \t Batch 200 \t Training Loss: 46.367072162628176\n",
      "Epoch 59 \t Batch 220 \t Training Loss: 46.3293265949596\n",
      "Epoch 59 \t Batch 240 \t Training Loss: 46.28379068374634\n",
      "Epoch 59 \t Batch 260 \t Training Loss: 46.329890456566446\n",
      "Epoch 59 \t Batch 280 \t Training Loss: 46.374223123277936\n",
      "Epoch 59 \t Batch 300 \t Training Loss: 46.43102738698324\n",
      "Epoch 59 \t Batch 320 \t Training Loss: 46.48626304864884\n",
      "Epoch 59 \t Batch 340 \t Training Loss: 46.46614853354061\n",
      "Epoch 59 \t Batch 360 \t Training Loss: 46.54370420244005\n",
      "Epoch 59 \t Batch 380 \t Training Loss: 46.54239421643709\n",
      "Epoch 59 \t Batch 400 \t Training Loss: 46.46300402641296\n",
      "Epoch 59 \t Batch 420 \t Training Loss: 46.50255481175014\n",
      "Epoch 59 \t Batch 440 \t Training Loss: 46.50437554446134\n",
      "Epoch 59 \t Batch 460 \t Training Loss: 46.51282659613568\n",
      "Epoch 59 \t Batch 480 \t Training Loss: 46.483120210965474\n",
      "Epoch 59 \t Batch 500 \t Training Loss: 46.54095593261719\n",
      "Epoch 59 \t Batch 520 \t Training Loss: 46.54359288582435\n",
      "Epoch 59 \t Batch 540 \t Training Loss: 46.570269132543494\n",
      "Epoch 59 \t Batch 560 \t Training Loss: 46.566883884157455\n",
      "Epoch 59 \t Batch 580 \t Training Loss: 46.54602480921252\n",
      "Epoch 59 \t Batch 600 \t Training Loss: 46.51297681172689\n",
      "Epoch 59 \t Batch 620 \t Training Loss: 46.4656129652454\n",
      "Epoch 59 \t Batch 640 \t Training Loss: 46.43382051587105\n",
      "Epoch 59 \t Batch 660 \t Training Loss: 46.42676518758138\n",
      "Epoch 59 \t Batch 680 \t Training Loss: 46.39070229249842\n",
      "Epoch 59 \t Batch 700 \t Training Loss: 46.389239371163505\n",
      "Epoch 59 \t Batch 720 \t Training Loss: 46.37682560814751\n",
      "Epoch 59 \t Batch 740 \t Training Loss: 46.39981056419579\n",
      "Epoch 59 \t Batch 760 \t Training Loss: 46.37596127861425\n",
      "Epoch 59 \t Batch 780 \t Training Loss: 46.356823701124924\n",
      "Epoch 59 \t Batch 800 \t Training Loss: 46.367411437034605\n",
      "Epoch 59 \t Batch 820 \t Training Loss: 46.378557000509126\n",
      "Epoch 59 \t Batch 840 \t Training Loss: 46.36204725901286\n",
      "Epoch 59 \t Batch 860 \t Training Loss: 46.35734770575235\n",
      "Epoch 59 \t Batch 880 \t Training Loss: 46.360630403865464\n",
      "Epoch 59 \t Batch 900 \t Training Loss: 46.39719139946832\n",
      "Epoch 59 \t Batch 20 \t Validation Loss: 15.394275569915772\n",
      "Epoch 59 \t Batch 40 \t Validation Loss: 18.345996022224426\n",
      "Epoch 59 \t Batch 60 \t Validation Loss: 17.969136555989582\n",
      "Epoch 59 \t Batch 80 \t Validation Loss: 18.601155853271486\n",
      "Epoch 59 \t Batch 100 \t Validation Loss: 20.548289604187012\n",
      "Epoch 59 \t Batch 120 \t Validation Loss: 22.103086630503338\n",
      "Epoch 59 \t Batch 140 \t Validation Loss: 22.89487017222813\n",
      "Epoch 59 \t Batch 160 \t Validation Loss: 24.904875230789184\n",
      "Epoch 59 \t Batch 180 \t Validation Loss: 28.35375272432963\n",
      "Epoch 59 \t Batch 200 \t Validation Loss: 29.78093903064728\n",
      "Epoch 59 \t Batch 220 \t Validation Loss: 31.072689312154598\n",
      "Epoch 59 \t Batch 240 \t Validation Loss: 31.619529946645102\n",
      "Epoch 59 \t Batch 260 \t Validation Loss: 33.65211477279663\n",
      "Epoch 59 \t Batch 280 \t Validation Loss: 34.78767104489463\n",
      "Epoch 59 \t Batch 300 \t Validation Loss: 35.8463920434316\n",
      "Epoch 59 \t Batch 320 \t Validation Loss: 36.40841164290905\n",
      "Epoch 59 \t Batch 340 \t Validation Loss: 36.42245994175182\n",
      "Epoch 59 \t Batch 360 \t Validation Loss: 36.306601119041446\n",
      "Epoch 59 \t Batch 380 \t Validation Loss: 36.60888797107496\n",
      "Epoch 59 \t Batch 400 \t Validation Loss: 36.29559393167496\n",
      "Epoch 59 \t Batch 420 \t Validation Loss: 36.40633976573036\n",
      "Epoch 59 \t Batch 440 \t Validation Loss: 36.187642299045216\n",
      "Epoch 59 \t Batch 460 \t Validation Loss: 36.50366193315257\n",
      "Epoch 59 \t Batch 480 \t Validation Loss: 37.04427314400673\n",
      "Epoch 59 \t Batch 500 \t Validation Loss: 36.794820108413695\n",
      "Epoch 59 \t Batch 520 \t Validation Loss: 36.60343498816857\n",
      "Epoch 59 \t Batch 540 \t Validation Loss: 36.432621761604594\n",
      "Epoch 59 \t Batch 560 \t Validation Loss: 36.29874388149806\n",
      "Epoch 59 \t Batch 580 \t Validation Loss: 36.04522307823444\n",
      "Epoch 59 \t Batch 600 \t Validation Loss: 36.376012341181436\n",
      "Epoch 59 Training Loss: 46.396511481398456 Validation Loss: 37.0703143361327\n",
      "Epoch 59 completed\n",
      "Epoch 60 \t Batch 20 \t Training Loss: 46.64633045196533\n",
      "Epoch 60 \t Batch 40 \t Training Loss: 46.63817911148071\n",
      "Epoch 60 \t Batch 60 \t Training Loss: 46.93136342366537\n",
      "Epoch 60 \t Batch 80 \t Training Loss: 46.395929622650144\n",
      "Epoch 60 \t Batch 100 \t Training Loss: 46.508834495544434\n",
      "Epoch 60 \t Batch 120 \t Training Loss: 46.335171413421634\n",
      "Epoch 60 \t Batch 140 \t Training Loss: 46.23544594900949\n",
      "Epoch 60 \t Batch 160 \t Training Loss: 46.13858604431152\n",
      "Epoch 60 \t Batch 180 \t Training Loss: 46.11622066497803\n",
      "Epoch 60 \t Batch 200 \t Training Loss: 46.14001184463501\n",
      "Epoch 60 \t Batch 220 \t Training Loss: 46.15787988142534\n",
      "Epoch 60 \t Batch 240 \t Training Loss: 46.2388009707133\n",
      "Epoch 60 \t Batch 260 \t Training Loss: 46.241008142324596\n",
      "Epoch 60 \t Batch 280 \t Training Loss: 46.307258646828785\n",
      "Epoch 60 \t Batch 300 \t Training Loss: 46.34133324940999\n",
      "Epoch 60 \t Batch 320 \t Training Loss: 46.37148529291153\n",
      "Epoch 60 \t Batch 340 \t Training Loss: 46.44725648094626\n",
      "Epoch 60 \t Batch 360 \t Training Loss: 46.39733397165934\n",
      "Epoch 60 \t Batch 380 \t Training Loss: 46.431123924255374\n",
      "Epoch 60 \t Batch 400 \t Training Loss: 46.43262690544128\n",
      "Epoch 60 \t Batch 420 \t Training Loss: 46.43255687895275\n",
      "Epoch 60 \t Batch 440 \t Training Loss: 46.41452165950428\n",
      "Epoch 60 \t Batch 460 \t Training Loss: 46.424732954605766\n",
      "Epoch 60 \t Batch 480 \t Training Loss: 46.42812904516856\n",
      "Epoch 60 \t Batch 500 \t Training Loss: 46.431344528198245\n",
      "Epoch 60 \t Batch 520 \t Training Loss: 46.41133637795082\n",
      "Epoch 60 \t Batch 540 \t Training Loss: 46.41085774457013\n",
      "Epoch 60 \t Batch 560 \t Training Loss: 46.42621785572597\n",
      "Epoch 60 \t Batch 580 \t Training Loss: 46.411348296856055\n",
      "Epoch 60 \t Batch 600 \t Training Loss: 46.40900084813436\n",
      "Epoch 60 \t Batch 620 \t Training Loss: 46.40185233085386\n",
      "Epoch 60 \t Batch 640 \t Training Loss: 46.36113973855972\n",
      "Epoch 60 \t Batch 660 \t Training Loss: 46.370730457883894\n",
      "Epoch 60 \t Batch 680 \t Training Loss: 46.34769390330595\n",
      "Epoch 60 \t Batch 700 \t Training Loss: 46.36782895224435\n",
      "Epoch 60 \t Batch 720 \t Training Loss: 46.40937773386637\n",
      "Epoch 60 \t Batch 740 \t Training Loss: 46.39363277538403\n",
      "Epoch 60 \t Batch 760 \t Training Loss: 46.38811032144647\n",
      "Epoch 60 \t Batch 780 \t Training Loss: 46.41466236603566\n",
      "Epoch 60 \t Batch 800 \t Training Loss: 46.39033420562744\n",
      "Epoch 60 \t Batch 820 \t Training Loss: 46.38482505054009\n",
      "Epoch 60 \t Batch 840 \t Training Loss: 46.35975720995948\n",
      "Epoch 60 \t Batch 860 \t Training Loss: 46.34958201563636\n",
      "Epoch 60 \t Batch 880 \t Training Loss: 46.31983891833912\n",
      "Epoch 60 \t Batch 900 \t Training Loss: 46.32241325378418\n",
      "Epoch 60 \t Batch 20 \t Validation Loss: 15.906209325790405\n",
      "Epoch 60 \t Batch 40 \t Validation Loss: 18.647393465042114\n",
      "Epoch 60 \t Batch 60 \t Validation Loss: 18.433572483062743\n",
      "Epoch 60 \t Batch 80 \t Validation Loss: 19.247196280956267\n",
      "Epoch 60 \t Batch 100 \t Validation Loss: 21.271606779098512\n",
      "Epoch 60 \t Batch 120 \t Validation Loss: 22.670640166600545\n",
      "Epoch 60 \t Batch 140 \t Validation Loss: 23.48776544843401\n",
      "Epoch 60 \t Batch 160 \t Validation Loss: 25.777830874919893\n",
      "Epoch 60 \t Batch 180 \t Validation Loss: 29.665930710898504\n",
      "Epoch 60 \t Batch 200 \t Validation Loss: 31.395599331855774\n",
      "Epoch 60 \t Batch 220 \t Validation Loss: 32.878279360857874\n",
      "Epoch 60 \t Batch 240 \t Validation Loss: 33.49276419878006\n",
      "Epoch 60 \t Batch 260 \t Validation Loss: 35.764562181326056\n",
      "Epoch 60 \t Batch 280 \t Validation Loss: 37.031198852402824\n",
      "Epoch 60 \t Batch 300 \t Validation Loss: 38.21385909080505\n",
      "Epoch 60 \t Batch 320 \t Validation Loss: 38.76398166716099\n",
      "Epoch 60 \t Batch 340 \t Validation Loss: 38.74551814303678\n",
      "Epoch 60 \t Batch 360 \t Validation Loss: 38.66114458243052\n",
      "Epoch 60 \t Batch 380 \t Validation Loss: 38.96390082961634\n",
      "Epoch 60 \t Batch 400 \t Validation Loss: 38.59626517534256\n",
      "Epoch 60 \t Batch 420 \t Validation Loss: 38.62487698736645\n",
      "Epoch 60 \t Batch 440 \t Validation Loss: 38.33886510025371\n",
      "Epoch 60 \t Batch 460 \t Validation Loss: 38.627271082090296\n",
      "Epoch 60 \t Batch 480 \t Validation Loss: 39.135196977853774\n",
      "Epoch 60 \t Batch 500 \t Validation Loss: 38.892688756942746\n",
      "Epoch 60 \t Batch 520 \t Validation Loss: 38.711805591216454\n",
      "Epoch 60 \t Batch 540 \t Validation Loss: 38.52152448760138\n",
      "Epoch 60 \t Batch 560 \t Validation Loss: 38.376081315108706\n",
      "Epoch 60 \t Batch 580 \t Validation Loss: 38.25751316629607\n",
      "Epoch 60 \t Batch 600 \t Validation Loss: 38.50270215511322\n",
      "Epoch 60 Training Loss: 46.34846573541573 Validation Loss: 39.27690009482495\n",
      "Epoch 60 completed\n",
      "Epoch 61 \t Batch 20 \t Training Loss: 45.34423007965088\n",
      "Epoch 61 \t Batch 40 \t Training Loss: 46.42462215423584\n",
      "Epoch 61 \t Batch 60 \t Training Loss: 46.20954418182373\n",
      "Epoch 61 \t Batch 80 \t Training Loss: 46.10588083267212\n",
      "Epoch 61 \t Batch 100 \t Training Loss: 45.94990264892578\n",
      "Epoch 61 \t Batch 120 \t Training Loss: 45.82960316340129\n",
      "Epoch 61 \t Batch 140 \t Training Loss: 45.7958169392177\n",
      "Epoch 61 \t Batch 160 \t Training Loss: 45.90291578769684\n",
      "Epoch 61 \t Batch 180 \t Training Loss: 45.94779733022054\n",
      "Epoch 61 \t Batch 200 \t Training Loss: 46.01554773330688\n",
      "Epoch 61 \t Batch 220 \t Training Loss: 46.13818426999179\n",
      "Epoch 61 \t Batch 240 \t Training Loss: 46.106102021535236\n",
      "Epoch 61 \t Batch 260 \t Training Loss: 46.01405222966121\n",
      "Epoch 61 \t Batch 280 \t Training Loss: 45.93381438936506\n",
      "Epoch 61 \t Batch 300 \t Training Loss: 45.95829430898031\n",
      "Epoch 61 \t Batch 320 \t Training Loss: 45.97089149951935\n",
      "Epoch 61 \t Batch 340 \t Training Loss: 45.93250387977152\n",
      "Epoch 61 \t Batch 360 \t Training Loss: 46.06680526733398\n",
      "Epoch 61 \t Batch 380 \t Training Loss: 46.073771105314556\n",
      "Epoch 61 \t Batch 400 \t Training Loss: 46.19539939880371\n",
      "Epoch 61 \t Batch 420 \t Training Loss: 46.24434971582322\n",
      "Epoch 61 \t Batch 440 \t Training Loss: 46.29686714519154\n",
      "Epoch 61 \t Batch 460 \t Training Loss: 46.35354225324548\n",
      "Epoch 61 \t Batch 480 \t Training Loss: 46.34121197064717\n",
      "Epoch 61 \t Batch 500 \t Training Loss: 46.41993444061279\n",
      "Epoch 61 \t Batch 520 \t Training Loss: 46.34027550770686\n",
      "Epoch 61 \t Batch 540 \t Training Loss: 46.301608883893046\n",
      "Epoch 61 \t Batch 560 \t Training Loss: 46.26198603766305\n",
      "Epoch 61 \t Batch 580 \t Training Loss: 46.22827685125943\n",
      "Epoch 61 \t Batch 600 \t Training Loss: 46.18932704925537\n",
      "Epoch 61 \t Batch 620 \t Training Loss: 46.194590020948844\n",
      "Epoch 61 \t Batch 640 \t Training Loss: 46.29694812893867\n",
      "Epoch 61 \t Batch 660 \t Training Loss: 46.289125997369936\n",
      "Epoch 61 \t Batch 680 \t Training Loss: 46.30724280862247\n",
      "Epoch 61 \t Batch 700 \t Training Loss: 46.32841110229492\n",
      "Epoch 61 \t Batch 720 \t Training Loss: 46.298503732681276\n",
      "Epoch 61 \t Batch 740 \t Training Loss: 46.32159929017763\n",
      "Epoch 61 \t Batch 760 \t Training Loss: 46.300617037321395\n",
      "Epoch 61 \t Batch 780 \t Training Loss: 46.259030689337315\n",
      "Epoch 61 \t Batch 800 \t Training Loss: 46.280947213172915\n",
      "Epoch 61 \t Batch 820 \t Training Loss: 46.28764659602468\n",
      "Epoch 61 \t Batch 840 \t Training Loss: 46.298725119091216\n",
      "Epoch 61 \t Batch 860 \t Training Loss: 46.330868725444\n",
      "Epoch 61 \t Batch 880 \t Training Loss: 46.35398897691206\n",
      "Epoch 61 \t Batch 900 \t Training Loss: 46.35050162845188\n",
      "Epoch 61 \t Batch 20 \t Validation Loss: 17.227399349212646\n",
      "Epoch 61 \t Batch 40 \t Validation Loss: 20.495351099967955\n",
      "Epoch 61 \t Batch 60 \t Validation Loss: 19.738951031366984\n",
      "Epoch 61 \t Batch 80 \t Validation Loss: 20.37495502233505\n",
      "Epoch 61 \t Batch 100 \t Validation Loss: 22.138878355026247\n",
      "Epoch 61 \t Batch 120 \t Validation Loss: 23.453190716107688\n",
      "Epoch 61 \t Batch 140 \t Validation Loss: 24.079055070877075\n",
      "Epoch 61 \t Batch 160 \t Validation Loss: 25.792891043424607\n",
      "Epoch 61 \t Batch 180 \t Validation Loss: 28.895010418362087\n",
      "Epoch 61 \t Batch 200 \t Validation Loss: 30.22252311706543\n",
      "Epoch 61 \t Batch 220 \t Validation Loss: 31.225389810041946\n",
      "Epoch 61 \t Batch 240 \t Validation Loss: 31.57732105255127\n",
      "Epoch 61 \t Batch 260 \t Validation Loss: 33.47301139831543\n",
      "Epoch 61 \t Batch 280 \t Validation Loss: 34.50679012026106\n",
      "Epoch 61 \t Batch 300 \t Validation Loss: 35.44004097620646\n",
      "Epoch 61 \t Batch 320 \t Validation Loss: 35.89064507186413\n",
      "Epoch 61 \t Batch 340 \t Validation Loss: 35.92585297472337\n",
      "Epoch 61 \t Batch 360 \t Validation Loss: 35.76639793448978\n",
      "Epoch 61 \t Batch 380 \t Validation Loss: 36.033537736691926\n",
      "Epoch 61 \t Batch 400 \t Validation Loss: 35.780897524356845\n",
      "Epoch 61 \t Batch 420 \t Validation Loss: 35.92867966606504\n",
      "Epoch 61 \t Batch 440 \t Validation Loss: 35.77065523320978\n",
      "Epoch 61 \t Batch 460 \t Validation Loss: 36.15625778281171\n",
      "Epoch 61 \t Batch 480 \t Validation Loss: 36.7032457391421\n",
      "Epoch 61 \t Batch 500 \t Validation Loss: 36.48976919174194\n",
      "Epoch 61 \t Batch 520 \t Validation Loss: 36.41147624529325\n",
      "Epoch 61 \t Batch 540 \t Validation Loss: 36.22080209166916\n",
      "Epoch 61 \t Batch 560 \t Validation Loss: 36.09618284021105\n",
      "Epoch 61 \t Batch 580 \t Validation Loss: 35.928110231202226\n",
      "Epoch 61 \t Batch 600 \t Validation Loss: 36.20019090970357\n",
      "Epoch 61 Training Loss: 46.342116477575274 Validation Loss: 36.926680704215904\n",
      "Epoch 61 completed\n",
      "Epoch 62 \t Batch 20 \t Training Loss: 45.27164173126221\n",
      "Epoch 62 \t Batch 40 \t Training Loss: 46.138266468048094\n",
      "Epoch 62 \t Batch 60 \t Training Loss: 46.54865678151449\n",
      "Epoch 62 \t Batch 80 \t Training Loss: 46.94145965576172\n",
      "Epoch 62 \t Batch 100 \t Training Loss: 46.68976089477539\n",
      "Epoch 62 \t Batch 120 \t Training Loss: 46.705738639831544\n",
      "Epoch 62 \t Batch 140 \t Training Loss: 46.51673744746617\n",
      "Epoch 62 \t Batch 160 \t Training Loss: 46.37042841911316\n",
      "Epoch 62 \t Batch 180 \t Training Loss: 46.38152741326226\n",
      "Epoch 62 \t Batch 200 \t Training Loss: 46.38281904220581\n",
      "Epoch 62 \t Batch 220 \t Training Loss: 46.309774260087444\n",
      "Epoch 62 \t Batch 240 \t Training Loss: 46.36601831118266\n",
      "Epoch 62 \t Batch 260 \t Training Loss: 46.35848959409274\n",
      "Epoch 62 \t Batch 280 \t Training Loss: 46.28874827793666\n",
      "Epoch 62 \t Batch 300 \t Training Loss: 46.294182192484534\n",
      "Epoch 62 \t Batch 320 \t Training Loss: 46.325763034820554\n",
      "Epoch 62 \t Batch 340 \t Training Loss: 46.343087925630456\n",
      "Epoch 62 \t Batch 360 \t Training Loss: 46.3793704032898\n",
      "Epoch 62 \t Batch 380 \t Training Loss: 46.38016261050576\n",
      "Epoch 62 \t Batch 400 \t Training Loss: 46.299787940979\n",
      "Epoch 62 \t Batch 420 \t Training Loss: 46.338773909069246\n",
      "Epoch 62 \t Batch 440 \t Training Loss: 46.37538845755837\n",
      "Epoch 62 \t Batch 460 \t Training Loss: 46.42676393260127\n",
      "Epoch 62 \t Batch 480 \t Training Loss: 46.35681233406067\n",
      "Epoch 62 \t Batch 500 \t Training Loss: 46.3729254989624\n",
      "Epoch 62 \t Batch 520 \t Training Loss: 46.343870221651514\n",
      "Epoch 62 \t Batch 540 \t Training Loss: 46.31118594982006\n",
      "Epoch 62 \t Batch 560 \t Training Loss: 46.33471543448312\n",
      "Epoch 62 \t Batch 580 \t Training Loss: 46.354049281416266\n",
      "Epoch 62 \t Batch 600 \t Training Loss: 46.37926122029622\n",
      "Epoch 62 \t Batch 620 \t Training Loss: 46.35704468142602\n",
      "Epoch 62 \t Batch 640 \t Training Loss: 46.37918176054954\n",
      "Epoch 62 \t Batch 660 \t Training Loss: 46.34118086496989\n",
      "Epoch 62 \t Batch 680 \t Training Loss: 46.304967106089876\n",
      "Epoch 62 \t Batch 700 \t Training Loss: 46.30481573922294\n",
      "Epoch 62 \t Batch 720 \t Training Loss: 46.30367758538988\n",
      "Epoch 62 \t Batch 740 \t Training Loss: 46.325533335917704\n",
      "Epoch 62 \t Batch 760 \t Training Loss: 46.29163449437995\n",
      "Epoch 62 \t Batch 780 \t Training Loss: 46.31079540252686\n",
      "Epoch 62 \t Batch 800 \t Training Loss: 46.30000826835632\n",
      "Epoch 62 \t Batch 820 \t Training Loss: 46.3137749136948\n",
      "Epoch 62 \t Batch 840 \t Training Loss: 46.306601115635466\n",
      "Epoch 62 \t Batch 860 \t Training Loss: 46.3393731183784\n",
      "Epoch 62 \t Batch 880 \t Training Loss: 46.32163136655634\n",
      "Epoch 62 \t Batch 900 \t Training Loss: 46.30596271514892\n",
      "Epoch 62 \t Batch 20 \t Validation Loss: 19.188430738449096\n",
      "Epoch 62 \t Batch 40 \t Validation Loss: 21.453757810592652\n",
      "Epoch 62 \t Batch 60 \t Validation Loss: 21.076463635762533\n",
      "Epoch 62 \t Batch 80 \t Validation Loss: 21.62386189699173\n",
      "Epoch 62 \t Batch 100 \t Validation Loss: 23.010763177871706\n",
      "Epoch 62 \t Batch 120 \t Validation Loss: 24.103934772809346\n",
      "Epoch 62 \t Batch 140 \t Validation Loss: 24.606542934690204\n",
      "Epoch 62 \t Batch 160 \t Validation Loss: 26.369946998357772\n",
      "Epoch 62 \t Batch 180 \t Validation Loss: 29.712095250023737\n",
      "Epoch 62 \t Batch 200 \t Validation Loss: 31.076852006912233\n",
      "Epoch 62 \t Batch 220 \t Validation Loss: 32.18596280704845\n",
      "Epoch 62 \t Batch 240 \t Validation Loss: 32.606899964809415\n",
      "Epoch 62 \t Batch 260 \t Validation Loss: 34.56884034963755\n",
      "Epoch 62 \t Batch 280 \t Validation Loss: 35.642030354908535\n",
      "Epoch 62 \t Batch 300 \t Validation Loss: 36.64782474517822\n",
      "Epoch 62 \t Batch 320 \t Validation Loss: 37.125139421224596\n",
      "Epoch 62 \t Batch 340 \t Validation Loss: 37.09814321854535\n",
      "Epoch 62 \t Batch 360 \t Validation Loss: 36.929858424928454\n",
      "Epoch 62 \t Batch 380 \t Validation Loss: 37.17209579066226\n",
      "Epoch 62 \t Batch 400 \t Validation Loss: 36.83532765388489\n",
      "Epoch 62 \t Batch 420 \t Validation Loss: 36.90470967292786\n",
      "Epoch 62 \t Batch 440 \t Validation Loss: 36.65860631032424\n",
      "Epoch 62 \t Batch 460 \t Validation Loss: 36.98041120819423\n",
      "Epoch 62 \t Batch 480 \t Validation Loss: 37.50848827958107\n",
      "Epoch 62 \t Batch 500 \t Validation Loss: 37.25748746681214\n",
      "Epoch 62 \t Batch 520 \t Validation Loss: 37.094159265664906\n",
      "Epoch 62 \t Batch 540 \t Validation Loss: 36.883547132986564\n",
      "Epoch 62 \t Batch 560 \t Validation Loss: 36.69672884941101\n",
      "Epoch 62 \t Batch 580 \t Validation Loss: 36.44031683165451\n",
      "Epoch 62 \t Batch 600 \t Validation Loss: 36.7029932975769\n",
      "Epoch 62 Training Loss: 46.3172102192886 Validation Loss: 37.38380230866469\n",
      "Epoch 62 completed\n",
      "Epoch 63 \t Batch 20 \t Training Loss: 46.86773242950439\n",
      "Epoch 63 \t Batch 40 \t Training Loss: 46.47904605865479\n",
      "Epoch 63 \t Batch 60 \t Training Loss: 46.3564562479655\n",
      "Epoch 63 \t Batch 80 \t Training Loss: 46.148784971237184\n",
      "Epoch 63 \t Batch 100 \t Training Loss: 46.34793697357178\n",
      "Epoch 63 \t Batch 120 \t Training Loss: 46.45968723297119\n",
      "Epoch 63 \t Batch 140 \t Training Loss: 46.35463406699044\n",
      "Epoch 63 \t Batch 160 \t Training Loss: 46.40276110172272\n",
      "Epoch 63 \t Batch 180 \t Training Loss: 46.32182473076715\n",
      "Epoch 63 \t Batch 200 \t Training Loss: 46.40293043136597\n",
      "Epoch 63 \t Batch 220 \t Training Loss: 46.45831635215065\n",
      "Epoch 63 \t Batch 240 \t Training Loss: 46.40232327779134\n",
      "Epoch 63 \t Batch 260 \t Training Loss: 46.210877932035004\n",
      "Epoch 63 \t Batch 280 \t Training Loss: 46.28735719408308\n",
      "Epoch 63 \t Batch 300 \t Training Loss: 46.27156167348226\n",
      "Epoch 63 \t Batch 320 \t Training Loss: 46.411958682537076\n",
      "Epoch 63 \t Batch 340 \t Training Loss: 46.31669563966639\n",
      "Epoch 63 \t Batch 360 \t Training Loss: 46.37359636094835\n",
      "Epoch 63 \t Batch 380 \t Training Loss: 46.3804739901894\n",
      "Epoch 63 \t Batch 400 \t Training Loss: 46.356711769104\n",
      "Epoch 63 \t Batch 420 \t Training Loss: 46.41857122693743\n",
      "Epoch 63 \t Batch 440 \t Training Loss: 46.401817321777344\n",
      "Epoch 63 \t Batch 460 \t Training Loss: 46.38100975700047\n",
      "Epoch 63 \t Batch 480 \t Training Loss: 46.35290411313375\n",
      "Epoch 63 \t Batch 500 \t Training Loss: 46.33147408294678\n",
      "Epoch 63 \t Batch 520 \t Training Loss: 46.33074444257296\n",
      "Epoch 63 \t Batch 540 \t Training Loss: 46.32965189615886\n",
      "Epoch 63 \t Batch 560 \t Training Loss: 46.30896449770246\n",
      "Epoch 63 \t Batch 580 \t Training Loss: 46.34558170581686\n",
      "Epoch 63 \t Batch 600 \t Training Loss: 46.344575061798096\n",
      "Epoch 63 \t Batch 620 \t Training Loss: 46.30644668456047\n",
      "Epoch 63 \t Batch 640 \t Training Loss: 46.33479889631271\n",
      "Epoch 63 \t Batch 660 \t Training Loss: 46.29782236850623\n",
      "Epoch 63 \t Batch 680 \t Training Loss: 46.332033219056974\n",
      "Epoch 63 \t Batch 700 \t Training Loss: 46.34767613002232\n",
      "Epoch 63 \t Batch 720 \t Training Loss: 46.33544928232829\n",
      "Epoch 63 \t Batch 740 \t Training Loss: 46.37276082941004\n",
      "Epoch 63 \t Batch 760 \t Training Loss: 46.35328061957108\n",
      "Epoch 63 \t Batch 780 \t Training Loss: 46.30515922155136\n",
      "Epoch 63 \t Batch 800 \t Training Loss: 46.30968525409698\n",
      "Epoch 63 \t Batch 820 \t Training Loss: 46.290765078474834\n",
      "Epoch 63 \t Batch 840 \t Training Loss: 46.28706180481684\n",
      "Epoch 63 \t Batch 860 \t Training Loss: 46.29380065119544\n",
      "Epoch 63 \t Batch 880 \t Training Loss: 46.28024600202387\n",
      "Epoch 63 \t Batch 900 \t Training Loss: 46.28670286814372\n",
      "Epoch 63 \t Batch 20 \t Validation Loss: 17.89199275970459\n",
      "Epoch 63 \t Batch 40 \t Validation Loss: 20.498099064826967\n",
      "Epoch 63 \t Batch 60 \t Validation Loss: 20.009263912836712\n",
      "Epoch 63 \t Batch 80 \t Validation Loss: 20.514654922485352\n",
      "Epoch 63 \t Batch 100 \t Validation Loss: 22.218338565826414\n",
      "Epoch 63 \t Batch 120 \t Validation Loss: 23.60347462495168\n",
      "Epoch 63 \t Batch 140 \t Validation Loss: 24.287751027515956\n",
      "Epoch 63 \t Batch 160 \t Validation Loss: 26.427940994501114\n",
      "Epoch 63 \t Batch 180 \t Validation Loss: 30.32178800370958\n",
      "Epoch 63 \t Batch 200 \t Validation Loss: 31.96952799797058\n",
      "Epoch 63 \t Batch 220 \t Validation Loss: 33.34445312673395\n",
      "Epoch 63 \t Batch 240 \t Validation Loss: 33.936094303925834\n",
      "Epoch 63 \t Batch 260 \t Validation Loss: 36.10066014436575\n",
      "Epoch 63 \t Batch 280 \t Validation Loss: 37.268909328324455\n",
      "Epoch 63 \t Batch 300 \t Validation Loss: 38.53386162757874\n",
      "Epoch 63 \t Batch 320 \t Validation Loss: 39.11628873646259\n",
      "Epoch 63 \t Batch 340 \t Validation Loss: 39.06893406194799\n",
      "Epoch 63 \t Batch 360 \t Validation Loss: 38.957426857948306\n",
      "Epoch 63 \t Batch 380 \t Validation Loss: 39.20123623044867\n",
      "Epoch 63 \t Batch 400 \t Validation Loss: 38.802620027065274\n",
      "Epoch 63 \t Batch 420 \t Validation Loss: 38.85229092325483\n",
      "Epoch 63 \t Batch 440 \t Validation Loss: 38.557074462283744\n",
      "Epoch 63 \t Batch 460 \t Validation Loss: 38.822812563440074\n",
      "Epoch 63 \t Batch 480 \t Validation Loss: 39.32719418803851\n",
      "Epoch 63 \t Batch 500 \t Validation Loss: 39.08182350349426\n",
      "Epoch 63 \t Batch 520 \t Validation Loss: 38.88157637852889\n",
      "Epoch 63 \t Batch 540 \t Validation Loss: 38.60743004657604\n",
      "Epoch 63 \t Batch 560 \t Validation Loss: 38.369527879783085\n",
      "Epoch 63 \t Batch 580 \t Validation Loss: 38.06333481854406\n",
      "Epoch 63 \t Batch 600 \t Validation Loss: 38.263044244448345\n",
      "Epoch 63 Training Loss: 46.30668672584525 Validation Loss: 38.903299794568646\n",
      "Epoch 63 completed\n",
      "Epoch 64 \t Batch 20 \t Training Loss: 44.956271171569824\n",
      "Epoch 64 \t Batch 40 \t Training Loss: 45.01478881835938\n",
      "Epoch 64 \t Batch 60 \t Training Loss: 45.709017817179365\n",
      "Epoch 64 \t Batch 80 \t Training Loss: 45.83777093887329\n",
      "Epoch 64 \t Batch 100 \t Training Loss: 45.60835609436035\n",
      "Epoch 64 \t Batch 120 \t Training Loss: 45.667824776967365\n",
      "Epoch 64 \t Batch 140 \t Training Loss: 45.69536448887416\n",
      "Epoch 64 \t Batch 160 \t Training Loss: 45.875370383262634\n",
      "Epoch 64 \t Batch 180 \t Training Loss: 45.713630273607045\n",
      "Epoch 64 \t Batch 200 \t Training Loss: 45.75734432220459\n",
      "Epoch 64 \t Batch 220 \t Training Loss: 45.87520080913197\n",
      "Epoch 64 \t Batch 240 \t Training Loss: 45.949364026387535\n",
      "Epoch 64 \t Batch 260 \t Training Loss: 46.09733073894794\n",
      "Epoch 64 \t Batch 280 \t Training Loss: 46.15975750514439\n",
      "Epoch 64 \t Batch 300 \t Training Loss: 46.09251784006754\n",
      "Epoch 64 \t Batch 320 \t Training Loss: 45.99534697532654\n",
      "Epoch 64 \t Batch 340 \t Training Loss: 46.039709663391115\n",
      "Epoch 64 \t Batch 360 \t Training Loss: 46.00901718139649\n",
      "Epoch 64 \t Batch 380 \t Training Loss: 46.04679940876208\n",
      "Epoch 64 \t Batch 400 \t Training Loss: 46.102455110549926\n",
      "Epoch 64 \t Batch 420 \t Training Loss: 46.11167873200916\n",
      "Epoch 64 \t Batch 440 \t Training Loss: 46.13883303728971\n",
      "Epoch 64 \t Batch 460 \t Training Loss: 46.14507253895635\n",
      "Epoch 64 \t Batch 480 \t Training Loss: 46.27179075082143\n",
      "Epoch 64 \t Batch 500 \t Training Loss: 46.2584944152832\n",
      "Epoch 64 \t Batch 520 \t Training Loss: 46.19574331870446\n",
      "Epoch 64 \t Batch 540 \t Training Loss: 46.12024875923439\n",
      "Epoch 64 \t Batch 560 \t Training Loss: 46.06552978243147\n",
      "Epoch 64 \t Batch 580 \t Training Loss: 46.08320122422843\n",
      "Epoch 64 \t Batch 600 \t Training Loss: 46.0762691561381\n",
      "Epoch 64 \t Batch 620 \t Training Loss: 46.107600661247005\n",
      "Epoch 64 \t Batch 640 \t Training Loss: 46.13350207209587\n",
      "Epoch 64 \t Batch 660 \t Training Loss: 46.18572831587358\n",
      "Epoch 64 \t Batch 680 \t Training Loss: 46.183421443490424\n",
      "Epoch 64 \t Batch 700 \t Training Loss: 46.18232597896031\n",
      "Epoch 64 \t Batch 720 \t Training Loss: 46.172473928663464\n",
      "Epoch 64 \t Batch 740 \t Training Loss: 46.19916246775034\n",
      "Epoch 64 \t Batch 760 \t Training Loss: 46.23007527903506\n",
      "Epoch 64 \t Batch 780 \t Training Loss: 46.2027897859231\n",
      "Epoch 64 \t Batch 800 \t Training Loss: 46.23704240322113\n",
      "Epoch 64 \t Batch 820 \t Training Loss: 46.21460980671208\n",
      "Epoch 64 \t Batch 840 \t Training Loss: 46.186636093684605\n",
      "Epoch 64 \t Batch 860 \t Training Loss: 46.1856287357419\n",
      "Epoch 64 \t Batch 880 \t Training Loss: 46.194737165624446\n",
      "Epoch 64 \t Batch 900 \t Training Loss: 46.25281966315375\n",
      "Epoch 64 \t Batch 20 \t Validation Loss: 22.568461322784422\n",
      "Epoch 64 \t Batch 40 \t Validation Loss: 24.25785925388336\n",
      "Epoch 64 \t Batch 60 \t Validation Loss: 24.047367922465007\n",
      "Epoch 64 \t Batch 80 \t Validation Loss: 24.470228290557863\n",
      "Epoch 64 \t Batch 100 \t Validation Loss: 25.37796148300171\n",
      "Epoch 64 \t Batch 120 \t Validation Loss: 26.09241773287455\n",
      "Epoch 64 \t Batch 140 \t Validation Loss: 26.401518862588066\n",
      "Epoch 64 \t Batch 160 \t Validation Loss: 28.170365941524505\n",
      "Epoch 64 \t Batch 180 \t Validation Loss: 31.57765032450358\n",
      "Epoch 64 \t Batch 200 \t Validation Loss: 32.850034952163696\n",
      "Epoch 64 \t Batch 220 \t Validation Loss: 33.9593411619013\n",
      "Epoch 64 \t Batch 240 \t Validation Loss: 34.35543525616328\n",
      "Epoch 64 \t Batch 260 \t Validation Loss: 36.28377371567946\n",
      "Epoch 64 \t Batch 280 \t Validation Loss: 37.2904424054282\n",
      "Epoch 64 \t Batch 300 \t Validation Loss: 38.393694426218666\n",
      "Epoch 64 \t Batch 320 \t Validation Loss: 38.88040375113487\n",
      "Epoch 64 \t Batch 340 \t Validation Loss: 38.80001857420977\n",
      "Epoch 64 \t Batch 360 \t Validation Loss: 38.654171400600006\n",
      "Epoch 64 \t Batch 380 \t Validation Loss: 38.86564603353801\n",
      "Epoch 64 \t Batch 400 \t Validation Loss: 38.4668916964531\n",
      "Epoch 64 \t Batch 420 \t Validation Loss: 38.48252712885539\n",
      "Epoch 64 \t Batch 440 \t Validation Loss: 38.18016330545599\n",
      "Epoch 64 \t Batch 460 \t Validation Loss: 38.460548388439676\n",
      "Epoch 64 \t Batch 480 \t Validation Loss: 38.95331262350082\n",
      "Epoch 64 \t Batch 500 \t Validation Loss: 38.68517626571655\n",
      "Epoch 64 \t Batch 520 \t Validation Loss: 38.51031597577609\n",
      "Epoch 64 \t Batch 540 \t Validation Loss: 38.29774584946809\n",
      "Epoch 64 \t Batch 560 \t Validation Loss: 38.111558594022476\n",
      "Epoch 64 \t Batch 580 \t Validation Loss: 37.85746106114881\n",
      "Epoch 64 \t Batch 600 \t Validation Loss: 38.10949468930562\n",
      "Epoch 64 Training Loss: 46.25231869946129 Validation Loss: 38.77772882077601\n",
      "Epoch 64 completed\n",
      "Epoch 65 \t Batch 20 \t Training Loss: 46.58571357727051\n",
      "Epoch 65 \t Batch 40 \t Training Loss: 46.90013465881348\n",
      "Epoch 65 \t Batch 60 \t Training Loss: 46.92923170725505\n",
      "Epoch 65 \t Batch 80 \t Training Loss: 46.573199129104616\n",
      "Epoch 65 \t Batch 100 \t Training Loss: 46.25824977874756\n",
      "Epoch 65 \t Batch 120 \t Training Loss: 46.1804305712382\n",
      "Epoch 65 \t Batch 140 \t Training Loss: 46.12864824022566\n",
      "Epoch 65 \t Batch 160 \t Training Loss: 46.24586501121521\n",
      "Epoch 65 \t Batch 180 \t Training Loss: 46.53625250922309\n",
      "Epoch 65 \t Batch 200 \t Training Loss: 46.54983325958252\n",
      "Epoch 65 \t Batch 220 \t Training Loss: 46.53840389251709\n",
      "Epoch 65 \t Batch 240 \t Training Loss: 46.557321055730185\n",
      "Epoch 65 \t Batch 260 \t Training Loss: 46.510769154475284\n",
      "Epoch 65 \t Batch 280 \t Training Loss: 46.52542435782296\n",
      "Epoch 65 \t Batch 300 \t Training Loss: 46.62058216094971\n",
      "Epoch 65 \t Batch 320 \t Training Loss: 46.58103305101395\n",
      "Epoch 65 \t Batch 340 \t Training Loss: 46.50955314636231\n",
      "Epoch 65 \t Batch 360 \t Training Loss: 46.592449559105766\n",
      "Epoch 65 \t Batch 380 \t Training Loss: 46.514192400480574\n",
      "Epoch 65 \t Batch 400 \t Training Loss: 46.55045678138733\n",
      "Epoch 65 \t Batch 420 \t Training Loss: 46.54605160667783\n",
      "Epoch 65 \t Batch 440 \t Training Loss: 46.48635387420654\n",
      "Epoch 65 \t Batch 460 \t Training Loss: 46.46651272981063\n",
      "Epoch 65 \t Batch 480 \t Training Loss: 46.40259169737498\n",
      "Epoch 65 \t Batch 500 \t Training Loss: 46.37284093475342\n",
      "Epoch 65 \t Batch 520 \t Training Loss: 46.379631101168115\n",
      "Epoch 65 \t Batch 540 \t Training Loss: 46.325974231296115\n",
      "Epoch 65 \t Batch 560 \t Training Loss: 46.29844241142273\n",
      "Epoch 65 \t Batch 580 \t Training Loss: 46.289637710308206\n",
      "Epoch 65 \t Batch 600 \t Training Loss: 46.322442849477135\n",
      "Epoch 65 \t Batch 620 \t Training Loss: 46.34008060578377\n",
      "Epoch 65 \t Batch 640 \t Training Loss: 46.33938783407211\n",
      "Epoch 65 \t Batch 660 \t Training Loss: 46.341012105074796\n",
      "Epoch 65 \t Batch 680 \t Training Loss: 46.35274253172033\n",
      "Epoch 65 \t Batch 700 \t Training Loss: 46.339255670819966\n",
      "Epoch 65 \t Batch 720 \t Training Loss: 46.332246436013115\n",
      "Epoch 65 \t Batch 740 \t Training Loss: 46.31463328696586\n",
      "Epoch 65 \t Batch 760 \t Training Loss: 46.26268081665039\n",
      "Epoch 65 \t Batch 780 \t Training Loss: 46.223347272628395\n",
      "Epoch 65 \t Batch 800 \t Training Loss: 46.1889598941803\n",
      "Epoch 65 \t Batch 820 \t Training Loss: 46.1689581289524\n",
      "Epoch 65 \t Batch 840 \t Training Loss: 46.18050038019816\n",
      "Epoch 65 \t Batch 860 \t Training Loss: 46.192122268676755\n",
      "Epoch 65 \t Batch 880 \t Training Loss: 46.218513753197406\n",
      "Epoch 65 \t Batch 900 \t Training Loss: 46.2307869720459\n",
      "Epoch 65 \t Batch 20 \t Validation Loss: 18.25260987281799\n",
      "Epoch 65 \t Batch 40 \t Validation Loss: 20.385224366188048\n",
      "Epoch 65 \t Batch 60 \t Validation Loss: 20.048917738596597\n",
      "Epoch 65 \t Batch 80 \t Validation Loss: 20.578765857219697\n",
      "Epoch 65 \t Batch 100 \t Validation Loss: 22.578389368057252\n",
      "Epoch 65 \t Batch 120 \t Validation Loss: 23.999680026372275\n",
      "Epoch 65 \t Batch 140 \t Validation Loss: 24.64933317729405\n",
      "Epoch 65 \t Batch 160 \t Validation Loss: 26.355065798759462\n",
      "Epoch 65 \t Batch 180 \t Validation Loss: 29.586310847600302\n",
      "Epoch 65 \t Batch 200 \t Validation Loss: 30.857752079963685\n",
      "Epoch 65 \t Batch 220 \t Validation Loss: 31.86666703224182\n",
      "Epoch 65 \t Batch 240 \t Validation Loss: 32.258324984709425\n",
      "Epoch 65 \t Batch 260 \t Validation Loss: 34.121567032887384\n",
      "Epoch 65 \t Batch 280 \t Validation Loss: 35.11225672108787\n",
      "Epoch 65 \t Batch 300 \t Validation Loss: 36.13623610496521\n",
      "Epoch 65 \t Batch 320 \t Validation Loss: 36.60268296599388\n",
      "Epoch 65 \t Batch 340 \t Validation Loss: 36.59815339481129\n",
      "Epoch 65 \t Batch 360 \t Validation Loss: 36.45516512129042\n",
      "Epoch 65 \t Batch 380 \t Validation Loss: 36.67919141367862\n",
      "Epoch 65 \t Batch 400 \t Validation Loss: 36.386816477775575\n",
      "Epoch 65 \t Batch 420 \t Validation Loss: 36.473531632196334\n",
      "Epoch 65 \t Batch 440 \t Validation Loss: 36.26809189969843\n",
      "Epoch 65 \t Batch 460 \t Validation Loss: 36.57531224955683\n",
      "Epoch 65 \t Batch 480 \t Validation Loss: 37.09944216410319\n",
      "Epoch 65 \t Batch 500 \t Validation Loss: 36.84127836990356\n",
      "Epoch 65 \t Batch 520 \t Validation Loss: 36.68217666882735\n",
      "Epoch 65 \t Batch 540 \t Validation Loss: 36.53568586243524\n",
      "Epoch 65 \t Batch 560 \t Validation Loss: 36.41898339305605\n",
      "Epoch 65 \t Batch 580 \t Validation Loss: 36.21308910271217\n",
      "Epoch 65 \t Batch 600 \t Validation Loss: 36.52705919424693\n",
      "Epoch 65 Training Loss: 46.23376726922188 Validation Loss: 37.237875813013545\n",
      "Epoch 65 completed\n",
      "Epoch 66 \t Batch 20 \t Training Loss: 46.90915069580078\n",
      "Epoch 66 \t Batch 40 \t Training Loss: 45.46186408996582\n",
      "Epoch 66 \t Batch 60 \t Training Loss: 45.797385597229\n",
      "Epoch 66 \t Batch 80 \t Training Loss: 45.920849323272705\n",
      "Epoch 66 \t Batch 100 \t Training Loss: 46.12014144897461\n",
      "Epoch 66 \t Batch 120 \t Training Loss: 46.33043270111084\n",
      "Epoch 66 \t Batch 140 \t Training Loss: 46.382685034615655\n",
      "Epoch 66 \t Batch 160 \t Training Loss: 46.27957379817963\n",
      "Epoch 66 \t Batch 180 \t Training Loss: 46.47069784800212\n",
      "Epoch 66 \t Batch 200 \t Training Loss: 46.317412109375\n",
      "Epoch 66 \t Batch 220 \t Training Loss: 46.20098824934526\n",
      "Epoch 66 \t Batch 240 \t Training Loss: 46.34984563191732\n",
      "Epoch 66 \t Batch 260 \t Training Loss: 46.27136620741624\n",
      "Epoch 66 \t Batch 280 \t Training Loss: 46.22799295697894\n",
      "Epoch 66 \t Batch 300 \t Training Loss: 46.2283736928304\n",
      "Epoch 66 \t Batch 320 \t Training Loss: 46.25640732049942\n",
      "Epoch 66 \t Batch 340 \t Training Loss: 46.19079934288474\n",
      "Epoch 66 \t Batch 360 \t Training Loss: 46.19876601960924\n",
      "Epoch 66 \t Batch 380 \t Training Loss: 46.20308576885023\n",
      "Epoch 66 \t Batch 400 \t Training Loss: 46.21125597953797\n",
      "Epoch 66 \t Batch 420 \t Training Loss: 46.18806963421049\n",
      "Epoch 66 \t Batch 440 \t Training Loss: 46.16419883207841\n",
      "Epoch 66 \t Batch 460 \t Training Loss: 46.17937651095183\n",
      "Epoch 66 \t Batch 480 \t Training Loss: 46.13293316364288\n",
      "Epoch 66 \t Batch 500 \t Training Loss: 46.14347665405273\n",
      "Epoch 66 \t Batch 520 \t Training Loss: 46.14148415051974\n",
      "Epoch 66 \t Batch 540 \t Training Loss: 46.09072785554109\n",
      "Epoch 66 \t Batch 560 \t Training Loss: 46.08784055028643\n",
      "Epoch 66 \t Batch 580 \t Training Loss: 46.07095965681405\n",
      "Epoch 66 \t Batch 600 \t Training Loss: 46.06403868357341\n",
      "Epoch 66 \t Batch 620 \t Training Loss: 46.02907282306302\n",
      "Epoch 66 \t Batch 640 \t Training Loss: 46.06502861380577\n",
      "Epoch 66 \t Batch 660 \t Training Loss: 46.0920818271059\n",
      "Epoch 66 \t Batch 680 \t Training Loss: 46.074865268258485\n",
      "Epoch 66 \t Batch 700 \t Training Loss: 46.1336447089059\n",
      "Epoch 66 \t Batch 720 \t Training Loss: 46.14617535803053\n",
      "Epoch 66 \t Batch 740 \t Training Loss: 46.14976964383512\n",
      "Epoch 66 \t Batch 760 \t Training Loss: 46.18488329335263\n",
      "Epoch 66 \t Batch 780 \t Training Loss: 46.186794711381964\n",
      "Epoch 66 \t Batch 800 \t Training Loss: 46.21693521976471\n",
      "Epoch 66 \t Batch 820 \t Training Loss: 46.19660372850372\n",
      "Epoch 66 \t Batch 840 \t Training Loss: 46.18954671678089\n",
      "Epoch 66 \t Batch 860 \t Training Loss: 46.19707800842995\n",
      "Epoch 66 \t Batch 880 \t Training Loss: 46.21063873984597\n",
      "Epoch 66 \t Batch 900 \t Training Loss: 46.19766934288872\n",
      "Epoch 66 \t Batch 20 \t Validation Loss: 24.13394112586975\n",
      "Epoch 66 \t Batch 40 \t Validation Loss: 25.367668390274048\n",
      "Epoch 66 \t Batch 60 \t Validation Loss: 25.17095683415731\n",
      "Epoch 66 \t Batch 80 \t Validation Loss: 25.70315055847168\n",
      "Epoch 66 \t Batch 100 \t Validation Loss: 26.269308624267577\n",
      "Epoch 66 \t Batch 120 \t Validation Loss: 26.860144639015196\n",
      "Epoch 66 \t Batch 140 \t Validation Loss: 27.083640595844813\n",
      "Epoch 66 \t Batch 160 \t Validation Loss: 28.994061917066574\n",
      "Epoch 66 \t Batch 180 \t Validation Loss: 32.755832539664375\n",
      "Epoch 66 \t Batch 200 \t Validation Loss: 34.31854853153229\n",
      "Epoch 66 \t Batch 220 \t Validation Loss: 35.553771136023784\n",
      "Epoch 66 \t Batch 240 \t Validation Loss: 36.020182712872824\n",
      "Epoch 66 \t Batch 260 \t Validation Loss: 38.08691300979027\n",
      "Epoch 66 \t Batch 280 \t Validation Loss: 39.15668761730194\n",
      "Epoch 66 \t Batch 300 \t Validation Loss: 40.36047911643982\n",
      "Epoch 66 \t Batch 320 \t Validation Loss: 40.85999037921429\n",
      "Epoch 66 \t Batch 340 \t Validation Loss: 40.75495377428391\n",
      "Epoch 66 \t Batch 360 \t Validation Loss: 40.58419472111596\n",
      "Epoch 66 \t Batch 380 \t Validation Loss: 40.773212058920606\n",
      "Epoch 66 \t Batch 400 \t Validation Loss: 40.33174322366715\n",
      "Epoch 66 \t Batch 420 \t Validation Loss: 40.33748101052784\n",
      "Epoch 66 \t Batch 440 \t Validation Loss: 39.986017476428636\n",
      "Epoch 66 \t Batch 460 \t Validation Loss: 40.21904411937879\n",
      "Epoch 66 \t Batch 480 \t Validation Loss: 40.67564515074094\n",
      "Epoch 66 \t Batch 500 \t Validation Loss: 40.41128080940246\n",
      "Epoch 66 \t Batch 520 \t Validation Loss: 40.18270392234509\n",
      "Epoch 66 \t Batch 540 \t Validation Loss: 39.867247650358415\n",
      "Epoch 66 \t Batch 560 \t Validation Loss: 39.60784056016377\n",
      "Epoch 66 \t Batch 580 \t Validation Loss: 39.31777009141856\n",
      "Epoch 66 \t Batch 600 \t Validation Loss: 39.463444177309675\n",
      "Epoch 66 Training Loss: 46.19297842527034 Validation Loss: 40.07451515847986\n",
      "Epoch 66 completed\n",
      "Epoch 67 \t Batch 20 \t Training Loss: 45.98468723297119\n",
      "Epoch 67 \t Batch 40 \t Training Loss: 45.81256113052368\n",
      "Epoch 67 \t Batch 60 \t Training Loss: 45.86428407033284\n",
      "Epoch 67 \t Batch 80 \t Training Loss: 45.85236458778381\n",
      "Epoch 67 \t Batch 100 \t Training Loss: 45.87497398376465\n",
      "Epoch 67 \t Batch 120 \t Training Loss: 45.79243233998616\n",
      "Epoch 67 \t Batch 140 \t Training Loss: 45.634529277256554\n",
      "Epoch 67 \t Batch 160 \t Training Loss: 45.80088105201721\n",
      "Epoch 67 \t Batch 180 \t Training Loss: 45.89597076839871\n",
      "Epoch 67 \t Batch 200 \t Training Loss: 45.95928176879883\n",
      "Epoch 67 \t Batch 220 \t Training Loss: 45.98837047923695\n",
      "Epoch 67 \t Batch 240 \t Training Loss: 46.01716237068176\n",
      "Epoch 67 \t Batch 260 \t Training Loss: 45.94305791121263\n",
      "Epoch 67 \t Batch 280 \t Training Loss: 45.99411189215524\n",
      "Epoch 67 \t Batch 300 \t Training Loss: 46.05366133371989\n",
      "Epoch 67 \t Batch 320 \t Training Loss: 46.17226763963699\n",
      "Epoch 67 \t Batch 340 \t Training Loss: 46.178481831270105\n",
      "Epoch 67 \t Batch 360 \t Training Loss: 46.190031645033095\n",
      "Epoch 67 \t Batch 380 \t Training Loss: 46.16308817612497\n",
      "Epoch 67 \t Batch 400 \t Training Loss: 46.20719082832336\n",
      "Epoch 67 \t Batch 420 \t Training Loss: 46.259567042759485\n",
      "Epoch 67 \t Batch 440 \t Training Loss: 46.18081156990745\n",
      "Epoch 67 \t Batch 460 \t Training Loss: 46.139435262265415\n",
      "Epoch 67 \t Batch 480 \t Training Loss: 46.19152443408966\n",
      "Epoch 67 \t Batch 500 \t Training Loss: 46.16635236358643\n",
      "Epoch 67 \t Batch 520 \t Training Loss: 46.1401403940641\n",
      "Epoch 67 \t Batch 540 \t Training Loss: 46.18322546923602\n",
      "Epoch 67 \t Batch 560 \t Training Loss: 46.21887405940465\n",
      "Epoch 67 \t Batch 580 \t Training Loss: 46.167826231594745\n",
      "Epoch 67 \t Batch 600 \t Training Loss: 46.116758785247804\n",
      "Epoch 67 \t Batch 620 \t Training Loss: 46.109264706027126\n",
      "Epoch 67 \t Batch 640 \t Training Loss: 46.1033048927784\n",
      "Epoch 67 \t Batch 660 \t Training Loss: 46.1107296221184\n",
      "Epoch 67 \t Batch 680 \t Training Loss: 46.07682365529678\n",
      "Epoch 67 \t Batch 700 \t Training Loss: 46.09217657906669\n",
      "Epoch 67 \t Batch 720 \t Training Loss: 46.090262264675566\n",
      "Epoch 67 \t Batch 740 \t Training Loss: 46.15447249025912\n",
      "Epoch 67 \t Batch 760 \t Training Loss: 46.17478107151232\n",
      "Epoch 67 \t Batch 780 \t Training Loss: 46.170305217840735\n",
      "Epoch 67 \t Batch 800 \t Training Loss: 46.17679421424866\n",
      "Epoch 67 \t Batch 820 \t Training Loss: 46.174612054592224\n",
      "Epoch 67 \t Batch 840 \t Training Loss: 46.131577182951425\n",
      "Epoch 67 \t Batch 860 \t Training Loss: 46.12193568030069\n",
      "Epoch 67 \t Batch 880 \t Training Loss: 46.15183628255671\n",
      "Epoch 67 \t Batch 900 \t Training Loss: 46.17207663642036\n",
      "Epoch 67 \t Batch 20 \t Validation Loss: 27.360041522979735\n",
      "Epoch 67 \t Batch 40 \t Validation Loss: 27.25009753704071\n",
      "Epoch 67 \t Batch 60 \t Validation Loss: 27.40855180422465\n",
      "Epoch 67 \t Batch 80 \t Validation Loss: 27.51366468667984\n",
      "Epoch 67 \t Batch 100 \t Validation Loss: 27.79432801246643\n",
      "Epoch 67 \t Batch 120 \t Validation Loss: 28.263786005973817\n",
      "Epoch 67 \t Batch 140 \t Validation Loss: 28.266404376711165\n",
      "Epoch 67 \t Batch 160 \t Validation Loss: 29.95033993124962\n",
      "Epoch 67 \t Batch 180 \t Validation Loss: 33.421216911739776\n",
      "Epoch 67 \t Batch 200 \t Validation Loss: 34.66269082069397\n",
      "Epoch 67 \t Batch 220 \t Validation Loss: 35.86764685890891\n",
      "Epoch 67 \t Batch 240 \t Validation Loss: 36.26903772354126\n",
      "Epoch 67 \t Batch 260 \t Validation Loss: 38.305969043878406\n",
      "Epoch 67 \t Batch 280 \t Validation Loss: 39.33005639484951\n",
      "Epoch 67 \t Batch 300 \t Validation Loss: 40.378961181640626\n",
      "Epoch 67 \t Batch 320 \t Validation Loss: 40.845495593547824\n",
      "Epoch 67 \t Batch 340 \t Validation Loss: 40.70307338378009\n",
      "Epoch 67 \t Batch 360 \t Validation Loss: 40.470470666885376\n",
      "Epoch 67 \t Batch 380 \t Validation Loss: 40.65274254648309\n",
      "Epoch 67 \t Batch 400 \t Validation Loss: 40.15108026027679\n",
      "Epoch 67 \t Batch 420 \t Validation Loss: 40.10773730959211\n",
      "Epoch 67 \t Batch 440 \t Validation Loss: 39.74365147677335\n",
      "Epoch 67 \t Batch 460 \t Validation Loss: 39.985327301854674\n",
      "Epoch 67 \t Batch 480 \t Validation Loss: 40.42041020790736\n",
      "Epoch 67 \t Batch 500 \t Validation Loss: 40.103175857543945\n",
      "Epoch 67 \t Batch 520 \t Validation Loss: 39.84882847529191\n",
      "Epoch 67 \t Batch 540 \t Validation Loss: 39.56227555274963\n",
      "Epoch 67 \t Batch 560 \t Validation Loss: 39.2913152882031\n",
      "Epoch 67 \t Batch 580 \t Validation Loss: 38.91546817648\n",
      "Epoch 67 \t Batch 600 \t Validation Loss: 39.13882672945658\n",
      "Epoch 67 Training Loss: 46.1779223892395 Validation Loss: 39.745309077300035\n",
      "Epoch 67 completed\n",
      "Epoch 68 \t Batch 20 \t Training Loss: 46.30909442901611\n",
      "Epoch 68 \t Batch 40 \t Training Loss: 45.756150531768796\n",
      "Epoch 68 \t Batch 60 \t Training Loss: 45.71549441019694\n",
      "Epoch 68 \t Batch 80 \t Training Loss: 45.908285188674924\n",
      "Epoch 68 \t Batch 100 \t Training Loss: 45.94674175262451\n",
      "Epoch 68 \t Batch 120 \t Training Loss: 45.768852392832436\n",
      "Epoch 68 \t Batch 140 \t Training Loss: 45.867090116228376\n",
      "Epoch 68 \t Batch 160 \t Training Loss: 45.935150146484375\n",
      "Epoch 68 \t Batch 180 \t Training Loss: 45.91309725443522\n",
      "Epoch 68 \t Batch 200 \t Training Loss: 46.03244417190552\n",
      "Epoch 68 \t Batch 220 \t Training Loss: 46.0287070534446\n",
      "Epoch 68 \t Batch 240 \t Training Loss: 46.019217268625894\n",
      "Epoch 68 \t Batch 260 \t Training Loss: 46.054039353590746\n",
      "Epoch 68 \t Batch 280 \t Training Loss: 46.01345169884818\n",
      "Epoch 68 \t Batch 300 \t Training Loss: 46.02675392150879\n",
      "Epoch 68 \t Batch 320 \t Training Loss: 46.11161106824875\n",
      "Epoch 68 \t Batch 340 \t Training Loss: 46.10928241505342\n",
      "Epoch 68 \t Batch 360 \t Training Loss: 46.20633435779148\n",
      "Epoch 68 \t Batch 380 \t Training Loss: 46.164552748830694\n",
      "Epoch 68 \t Batch 400 \t Training Loss: 46.173514184951785\n",
      "Epoch 68 \t Batch 420 \t Training Loss: 46.157371230352496\n",
      "Epoch 68 \t Batch 440 \t Training Loss: 46.1597299749201\n",
      "Epoch 68 \t Batch 460 \t Training Loss: 46.201390283004095\n",
      "Epoch 68 \t Batch 480 \t Training Loss: 46.307024677594505\n",
      "Epoch 68 \t Batch 500 \t Training Loss: 46.364195960998536\n",
      "Epoch 68 \t Batch 520 \t Training Loss: 46.31433382767897\n",
      "Epoch 68 \t Batch 540 \t Training Loss: 46.28884617134377\n",
      "Epoch 68 \t Batch 560 \t Training Loss: 46.2759249482836\n",
      "Epoch 68 \t Batch 580 \t Training Loss: 46.285718233832\n",
      "Epoch 68 \t Batch 600 \t Training Loss: 46.28015627543132\n",
      "Epoch 68 \t Batch 620 \t Training Loss: 46.267011396346554\n",
      "Epoch 68 \t Batch 640 \t Training Loss: 46.22972425818443\n",
      "Epoch 68 \t Batch 660 \t Training Loss: 46.197893870960584\n",
      "Epoch 68 \t Batch 680 \t Training Loss: 46.17872403649723\n",
      "Epoch 68 \t Batch 700 \t Training Loss: 46.19530112675258\n",
      "Epoch 68 \t Batch 720 \t Training Loss: 46.203761148452756\n",
      "Epoch 68 \t Batch 740 \t Training Loss: 46.17235315683726\n",
      "Epoch 68 \t Batch 760 \t Training Loss: 46.17710258584273\n",
      "Epoch 68 \t Batch 780 \t Training Loss: 46.17936216501089\n",
      "Epoch 68 \t Batch 800 \t Training Loss: 46.170239396095276\n",
      "Epoch 68 \t Batch 820 \t Training Loss: 46.15951613914676\n",
      "Epoch 68 \t Batch 840 \t Training Loss: 46.13483490262713\n",
      "Epoch 68 \t Batch 860 \t Training Loss: 46.15243204249892\n",
      "Epoch 68 \t Batch 880 \t Training Loss: 46.15515838536349\n",
      "Epoch 68 \t Batch 900 \t Training Loss: 46.151688202752005\n",
      "Epoch 68 \t Batch 20 \t Validation Loss: 17.13194088935852\n",
      "Epoch 68 \t Batch 40 \t Validation Loss: 19.094373297691345\n",
      "Epoch 68 \t Batch 60 \t Validation Loss: 18.835528802871703\n",
      "Epoch 68 \t Batch 80 \t Validation Loss: 19.349822318553926\n",
      "Epoch 68 \t Batch 100 \t Validation Loss: 21.141306943893433\n",
      "Epoch 68 \t Batch 120 \t Validation Loss: 22.66875893274943\n",
      "Epoch 68 \t Batch 140 \t Validation Loss: 23.384479236602782\n",
      "Epoch 68 \t Batch 160 \t Validation Loss: 25.03137472867966\n",
      "Epoch 68 \t Batch 180 \t Validation Loss: 28.011856280432806\n",
      "Epoch 68 \t Batch 200 \t Validation Loss: 29.192538623809813\n",
      "Epoch 68 \t Batch 220 \t Validation Loss: 30.12224066474221\n",
      "Epoch 68 \t Batch 240 \t Validation Loss: 30.468599859873454\n",
      "Epoch 68 \t Batch 260 \t Validation Loss: 32.22954529982347\n",
      "Epoch 68 \t Batch 280 \t Validation Loss: 33.184015597615925\n",
      "Epoch 68 \t Batch 300 \t Validation Loss: 34.091068747838335\n",
      "Epoch 68 \t Batch 320 \t Validation Loss: 34.53881759941578\n",
      "Epoch 68 \t Batch 340 \t Validation Loss: 34.63248229587779\n",
      "Epoch 68 \t Batch 360 \t Validation Loss: 34.50994941658444\n",
      "Epoch 68 \t Batch 380 \t Validation Loss: 34.774491658963655\n",
      "Epoch 68 \t Batch 400 \t Validation Loss: 34.60768402338028\n",
      "Epoch 68 \t Batch 420 \t Validation Loss: 34.79387578737168\n",
      "Epoch 68 \t Batch 440 \t Validation Loss: 34.70573071349751\n",
      "Epoch 68 \t Batch 460 \t Validation Loss: 35.09686040670975\n",
      "Epoch 68 \t Batch 480 \t Validation Loss: 35.6693265457948\n",
      "Epoch 68 \t Batch 500 \t Validation Loss: 35.46305890846252\n",
      "Epoch 68 \t Batch 520 \t Validation Loss: 35.35138433346381\n",
      "Epoch 68 \t Batch 540 \t Validation Loss: 35.25396499810395\n",
      "Epoch 68 \t Batch 560 \t Validation Loss: 35.194209795338764\n",
      "Epoch 68 \t Batch 580 \t Validation Loss: 35.05278716416195\n",
      "Epoch 68 \t Batch 600 \t Validation Loss: 35.40114809830983\n",
      "Epoch 68 Training Loss: 46.15955737996907 Validation Loss: 36.14901712807742\n",
      "Validation Loss Decreased(22566.078581809998--->22267.79455089569) Saving The Model\n",
      "Epoch 68 completed\n",
      "Epoch 69 \t Batch 20 \t Training Loss: 47.25823783874512\n",
      "Epoch 69 \t Batch 40 \t Training Loss: 46.656387138366696\n",
      "Epoch 69 \t Batch 60 \t Training Loss: 46.100599924723305\n",
      "Epoch 69 \t Batch 80 \t Training Loss: 45.83345522880554\n",
      "Epoch 69 \t Batch 100 \t Training Loss: 46.21533466339111\n",
      "Epoch 69 \t Batch 120 \t Training Loss: 46.241819349924725\n",
      "Epoch 69 \t Batch 140 \t Training Loss: 46.49229052407401\n",
      "Epoch 69 \t Batch 160 \t Training Loss: 46.38465759754181\n",
      "Epoch 69 \t Batch 180 \t Training Loss: 46.31831005944146\n",
      "Epoch 69 \t Batch 200 \t Training Loss: 46.251552753448486\n",
      "Epoch 69 \t Batch 220 \t Training Loss: 46.365697028420186\n",
      "Epoch 69 \t Batch 240 \t Training Loss: 46.2425240834554\n",
      "Epoch 69 \t Batch 260 \t Training Loss: 46.25634062840388\n",
      "Epoch 69 \t Batch 280 \t Training Loss: 46.16768445968628\n",
      "Epoch 69 \t Batch 300 \t Training Loss: 46.19400143941243\n",
      "Epoch 69 \t Batch 320 \t Training Loss: 46.15489671230316\n",
      "Epoch 69 \t Batch 340 \t Training Loss: 46.15182340285357\n",
      "Epoch 69 \t Batch 360 \t Training Loss: 46.17990013758342\n",
      "Epoch 69 \t Batch 380 \t Training Loss: 46.21490734501889\n",
      "Epoch 69 \t Batch 400 \t Training Loss: 46.290935068130494\n",
      "Epoch 69 \t Batch 420 \t Training Loss: 46.30405863807315\n",
      "Epoch 69 \t Batch 440 \t Training Loss: 46.305036813562566\n",
      "Epoch 69 \t Batch 460 \t Training Loss: 46.217065753107484\n",
      "Epoch 69 \t Batch 480 \t Training Loss: 46.15645821889242\n",
      "Epoch 69 \t Batch 500 \t Training Loss: 46.13283856201172\n",
      "Epoch 69 \t Batch 520 \t Training Loss: 46.13916203425481\n",
      "Epoch 69 \t Batch 540 \t Training Loss: 46.13010631137424\n",
      "Epoch 69 \t Batch 560 \t Training Loss: 46.12457276753017\n",
      "Epoch 69 \t Batch 580 \t Training Loss: 46.120472355546624\n",
      "Epoch 69 \t Batch 600 \t Training Loss: 46.091169611612955\n",
      "Epoch 69 \t Batch 620 \t Training Loss: 46.08659580599877\n",
      "Epoch 69 \t Batch 640 \t Training Loss: 46.12286628484726\n",
      "Epoch 69 \t Batch 660 \t Training Loss: 46.11320343017578\n",
      "Epoch 69 \t Batch 680 \t Training Loss: 46.14377031887279\n",
      "Epoch 69 \t Batch 700 \t Training Loss: 46.12987977709089\n",
      "Epoch 69 \t Batch 720 \t Training Loss: 46.10653627183702\n",
      "Epoch 69 \t Batch 740 \t Training Loss: 46.121862865138695\n",
      "Epoch 69 \t Batch 760 \t Training Loss: 46.15610728514822\n",
      "Epoch 69 \t Batch 780 \t Training Loss: 46.16971550476857\n",
      "Epoch 69 \t Batch 800 \t Training Loss: 46.15132777690887\n",
      "Epoch 69 \t Batch 820 \t Training Loss: 46.15450942807081\n",
      "Epoch 69 \t Batch 840 \t Training Loss: 46.13046459924607\n",
      "Epoch 69 \t Batch 860 \t Training Loss: 46.1510554424552\n",
      "Epoch 69 \t Batch 880 \t Training Loss: 46.165511309016836\n",
      "Epoch 69 \t Batch 900 \t Training Loss: 46.16179796006944\n",
      "Epoch 69 \t Batch 20 \t Validation Loss: 17.559342908859254\n",
      "Epoch 69 \t Batch 40 \t Validation Loss: 21.097504687309264\n",
      "Epoch 69 \t Batch 60 \t Validation Loss: 20.364514207839967\n",
      "Epoch 69 \t Batch 80 \t Validation Loss: 21.05100977420807\n",
      "Epoch 69 \t Batch 100 \t Validation Loss: 22.489868888854982\n",
      "Epoch 69 \t Batch 120 \t Validation Loss: 23.70907831986745\n",
      "Epoch 69 \t Batch 140 \t Validation Loss: 24.454196541649953\n",
      "Epoch 69 \t Batch 160 \t Validation Loss: 26.590423148870467\n",
      "Epoch 69 \t Batch 180 \t Validation Loss: 30.28165422545539\n",
      "Epoch 69 \t Batch 200 \t Validation Loss: 31.861634345054625\n",
      "Epoch 69 \t Batch 220 \t Validation Loss: 33.19760850126093\n",
      "Epoch 69 \t Batch 240 \t Validation Loss: 33.78342283566793\n",
      "Epoch 69 \t Batch 260 \t Validation Loss: 35.87735234774076\n",
      "Epoch 69 \t Batch 280 \t Validation Loss: 37.02528784275055\n",
      "Epoch 69 \t Batch 300 \t Validation Loss: 38.15323142051697\n",
      "Epoch 69 \t Batch 320 \t Validation Loss: 38.69240681827068\n",
      "Epoch 69 \t Batch 340 \t Validation Loss: 38.70073082026313\n",
      "Epoch 69 \t Batch 360 \t Validation Loss: 38.63088867134518\n",
      "Epoch 69 \t Batch 380 \t Validation Loss: 38.916843843460086\n",
      "Epoch 69 \t Batch 400 \t Validation Loss: 38.61105843305588\n",
      "Epoch 69 \t Batch 420 \t Validation Loss: 38.63944143794832\n",
      "Epoch 69 \t Batch 440 \t Validation Loss: 38.39873800060966\n",
      "Epoch 69 \t Batch 460 \t Validation Loss: 38.6998245716095\n",
      "Epoch 69 \t Batch 480 \t Validation Loss: 39.19412225286166\n",
      "Epoch 69 \t Batch 500 \t Validation Loss: 38.94934649085999\n",
      "Epoch 69 \t Batch 520 \t Validation Loss: 38.81857260373923\n",
      "Epoch 69 \t Batch 540 \t Validation Loss: 38.635992730105365\n",
      "Epoch 69 \t Batch 560 \t Validation Loss: 38.494985377788545\n",
      "Epoch 69 \t Batch 580 \t Validation Loss: 38.328277119274794\n",
      "Epoch 69 \t Batch 600 \t Validation Loss: 38.56138361771902\n",
      "Epoch 69 Training Loss: 46.135728621820846 Validation Loss: 39.25690009222402\n",
      "Epoch 69 completed\n",
      "Epoch 70 \t Batch 20 \t Training Loss: 46.665808486938474\n",
      "Epoch 70 \t Batch 40 \t Training Loss: 46.53782052993775\n",
      "Epoch 70 \t Batch 60 \t Training Loss: 46.653780174255374\n",
      "Epoch 70 \t Batch 80 \t Training Loss: 46.24922180175781\n",
      "Epoch 70 \t Batch 100 \t Training Loss: 46.123890762329104\n",
      "Epoch 70 \t Batch 120 \t Training Loss: 46.21301240921021\n",
      "Epoch 70 \t Batch 140 \t Training Loss: 46.16918806348528\n",
      "Epoch 70 \t Batch 160 \t Training Loss: 46.14143342971802\n",
      "Epoch 70 \t Batch 180 \t Training Loss: 46.1326270421346\n",
      "Epoch 70 \t Batch 200 \t Training Loss: 46.13588449478149\n",
      "Epoch 70 \t Batch 220 \t Training Loss: 46.12569361600009\n",
      "Epoch 70 \t Batch 240 \t Training Loss: 46.068580118815106\n",
      "Epoch 70 \t Batch 260 \t Training Loss: 46.09591489938589\n",
      "Epoch 70 \t Batch 280 \t Training Loss: 46.07315951756069\n",
      "Epoch 70 \t Batch 300 \t Training Loss: 46.10428384145101\n",
      "Epoch 70 \t Batch 320 \t Training Loss: 46.165136539936064\n",
      "Epoch 70 \t Batch 340 \t Training Loss: 46.15181321536794\n",
      "Epoch 70 \t Batch 360 \t Training Loss: 46.165919134351945\n",
      "Epoch 70 \t Batch 380 \t Training Loss: 46.15435087304366\n",
      "Epoch 70 \t Batch 400 \t Training Loss: 46.24956542015076\n",
      "Epoch 70 \t Batch 420 \t Training Loss: 46.276954178583054\n",
      "Epoch 70 \t Batch 440 \t Training Loss: 46.29356321855025\n",
      "Epoch 70 \t Batch 460 \t Training Loss: 46.2662713755732\n",
      "Epoch 70 \t Batch 480 \t Training Loss: 46.31275641123454\n",
      "Epoch 70 \t Batch 500 \t Training Loss: 46.29530074310303\n",
      "Epoch 70 \t Batch 520 \t Training Loss: 46.29361268557035\n",
      "Epoch 70 \t Batch 540 \t Training Loss: 46.289818431712966\n",
      "Epoch 70 \t Batch 560 \t Training Loss: 46.281356470925466\n",
      "Epoch 70 \t Batch 580 \t Training Loss: 46.27419713776687\n",
      "Epoch 70 \t Batch 600 \t Training Loss: 46.29901357014974\n",
      "Epoch 70 \t Batch 620 \t Training Loss: 46.24575624773579\n",
      "Epoch 70 \t Batch 640 \t Training Loss: 46.18510257005691\n",
      "Epoch 70 \t Batch 660 \t Training Loss: 46.24077417778246\n",
      "Epoch 70 \t Batch 680 \t Training Loss: 46.20186273350435\n",
      "Epoch 70 \t Batch 700 \t Training Loss: 46.225287061418804\n",
      "Epoch 70 \t Batch 720 \t Training Loss: 46.225591044955785\n",
      "Epoch 70 \t Batch 740 \t Training Loss: 46.21659618583885\n",
      "Epoch 70 \t Batch 760 \t Training Loss: 46.233486351213955\n",
      "Epoch 70 \t Batch 780 \t Training Loss: 46.20081854111109\n",
      "Epoch 70 \t Batch 800 \t Training Loss: 46.18110249996185\n",
      "Epoch 70 \t Batch 820 \t Training Loss: 46.198725760855325\n",
      "Epoch 70 \t Batch 840 \t Training Loss: 46.17060461498442\n",
      "Epoch 70 \t Batch 860 \t Training Loss: 46.1707648432532\n",
      "Epoch 70 \t Batch 880 \t Training Loss: 46.15781266472556\n",
      "Epoch 70 \t Batch 900 \t Training Loss: 46.119675657484265\n",
      "Epoch 70 \t Batch 20 \t Validation Loss: 19.514902210235597\n",
      "Epoch 70 \t Batch 40 \t Validation Loss: 21.651131939888\n",
      "Epoch 70 \t Batch 60 \t Validation Loss: 21.42578059832255\n",
      "Epoch 70 \t Batch 80 \t Validation Loss: 22.32884030342102\n",
      "Epoch 70 \t Batch 100 \t Validation Loss: 23.738493194580077\n",
      "Epoch 70 \t Batch 120 \t Validation Loss: 24.906484190622965\n",
      "Epoch 70 \t Batch 140 \t Validation Loss: 25.370779282706124\n",
      "Epoch 70 \t Batch 160 \t Validation Loss: 27.24584184885025\n",
      "Epoch 70 \t Batch 180 \t Validation Loss: 30.823156107796564\n",
      "Epoch 70 \t Batch 200 \t Validation Loss: 32.23750127315521\n",
      "Epoch 70 \t Batch 220 \t Validation Loss: 33.50413167693398\n",
      "Epoch 70 \t Batch 240 \t Validation Loss: 34.011580828825636\n",
      "Epoch 70 \t Batch 260 \t Validation Loss: 36.04632106194129\n",
      "Epoch 70 \t Batch 280 \t Validation Loss: 37.14830341679709\n",
      "Epoch 70 \t Batch 300 \t Validation Loss: 38.25744417508443\n",
      "Epoch 70 \t Batch 320 \t Validation Loss: 38.77447000443935\n",
      "Epoch 70 \t Batch 340 \t Validation Loss: 38.72696812293109\n",
      "Epoch 70 \t Batch 360 \t Validation Loss: 38.57018596066369\n",
      "Epoch 70 \t Batch 380 \t Validation Loss: 38.79478740943106\n",
      "Epoch 70 \t Batch 400 \t Validation Loss: 38.403796331882475\n",
      "Epoch 70 \t Batch 420 \t Validation Loss: 38.45218931833903\n",
      "Epoch 70 \t Batch 440 \t Validation Loss: 38.17264433774081\n",
      "Epoch 70 \t Batch 460 \t Validation Loss: 38.4307686764261\n",
      "Epoch 70 \t Batch 480 \t Validation Loss: 38.92724945942561\n",
      "Epoch 70 \t Batch 500 \t Validation Loss: 38.65917298126221\n",
      "Epoch 70 \t Batch 520 \t Validation Loss: 38.41628370651832\n",
      "Epoch 70 \t Batch 540 \t Validation Loss: 38.170902969219064\n",
      "Epoch 70 \t Batch 560 \t Validation Loss: 37.96648144040789\n",
      "Epoch 70 \t Batch 580 \t Validation Loss: 37.66427674786798\n",
      "Epoch 70 \t Batch 600 \t Validation Loss: 37.90658599217733\n",
      "Epoch 70 Training Loss: 46.1040909079715 Validation Loss: 38.60813829186675\n",
      "Epoch 70 completed\n",
      "Epoch 71 \t Batch 20 \t Training Loss: 45.74554576873779\n",
      "Epoch 71 \t Batch 40 \t Training Loss: 46.08116283416748\n",
      "Epoch 71 \t Batch 60 \t Training Loss: 46.28903636932373\n",
      "Epoch 71 \t Batch 80 \t Training Loss: 46.13487467765808\n",
      "Epoch 71 \t Batch 100 \t Training Loss: 46.32139595031738\n",
      "Epoch 71 \t Batch 120 \t Training Loss: 46.23814223607381\n",
      "Epoch 71 \t Batch 140 \t Training Loss: 46.24696829659598\n",
      "Epoch 71 \t Batch 160 \t Training Loss: 46.155359983444214\n",
      "Epoch 71 \t Batch 180 \t Training Loss: 46.15694046020508\n",
      "Epoch 71 \t Batch 200 \t Training Loss: 46.11397613525391\n",
      "Epoch 71 \t Batch 220 \t Training Loss: 46.124088270013985\n",
      "Epoch 71 \t Batch 240 \t Training Loss: 46.04109547932943\n",
      "Epoch 71 \t Batch 260 \t Training Loss: 46.13063765305739\n",
      "Epoch 71 \t Batch 280 \t Training Loss: 46.10515493665423\n",
      "Epoch 71 \t Batch 300 \t Training Loss: 46.04088692982992\n",
      "Epoch 71 \t Batch 320 \t Training Loss: 46.01678028106689\n",
      "Epoch 71 \t Batch 340 \t Training Loss: 46.07551156212302\n",
      "Epoch 71 \t Batch 360 \t Training Loss: 46.05544271469116\n",
      "Epoch 71 \t Batch 380 \t Training Loss: 46.039319530286285\n",
      "Epoch 71 \t Batch 400 \t Training Loss: 45.96777263641357\n",
      "Epoch 71 \t Batch 420 \t Training Loss: 45.96387048448835\n",
      "Epoch 71 \t Batch 440 \t Training Loss: 46.0312219099565\n",
      "Epoch 71 \t Batch 460 \t Training Loss: 46.03322205750838\n",
      "Epoch 71 \t Batch 480 \t Training Loss: 46.003385655085246\n",
      "Epoch 71 \t Batch 500 \t Training Loss: 46.00662738800049\n",
      "Epoch 71 \t Batch 520 \t Training Loss: 46.02907798473652\n",
      "Epoch 71 \t Batch 540 \t Training Loss: 46.01957971078378\n",
      "Epoch 71 \t Batch 560 \t Training Loss: 45.97613946369716\n",
      "Epoch 71 \t Batch 580 \t Training Loss: 45.964127816825076\n",
      "Epoch 71 \t Batch 600 \t Training Loss: 45.998992379506426\n",
      "Epoch 71 \t Batch 620 \t Training Loss: 46.00154788109564\n",
      "Epoch 71 \t Batch 640 \t Training Loss: 46.03677521944046\n",
      "Epoch 71 \t Batch 660 \t Training Loss: 46.09056491273822\n",
      "Epoch 71 \t Batch 680 \t Training Loss: 46.088878816716814\n",
      "Epoch 71 \t Batch 700 \t Training Loss: 46.069229469299316\n",
      "Epoch 71 \t Batch 720 \t Training Loss: 46.05691984494527\n",
      "Epoch 71 \t Batch 740 \t Training Loss: 46.01777166418127\n",
      "Epoch 71 \t Batch 760 \t Training Loss: 46.04098120739585\n",
      "Epoch 71 \t Batch 780 \t Training Loss: 46.05386308034261\n",
      "Epoch 71 \t Batch 800 \t Training Loss: 46.045988721847536\n",
      "Epoch 71 \t Batch 820 \t Training Loss: 46.04794462715707\n",
      "Epoch 71 \t Batch 840 \t Training Loss: 46.03390428452265\n",
      "Epoch 71 \t Batch 860 \t Training Loss: 46.03757483016613\n",
      "Epoch 71 \t Batch 880 \t Training Loss: 46.01834169734608\n",
      "Epoch 71 \t Batch 900 \t Training Loss: 46.02346451229519\n",
      "Epoch 71 \t Batch 20 \t Validation Loss: 21.812334060668945\n",
      "Epoch 71 \t Batch 40 \t Validation Loss: 23.36904001235962\n",
      "Epoch 71 \t Batch 60 \t Validation Loss: 23.120281966527305\n",
      "Epoch 71 \t Batch 80 \t Validation Loss: 23.776695322990417\n",
      "Epoch 71 \t Batch 100 \t Validation Loss: 24.897098693847656\n",
      "Epoch 71 \t Batch 120 \t Validation Loss: 25.7350146373113\n",
      "Epoch 71 \t Batch 140 \t Validation Loss: 26.03505986758641\n",
      "Epoch 71 \t Batch 160 \t Validation Loss: 27.71576616168022\n",
      "Epoch 71 \t Batch 180 \t Validation Loss: 31.07258399327596\n",
      "Epoch 71 \t Batch 200 \t Validation Loss: 32.32552680015564\n",
      "Epoch 71 \t Batch 220 \t Validation Loss: 33.405151393196796\n",
      "Epoch 71 \t Batch 240 \t Validation Loss: 33.80425541003545\n",
      "Epoch 71 \t Batch 260 \t Validation Loss: 35.75631748346182\n",
      "Epoch 71 \t Batch 280 \t Validation Loss: 36.815900683403015\n",
      "Epoch 71 \t Batch 300 \t Validation Loss: 37.85030515988668\n",
      "Epoch 71 \t Batch 320 \t Validation Loss: 38.31450731754303\n",
      "Epoch 71 \t Batch 340 \t Validation Loss: 38.259056153016935\n",
      "Epoch 71 \t Batch 360 \t Validation Loss: 38.0643113984002\n",
      "Epoch 71 \t Batch 380 \t Validation Loss: 38.263159064242714\n",
      "Epoch 71 \t Batch 400 \t Validation Loss: 37.89239234447479\n",
      "Epoch 71 \t Batch 420 \t Validation Loss: 37.90558622905186\n",
      "Epoch 71 \t Batch 440 \t Validation Loss: 37.62841849760576\n",
      "Epoch 71 \t Batch 460 \t Validation Loss: 37.893803807963494\n",
      "Epoch 71 \t Batch 480 \t Validation Loss: 38.391422001520795\n",
      "Epoch 71 \t Batch 500 \t Validation Loss: 38.113831497192386\n",
      "Epoch 71 \t Batch 520 \t Validation Loss: 37.89897005374615\n",
      "Epoch 71 \t Batch 540 \t Validation Loss: 37.68509190877278\n",
      "Epoch 71 \t Batch 560 \t Validation Loss: 37.499448367527556\n",
      "Epoch 71 \t Batch 580 \t Validation Loss: 37.23026391391097\n",
      "Epoch 71 \t Batch 600 \t Validation Loss: 37.48772885004679\n",
      "Epoch 71 Training Loss: 46.03510606405909 Validation Loss: 38.14046421608368\n",
      "Epoch 71 completed\n",
      "Epoch 72 \t Batch 20 \t Training Loss: 46.72772693634033\n",
      "Epoch 72 \t Batch 40 \t Training Loss: 45.977992343902585\n",
      "Epoch 72 \t Batch 60 \t Training Loss: 46.13856474558512\n",
      "Epoch 72 \t Batch 80 \t Training Loss: 46.19408864974976\n",
      "Epoch 72 \t Batch 100 \t Training Loss: 46.34941562652588\n",
      "Epoch 72 \t Batch 120 \t Training Loss: 46.449722862243654\n",
      "Epoch 72 \t Batch 140 \t Training Loss: 46.6211101259504\n",
      "Epoch 72 \t Batch 160 \t Training Loss: 46.32423529624939\n",
      "Epoch 72 \t Batch 180 \t Training Loss: 46.24299059973823\n",
      "Epoch 72 \t Batch 200 \t Training Loss: 46.173807334899905\n",
      "Epoch 72 \t Batch 220 \t Training Loss: 46.180499614368784\n",
      "Epoch 72 \t Batch 240 \t Training Loss: 46.10563570658366\n",
      "Epoch 72 \t Batch 260 \t Training Loss: 46.15814380645752\n",
      "Epoch 72 \t Batch 280 \t Training Loss: 46.17582174028669\n",
      "Epoch 72 \t Batch 300 \t Training Loss: 46.2131808980306\n",
      "Epoch 72 \t Batch 320 \t Training Loss: 46.22053079605102\n",
      "Epoch 72 \t Batch 340 \t Training Loss: 46.253056806676526\n",
      "Epoch 72 \t Batch 360 \t Training Loss: 46.24730296664768\n",
      "Epoch 72 \t Batch 380 \t Training Loss: 46.248225934881916\n",
      "Epoch 72 \t Batch 400 \t Training Loss: 46.28792950630188\n",
      "Epoch 72 \t Batch 420 \t Training Loss: 46.295814450581865\n",
      "Epoch 72 \t Batch 440 \t Training Loss: 46.27206536206332\n",
      "Epoch 72 \t Batch 460 \t Training Loss: 46.2728469931561\n",
      "Epoch 72 \t Batch 480 \t Training Loss: 46.13182830810547\n",
      "Epoch 72 \t Batch 500 \t Training Loss: 46.07574115753174\n",
      "Epoch 72 \t Batch 520 \t Training Loss: 46.033980479607216\n",
      "Epoch 72 \t Batch 540 \t Training Loss: 46.06857449566876\n",
      "Epoch 72 \t Batch 560 \t Training Loss: 46.031660495485575\n",
      "Epoch 72 \t Batch 580 \t Training Loss: 46.074274286730535\n",
      "Epoch 72 \t Batch 600 \t Training Loss: 46.001813888549805\n",
      "Epoch 72 \t Batch 620 \t Training Loss: 45.97260349027572\n",
      "Epoch 72 \t Batch 640 \t Training Loss: 45.95887733101845\n",
      "Epoch 72 \t Batch 660 \t Training Loss: 45.97417151711204\n",
      "Epoch 72 \t Batch 680 \t Training Loss: 45.95057136872236\n",
      "Epoch 72 \t Batch 700 \t Training Loss: 45.948240618024556\n",
      "Epoch 72 \t Batch 720 \t Training Loss: 45.984221654468115\n",
      "Epoch 72 \t Batch 740 \t Training Loss: 46.001489742382155\n",
      "Epoch 72 \t Batch 760 \t Training Loss: 46.01158823716013\n",
      "Epoch 72 \t Batch 780 \t Training Loss: 45.98761029854799\n",
      "Epoch 72 \t Batch 800 \t Training Loss: 45.99415057182312\n",
      "Epoch 72 \t Batch 820 \t Training Loss: 45.98762658282024\n",
      "Epoch 72 \t Batch 840 \t Training Loss: 45.993533770243324\n",
      "Epoch 72 \t Batch 860 \t Training Loss: 46.0064763313116\n",
      "Epoch 72 \t Batch 880 \t Training Loss: 46.03000701557506\n",
      "Epoch 72 \t Batch 900 \t Training Loss: 46.02444527943929\n",
      "Epoch 72 \t Batch 20 \t Validation Loss: 21.351940059661864\n",
      "Epoch 72 \t Batch 40 \t Validation Loss: 22.245907044410707\n",
      "Epoch 72 \t Batch 60 \t Validation Loss: 22.401793877283733\n",
      "Epoch 72 \t Batch 80 \t Validation Loss: 22.871798551082613\n",
      "Epoch 72 \t Batch 100 \t Validation Loss: 24.29703791618347\n",
      "Epoch 72 \t Batch 120 \t Validation Loss: 25.31550752321879\n",
      "Epoch 72 \t Batch 140 \t Validation Loss: 25.711467157091413\n",
      "Epoch 72 \t Batch 160 \t Validation Loss: 27.43401074409485\n",
      "Epoch 72 \t Batch 180 \t Validation Loss: 30.913595056533815\n",
      "Epoch 72 \t Batch 200 \t Validation Loss: 32.24327575206757\n",
      "Epoch 72 \t Batch 220 \t Validation Loss: 33.423340038819745\n",
      "Epoch 72 \t Batch 240 \t Validation Loss: 33.88232024510702\n",
      "Epoch 72 \t Batch 260 \t Validation Loss: 35.86239582942083\n",
      "Epoch 72 \t Batch 280 \t Validation Loss: 36.95454283441816\n",
      "Epoch 72 \t Batch 300 \t Validation Loss: 38.0649982770284\n",
      "Epoch 72 \t Batch 320 \t Validation Loss: 38.53793489336967\n",
      "Epoch 72 \t Batch 340 \t Validation Loss: 38.45195635066313\n",
      "Epoch 72 \t Batch 360 \t Validation Loss: 38.25404314464993\n",
      "Epoch 72 \t Batch 380 \t Validation Loss: 38.441332054138186\n",
      "Epoch 72 \t Batch 400 \t Validation Loss: 38.025340156555174\n",
      "Epoch 72 \t Batch 420 \t Validation Loss: 38.048986357734314\n",
      "Epoch 72 \t Batch 440 \t Validation Loss: 37.77182727727023\n",
      "Epoch 72 \t Batch 460 \t Validation Loss: 37.99225013152413\n",
      "Epoch 72 \t Batch 480 \t Validation Loss: 38.46567192872365\n",
      "Epoch 72 \t Batch 500 \t Validation Loss: 38.16433973693848\n",
      "Epoch 72 \t Batch 520 \t Validation Loss: 37.918441457014815\n",
      "Epoch 72 \t Batch 540 \t Validation Loss: 37.669407516055635\n",
      "Epoch 72 \t Batch 560 \t Validation Loss: 37.46911295822689\n",
      "Epoch 72 \t Batch 580 \t Validation Loss: 37.19310002162539\n",
      "Epoch 72 \t Batch 600 \t Validation Loss: 37.42415651957194\n",
      "Epoch 72 Training Loss: 46.03045196699602 Validation Loss: 38.089132742448285\n",
      "Epoch 72 completed\n",
      "Epoch 73 \t Batch 20 \t Training Loss: 46.89062004089355\n",
      "Epoch 73 \t Batch 40 \t Training Loss: 46.501688957214355\n",
      "Epoch 73 \t Batch 60 \t Training Loss: 46.45048224131266\n",
      "Epoch 73 \t Batch 80 \t Training Loss: 46.31293225288391\n",
      "Epoch 73 \t Batch 100 \t Training Loss: 46.45606170654297\n",
      "Epoch 73 \t Batch 120 \t Training Loss: 46.31801274617513\n",
      "Epoch 73 \t Batch 140 \t Training Loss: 46.395682907104494\n",
      "Epoch 73 \t Batch 160 \t Training Loss: 46.34221346378327\n",
      "Epoch 73 \t Batch 180 \t Training Loss: 46.180485513475205\n",
      "Epoch 73 \t Batch 200 \t Training Loss: 46.25629377365112\n",
      "Epoch 73 \t Batch 220 \t Training Loss: 46.21293910633434\n",
      "Epoch 73 \t Batch 240 \t Training Loss: 46.274988412857056\n",
      "Epoch 73 \t Batch 260 \t Training Loss: 46.25827187758226\n",
      "Epoch 73 \t Batch 280 \t Training Loss: 46.26125834328788\n",
      "Epoch 73 \t Batch 300 \t Training Loss: 46.299518559773766\n",
      "Epoch 73 \t Batch 320 \t Training Loss: 46.248018217086795\n",
      "Epoch 73 \t Batch 340 \t Training Loss: 46.205596160888675\n",
      "Epoch 73 \t Batch 360 \t Training Loss: 46.152988794114854\n",
      "Epoch 73 \t Batch 380 \t Training Loss: 46.11939139115183\n",
      "Epoch 73 \t Batch 400 \t Training Loss: 46.165704298019406\n",
      "Epoch 73 \t Batch 420 \t Training Loss: 46.10821834745861\n",
      "Epoch 73 \t Batch 440 \t Training Loss: 46.06461555307562\n",
      "Epoch 73 \t Batch 460 \t Training Loss: 46.03541968801747\n",
      "Epoch 73 \t Batch 480 \t Training Loss: 45.99515967369079\n",
      "Epoch 73 \t Batch 500 \t Training Loss: 45.99716667175293\n",
      "Epoch 73 \t Batch 520 \t Training Loss: 45.98688073525062\n",
      "Epoch 73 \t Batch 540 \t Training Loss: 46.00693460393835\n",
      "Epoch 73 \t Batch 560 \t Training Loss: 45.968938323429654\n",
      "Epoch 73 \t Batch 580 \t Training Loss: 45.9792368066722\n",
      "Epoch 73 \t Batch 600 \t Training Loss: 45.97586852391561\n",
      "Epoch 73 \t Batch 620 \t Training Loss: 45.98382819391066\n",
      "Epoch 73 \t Batch 640 \t Training Loss: 45.978526616096495\n",
      "Epoch 73 \t Batch 660 \t Training Loss: 45.98784998113459\n",
      "Epoch 73 \t Batch 680 \t Training Loss: 45.97429796106675\n",
      "Epoch 73 \t Batch 700 \t Training Loss: 45.997480109078545\n",
      "Epoch 73 \t Batch 720 \t Training Loss: 45.989296701219345\n",
      "Epoch 73 \t Batch 740 \t Training Loss: 46.00564787065661\n",
      "Epoch 73 \t Batch 760 \t Training Loss: 46.01033855739393\n",
      "Epoch 73 \t Batch 780 \t Training Loss: 46.01465550936185\n",
      "Epoch 73 \t Batch 800 \t Training Loss: 46.02197802066803\n",
      "Epoch 73 \t Batch 820 \t Training Loss: 46.06082740411526\n",
      "Epoch 73 \t Batch 840 \t Training Loss: 46.08010502769834\n",
      "Epoch 73 \t Batch 860 \t Training Loss: 46.09914779663086\n",
      "Epoch 73 \t Batch 880 \t Training Loss: 46.07811119773171\n",
      "Epoch 73 \t Batch 900 \t Training Loss: 46.070409821404354\n",
      "Epoch 73 \t Batch 20 \t Validation Loss: 16.964055156707765\n",
      "Epoch 73 \t Batch 40 \t Validation Loss: 20.216699802875517\n",
      "Epoch 73 \t Batch 60 \t Validation Loss: 19.803913489977518\n",
      "Epoch 73 \t Batch 80 \t Validation Loss: 20.437487441301347\n",
      "Epoch 73 \t Batch 100 \t Validation Loss: 22.113111863136293\n",
      "Epoch 73 \t Batch 120 \t Validation Loss: 23.67932545741399\n",
      "Epoch 73 \t Batch 140 \t Validation Loss: 24.43802434716906\n",
      "Epoch 73 \t Batch 160 \t Validation Loss: 26.59869660437107\n",
      "Epoch 73 \t Batch 180 \t Validation Loss: 30.582145852512785\n",
      "Epoch 73 \t Batch 200 \t Validation Loss: 32.21198242425918\n",
      "Epoch 73 \t Batch 220 \t Validation Loss: 33.640919609503314\n",
      "Epoch 73 \t Batch 240 \t Validation Loss: 34.25118136604627\n",
      "Epoch 73 \t Batch 260 \t Validation Loss: 36.50789026663853\n",
      "Epoch 73 \t Batch 280 \t Validation Loss: 37.73524947336742\n",
      "Epoch 73 \t Batch 300 \t Validation Loss: 38.995931073824565\n",
      "Epoch 73 \t Batch 320 \t Validation Loss: 39.563768507540225\n",
      "Epoch 73 \t Batch 340 \t Validation Loss: 39.50690800302169\n",
      "Epoch 73 \t Batch 360 \t Validation Loss: 39.37258837090598\n",
      "Epoch 73 \t Batch 380 \t Validation Loss: 39.598114672460056\n",
      "Epoch 73 \t Batch 400 \t Validation Loss: 39.162720428705214\n",
      "Epoch 73 \t Batch 420 \t Validation Loss: 39.164588429814295\n",
      "Epoch 73 \t Batch 440 \t Validation Loss: 38.82595315521414\n",
      "Epoch 73 \t Batch 460 \t Validation Loss: 39.05674209698387\n",
      "Epoch 73 \t Batch 480 \t Validation Loss: 39.53368159631888\n",
      "Epoch 73 \t Batch 500 \t Validation Loss: 39.25884946155548\n",
      "Epoch 73 \t Batch 520 \t Validation Loss: 39.00070952727244\n",
      "Epoch 73 \t Batch 540 \t Validation Loss: 38.74428030031699\n",
      "Epoch 73 \t Batch 560 \t Validation Loss: 38.51166906441961\n",
      "Epoch 73 \t Batch 580 \t Validation Loss: 38.236773498304956\n",
      "Epoch 73 \t Batch 600 \t Validation Loss: 38.444186516602834\n",
      "Epoch 73 Training Loss: 46.05444298913721 Validation Loss: 39.09440762114215\n",
      "Epoch 73 completed\n",
      "Epoch 74 \t Batch 20 \t Training Loss: 46.24335689544678\n",
      "Epoch 74 \t Batch 40 \t Training Loss: 46.31273603439331\n",
      "Epoch 74 \t Batch 60 \t Training Loss: 46.262793922424315\n",
      "Epoch 74 \t Batch 80 \t Training Loss: 46.427654123306276\n",
      "Epoch 74 \t Batch 100 \t Training Loss: 46.37709991455078\n",
      "Epoch 74 \t Batch 120 \t Training Loss: 46.24182252883911\n",
      "Epoch 74 \t Batch 140 \t Training Loss: 46.35232650211879\n",
      "Epoch 74 \t Batch 160 \t Training Loss: 46.096100378036496\n",
      "Epoch 74 \t Batch 180 \t Training Loss: 46.19987138112386\n",
      "Epoch 74 \t Batch 200 \t Training Loss: 46.1764786529541\n",
      "Epoch 74 \t Batch 220 \t Training Loss: 46.17739994742654\n",
      "Epoch 74 \t Batch 240 \t Training Loss: 46.21935796737671\n",
      "Epoch 74 \t Batch 260 \t Training Loss: 46.27883708660419\n",
      "Epoch 74 \t Batch 280 \t Training Loss: 46.37676830291748\n",
      "Epoch 74 \t Batch 300 \t Training Loss: 46.4519650777181\n",
      "Epoch 74 \t Batch 320 \t Training Loss: 46.34172282218933\n",
      "Epoch 74 \t Batch 340 \t Training Loss: 46.28489778182086\n",
      "Epoch 74 \t Batch 360 \t Training Loss: 46.37622283299764\n",
      "Epoch 74 \t Batch 380 \t Training Loss: 46.38546398564389\n",
      "Epoch 74 \t Batch 400 \t Training Loss: 46.34010263442993\n",
      "Epoch 74 \t Batch 420 \t Training Loss: 46.29789291563488\n",
      "Epoch 74 \t Batch 440 \t Training Loss: 46.24846628362482\n",
      "Epoch 74 \t Batch 460 \t Training Loss: 46.209209301160726\n",
      "Epoch 74 \t Batch 480 \t Training Loss: 46.16631433169047\n",
      "Epoch 74 \t Batch 500 \t Training Loss: 46.139054901123046\n",
      "Epoch 74 \t Batch 520 \t Training Loss: 46.119443878760706\n",
      "Epoch 74 \t Batch 540 \t Training Loss: 46.13590176193802\n",
      "Epoch 74 \t Batch 560 \t Training Loss: 46.11756700788226\n",
      "Epoch 74 \t Batch 580 \t Training Loss: 46.1215977241253\n",
      "Epoch 74 \t Batch 600 \t Training Loss: 46.133036092122396\n",
      "Epoch 74 \t Batch 620 \t Training Loss: 46.11700189651982\n",
      "Epoch 74 \t Batch 640 \t Training Loss: 46.12390891313553\n",
      "Epoch 74 \t Batch 660 \t Training Loss: 46.15130733721184\n",
      "Epoch 74 \t Batch 680 \t Training Loss: 46.13496552074657\n",
      "Epoch 74 \t Batch 700 \t Training Loss: 46.15918577466692\n",
      "Epoch 74 \t Batch 720 \t Training Loss: 46.09021500481499\n",
      "Epoch 74 \t Batch 740 \t Training Loss: 46.098678016662596\n",
      "Epoch 74 \t Batch 760 \t Training Loss: 46.07551509957565\n",
      "Epoch 74 \t Batch 780 \t Training Loss: 46.044804802919046\n",
      "Epoch 74 \t Batch 800 \t Training Loss: 46.03208611488342\n",
      "Epoch 74 \t Batch 820 \t Training Loss: 46.038673968431425\n",
      "Epoch 74 \t Batch 840 \t Training Loss: 46.0466627348037\n",
      "Epoch 74 \t Batch 860 \t Training Loss: 46.0308975663296\n",
      "Epoch 74 \t Batch 880 \t Training Loss: 45.9987250328064\n",
      "Epoch 74 \t Batch 900 \t Training Loss: 45.994277771843805\n",
      "Epoch 74 \t Batch 20 \t Validation Loss: 21.00343370437622\n",
      "Epoch 74 \t Batch 40 \t Validation Loss: 21.78372151851654\n",
      "Epoch 74 \t Batch 60 \t Validation Loss: 21.855440870920816\n",
      "Epoch 74 \t Batch 80 \t Validation Loss: 22.204053890705108\n",
      "Epoch 74 \t Batch 100 \t Validation Loss: 23.593043184280397\n",
      "Epoch 74 \t Batch 120 \t Validation Loss: 24.872613008817037\n",
      "Epoch 74 \t Batch 140 \t Validation Loss: 25.40922067505973\n",
      "Epoch 74 \t Batch 160 \t Validation Loss: 27.171321445703505\n",
      "Epoch 74 \t Batch 180 \t Validation Loss: 30.753376065360175\n",
      "Epoch 74 \t Batch 200 \t Validation Loss: 32.145162014961244\n",
      "Epoch 74 \t Batch 220 \t Validation Loss: 33.23616992343556\n",
      "Epoch 74 \t Batch 240 \t Validation Loss: 33.66158185799917\n",
      "Epoch 74 \t Batch 260 \t Validation Loss: 35.66410115315364\n",
      "Epoch 74 \t Batch 280 \t Validation Loss: 36.73082614966801\n",
      "Epoch 74 \t Batch 300 \t Validation Loss: 37.8460929775238\n",
      "Epoch 74 \t Batch 320 \t Validation Loss: 38.314531716704366\n",
      "Epoch 74 \t Batch 340 \t Validation Loss: 38.24636131454916\n",
      "Epoch 74 \t Batch 360 \t Validation Loss: 38.0685341808531\n",
      "Epoch 74 \t Batch 380 \t Validation Loss: 38.262141862668486\n",
      "Epoch 74 \t Batch 400 \t Validation Loss: 37.88318577528\n",
      "Epoch 74 \t Batch 420 \t Validation Loss: 37.90328045118423\n",
      "Epoch 74 \t Batch 440 \t Validation Loss: 37.61934146230871\n",
      "Epoch 74 \t Batch 460 \t Validation Loss: 37.83948265780573\n",
      "Epoch 74 \t Batch 480 \t Validation Loss: 38.34631843765577\n",
      "Epoch 74 \t Batch 500 \t Validation Loss: 38.05627500343323\n",
      "Epoch 74 \t Batch 520 \t Validation Loss: 37.82605336079231\n",
      "Epoch 74 \t Batch 540 \t Validation Loss: 37.61135319073995\n",
      "Epoch 74 \t Batch 560 \t Validation Loss: 37.454013124534065\n",
      "Epoch 74 \t Batch 580 \t Validation Loss: 37.268706514095435\n",
      "Epoch 74 \t Batch 600 \t Validation Loss: 37.51832220872243\n",
      "Epoch 74 Training Loss: 46.00759356466479 Validation Loss: 38.21308003617572\n",
      "Epoch 74 completed\n",
      "Epoch 75 \t Batch 20 \t Training Loss: 44.879541015625\n",
      "Epoch 75 \t Batch 40 \t Training Loss: 44.83848400115967\n",
      "Epoch 75 \t Batch 60 \t Training Loss: 45.04209912618001\n",
      "Epoch 75 \t Batch 80 \t Training Loss: 45.00201907157898\n",
      "Epoch 75 \t Batch 100 \t Training Loss: 44.90049125671387\n",
      "Epoch 75 \t Batch 120 \t Training Loss: 45.05032447179158\n",
      "Epoch 75 \t Batch 140 \t Training Loss: 45.24799807412284\n",
      "Epoch 75 \t Batch 160 \t Training Loss: 45.32981543540954\n",
      "Epoch 75 \t Batch 180 \t Training Loss: 45.38873920440674\n",
      "Epoch 75 \t Batch 200 \t Training Loss: 45.42277250289917\n",
      "Epoch 75 \t Batch 220 \t Training Loss: 45.39834778525613\n",
      "Epoch 75 \t Batch 240 \t Training Loss: 45.352941354115806\n",
      "Epoch 75 \t Batch 260 \t Training Loss: 45.41490164536696\n",
      "Epoch 75 \t Batch 280 \t Training Loss: 45.43596933909825\n",
      "Epoch 75 \t Batch 300 \t Training Loss: 45.527148729960125\n",
      "Epoch 75 \t Batch 320 \t Training Loss: 45.55863258838654\n",
      "Epoch 75 \t Batch 340 \t Training Loss: 45.57891979217529\n",
      "Epoch 75 \t Batch 360 \t Training Loss: 45.61549676259359\n",
      "Epoch 75 \t Batch 380 \t Training Loss: 45.75173547644364\n",
      "Epoch 75 \t Batch 400 \t Training Loss: 45.77624543190002\n",
      "Epoch 75 \t Batch 420 \t Training Loss: 45.81805333637056\n",
      "Epoch 75 \t Batch 440 \t Training Loss: 45.87511056553234\n",
      "Epoch 75 \t Batch 460 \t Training Loss: 45.877030704332434\n",
      "Epoch 75 \t Batch 480 \t Training Loss: 45.84284099737803\n",
      "Epoch 75 \t Batch 500 \t Training Loss: 45.8217816696167\n",
      "Epoch 75 \t Batch 520 \t Training Loss: 45.79636163711548\n",
      "Epoch 75 \t Batch 540 \t Training Loss: 45.80936503233733\n",
      "Epoch 75 \t Batch 560 \t Training Loss: 45.83661146163941\n",
      "Epoch 75 \t Batch 580 \t Training Loss: 45.87515671828697\n",
      "Epoch 75 \t Batch 600 \t Training Loss: 45.92621831258138\n",
      "Epoch 75 \t Batch 620 \t Training Loss: 45.91165199279785\n",
      "Epoch 75 \t Batch 640 \t Training Loss: 45.88986758589745\n",
      "Epoch 75 \t Batch 660 \t Training Loss: 45.9241445830374\n",
      "Epoch 75 \t Batch 680 \t Training Loss: 45.91225836136762\n",
      "Epoch 75 \t Batch 700 \t Training Loss: 45.940261028834755\n",
      "Epoch 75 \t Batch 720 \t Training Loss: 45.93694622251723\n",
      "Epoch 75 \t Batch 740 \t Training Loss: 45.943607252997325\n",
      "Epoch 75 \t Batch 760 \t Training Loss: 45.93465822119462\n",
      "Epoch 75 \t Batch 780 \t Training Loss: 45.97264508467454\n",
      "Epoch 75 \t Batch 800 \t Training Loss: 45.96786166667938\n",
      "Epoch 75 \t Batch 820 \t Training Loss: 45.98272906047542\n",
      "Epoch 75 \t Batch 840 \t Training Loss: 45.96704647881644\n",
      "Epoch 75 \t Batch 860 \t Training Loss: 45.9436889249225\n",
      "Epoch 75 \t Batch 880 \t Training Loss: 45.97191565253518\n",
      "Epoch 75 \t Batch 900 \t Training Loss: 45.954662259419756\n",
      "Epoch 75 \t Batch 20 \t Validation Loss: 25.994501495361327\n",
      "Epoch 75 \t Batch 40 \t Validation Loss: 26.50143449306488\n",
      "Epoch 75 \t Batch 60 \t Validation Loss: 26.32303156852722\n",
      "Epoch 75 \t Batch 80 \t Validation Loss: 26.47770447731018\n",
      "Epoch 75 \t Batch 100 \t Validation Loss: 27.021981201171876\n",
      "Epoch 75 \t Batch 120 \t Validation Loss: 27.611935289700828\n",
      "Epoch 75 \t Batch 140 \t Validation Loss: 27.74743381908962\n",
      "Epoch 75 \t Batch 160 \t Validation Loss: 29.193039590120314\n",
      "Epoch 75 \t Batch 180 \t Validation Loss: 32.34834877120124\n",
      "Epoch 75 \t Batch 200 \t Validation Loss: 33.470834403038026\n",
      "Epoch 75 \t Batch 220 \t Validation Loss: 34.3915613044392\n",
      "Epoch 75 \t Batch 240 \t Validation Loss: 34.68825887441635\n",
      "Epoch 75 \t Batch 260 \t Validation Loss: 36.491167167516856\n",
      "Epoch 75 \t Batch 280 \t Validation Loss: 37.37493270805904\n",
      "Epoch 75 \t Batch 300 \t Validation Loss: 38.416378761927284\n",
      "Epoch 75 \t Batch 320 \t Validation Loss: 38.85752310156822\n",
      "Epoch 75 \t Batch 340 \t Validation Loss: 38.75907328549553\n",
      "Epoch 75 \t Batch 360 \t Validation Loss: 38.558890671200224\n",
      "Epoch 75 \t Batch 380 \t Validation Loss: 38.72502152794286\n",
      "Epoch 75 \t Batch 400 \t Validation Loss: 38.33581276893616\n",
      "Epoch 75 \t Batch 420 \t Validation Loss: 38.33290835562207\n",
      "Epoch 75 \t Batch 440 \t Validation Loss: 38.03284013271332\n",
      "Epoch 75 \t Batch 460 \t Validation Loss: 38.27351623825405\n",
      "Epoch 75 \t Batch 480 \t Validation Loss: 38.767716974020004\n",
      "Epoch 75 \t Batch 500 \t Validation Loss: 38.465906732559205\n",
      "Epoch 75 \t Batch 520 \t Validation Loss: 38.26521771871126\n",
      "Epoch 75 \t Batch 540 \t Validation Loss: 38.030004536664045\n",
      "Epoch 75 \t Batch 560 \t Validation Loss: 37.833474608830045\n",
      "Epoch 75 \t Batch 580 \t Validation Loss: 37.60348803092693\n",
      "Epoch 75 \t Batch 600 \t Validation Loss: 37.8192138004303\n",
      "Epoch 75 Training Loss: 45.96114796146572 Validation Loss: 38.49151255867698\n",
      "Epoch 75 completed\n",
      "Epoch 76 \t Batch 20 \t Training Loss: 44.74197006225586\n",
      "Epoch 76 \t Batch 40 \t Training Loss: 45.56991558074951\n",
      "Epoch 76 \t Batch 60 \t Training Loss: 45.43417542775472\n",
      "Epoch 76 \t Batch 80 \t Training Loss: 45.67200403213501\n",
      "Epoch 76 \t Batch 100 \t Training Loss: 45.87525356292725\n",
      "Epoch 76 \t Batch 120 \t Training Loss: 45.78669315973918\n",
      "Epoch 76 \t Batch 140 \t Training Loss: 45.87885284423828\n",
      "Epoch 76 \t Batch 160 \t Training Loss: 45.70311694145202\n",
      "Epoch 76 \t Batch 180 \t Training Loss: 45.70541583167182\n",
      "Epoch 76 \t Batch 200 \t Training Loss: 45.697364044189456\n",
      "Epoch 76 \t Batch 220 \t Training Loss: 45.79436109716242\n",
      "Epoch 76 \t Batch 240 \t Training Loss: 45.87073726654053\n",
      "Epoch 76 \t Batch 260 \t Training Loss: 45.961567115783694\n",
      "Epoch 76 \t Batch 280 \t Training Loss: 45.95700454711914\n",
      "Epoch 76 \t Batch 300 \t Training Loss: 45.96196564992269\n",
      "Epoch 76 \t Batch 320 \t Training Loss: 45.836986446380614\n",
      "Epoch 76 \t Batch 340 \t Training Loss: 45.91619189767277\n",
      "Epoch 76 \t Batch 360 \t Training Loss: 45.95765161514282\n",
      "Epoch 76 \t Batch 380 \t Training Loss: 45.87073794917056\n",
      "Epoch 76 \t Batch 400 \t Training Loss: 45.84146018981934\n",
      "Epoch 76 \t Batch 420 \t Training Loss: 45.79127175467355\n",
      "Epoch 76 \t Batch 440 \t Training Loss: 45.750116443634035\n",
      "Epoch 76 \t Batch 460 \t Training Loss: 45.7615794306216\n",
      "Epoch 76 \t Batch 480 \t Training Loss: 45.77849686940511\n",
      "Epoch 76 \t Batch 500 \t Training Loss: 45.83100157165527\n",
      "Epoch 76 \t Batch 520 \t Training Loss: 45.88946951352633\n",
      "Epoch 76 \t Batch 540 \t Training Loss: 45.8713596979777\n",
      "Epoch 76 \t Batch 560 \t Training Loss: 45.872677721296036\n",
      "Epoch 76 \t Batch 580 \t Training Loss: 45.81740062976706\n",
      "Epoch 76 \t Batch 600 \t Training Loss: 45.804313036600746\n",
      "Epoch 76 \t Batch 620 \t Training Loss: 45.84986174183507\n",
      "Epoch 76 \t Batch 640 \t Training Loss: 45.88907681703567\n",
      "Epoch 76 \t Batch 660 \t Training Loss: 45.88045087294145\n",
      "Epoch 76 \t Batch 680 \t Training Loss: 45.89397425932043\n",
      "Epoch 76 \t Batch 700 \t Training Loss: 45.89146607535226\n",
      "Epoch 76 \t Batch 720 \t Training Loss: 45.890656312306724\n",
      "Epoch 76 \t Batch 740 \t Training Loss: 45.89195955121839\n",
      "Epoch 76 \t Batch 760 \t Training Loss: 45.913148854908194\n",
      "Epoch 76 \t Batch 780 \t Training Loss: 45.899828407091974\n",
      "Epoch 76 \t Batch 800 \t Training Loss: 45.852390117645264\n",
      "Epoch 76 \t Batch 820 \t Training Loss: 45.862861614692505\n",
      "Epoch 76 \t Batch 840 \t Training Loss: 45.91364670708066\n",
      "Epoch 76 \t Batch 860 \t Training Loss: 45.91458057137423\n",
      "Epoch 76 \t Batch 880 \t Training Loss: 45.9287407875061\n",
      "Epoch 76 \t Batch 900 \t Training Loss: 45.90816822052002\n",
      "Epoch 76 \t Batch 20 \t Validation Loss: 21.328369474411012\n",
      "Epoch 76 \t Batch 40 \t Validation Loss: 22.79219431877136\n",
      "Epoch 76 \t Batch 60 \t Validation Loss: 22.564936542510985\n",
      "Epoch 76 \t Batch 80 \t Validation Loss: 23.13907766342163\n",
      "Epoch 76 \t Batch 100 \t Validation Loss: 24.297067203521728\n",
      "Epoch 76 \t Batch 120 \t Validation Loss: 25.393853775660197\n",
      "Epoch 76 \t Batch 140 \t Validation Loss: 25.87550424848284\n",
      "Epoch 76 \t Batch 160 \t Validation Loss: 27.747476410865783\n",
      "Epoch 76 \t Batch 180 \t Validation Loss: 31.335653999116687\n",
      "Epoch 76 \t Batch 200 \t Validation Loss: 32.75990959644318\n",
      "Epoch 76 \t Batch 220 \t Validation Loss: 33.993661390651354\n",
      "Epoch 76 \t Batch 240 \t Validation Loss: 34.479746933778124\n",
      "Epoch 76 \t Batch 260 \t Validation Loss: 36.57486646725581\n",
      "Epoch 76 \t Batch 280 \t Validation Loss: 37.71029270717076\n",
      "Epoch 76 \t Batch 300 \t Validation Loss: 38.81459192276001\n",
      "Epoch 76 \t Batch 320 \t Validation Loss: 39.2686618745327\n",
      "Epoch 76 \t Batch 340 \t Validation Loss: 39.18177456575282\n",
      "Epoch 76 \t Batch 360 \t Validation Loss: 39.01898908085293\n",
      "Epoch 76 \t Batch 380 \t Validation Loss: 39.23816120248092\n",
      "Epoch 76 \t Batch 400 \t Validation Loss: 38.82813296794891\n",
      "Epoch 76 \t Batch 420 \t Validation Loss: 38.80046243213472\n",
      "Epoch 76 \t Batch 440 \t Validation Loss: 38.48919689872048\n",
      "Epoch 76 \t Batch 460 \t Validation Loss: 38.70801132865574\n",
      "Epoch 76 \t Batch 480 \t Validation Loss: 39.17663252353668\n",
      "Epoch 76 \t Batch 500 \t Validation Loss: 38.8648112487793\n",
      "Epoch 76 \t Batch 520 \t Validation Loss: 38.67212822987483\n",
      "Epoch 76 \t Batch 540 \t Validation Loss: 38.45753778881497\n",
      "Epoch 76 \t Batch 560 \t Validation Loss: 38.29891545772553\n",
      "Epoch 76 \t Batch 580 \t Validation Loss: 38.10408670491186\n",
      "Epoch 76 \t Batch 600 \t Validation Loss: 38.33839781125386\n",
      "Epoch 76 Training Loss: 45.93981200606118 Validation Loss: 39.02670118096587\n",
      "Epoch 76 completed\n",
      "Epoch 77 \t Batch 20 \t Training Loss: 46.293533897399904\n",
      "Epoch 77 \t Batch 40 \t Training Loss: 45.72378168106079\n",
      "Epoch 77 \t Batch 60 \t Training Loss: 45.58781960805257\n",
      "Epoch 77 \t Batch 80 \t Training Loss: 45.642314195632935\n",
      "Epoch 77 \t Batch 100 \t Training Loss: 45.74908393859863\n",
      "Epoch 77 \t Batch 120 \t Training Loss: 45.906432978312175\n",
      "Epoch 77 \t Batch 140 \t Training Loss: 45.882130541120254\n",
      "Epoch 77 \t Batch 160 \t Training Loss: 46.187704992294314\n",
      "Epoch 77 \t Batch 180 \t Training Loss: 46.067231051127116\n",
      "Epoch 77 \t Batch 200 \t Training Loss: 46.02602893829346\n",
      "Epoch 77 \t Batch 220 \t Training Loss: 45.952613067626956\n",
      "Epoch 77 \t Batch 240 \t Training Loss: 46.02946802775065\n",
      "Epoch 77 \t Batch 260 \t Training Loss: 46.02849148970384\n",
      "Epoch 77 \t Batch 280 \t Training Loss: 46.032537419455394\n",
      "Epoch 77 \t Batch 300 \t Training Loss: 46.028969332377116\n",
      "Epoch 77 \t Batch 320 \t Training Loss: 46.077040183544156\n",
      "Epoch 77 \t Batch 340 \t Training Loss: 46.02749404907227\n",
      "Epoch 77 \t Batch 360 \t Training Loss: 45.99792579015096\n",
      "Epoch 77 \t Batch 380 \t Training Loss: 45.90612673508493\n",
      "Epoch 77 \t Batch 400 \t Training Loss: 45.9090385723114\n",
      "Epoch 77 \t Batch 420 \t Training Loss: 45.86585838681176\n",
      "Epoch 77 \t Batch 440 \t Training Loss: 45.856694438240744\n",
      "Epoch 77 \t Batch 460 \t Training Loss: 45.878374049974525\n",
      "Epoch 77 \t Batch 480 \t Training Loss: 45.88160512447357\n",
      "Epoch 77 \t Batch 500 \t Training Loss: 45.91569275665283\n",
      "Epoch 77 \t Batch 520 \t Training Loss: 45.898229158841644\n",
      "Epoch 77 \t Batch 540 \t Training Loss: 45.8990557635272\n",
      "Epoch 77 \t Batch 560 \t Training Loss: 45.869857454299925\n",
      "Epoch 77 \t Batch 580 \t Training Loss: 45.83626449848043\n",
      "Epoch 77 \t Batch 600 \t Training Loss: 45.88065660476685\n",
      "Epoch 77 \t Batch 620 \t Training Loss: 45.90333685721121\n",
      "Epoch 77 \t Batch 640 \t Training Loss: 45.93212042450905\n",
      "Epoch 77 \t Batch 660 \t Training Loss: 45.92311584010269\n",
      "Epoch 77 \t Batch 680 \t Training Loss: 45.963355086831484\n",
      "Epoch 77 \t Batch 700 \t Training Loss: 45.94647212437221\n",
      "Epoch 77 \t Batch 720 \t Training Loss: 45.965591494242354\n",
      "Epoch 77 \t Batch 740 \t Training Loss: 45.97215802476213\n",
      "Epoch 77 \t Batch 760 \t Training Loss: 45.96282290408486\n",
      "Epoch 77 \t Batch 780 \t Training Loss: 45.93932053003556\n",
      "Epoch 77 \t Batch 800 \t Training Loss: 45.949118089675906\n",
      "Epoch 77 \t Batch 820 \t Training Loss: 45.952355570909454\n",
      "Epoch 77 \t Batch 840 \t Training Loss: 45.96361096245902\n",
      "Epoch 77 \t Batch 860 \t Training Loss: 45.915932216200716\n",
      "Epoch 77 \t Batch 880 \t Training Loss: 45.90665338689631\n",
      "Epoch 77 \t Batch 900 \t Training Loss: 45.926841523912216\n",
      "Epoch 77 \t Batch 20 \t Validation Loss: 14.378045463562012\n",
      "Epoch 77 \t Batch 40 \t Validation Loss: 17.544942688941955\n",
      "Epoch 77 \t Batch 60 \t Validation Loss: 17.265874973932902\n",
      "Epoch 77 \t Batch 80 \t Validation Loss: 18.184579455852507\n",
      "Epoch 77 \t Batch 100 \t Validation Loss: 20.446060190200807\n",
      "Epoch 77 \t Batch 120 \t Validation Loss: 22.02577633857727\n",
      "Epoch 77 \t Batch 140 \t Validation Loss: 22.951694079807826\n",
      "Epoch 77 \t Batch 160 \t Validation Loss: 24.94815477132797\n",
      "Epoch 77 \t Batch 180 \t Validation Loss: 28.231912485758464\n",
      "Epoch 77 \t Batch 200 \t Validation Loss: 29.591637029647828\n",
      "Epoch 77 \t Batch 220 \t Validation Loss: 30.69183287187056\n",
      "Epoch 77 \t Batch 240 \t Validation Loss: 31.153675774733227\n",
      "Epoch 77 \t Batch 260 \t Validation Loss: 33.10036887755761\n",
      "Epoch 77 \t Batch 280 \t Validation Loss: 34.19874109881265\n",
      "Epoch 77 \t Batch 300 \t Validation Loss: 35.22325825691223\n",
      "Epoch 77 \t Batch 320 \t Validation Loss: 35.69455129802227\n",
      "Epoch 77 \t Batch 340 \t Validation Loss: 35.7673660699059\n",
      "Epoch 77 \t Batch 360 \t Validation Loss: 35.70298267735375\n",
      "Epoch 77 \t Batch 380 \t Validation Loss: 36.01187600336577\n",
      "Epoch 77 \t Batch 400 \t Validation Loss: 35.81508511781693\n",
      "Epoch 77 \t Batch 420 \t Validation Loss: 35.9440091019585\n",
      "Epoch 77 \t Batch 440 \t Validation Loss: 35.80606518875469\n",
      "Epoch 77 \t Batch 460 \t Validation Loss: 36.16052640624668\n",
      "Epoch 77 \t Batch 480 \t Validation Loss: 36.70831427375476\n",
      "Epoch 77 \t Batch 500 \t Validation Loss: 36.4826486415863\n",
      "Epoch 77 \t Batch 520 \t Validation Loss: 36.4360226722864\n",
      "Epoch 77 \t Batch 540 \t Validation Loss: 36.324389943370115\n",
      "Epoch 77 \t Batch 560 \t Validation Loss: 36.2595300759588\n",
      "Epoch 77 \t Batch 580 \t Validation Loss: 36.194448295132865\n",
      "Epoch 77 \t Batch 600 \t Validation Loss: 36.48985560576121\n",
      "Epoch 77 Training Loss: 45.92353418947176 Validation Loss: 37.25162579641714\n",
      "Epoch 77 completed\n",
      "Epoch 78 \t Batch 20 \t Training Loss: 46.37069091796875\n",
      "Epoch 78 \t Batch 40 \t Training Loss: 46.38680648803711\n",
      "Epoch 78 \t Batch 60 \t Training Loss: 46.0965638478597\n",
      "Epoch 78 \t Batch 80 \t Training Loss: 46.090529441833496\n",
      "Epoch 78 \t Batch 100 \t Training Loss: 45.67766845703125\n",
      "Epoch 78 \t Batch 120 \t Training Loss: 45.69264612197876\n",
      "Epoch 78 \t Batch 140 \t Training Loss: 45.74647617340088\n",
      "Epoch 78 \t Batch 160 \t Training Loss: 45.69793701171875\n",
      "Epoch 78 \t Batch 180 \t Training Loss: 45.61192527347141\n",
      "Epoch 78 \t Batch 200 \t Training Loss: 45.7954549407959\n",
      "Epoch 78 \t Batch 220 \t Training Loss: 45.87101052024148\n",
      "Epoch 78 \t Batch 240 \t Training Loss: 45.91208769480387\n",
      "Epoch 78 \t Batch 260 \t Training Loss: 45.7472108400785\n",
      "Epoch 78 \t Batch 280 \t Training Loss: 45.65443992614746\n",
      "Epoch 78 \t Batch 300 \t Training Loss: 45.77997353871663\n",
      "Epoch 78 \t Batch 320 \t Training Loss: 45.889687061309814\n",
      "Epoch 78 \t Batch 340 \t Training Loss: 45.81699431924259\n",
      "Epoch 78 \t Batch 360 \t Training Loss: 45.83170644972059\n",
      "Epoch 78 \t Batch 380 \t Training Loss: 45.824011280662134\n",
      "Epoch 78 \t Batch 400 \t Training Loss: 45.86194738388062\n",
      "Epoch 78 \t Batch 420 \t Training Loss: 45.82525533040364\n",
      "Epoch 78 \t Batch 440 \t Training Loss: 45.840275062214246\n",
      "Epoch 78 \t Batch 460 \t Training Loss: 45.827216546431835\n",
      "Epoch 78 \t Batch 480 \t Training Loss: 45.894250837961835\n",
      "Epoch 78 \t Batch 500 \t Training Loss: 45.88184757232666\n",
      "Epoch 78 \t Batch 520 \t Training Loss: 45.926969066032996\n",
      "Epoch 78 \t Batch 540 \t Training Loss: 45.923230255974666\n",
      "Epoch 78 \t Batch 560 \t Training Loss: 45.90455631528582\n",
      "Epoch 78 \t Batch 580 \t Training Loss: 45.86327085823849\n",
      "Epoch 78 \t Batch 600 \t Training Loss: 45.86108992894491\n",
      "Epoch 78 \t Batch 620 \t Training Loss: 45.889080859768775\n",
      "Epoch 78 \t Batch 640 \t Training Loss: 45.88443056344986\n",
      "Epoch 78 \t Batch 660 \t Training Loss: 45.872817837108265\n",
      "Epoch 78 \t Batch 680 \t Training Loss: 45.86638304205502\n",
      "Epoch 78 \t Batch 700 \t Training Loss: 45.82797710963658\n",
      "Epoch 78 \t Batch 720 \t Training Loss: 45.838966867658826\n",
      "Epoch 78 \t Batch 740 \t Training Loss: 45.863758855252655\n",
      "Epoch 78 \t Batch 760 \t Training Loss: 45.886550401386465\n",
      "Epoch 78 \t Batch 780 \t Training Loss: 45.865377435928735\n",
      "Epoch 78 \t Batch 800 \t Training Loss: 45.87050443649292\n",
      "Epoch 78 \t Batch 820 \t Training Loss: 45.931915362288315\n",
      "Epoch 78 \t Batch 840 \t Training Loss: 45.92164779844738\n",
      "Epoch 78 \t Batch 860 \t Training Loss: 45.91257908399715\n",
      "Epoch 78 \t Batch 880 \t Training Loss: 45.92284505150535\n",
      "Epoch 78 \t Batch 900 \t Training Loss: 45.90609349568685\n",
      "Epoch 78 \t Batch 20 \t Validation Loss: 23.141295385360717\n",
      "Epoch 78 \t Batch 40 \t Validation Loss: 23.686216521263123\n",
      "Epoch 78 \t Batch 60 \t Validation Loss: 23.793142127990723\n",
      "Epoch 78 \t Batch 80 \t Validation Loss: 24.27726970911026\n",
      "Epoch 78 \t Batch 100 \t Validation Loss: 25.12379448890686\n",
      "Epoch 78 \t Batch 120 \t Validation Loss: 26.018994800249736\n",
      "Epoch 78 \t Batch 140 \t Validation Loss: 26.362347269058226\n",
      "Epoch 78 \t Batch 160 \t Validation Loss: 28.105773907899856\n",
      "Epoch 78 \t Batch 180 \t Validation Loss: 31.533372916115656\n",
      "Epoch 78 \t Batch 200 \t Validation Loss: 32.86515995502472\n",
      "Epoch 78 \t Batch 220 \t Validation Loss: 33.983436129309915\n",
      "Epoch 78 \t Batch 240 \t Validation Loss: 34.38745985428492\n",
      "Epoch 78 \t Batch 260 \t Validation Loss: 36.39013194671044\n",
      "Epoch 78 \t Batch 280 \t Validation Loss: 37.458427381515506\n",
      "Epoch 78 \t Batch 300 \t Validation Loss: 38.54609692891439\n",
      "Epoch 78 \t Batch 320 \t Validation Loss: 39.008868008852005\n",
      "Epoch 78 \t Batch 340 \t Validation Loss: 38.93080810098087\n",
      "Epoch 78 \t Batch 360 \t Validation Loss: 38.76126262876723\n",
      "Epoch 78 \t Batch 380 \t Validation Loss: 38.96400454169825\n",
      "Epoch 78 \t Batch 400 \t Validation Loss: 38.563194756507876\n",
      "Epoch 78 \t Batch 420 \t Validation Loss: 38.548025762467155\n",
      "Epoch 78 \t Batch 440 \t Validation Loss: 38.23764788020741\n",
      "Epoch 78 \t Batch 460 \t Validation Loss: 38.46658338049184\n",
      "Epoch 78 \t Batch 480 \t Validation Loss: 38.95334704319636\n",
      "Epoch 78 \t Batch 500 \t Validation Loss: 38.662114917755126\n",
      "Epoch 78 \t Batch 520 \t Validation Loss: 38.45055595361269\n",
      "Epoch 78 \t Batch 540 \t Validation Loss: 38.246688003893254\n",
      "Epoch 78 \t Batch 560 \t Validation Loss: 38.09278156587056\n",
      "Epoch 78 \t Batch 580 \t Validation Loss: 37.92373541634658\n",
      "Epoch 78 \t Batch 600 \t Validation Loss: 38.15303439617157\n",
      "Epoch 78 Training Loss: 45.92300360959385 Validation Loss: 38.85731239287884\n",
      "Epoch 78 completed\n",
      "Epoch 79 \t Batch 20 \t Training Loss: 44.31928672790527\n",
      "Epoch 79 \t Batch 40 \t Training Loss: 44.89905128479004\n",
      "Epoch 79 \t Batch 60 \t Training Loss: 45.64284076690674\n",
      "Epoch 79 \t Batch 80 \t Training Loss: 45.284605741500854\n",
      "Epoch 79 \t Batch 100 \t Training Loss: 45.34233261108398\n",
      "Epoch 79 \t Batch 120 \t Training Loss: 45.58667914072672\n",
      "Epoch 79 \t Batch 140 \t Training Loss: 45.681646728515624\n",
      "Epoch 79 \t Batch 160 \t Training Loss: 45.66210958957672\n",
      "Epoch 79 \t Batch 180 \t Training Loss: 45.664067628648546\n",
      "Epoch 79 \t Batch 200 \t Training Loss: 45.651189403533934\n",
      "Epoch 79 \t Batch 220 \t Training Loss: 45.81519607197155\n",
      "Epoch 79 \t Batch 240 \t Training Loss: 45.85562980969747\n",
      "Epoch 79 \t Batch 260 \t Training Loss: 45.85714357816256\n",
      "Epoch 79 \t Batch 280 \t Training Loss: 45.89452238082886\n",
      "Epoch 79 \t Batch 300 \t Training Loss: 45.869901504516605\n",
      "Epoch 79 \t Batch 320 \t Training Loss: 45.858856523036955\n",
      "Epoch 79 \t Batch 340 \t Training Loss: 45.859021400002874\n",
      "Epoch 79 \t Batch 360 \t Training Loss: 45.83531787660387\n",
      "Epoch 79 \t Batch 380 \t Training Loss: 45.82012127324155\n",
      "Epoch 79 \t Batch 400 \t Training Loss: 45.82050485610962\n",
      "Epoch 79 \t Batch 420 \t Training Loss: 45.856612323579334\n",
      "Epoch 79 \t Batch 440 \t Training Loss: 45.80923653515902\n",
      "Epoch 79 \t Batch 460 \t Training Loss: 45.81536900064219\n",
      "Epoch 79 \t Batch 480 \t Training Loss: 45.78611947695414\n",
      "Epoch 79 \t Batch 500 \t Training Loss: 45.79040087127686\n",
      "Epoch 79 \t Batch 520 \t Training Loss: 45.88325835741483\n",
      "Epoch 79 \t Batch 540 \t Training Loss: 45.90065046239782\n",
      "Epoch 79 \t Batch 560 \t Training Loss: 45.905310249328615\n",
      "Epoch 79 \t Batch 580 \t Training Loss: 45.91387905910097\n",
      "Epoch 79 \t Batch 600 \t Training Loss: 45.893077869415286\n",
      "Epoch 79 \t Batch 620 \t Training Loss: 45.861119282630185\n",
      "Epoch 79 \t Batch 640 \t Training Loss: 45.90591455101967\n",
      "Epoch 79 \t Batch 660 \t Training Loss: 45.88731842619\n",
      "Epoch 79 \t Batch 680 \t Training Loss: 45.881193374185\n",
      "Epoch 79 \t Batch 700 \t Training Loss: 45.883495363507954\n",
      "Epoch 79 \t Batch 720 \t Training Loss: 45.8932395723131\n",
      "Epoch 79 \t Batch 740 \t Training Loss: 45.88967543421565\n",
      "Epoch 79 \t Batch 760 \t Training Loss: 45.8756119025381\n",
      "Epoch 79 \t Batch 780 \t Training Loss: 45.889157662024864\n",
      "Epoch 79 \t Batch 800 \t Training Loss: 45.89262234210968\n",
      "Epoch 79 \t Batch 820 \t Training Loss: 45.92548818355653\n",
      "Epoch 79 \t Batch 840 \t Training Loss: 45.89360480535598\n",
      "Epoch 79 \t Batch 860 \t Training Loss: 45.89098986692207\n",
      "Epoch 79 \t Batch 880 \t Training Loss: 45.879443207654084\n",
      "Epoch 79 \t Batch 900 \t Training Loss: 45.86862558576796\n",
      "Epoch 79 \t Batch 20 \t Validation Loss: 24.505459117889405\n",
      "Epoch 79 \t Batch 40 \t Validation Loss: 25.739871859550476\n",
      "Epoch 79 \t Batch 60 \t Validation Loss: 25.633701292673745\n",
      "Epoch 79 \t Batch 80 \t Validation Loss: 26.127228307724\n",
      "Epoch 79 \t Batch 100 \t Validation Loss: 26.548246250152587\n",
      "Epoch 79 \t Batch 120 \t Validation Loss: 27.166061282157898\n",
      "Epoch 79 \t Batch 140 \t Validation Loss: 27.366454185758318\n",
      "Epoch 79 \t Batch 160 \t Validation Loss: 29.199920457601547\n",
      "Epoch 79 \t Batch 180 \t Validation Loss: 32.97826200591193\n",
      "Epoch 79 \t Batch 200 \t Validation Loss: 34.374302382469175\n",
      "Epoch 79 \t Batch 220 \t Validation Loss: 35.67226971279491\n",
      "Epoch 79 \t Batch 240 \t Validation Loss: 36.154651113351186\n",
      "Epoch 79 \t Batch 260 \t Validation Loss: 38.275872586323665\n",
      "Epoch 79 \t Batch 280 \t Validation Loss: 39.346978078569684\n",
      "Epoch 79 \t Batch 300 \t Validation Loss: 40.54159852345784\n",
      "Epoch 79 \t Batch 320 \t Validation Loss: 41.026812601089475\n",
      "Epoch 79 \t Batch 340 \t Validation Loss: 40.89420413970947\n",
      "Epoch 79 \t Batch 360 \t Validation Loss: 40.71713814735413\n",
      "Epoch 79 \t Batch 380 \t Validation Loss: 40.874605901617755\n",
      "Epoch 79 \t Batch 400 \t Validation Loss: 40.366842980384824\n",
      "Epoch 79 \t Batch 420 \t Validation Loss: 40.295734287443615\n",
      "Epoch 79 \t Batch 440 \t Validation Loss: 39.91335800777782\n",
      "Epoch 79 \t Batch 460 \t Validation Loss: 40.053142705171005\n",
      "Epoch 79 \t Batch 480 \t Validation Loss: 40.501685913403826\n",
      "Epoch 79 \t Batch 500 \t Validation Loss: 40.182213897705076\n",
      "Epoch 79 \t Batch 520 \t Validation Loss: 39.87506622167734\n",
      "Epoch 79 \t Batch 540 \t Validation Loss: 39.60469262864854\n",
      "Epoch 79 \t Batch 560 \t Validation Loss: 39.39041478293283\n",
      "Epoch 79 \t Batch 580 \t Validation Loss: 39.14028382794611\n",
      "Epoch 79 \t Batch 600 \t Validation Loss: 39.33143053372701\n",
      "Epoch 79 Training Loss: 45.87459812975502 Validation Loss: 39.98750731852147\n",
      "Epoch 79 completed\n",
      "Epoch 80 \t Batch 20 \t Training Loss: 46.84276237487793\n",
      "Epoch 80 \t Batch 40 \t Training Loss: 45.61250123977661\n",
      "Epoch 80 \t Batch 60 \t Training Loss: 45.49345830281575\n",
      "Epoch 80 \t Batch 80 \t Training Loss: 45.69582433700562\n",
      "Epoch 80 \t Batch 100 \t Training Loss: 45.67744491577148\n",
      "Epoch 80 \t Batch 120 \t Training Loss: 45.668172963460286\n",
      "Epoch 80 \t Batch 140 \t Training Loss: 45.667847415379114\n",
      "Epoch 80 \t Batch 160 \t Training Loss: 45.750315284729005\n",
      "Epoch 80 \t Batch 180 \t Training Loss: 45.64714643690321\n",
      "Epoch 80 \t Batch 200 \t Training Loss: 45.5355345916748\n",
      "Epoch 80 \t Batch 220 \t Training Loss: 45.69451359835538\n",
      "Epoch 80 \t Batch 240 \t Training Loss: 45.547596073150636\n",
      "Epoch 80 \t Batch 260 \t Training Loss: 45.60708092909593\n",
      "Epoch 80 \t Batch 280 \t Training Loss: 45.62642741884504\n",
      "Epoch 80 \t Batch 300 \t Training Loss: 45.637300720214846\n",
      "Epoch 80 \t Batch 320 \t Training Loss: 45.755859684944156\n",
      "Epoch 80 \t Batch 340 \t Training Loss: 45.76949210447424\n",
      "Epoch 80 \t Batch 360 \t Training Loss: 45.736685053507486\n",
      "Epoch 80 \t Batch 380 \t Training Loss: 45.83083743045204\n",
      "Epoch 80 \t Batch 400 \t Training Loss: 45.86470093727112\n",
      "Epoch 80 \t Batch 420 \t Training Loss: 45.88821095966158\n",
      "Epoch 80 \t Batch 440 \t Training Loss: 45.84156544425271\n",
      "Epoch 80 \t Batch 460 \t Training Loss: 45.82633851922077\n",
      "Epoch 80 \t Batch 480 \t Training Loss: 45.79972903728485\n",
      "Epoch 80 \t Batch 500 \t Training Loss: 45.81449897766113\n",
      "Epoch 80 \t Batch 520 \t Training Loss: 45.82648944121141\n",
      "Epoch 80 \t Batch 540 \t Training Loss: 45.89221691555447\n",
      "Epoch 80 \t Batch 560 \t Training Loss: 45.83277237074716\n",
      "Epoch 80 \t Batch 580 \t Training Loss: 45.78858209149591\n",
      "Epoch 80 \t Batch 600 \t Training Loss: 45.779629936218264\n",
      "Epoch 80 \t Batch 620 \t Training Loss: 45.76076120561169\n",
      "Epoch 80 \t Batch 640 \t Training Loss: 45.752476239204405\n",
      "Epoch 80 \t Batch 660 \t Training Loss: 45.758986305468014\n",
      "Epoch 80 \t Batch 680 \t Training Loss: 45.75603511473712\n",
      "Epoch 80 \t Batch 700 \t Training Loss: 45.786571001325335\n",
      "Epoch 80 \t Batch 720 \t Training Loss: 45.816999430126614\n",
      "Epoch 80 \t Batch 740 \t Training Loss: 45.81222657899599\n",
      "Epoch 80 \t Batch 760 \t Training Loss: 45.77717766510813\n",
      "Epoch 80 \t Batch 780 \t Training Loss: 45.80708269461607\n",
      "Epoch 80 \t Batch 800 \t Training Loss: 45.755749859809875\n",
      "Epoch 80 \t Batch 820 \t Training Loss: 45.771570359206784\n",
      "Epoch 80 \t Batch 840 \t Training Loss: 45.81147313345046\n",
      "Epoch 80 \t Batch 860 \t Training Loss: 45.77600267543349\n",
      "Epoch 80 \t Batch 880 \t Training Loss: 45.77801049839366\n",
      "Epoch 80 \t Batch 900 \t Training Loss: 45.78443463643392\n",
      "Epoch 80 \t Batch 20 \t Validation Loss: 18.24895052909851\n",
      "Epoch 80 \t Batch 40 \t Validation Loss: 20.633902168273927\n",
      "Epoch 80 \t Batch 60 \t Validation Loss: 20.17905395825704\n",
      "Epoch 80 \t Batch 80 \t Validation Loss: 20.87044734954834\n",
      "Epoch 80 \t Batch 100 \t Validation Loss: 22.69264362335205\n",
      "Epoch 80 \t Batch 120 \t Validation Loss: 23.863038023312885\n",
      "Epoch 80 \t Batch 140 \t Validation Loss: 24.51239378111703\n",
      "Epoch 80 \t Batch 160 \t Validation Loss: 26.54794251322746\n",
      "Epoch 80 \t Batch 180 \t Validation Loss: 30.121197313732573\n",
      "Epoch 80 \t Batch 200 \t Validation Loss: 31.6589657831192\n",
      "Epoch 80 \t Batch 220 \t Validation Loss: 32.94329139102589\n",
      "Epoch 80 \t Batch 240 \t Validation Loss: 33.48401700258255\n",
      "Epoch 80 \t Batch 260 \t Validation Loss: 35.53956635915316\n",
      "Epoch 80 \t Batch 280 \t Validation Loss: 36.649742589678084\n",
      "Epoch 80 \t Batch 300 \t Validation Loss: 37.75237822214763\n",
      "Epoch 80 \t Batch 320 \t Validation Loss: 38.281628674268724\n",
      "Epoch 80 \t Batch 340 \t Validation Loss: 38.25606089760275\n",
      "Epoch 80 \t Batch 360 \t Validation Loss: 38.134956979751585\n",
      "Epoch 80 \t Batch 380 \t Validation Loss: 38.3898145123532\n",
      "Epoch 80 \t Batch 400 \t Validation Loss: 38.03935447692871\n",
      "Epoch 80 \t Batch 420 \t Validation Loss: 38.08363767351423\n",
      "Epoch 80 \t Batch 440 \t Validation Loss: 37.819206940044054\n",
      "Epoch 80 \t Batch 460 \t Validation Loss: 38.09658846647843\n",
      "Epoch 80 \t Batch 480 \t Validation Loss: 38.59276715914408\n",
      "Epoch 80 \t Batch 500 \t Validation Loss: 38.33736678314209\n",
      "Epoch 80 \t Batch 520 \t Validation Loss: 38.15530181848086\n",
      "Epoch 80 \t Batch 540 \t Validation Loss: 37.963356758047034\n",
      "Epoch 80 \t Batch 560 \t Validation Loss: 37.82497284242085\n",
      "Epoch 80 \t Batch 580 \t Validation Loss: 37.65930718224624\n",
      "Epoch 80 \t Batch 600 \t Validation Loss: 37.9002214606603\n",
      "Epoch 80 Training Loss: 45.810637579099435 Validation Loss: 38.610081404834595\n",
      "Epoch 80 completed\n",
      "Epoch 81 \t Batch 20 \t Training Loss: 46.04818477630615\n",
      "Epoch 81 \t Batch 40 \t Training Loss: 46.81020240783691\n",
      "Epoch 81 \t Batch 60 \t Training Loss: 46.16439723968506\n",
      "Epoch 81 \t Batch 80 \t Training Loss: 45.6435884475708\n",
      "Epoch 81 \t Batch 100 \t Training Loss: 45.67534118652344\n",
      "Epoch 81 \t Batch 120 \t Training Loss: 45.781624507904056\n",
      "Epoch 81 \t Batch 140 \t Training Loss: 45.87035124642508\n",
      "Epoch 81 \t Batch 160 \t Training Loss: 45.853226232528684\n",
      "Epoch 81 \t Batch 180 \t Training Loss: 45.76093525356717\n",
      "Epoch 81 \t Batch 200 \t Training Loss: 45.784356842041014\n",
      "Epoch 81 \t Batch 220 \t Training Loss: 45.89432452808727\n",
      "Epoch 81 \t Batch 240 \t Training Loss: 45.95782335599264\n",
      "Epoch 81 \t Batch 260 \t Training Loss: 45.924136660649225\n",
      "Epoch 81 \t Batch 280 \t Training Loss: 45.86278907230922\n",
      "Epoch 81 \t Batch 300 \t Training Loss: 45.86819357554118\n",
      "Epoch 81 \t Batch 320 \t Training Loss: 45.88215461969376\n",
      "Epoch 81 \t Batch 340 \t Training Loss: 45.838814017351936\n",
      "Epoch 81 \t Batch 360 \t Training Loss: 45.76043710708618\n",
      "Epoch 81 \t Batch 380 \t Training Loss: 45.78123533349288\n",
      "Epoch 81 \t Batch 400 \t Training Loss: 45.799986896514895\n",
      "Epoch 81 \t Batch 420 \t Training Loss: 45.870357622419085\n",
      "Epoch 81 \t Batch 440 \t Training Loss: 45.813707022233444\n",
      "Epoch 81 \t Batch 460 \t Training Loss: 45.75268582053806\n",
      "Epoch 81 \t Batch 480 \t Training Loss: 45.79887065092723\n",
      "Epoch 81 \t Batch 500 \t Training Loss: 45.77074465179443\n",
      "Epoch 81 \t Batch 520 \t Training Loss: 45.78194649036114\n",
      "Epoch 81 \t Batch 540 \t Training Loss: 45.73552335103353\n",
      "Epoch 81 \t Batch 560 \t Training Loss: 45.80217125075204\n",
      "Epoch 81 \t Batch 580 \t Training Loss: 45.733471219293\n",
      "Epoch 81 \t Batch 600 \t Training Loss: 45.757476603190106\n",
      "Epoch 81 \t Batch 620 \t Training Loss: 45.811985065091044\n",
      "Epoch 81 \t Batch 640 \t Training Loss: 45.783957469463346\n",
      "Epoch 81 \t Batch 660 \t Training Loss: 45.81666174223929\n",
      "Epoch 81 \t Batch 680 \t Training Loss: 45.81715108647066\n",
      "Epoch 81 \t Batch 700 \t Training Loss: 45.815522646222796\n",
      "Epoch 81 \t Batch 720 \t Training Loss: 45.83690350850423\n",
      "Epoch 81 \t Batch 740 \t Training Loss: 45.83990379539696\n",
      "Epoch 81 \t Batch 760 \t Training Loss: 45.85343327271311\n",
      "Epoch 81 \t Batch 780 \t Training Loss: 45.85150387103741\n",
      "Epoch 81 \t Batch 800 \t Training Loss: 45.815081286430356\n",
      "Epoch 81 \t Batch 820 \t Training Loss: 45.81219522429676\n",
      "Epoch 81 \t Batch 840 \t Training Loss: 45.800835400535945\n",
      "Epoch 81 \t Batch 860 \t Training Loss: 45.79271312536195\n",
      "Epoch 81 \t Batch 880 \t Training Loss: 45.78254634250294\n",
      "Epoch 81 \t Batch 900 \t Training Loss: 45.79606600443522\n",
      "Epoch 81 \t Batch 20 \t Validation Loss: 17.72460150718689\n",
      "Epoch 81 \t Batch 40 \t Validation Loss: 20.19445412158966\n",
      "Epoch 81 \t Batch 60 \t Validation Loss: 19.819504960378012\n",
      "Epoch 81 \t Batch 80 \t Validation Loss: 20.504621303081514\n",
      "Epoch 81 \t Batch 100 \t Validation Loss: 22.303639001846314\n",
      "Epoch 81 \t Batch 120 \t Validation Loss: 23.537356503804524\n",
      "Epoch 81 \t Batch 140 \t Validation Loss: 24.220962946755545\n",
      "Epoch 81 \t Batch 160 \t Validation Loss: 26.312452006340028\n",
      "Epoch 81 \t Batch 180 \t Validation Loss: 30.023851304584078\n",
      "Epoch 81 \t Batch 200 \t Validation Loss: 31.580658164024353\n",
      "Epoch 81 \t Batch 220 \t Validation Loss: 32.905257723548196\n",
      "Epoch 81 \t Batch 240 \t Validation Loss: 33.46702487866084\n",
      "Epoch 81 \t Batch 260 \t Validation Loss: 35.60496661112859\n",
      "Epoch 81 \t Batch 280 \t Validation Loss: 36.76997992651803\n",
      "Epoch 81 \t Batch 300 \t Validation Loss: 37.945217018127444\n",
      "Epoch 81 \t Batch 320 \t Validation Loss: 38.49750018417835\n",
      "Epoch 81 \t Batch 340 \t Validation Loss: 38.47437050482806\n",
      "Epoch 81 \t Batch 360 \t Validation Loss: 38.361603718333775\n",
      "Epoch 81 \t Batch 380 \t Validation Loss: 38.61587604472512\n",
      "Epoch 81 \t Batch 400 \t Validation Loss: 38.221157815456394\n",
      "Epoch 81 \t Batch 420 \t Validation Loss: 38.25039253916059\n",
      "Epoch 81 \t Batch 440 \t Validation Loss: 37.95638336485082\n",
      "Epoch 81 \t Batch 460 \t Validation Loss: 38.2677040659863\n",
      "Epoch 81 \t Batch 480 \t Validation Loss: 38.77518963217735\n",
      "Epoch 81 \t Batch 500 \t Validation Loss: 38.53510758781433\n",
      "Epoch 81 \t Batch 520 \t Validation Loss: 38.33709051975837\n",
      "Epoch 81 \t Batch 540 \t Validation Loss: 38.124065549285326\n",
      "Epoch 81 \t Batch 560 \t Validation Loss: 37.942635057653696\n",
      "Epoch 81 \t Batch 580 \t Validation Loss: 37.71206408040277\n",
      "Epoch 81 \t Batch 600 \t Validation Loss: 37.94936857064565\n",
      "Epoch 81 Training Loss: 45.816841920043544 Validation Loss: 38.61205472729423\n",
      "Epoch 81 completed\n",
      "Epoch 82 \t Batch 20 \t Training Loss: 45.153595542907716\n",
      "Epoch 82 \t Batch 40 \t Training Loss: 45.26352834701538\n",
      "Epoch 82 \t Batch 60 \t Training Loss: 45.41664740244548\n",
      "Epoch 82 \t Batch 80 \t Training Loss: 45.24137940406799\n",
      "Epoch 82 \t Batch 100 \t Training Loss: 45.397225799560545\n",
      "Epoch 82 \t Batch 120 \t Training Loss: 45.50264056523641\n",
      "Epoch 82 \t Batch 140 \t Training Loss: 45.48165231432233\n",
      "Epoch 82 \t Batch 160 \t Training Loss: 45.53681223392486\n",
      "Epoch 82 \t Batch 180 \t Training Loss: 45.64280868106418\n",
      "Epoch 82 \t Batch 200 \t Training Loss: 45.76717163085937\n",
      "Epoch 82 \t Batch 220 \t Training Loss: 45.77685983831232\n",
      "Epoch 82 \t Batch 240 \t Training Loss: 45.763311306635536\n",
      "Epoch 82 \t Batch 260 \t Training Loss: 45.83830980154184\n",
      "Epoch 82 \t Batch 280 \t Training Loss: 45.86995692934309\n",
      "Epoch 82 \t Batch 300 \t Training Loss: 45.822825558980306\n",
      "Epoch 82 \t Batch 320 \t Training Loss: 45.9144146323204\n",
      "Epoch 82 \t Batch 340 \t Training Loss: 45.90376264908735\n",
      "Epoch 82 \t Batch 360 \t Training Loss: 45.9673865530226\n",
      "Epoch 82 \t Batch 380 \t Training Loss: 45.93043600383558\n",
      "Epoch 82 \t Batch 400 \t Training Loss: 45.93285276412964\n",
      "Epoch 82 \t Batch 420 \t Training Loss: 45.91070522126697\n",
      "Epoch 82 \t Batch 440 \t Training Loss: 45.872163625196976\n",
      "Epoch 82 \t Batch 460 \t Training Loss: 45.868029271001404\n",
      "Epoch 82 \t Batch 480 \t Training Loss: 45.861213000615436\n",
      "Epoch 82 \t Batch 500 \t Training Loss: 45.82627333068848\n",
      "Epoch 82 \t Batch 520 \t Training Loss: 45.86892336331881\n",
      "Epoch 82 \t Batch 540 \t Training Loss: 45.81376293323658\n",
      "Epoch 82 \t Batch 560 \t Training Loss: 45.82539993694851\n",
      "Epoch 82 \t Batch 580 \t Training Loss: 45.869159987877154\n",
      "Epoch 82 \t Batch 600 \t Training Loss: 45.8644407526652\n",
      "Epoch 82 \t Batch 620 \t Training Loss: 45.829413918525944\n",
      "Epoch 82 \t Batch 640 \t Training Loss: 45.82548747062683\n",
      "Epoch 82 \t Batch 660 \t Training Loss: 45.8448348941225\n",
      "Epoch 82 \t Batch 680 \t Training Loss: 45.844592722724464\n",
      "Epoch 82 \t Batch 700 \t Training Loss: 45.83368535723005\n",
      "Epoch 82 \t Batch 720 \t Training Loss: 45.79358303811815\n",
      "Epoch 82 \t Batch 740 \t Training Loss: 45.76930652309108\n",
      "Epoch 82 \t Batch 760 \t Training Loss: 45.76118921982614\n",
      "Epoch 82 \t Batch 780 \t Training Loss: 45.77707951374543\n",
      "Epoch 82 \t Batch 800 \t Training Loss: 45.763991417884824\n",
      "Epoch 82 \t Batch 820 \t Training Loss: 45.777733746970576\n",
      "Epoch 82 \t Batch 840 \t Training Loss: 45.79392221087501\n",
      "Epoch 82 \t Batch 860 \t Training Loss: 45.80357343540635\n",
      "Epoch 82 \t Batch 880 \t Training Loss: 45.780541224913165\n",
      "Epoch 82 \t Batch 900 \t Training Loss: 45.81449927859836\n",
      "Epoch 82 \t Batch 20 \t Validation Loss: 19.36070351600647\n",
      "Epoch 82 \t Batch 40 \t Validation Loss: 21.21952543258667\n",
      "Epoch 82 \t Batch 60 \t Validation Loss: 20.87761796315511\n",
      "Epoch 82 \t Batch 80 \t Validation Loss: 21.409016954898835\n",
      "Epoch 82 \t Batch 100 \t Validation Loss: 22.95933844566345\n",
      "Epoch 82 \t Batch 120 \t Validation Loss: 24.156989336013794\n",
      "Epoch 82 \t Batch 140 \t Validation Loss: 24.711299964359828\n",
      "Epoch 82 \t Batch 160 \t Validation Loss: 26.67109011411667\n",
      "Epoch 82 \t Batch 180 \t Validation Loss: 30.159489880667792\n",
      "Epoch 82 \t Batch 200 \t Validation Loss: 31.532069630622864\n",
      "Epoch 82 \t Batch 220 \t Validation Loss: 32.738566940481014\n",
      "Epoch 82 \t Batch 240 \t Validation Loss: 33.23610537449519\n",
      "Epoch 82 \t Batch 260 \t Validation Loss: 35.231617054572475\n",
      "Epoch 82 \t Batch 280 \t Validation Loss: 36.340211503846305\n",
      "Epoch 82 \t Batch 300 \t Validation Loss: 37.45967526435852\n",
      "Epoch 82 \t Batch 320 \t Validation Loss: 38.00438189804554\n",
      "Epoch 82 \t Batch 340 \t Validation Loss: 37.990875005722046\n",
      "Epoch 82 \t Batch 360 \t Validation Loss: 37.86535856458876\n",
      "Epoch 82 \t Batch 380 \t Validation Loss: 38.138759432340926\n",
      "Epoch 82 \t Batch 400 \t Validation Loss: 37.81450172901154\n",
      "Epoch 82 \t Batch 420 \t Validation Loss: 37.874078253337316\n",
      "Epoch 82 \t Batch 440 \t Validation Loss: 37.64136785810644\n",
      "Epoch 82 \t Batch 460 \t Validation Loss: 37.97424079438915\n",
      "Epoch 82 \t Batch 480 \t Validation Loss: 38.49072756171226\n",
      "Epoch 82 \t Batch 500 \t Validation Loss: 38.255841230392456\n",
      "Epoch 82 \t Batch 520 \t Validation Loss: 38.08629544698275\n",
      "Epoch 82 \t Batch 540 \t Validation Loss: 37.84576382460418\n",
      "Epoch 82 \t Batch 560 \t Validation Loss: 37.64906464985439\n",
      "Epoch 82 \t Batch 580 \t Validation Loss: 37.38087642275054\n",
      "Epoch 82 \t Batch 600 \t Validation Loss: 37.62039046287536\n",
      "Epoch 82 Training Loss: 45.806785263178 Validation Loss: 38.29324389742566\n",
      "Epoch 82 completed\n",
      "Epoch 83 \t Batch 20 \t Training Loss: 45.70782470703125\n",
      "Epoch 83 \t Batch 40 \t Training Loss: 45.795789051055905\n",
      "Epoch 83 \t Batch 60 \t Training Loss: 45.47934449513753\n",
      "Epoch 83 \t Batch 80 \t Training Loss: 45.49289875030517\n",
      "Epoch 83 \t Batch 100 \t Training Loss: 45.20906188964844\n",
      "Epoch 83 \t Batch 120 \t Training Loss: 45.14010636011759\n",
      "Epoch 83 \t Batch 140 \t Training Loss: 45.26040791102818\n",
      "Epoch 83 \t Batch 160 \t Training Loss: 45.25617690086365\n",
      "Epoch 83 \t Batch 180 \t Training Loss: 45.256476910909015\n",
      "Epoch 83 \t Batch 200 \t Training Loss: 45.31786243438721\n",
      "Epoch 83 \t Batch 220 \t Training Loss: 45.36168849251487\n",
      "Epoch 83 \t Batch 240 \t Training Loss: 45.44601612091064\n",
      "Epoch 83 \t Batch 260 \t Training Loss: 45.44189466329721\n",
      "Epoch 83 \t Batch 280 \t Training Loss: 45.46510253633772\n",
      "Epoch 83 \t Batch 300 \t Training Loss: 45.48854704538981\n",
      "Epoch 83 \t Batch 320 \t Training Loss: 45.53018723726272\n",
      "Epoch 83 \t Batch 340 \t Training Loss: 45.63242139255299\n",
      "Epoch 83 \t Batch 360 \t Training Loss: 45.578085305955675\n",
      "Epoch 83 \t Batch 380 \t Training Loss: 45.50583328447844\n",
      "Epoch 83 \t Batch 400 \t Training Loss: 45.46163866996765\n",
      "Epoch 83 \t Batch 420 \t Training Loss: 45.536721956162225\n",
      "Epoch 83 \t Batch 440 \t Training Loss: 45.611144655401056\n",
      "Epoch 83 \t Batch 460 \t Training Loss: 45.65433060189952\n",
      "Epoch 83 \t Batch 480 \t Training Loss: 45.638065775235496\n",
      "Epoch 83 \t Batch 500 \t Training Loss: 45.62085662841797\n",
      "Epoch 83 \t Batch 520 \t Training Loss: 45.657556570493256\n",
      "Epoch 83 \t Batch 540 \t Training Loss: 45.68720498968054\n",
      "Epoch 83 \t Batch 560 \t Training Loss: 45.69853641646249\n",
      "Epoch 83 \t Batch 580 \t Training Loss: 45.717730515578694\n",
      "Epoch 83 \t Batch 600 \t Training Loss: 45.680207640329996\n",
      "Epoch 83 \t Batch 620 \t Training Loss: 45.6950774100519\n",
      "Epoch 83 \t Batch 640 \t Training Loss: 45.66173837780953\n",
      "Epoch 83 \t Batch 660 \t Training Loss: 45.651804900891854\n",
      "Epoch 83 \t Batch 680 \t Training Loss: 45.681099717757284\n",
      "Epoch 83 \t Batch 700 \t Training Loss: 45.68388555254255\n",
      "Epoch 83 \t Batch 720 \t Training Loss: 45.665551376342776\n",
      "Epoch 83 \t Batch 740 \t Training Loss: 45.636751391436604\n",
      "Epoch 83 \t Batch 760 \t Training Loss: 45.655399262277705\n",
      "Epoch 83 \t Batch 780 \t Training Loss: 45.67068646748861\n",
      "Epoch 83 \t Batch 800 \t Training Loss: 45.707910451889035\n",
      "Epoch 83 \t Batch 820 \t Training Loss: 45.712527168087846\n",
      "Epoch 83 \t Batch 840 \t Training Loss: 45.74895068123227\n",
      "Epoch 83 \t Batch 860 \t Training Loss: 45.778267549913984\n",
      "Epoch 83 \t Batch 880 \t Training Loss: 45.807794445211236\n",
      "Epoch 83 \t Batch 900 \t Training Loss: 45.79413636525472\n",
      "Epoch 83 \t Batch 20 \t Validation Loss: 27.18476333618164\n",
      "Epoch 83 \t Batch 40 \t Validation Loss: 27.755480885505676\n",
      "Epoch 83 \t Batch 60 \t Validation Loss: 27.464978583653767\n",
      "Epoch 83 \t Batch 80 \t Validation Loss: 27.717893755435945\n",
      "Epoch 83 \t Batch 100 \t Validation Loss: 28.238446836471557\n",
      "Epoch 83 \t Batch 120 \t Validation Loss: 28.843607338269553\n",
      "Epoch 83 \t Batch 140 \t Validation Loss: 28.886218799863542\n",
      "Epoch 83 \t Batch 160 \t Validation Loss: 30.466708379983903\n",
      "Epoch 83 \t Batch 180 \t Validation Loss: 34.00397858089871\n",
      "Epoch 83 \t Batch 200 \t Validation Loss: 35.30526376724243\n",
      "Epoch 83 \t Batch 220 \t Validation Loss: 36.35312793905085\n",
      "Epoch 83 \t Batch 240 \t Validation Loss: 36.71777504682541\n",
      "Epoch 83 \t Batch 260 \t Validation Loss: 38.66130882776701\n",
      "Epoch 83 \t Batch 280 \t Validation Loss: 39.61986126559121\n",
      "Epoch 83 \t Batch 300 \t Validation Loss: 40.807806742986045\n",
      "Epoch 83 \t Batch 320 \t Validation Loss: 41.25750033855438\n",
      "Epoch 83 \t Batch 340 \t Validation Loss: 41.100867148006664\n",
      "Epoch 83 \t Batch 360 \t Validation Loss: 40.88590116500855\n",
      "Epoch 83 \t Batch 380 \t Validation Loss: 40.996512177115996\n",
      "Epoch 83 \t Batch 400 \t Validation Loss: 40.49839269638061\n",
      "Epoch 83 \t Batch 420 \t Validation Loss: 40.44978349095299\n",
      "Epoch 83 \t Batch 440 \t Validation Loss: 40.0737410957163\n",
      "Epoch 83 \t Batch 460 \t Validation Loss: 40.26259326727494\n",
      "Epoch 83 \t Batch 480 \t Validation Loss: 40.69693112572034\n",
      "Epoch 83 \t Batch 500 \t Validation Loss: 40.38217555427551\n",
      "Epoch 83 \t Batch 520 \t Validation Loss: 40.10749544913952\n",
      "Epoch 83 \t Batch 540 \t Validation Loss: 39.791562509536746\n",
      "Epoch 83 \t Batch 560 \t Validation Loss: 39.52885053668703\n",
      "Epoch 83 \t Batch 580 \t Validation Loss: 39.1914198694558\n",
      "Epoch 83 \t Batch 600 \t Validation Loss: 39.35760059197744\n",
      "Epoch 83 Training Loss: 45.77407961713319 Validation Loss: 39.974747931802426\n",
      "Epoch 83 completed\n",
      "Epoch 84 \t Batch 20 \t Training Loss: 45.07849407196045\n",
      "Epoch 84 \t Batch 40 \t Training Loss: 44.37182207107544\n",
      "Epoch 84 \t Batch 60 \t Training Loss: 44.55186538696289\n",
      "Epoch 84 \t Batch 80 \t Training Loss: 44.8941481590271\n",
      "Epoch 84 \t Batch 100 \t Training Loss: 44.9590270614624\n",
      "Epoch 84 \t Batch 120 \t Training Loss: 44.77654349009196\n",
      "Epoch 84 \t Batch 140 \t Training Loss: 45.03466900416783\n",
      "Epoch 84 \t Batch 160 \t Training Loss: 44.95976529121399\n",
      "Epoch 84 \t Batch 180 \t Training Loss: 45.09357003106011\n",
      "Epoch 84 \t Batch 200 \t Training Loss: 45.09432004928589\n",
      "Epoch 84 \t Batch 220 \t Training Loss: 45.06652414148504\n",
      "Epoch 84 \t Batch 240 \t Training Loss: 45.05484420458476\n",
      "Epoch 84 \t Batch 260 \t Training Loss: 45.047606585575984\n",
      "Epoch 84 \t Batch 280 \t Training Loss: 45.05825598580497\n",
      "Epoch 84 \t Batch 300 \t Training Loss: 45.13956251780192\n",
      "Epoch 84 \t Batch 320 \t Training Loss: 45.20044310092926\n",
      "Epoch 84 \t Batch 340 \t Training Loss: 45.28268968918744\n",
      "Epoch 84 \t Batch 360 \t Training Loss: 45.36357583999634\n",
      "Epoch 84 \t Batch 380 \t Training Loss: 45.411661399038216\n",
      "Epoch 84 \t Batch 400 \t Training Loss: 45.388386707305905\n",
      "Epoch 84 \t Batch 420 \t Training Loss: 45.40591475168864\n",
      "Epoch 84 \t Batch 440 \t Training Loss: 45.47113730690696\n",
      "Epoch 84 \t Batch 460 \t Training Loss: 45.55180308300516\n",
      "Epoch 84 \t Batch 480 \t Training Loss: 45.54013872941335\n",
      "Epoch 84 \t Batch 500 \t Training Loss: 45.60926345062256\n",
      "Epoch 84 \t Batch 520 \t Training Loss: 45.630297264686\n",
      "Epoch 84 \t Batch 540 \t Training Loss: 45.580065670719854\n",
      "Epoch 84 \t Batch 560 \t Training Loss: 45.55747931344168\n",
      "Epoch 84 \t Batch 580 \t Training Loss: 45.558379955949455\n",
      "Epoch 84 \t Batch 600 \t Training Loss: 45.5818572807312\n",
      "Epoch 84 \t Batch 620 \t Training Loss: 45.58213374230169\n",
      "Epoch 84 \t Batch 640 \t Training Loss: 45.57772404551506\n",
      "Epoch 84 \t Batch 660 \t Training Loss: 45.60550384521484\n",
      "Epoch 84 \t Batch 680 \t Training Loss: 45.62506565206191\n",
      "Epoch 84 \t Batch 700 \t Training Loss: 45.64979454585484\n",
      "Epoch 84 \t Batch 720 \t Training Loss: 45.64727299478319\n",
      "Epoch 84 \t Batch 740 \t Training Loss: 45.65492517625963\n",
      "Epoch 84 \t Batch 760 \t Training Loss: 45.66922475915206\n",
      "Epoch 84 \t Batch 780 \t Training Loss: 45.66805610167675\n",
      "Epoch 84 \t Batch 800 \t Training Loss: 45.69465487957001\n",
      "Epoch 84 \t Batch 820 \t Training Loss: 45.6913357525337\n",
      "Epoch 84 \t Batch 840 \t Training Loss: 45.6852132661002\n",
      "Epoch 84 \t Batch 860 \t Training Loss: 45.68677221342575\n",
      "Epoch 84 \t Batch 880 \t Training Loss: 45.73607498515736\n",
      "Epoch 84 \t Batch 900 \t Training Loss: 45.713693364461264\n",
      "Epoch 84 \t Batch 20 \t Validation Loss: 23.15762243270874\n",
      "Epoch 84 \t Batch 40 \t Validation Loss: 24.518169403076172\n",
      "Epoch 84 \t Batch 60 \t Validation Loss: 24.200014114379883\n",
      "Epoch 84 \t Batch 80 \t Validation Loss: 24.428268826007844\n",
      "Epoch 84 \t Batch 100 \t Validation Loss: 25.46262608528137\n",
      "Epoch 84 \t Batch 120 \t Validation Loss: 26.358112184206643\n",
      "Epoch 84 \t Batch 140 \t Validation Loss: 26.673442929131642\n",
      "Epoch 84 \t Batch 160 \t Validation Loss: 28.35919595360756\n",
      "Epoch 84 \t Batch 180 \t Validation Loss: 31.90900379286872\n",
      "Epoch 84 \t Batch 200 \t Validation Loss: 33.32644546985626\n",
      "Epoch 84 \t Batch 220 \t Validation Loss: 34.447929438677704\n",
      "Epoch 84 \t Batch 240 \t Validation Loss: 34.862146429220836\n",
      "Epoch 84 \t Batch 260 \t Validation Loss: 36.88135509857764\n",
      "Epoch 84 \t Batch 280 \t Validation Loss: 37.93527478490557\n",
      "Epoch 84 \t Batch 300 \t Validation Loss: 39.073317495981854\n",
      "Epoch 84 \t Batch 320 \t Validation Loss: 39.539323672652245\n",
      "Epoch 84 \t Batch 340 \t Validation Loss: 39.45076298152699\n",
      "Epoch 84 \t Batch 360 \t Validation Loss: 39.23778292602963\n",
      "Epoch 84 \t Batch 380 \t Validation Loss: 39.432676187314485\n",
      "Epoch 84 \t Batch 400 \t Validation Loss: 39.025357034206394\n",
      "Epoch 84 \t Batch 420 \t Validation Loss: 39.029319047927856\n",
      "Epoch 84 \t Batch 440 \t Validation Loss: 38.715813955393706\n",
      "Epoch 84 \t Batch 460 \t Validation Loss: 38.95407762320146\n",
      "Epoch 84 \t Batch 480 \t Validation Loss: 39.42061394254367\n",
      "Epoch 84 \t Batch 500 \t Validation Loss: 39.12607527351379\n",
      "Epoch 84 \t Batch 520 \t Validation Loss: 38.89771328889407\n",
      "Epoch 84 \t Batch 540 \t Validation Loss: 38.65679583196287\n",
      "Epoch 84 \t Batch 560 \t Validation Loss: 38.45344518082482\n",
      "Epoch 84 \t Batch 580 \t Validation Loss: 38.19999166850386\n",
      "Epoch 84 \t Batch 600 \t Validation Loss: 38.415758894284565\n",
      "Epoch 84 Training Loss: 45.72377776267614 Validation Loss: 39.077351478787214\n",
      "Epoch 84 completed\n",
      "Epoch 85 \t Batch 20 \t Training Loss: 45.30704975128174\n",
      "Epoch 85 \t Batch 40 \t Training Loss: 45.40926122665405\n",
      "Epoch 85 \t Batch 60 \t Training Loss: 45.63639920552571\n",
      "Epoch 85 \t Batch 80 \t Training Loss: 45.87326588630676\n",
      "Epoch 85 \t Batch 100 \t Training Loss: 45.89472408294678\n",
      "Epoch 85 \t Batch 120 \t Training Loss: 45.756081899007164\n",
      "Epoch 85 \t Batch 140 \t Training Loss: 45.77312831878662\n",
      "Epoch 85 \t Batch 160 \t Training Loss: 45.82743022441864\n",
      "Epoch 85 \t Batch 180 \t Training Loss: 45.810061645507815\n",
      "Epoch 85 \t Batch 200 \t Training Loss: 45.676182708740235\n",
      "Epoch 85 \t Batch 220 \t Training Loss: 45.56624370921742\n",
      "Epoch 85 \t Batch 240 \t Training Loss: 45.70571753184001\n",
      "Epoch 85 \t Batch 260 \t Training Loss: 45.76534940279447\n",
      "Epoch 85 \t Batch 280 \t Training Loss: 45.61587987627302\n",
      "Epoch 85 \t Batch 300 \t Training Loss: 45.65856093088786\n",
      "Epoch 85 \t Batch 320 \t Training Loss: 45.64311598539352\n",
      "Epoch 85 \t Batch 340 \t Training Loss: 45.60880195393282\n",
      "Epoch 85 \t Batch 360 \t Training Loss: 45.538663895924884\n",
      "Epoch 85 \t Batch 380 \t Training Loss: 45.531695667066074\n",
      "Epoch 85 \t Batch 400 \t Training Loss: 45.551142349243165\n",
      "Epoch 85 \t Batch 420 \t Training Loss: 45.51972496396019\n",
      "Epoch 85 \t Batch 440 \t Training Loss: 45.56111177964644\n",
      "Epoch 85 \t Batch 460 \t Training Loss: 45.622499051301375\n",
      "Epoch 85 \t Batch 480 \t Training Loss: 45.65924151738485\n",
      "Epoch 85 \t Batch 500 \t Training Loss: 45.683641548156736\n",
      "Epoch 85 \t Batch 520 \t Training Loss: 45.69444873516376\n",
      "Epoch 85 \t Batch 540 \t Training Loss: 45.744882887381095\n",
      "Epoch 85 \t Batch 560 \t Training Loss: 45.74170685495649\n",
      "Epoch 85 \t Batch 580 \t Training Loss: 45.74311184718691\n",
      "Epoch 85 \t Batch 600 \t Training Loss: 45.757654126485185\n",
      "Epoch 85 \t Batch 620 \t Training Loss: 45.766272661762855\n",
      "Epoch 85 \t Batch 640 \t Training Loss: 45.7674655854702\n",
      "Epoch 85 \t Batch 660 \t Training Loss: 45.793433385906795\n",
      "Epoch 85 \t Batch 680 \t Training Loss: 45.74287034764009\n",
      "Epoch 85 \t Batch 700 \t Training Loss: 45.73504159109933\n",
      "Epoch 85 \t Batch 720 \t Training Loss: 45.72037666108873\n",
      "Epoch 85 \t Batch 740 \t Training Loss: 45.72007444742564\n",
      "Epoch 85 \t Batch 760 \t Training Loss: 45.725968481365\n",
      "Epoch 85 \t Batch 780 \t Training Loss: 45.68101685841878\n",
      "Epoch 85 \t Batch 800 \t Training Loss: 45.72339293956757\n",
      "Epoch 85 \t Batch 820 \t Training Loss: 45.74179341851211\n",
      "Epoch 85 \t Batch 840 \t Training Loss: 45.741167222885856\n",
      "Epoch 85 \t Batch 860 \t Training Loss: 45.72110945235851\n",
      "Epoch 85 \t Batch 880 \t Training Loss: 45.72985419793562\n",
      "Epoch 85 \t Batch 900 \t Training Loss: 45.75747117784288\n",
      "Epoch 85 \t Batch 20 \t Validation Loss: 17.009098958969116\n",
      "Epoch 85 \t Batch 40 \t Validation Loss: 19.538953185081482\n",
      "Epoch 85 \t Batch 60 \t Validation Loss: 19.390314801534018\n",
      "Epoch 85 \t Batch 80 \t Validation Loss: 20.283128917217255\n",
      "Epoch 85 \t Batch 100 \t Validation Loss: 22.21344069480896\n",
      "Epoch 85 \t Batch 120 \t Validation Loss: 23.728316362698873\n",
      "Epoch 85 \t Batch 140 \t Validation Loss: 24.60102756364005\n",
      "Epoch 85 \t Batch 160 \t Validation Loss: 26.69353511929512\n",
      "Epoch 85 \t Batch 180 \t Validation Loss: 30.368707609176635\n",
      "Epoch 85 \t Batch 200 \t Validation Loss: 31.905035700798035\n",
      "Epoch 85 \t Batch 220 \t Validation Loss: 33.173936276002365\n",
      "Epoch 85 \t Batch 240 \t Validation Loss: 33.76195100943247\n",
      "Epoch 85 \t Batch 260 \t Validation Loss: 35.8145611506242\n",
      "Epoch 85 \t Batch 280 \t Validation Loss: 36.930067907060895\n",
      "Epoch 85 \t Batch 300 \t Validation Loss: 38.07795345306396\n",
      "Epoch 85 \t Batch 320 \t Validation Loss: 38.59624108672142\n",
      "Epoch 85 \t Batch 340 \t Validation Loss: 38.59389899197747\n",
      "Epoch 85 \t Batch 360 \t Validation Loss: 38.51764514181349\n",
      "Epoch 85 \t Batch 380 \t Validation Loss: 38.77663462789435\n",
      "Epoch 85 \t Batch 400 \t Validation Loss: 38.4447128868103\n",
      "Epoch 85 \t Batch 420 \t Validation Loss: 38.468694400787356\n",
      "Epoch 85 \t Batch 440 \t Validation Loss: 38.2139517957514\n",
      "Epoch 85 \t Batch 460 \t Validation Loss: 38.50066777519558\n",
      "Epoch 85 \t Batch 480 \t Validation Loss: 38.99148943026861\n",
      "Epoch 85 \t Batch 500 \t Validation Loss: 38.726322792053224\n",
      "Epoch 85 \t Batch 520 \t Validation Loss: 38.600107310368465\n",
      "Epoch 85 \t Batch 540 \t Validation Loss: 38.44830914956552\n",
      "Epoch 85 \t Batch 560 \t Validation Loss: 38.339820293017794\n",
      "Epoch 85 \t Batch 580 \t Validation Loss: 38.22409881723338\n",
      "Epoch 85 \t Batch 600 \t Validation Loss: 38.47911460876465\n",
      "Epoch 85 Training Loss: 45.73966896780965 Validation Loss: 39.236847047681934\n",
      "Epoch 85 completed\n",
      "Epoch 86 \t Batch 20 \t Training Loss: 44.519876098632814\n",
      "Epoch 86 \t Batch 40 \t Training Loss: 45.02533798217773\n",
      "Epoch 86 \t Batch 60 \t Training Loss: 45.66111793518066\n",
      "Epoch 86 \t Batch 80 \t Training Loss: 45.67886085510254\n",
      "Epoch 86 \t Batch 100 \t Training Loss: 45.38380428314209\n",
      "Epoch 86 \t Batch 120 \t Training Loss: 45.42264340718587\n",
      "Epoch 86 \t Batch 140 \t Training Loss: 45.66752708980015\n",
      "Epoch 86 \t Batch 160 \t Training Loss: 45.6499440908432\n",
      "Epoch 86 \t Batch 180 \t Training Loss: 45.62086279127333\n",
      "Epoch 86 \t Batch 200 \t Training Loss: 45.53777841567993\n",
      "Epoch 86 \t Batch 220 \t Training Loss: 45.41249708695845\n",
      "Epoch 86 \t Batch 240 \t Training Loss: 45.4178152402242\n",
      "Epoch 86 \t Batch 260 \t Training Loss: 45.45564397665171\n",
      "Epoch 86 \t Batch 280 \t Training Loss: 45.497687462397984\n",
      "Epoch 86 \t Batch 300 \t Training Loss: 45.53730242411296\n",
      "Epoch 86 \t Batch 320 \t Training Loss: 45.5643443107605\n",
      "Epoch 86 \t Batch 340 \t Training Loss: 45.58174232034122\n",
      "Epoch 86 \t Batch 360 \t Training Loss: 45.56985126071506\n",
      "Epoch 86 \t Batch 380 \t Training Loss: 45.56784126884059\n",
      "Epoch 86 \t Batch 400 \t Training Loss: 45.62779948234558\n",
      "Epoch 86 \t Batch 420 \t Training Loss: 45.65722042265392\n",
      "Epoch 86 \t Batch 440 \t Training Loss: 45.65754632949829\n",
      "Epoch 86 \t Batch 460 \t Training Loss: 45.707040372102156\n",
      "Epoch 86 \t Batch 480 \t Training Loss: 45.69785188039144\n",
      "Epoch 86 \t Batch 500 \t Training Loss: 45.698276756286624\n",
      "Epoch 86 \t Batch 520 \t Training Loss: 45.67459886257465\n",
      "Epoch 86 \t Batch 540 \t Training Loss: 45.72038438585069\n",
      "Epoch 86 \t Batch 560 \t Training Loss: 45.71472761290414\n",
      "Epoch 86 \t Batch 580 \t Training Loss: 45.73511564320531\n",
      "Epoch 86 \t Batch 600 \t Training Loss: 45.73643344243367\n",
      "Epoch 86 \t Batch 620 \t Training Loss: 45.74055895036267\n",
      "Epoch 86 \t Batch 640 \t Training Loss: 45.67896192669868\n",
      "Epoch 86 \t Batch 660 \t Training Loss: 45.68893059239243\n",
      "Epoch 86 \t Batch 680 \t Training Loss: 45.67598176282995\n",
      "Epoch 86 \t Batch 700 \t Training Loss: 45.696871577671594\n",
      "Epoch 86 \t Batch 720 \t Training Loss: 45.674138932757906\n",
      "Epoch 86 \t Batch 740 \t Training Loss: 45.68417144466091\n",
      "Epoch 86 \t Batch 760 \t Training Loss: 45.69935965788992\n",
      "Epoch 86 \t Batch 780 \t Training Loss: 45.69025011307154\n",
      "Epoch 86 \t Batch 800 \t Training Loss: 45.68542520046234\n",
      "Epoch 86 \t Batch 820 \t Training Loss: 45.670232363445\n",
      "Epoch 86 \t Batch 840 \t Training Loss: 45.68639365150815\n",
      "Epoch 86 \t Batch 860 \t Training Loss: 45.665761623826135\n",
      "Epoch 86 \t Batch 880 \t Training Loss: 45.678220142017715\n",
      "Epoch 86 \t Batch 900 \t Training Loss: 45.6739258617825\n",
      "Epoch 86 \t Batch 20 \t Validation Loss: 17.41769905090332\n",
      "Epoch 86 \t Batch 40 \t Validation Loss: 19.55734815597534\n",
      "Epoch 86 \t Batch 60 \t Validation Loss: 19.322488673528035\n",
      "Epoch 86 \t Batch 80 \t Validation Loss: 19.942937326431274\n",
      "Epoch 86 \t Batch 100 \t Validation Loss: 21.79227731704712\n",
      "Epoch 86 \t Batch 120 \t Validation Loss: 23.322898197174073\n",
      "Epoch 86 \t Batch 140 \t Validation Loss: 24.11478020804269\n",
      "Epoch 86 \t Batch 160 \t Validation Loss: 26.131515061855318\n",
      "Epoch 86 \t Batch 180 \t Validation Loss: 30.011997413635253\n",
      "Epoch 86 \t Batch 200 \t Validation Loss: 31.525997409820558\n",
      "Epoch 86 \t Batch 220 \t Validation Loss: 32.759502931074664\n",
      "Epoch 86 \t Batch 240 \t Validation Loss: 33.29306746323903\n",
      "Epoch 86 \t Batch 260 \t Validation Loss: 35.34730782142052\n",
      "Epoch 86 \t Batch 280 \t Validation Loss: 36.443624888147625\n",
      "Epoch 86 \t Batch 300 \t Validation Loss: 37.751857131322225\n",
      "Epoch 86 \t Batch 320 \t Validation Loss: 38.34502512216568\n",
      "Epoch 86 \t Batch 340 \t Validation Loss: 38.325011545069074\n",
      "Epoch 86 \t Batch 360 \t Validation Loss: 38.22401178677877\n",
      "Epoch 86 \t Batch 380 \t Validation Loss: 38.43322730817293\n",
      "Epoch 86 \t Batch 400 \t Validation Loss: 38.07208635807037\n",
      "Epoch 86 \t Batch 420 \t Validation Loss: 38.10018543515886\n",
      "Epoch 86 \t Batch 440 \t Validation Loss: 37.82811519882896\n",
      "Epoch 86 \t Batch 460 \t Validation Loss: 38.11180797245191\n",
      "Epoch 86 \t Batch 480 \t Validation Loss: 38.63341158231099\n",
      "Epoch 86 \t Batch 500 \t Validation Loss: 38.37302647018433\n",
      "Epoch 86 \t Batch 520 \t Validation Loss: 38.174689610187826\n",
      "Epoch 86 \t Batch 540 \t Validation Loss: 37.94758574697706\n",
      "Epoch 86 \t Batch 560 \t Validation Loss: 37.74809045961925\n",
      "Epoch 86 \t Batch 580 \t Validation Loss: 37.49175616132802\n",
      "Epoch 86 \t Batch 600 \t Validation Loss: 37.724733541806536\n",
      "Epoch 86 Training Loss: 45.65769646913951 Validation Loss: 38.396714927314164\n",
      "Epoch 86 completed\n",
      "Epoch 87 \t Batch 20 \t Training Loss: 44.54667644500732\n",
      "Epoch 87 \t Batch 40 \t Training Loss: 45.15454044342041\n",
      "Epoch 87 \t Batch 60 \t Training Loss: 45.38034184773763\n",
      "Epoch 87 \t Batch 80 \t Training Loss: 45.72237281799316\n",
      "Epoch 87 \t Batch 100 \t Training Loss: 45.71283500671387\n",
      "Epoch 87 \t Batch 120 \t Training Loss: 45.489662901560465\n",
      "Epoch 87 \t Batch 140 \t Training Loss: 45.33814577375139\n",
      "Epoch 87 \t Batch 160 \t Training Loss: 45.27358605861664\n",
      "Epoch 87 \t Batch 180 \t Training Loss: 45.257878091600205\n",
      "Epoch 87 \t Batch 200 \t Training Loss: 45.322135028839114\n",
      "Epoch 87 \t Batch 220 \t Training Loss: 45.436708346280184\n",
      "Epoch 87 \t Batch 240 \t Training Loss: 45.49454544385274\n",
      "Epoch 87 \t Batch 260 \t Training Loss: 45.43763907505916\n",
      "Epoch 87 \t Batch 280 \t Training Loss: 45.422538362230576\n",
      "Epoch 87 \t Batch 300 \t Training Loss: 45.407847023010255\n",
      "Epoch 87 \t Batch 320 \t Training Loss: 45.4108241558075\n",
      "Epoch 87 \t Batch 340 \t Training Loss: 45.406271732554714\n",
      "Epoch 87 \t Batch 360 \t Training Loss: 45.40727059046427\n",
      "Epoch 87 \t Batch 380 \t Training Loss: 45.31873176976254\n",
      "Epoch 87 \t Batch 400 \t Training Loss: 45.35280389785767\n",
      "Epoch 87 \t Batch 420 \t Training Loss: 45.47363177708217\n",
      "Epoch 87 \t Batch 440 \t Training Loss: 45.386188038912685\n",
      "Epoch 87 \t Batch 460 \t Training Loss: 45.437715596738066\n",
      "Epoch 87 \t Batch 480 \t Training Loss: 45.450212295850115\n",
      "Epoch 87 \t Batch 500 \t Training Loss: 45.50478125\n",
      "Epoch 87 \t Batch 520 \t Training Loss: 45.531904440659744\n",
      "Epoch 87 \t Batch 540 \t Training Loss: 45.56754424483688\n",
      "Epoch 87 \t Batch 560 \t Training Loss: 45.53243524006435\n",
      "Epoch 87 \t Batch 580 \t Training Loss: 45.49963810361665\n",
      "Epoch 87 \t Batch 600 \t Training Loss: 45.474799925486245\n",
      "Epoch 87 \t Batch 620 \t Training Loss: 45.46613209632135\n",
      "Epoch 87 \t Batch 640 \t Training Loss: 45.51688039302826\n",
      "Epoch 87 \t Batch 660 \t Training Loss: 45.553388982830626\n",
      "Epoch 87 \t Batch 680 \t Training Loss: 45.561088864943564\n",
      "Epoch 87 \t Batch 700 \t Training Loss: 45.554188755580356\n",
      "Epoch 87 \t Batch 720 \t Training Loss: 45.567831669913396\n",
      "Epoch 87 \t Batch 740 \t Training Loss: 45.584554522746316\n",
      "Epoch 87 \t Batch 760 \t Training Loss: 45.618801352852266\n",
      "Epoch 87 \t Batch 780 \t Training Loss: 45.609125704643056\n",
      "Epoch 87 \t Batch 800 \t Training Loss: 45.61974722385406\n",
      "Epoch 87 \t Batch 820 \t Training Loss: 45.61763862516822\n",
      "Epoch 87 \t Batch 840 \t Training Loss: 45.63533346085321\n",
      "Epoch 87 \t Batch 860 \t Training Loss: 45.63890889189964\n",
      "Epoch 87 \t Batch 880 \t Training Loss: 45.62553647648205\n",
      "Epoch 87 \t Batch 900 \t Training Loss: 45.63152759128147\n",
      "Epoch 87 \t Batch 20 \t Validation Loss: 25.400287246704103\n",
      "Epoch 87 \t Batch 40 \t Validation Loss: 26.527376675605773\n",
      "Epoch 87 \t Batch 60 \t Validation Loss: 26.25367194811503\n",
      "Epoch 87 \t Batch 80 \t Validation Loss: 26.57095127105713\n",
      "Epoch 87 \t Batch 100 \t Validation Loss: 27.273268241882324\n",
      "Epoch 87 \t Batch 120 \t Validation Loss: 27.874476369222005\n",
      "Epoch 87 \t Batch 140 \t Validation Loss: 28.0564500944955\n",
      "Epoch 87 \t Batch 160 \t Validation Loss: 29.83002381324768\n",
      "Epoch 87 \t Batch 180 \t Validation Loss: 33.353353050020004\n",
      "Epoch 87 \t Batch 200 \t Validation Loss: 34.63976171970367\n",
      "Epoch 87 \t Batch 220 \t Validation Loss: 35.79039520350369\n",
      "Epoch 87 \t Batch 240 \t Validation Loss: 36.17682124376297\n",
      "Epoch 87 \t Batch 260 \t Validation Loss: 38.167776412230275\n",
      "Epoch 87 \t Batch 280 \t Validation Loss: 39.19540538447244\n",
      "Epoch 87 \t Batch 300 \t Validation Loss: 40.297955675125124\n",
      "Epoch 87 \t Batch 320 \t Validation Loss: 40.73381526172161\n",
      "Epoch 87 \t Batch 340 \t Validation Loss: 40.61302880960352\n",
      "Epoch 87 \t Batch 360 \t Validation Loss: 40.44146586259206\n",
      "Epoch 87 \t Batch 380 \t Validation Loss: 40.575841148276076\n",
      "Epoch 87 \t Batch 400 \t Validation Loss: 40.08853627443314\n",
      "Epoch 87 \t Batch 420 \t Validation Loss: 40.01545411291576\n",
      "Epoch 87 \t Batch 440 \t Validation Loss: 39.64691320982846\n",
      "Epoch 87 \t Batch 460 \t Validation Loss: 39.86164792309636\n",
      "Epoch 87 \t Batch 480 \t Validation Loss: 40.30780557195346\n",
      "Epoch 87 \t Batch 500 \t Validation Loss: 40.014329084396365\n",
      "Epoch 87 \t Batch 520 \t Validation Loss: 39.757422199616066\n",
      "Epoch 87 \t Batch 540 \t Validation Loss: 39.49675159630952\n",
      "Epoch 87 \t Batch 560 \t Validation Loss: 39.327119396414076\n",
      "Epoch 87 \t Batch 580 \t Validation Loss: 39.11484914812549\n",
      "Epoch 87 \t Batch 600 \t Validation Loss: 39.315881024996436\n",
      "Epoch 87 Training Loss: 45.646087929362686 Validation Loss: 39.973047843227135\n",
      "Epoch 87 completed\n",
      "Epoch 88 \t Batch 20 \t Training Loss: 45.29569282531738\n",
      "Epoch 88 \t Batch 40 \t Training Loss: 45.89940280914307\n",
      "Epoch 88 \t Batch 60 \t Training Loss: 45.67570972442627\n",
      "Epoch 88 \t Batch 80 \t Training Loss: 45.51950368881226\n",
      "Epoch 88 \t Batch 100 \t Training Loss: 45.36676937103272\n",
      "Epoch 88 \t Batch 120 \t Training Loss: 45.33398100535075\n",
      "Epoch 88 \t Batch 140 \t Training Loss: 45.35377750396729\n",
      "Epoch 88 \t Batch 160 \t Training Loss: 45.41133136749268\n",
      "Epoch 88 \t Batch 180 \t Training Loss: 45.36068325042724\n",
      "Epoch 88 \t Batch 200 \t Training Loss: 45.35868942260742\n",
      "Epoch 88 \t Batch 220 \t Training Loss: 45.3506668437611\n",
      "Epoch 88 \t Batch 240 \t Training Loss: 45.33003565470378\n",
      "Epoch 88 \t Batch 260 \t Training Loss: 45.29283915299636\n",
      "Epoch 88 \t Batch 280 \t Training Loss: 45.28964125769479\n",
      "Epoch 88 \t Batch 300 \t Training Loss: 45.398428179423014\n",
      "Epoch 88 \t Batch 320 \t Training Loss: 45.35805398225784\n",
      "Epoch 88 \t Batch 340 \t Training Loss: 45.446475264605354\n",
      "Epoch 88 \t Batch 360 \t Training Loss: 45.49707815382216\n",
      "Epoch 88 \t Batch 380 \t Training Loss: 45.579075883564194\n",
      "Epoch 88 \t Batch 400 \t Training Loss: 45.65651490211487\n",
      "Epoch 88 \t Batch 420 \t Training Loss: 45.629813848223\n",
      "Epoch 88 \t Batch 440 \t Training Loss: 45.714007169550115\n",
      "Epoch 88 \t Batch 460 \t Training Loss: 45.702453132297684\n",
      "Epoch 88 \t Batch 480 \t Training Loss: 45.723177989323936\n",
      "Epoch 88 \t Batch 500 \t Training Loss: 45.65721952819824\n",
      "Epoch 88 \t Batch 520 \t Training Loss: 45.645751351576585\n",
      "Epoch 88 \t Batch 540 \t Training Loss: 45.62492193999114\n",
      "Epoch 88 \t Batch 560 \t Training Loss: 45.607774591445924\n",
      "Epoch 88 \t Batch 580 \t Training Loss: 45.54493505543676\n",
      "Epoch 88 \t Batch 600 \t Training Loss: 45.5357710202535\n",
      "Epoch 88 \t Batch 620 \t Training Loss: 45.55839713311965\n",
      "Epoch 88 \t Batch 640 \t Training Loss: 45.55031172037125\n",
      "Epoch 88 \t Batch 660 \t Training Loss: 45.52595620588823\n",
      "Epoch 88 \t Batch 680 \t Training Loss: 45.505084492178526\n",
      "Epoch 88 \t Batch 700 \t Training Loss: 45.54733660016741\n",
      "Epoch 88 \t Batch 720 \t Training Loss: 45.554689444435965\n",
      "Epoch 88 \t Batch 740 \t Training Loss: 45.562454708202466\n",
      "Epoch 88 \t Batch 760 \t Training Loss: 45.52830858732525\n",
      "Epoch 88 \t Batch 780 \t Training Loss: 45.558273877853004\n",
      "Epoch 88 \t Batch 800 \t Training Loss: 45.58445562362671\n",
      "Epoch 88 \t Batch 820 \t Training Loss: 45.59235314625065\n",
      "Epoch 88 \t Batch 840 \t Training Loss: 45.60511320659092\n",
      "Epoch 88 \t Batch 860 \t Training Loss: 45.610961794298746\n",
      "Epoch 88 \t Batch 880 \t Training Loss: 45.64920001896945\n",
      "Epoch 88 \t Batch 900 \t Training Loss: 45.65240111456977\n",
      "Epoch 88 \t Batch 20 \t Validation Loss: 16.358016633987425\n",
      "Epoch 88 \t Batch 40 \t Validation Loss: 17.641740918159485\n",
      "Epoch 88 \t Batch 60 \t Validation Loss: 17.749022054672242\n",
      "Epoch 88 \t Batch 80 \t Validation Loss: 18.450997269153596\n",
      "Epoch 88 \t Batch 100 \t Validation Loss: 20.35404631614685\n",
      "Epoch 88 \t Batch 120 \t Validation Loss: 21.884292260805765\n",
      "Epoch 88 \t Batch 140 \t Validation Loss: 22.763461419514247\n",
      "Epoch 88 \t Batch 160 \t Validation Loss: 24.95206623673439\n",
      "Epoch 88 \t Batch 180 \t Validation Loss: 28.775516976250543\n",
      "Epoch 88 \t Batch 200 \t Validation Loss: 30.42258006095886\n",
      "Epoch 88 \t Batch 220 \t Validation Loss: 31.758374058116566\n",
      "Epoch 88 \t Batch 240 \t Validation Loss: 32.36313805977503\n",
      "Epoch 88 \t Batch 260 \t Validation Loss: 34.52138105539175\n",
      "Epoch 88 \t Batch 280 \t Validation Loss: 35.72163031441825\n",
      "Epoch 88 \t Batch 300 \t Validation Loss: 36.95802335103353\n",
      "Epoch 88 \t Batch 320 \t Validation Loss: 37.53197935819626\n",
      "Epoch 88 \t Batch 340 \t Validation Loss: 37.55091881471522\n",
      "Epoch 88 \t Batch 360 \t Validation Loss: 37.45962536070082\n",
      "Epoch 88 \t Batch 380 \t Validation Loss: 37.75537661502236\n",
      "Epoch 88 \t Batch 400 \t Validation Loss: 37.43099478244781\n",
      "Epoch 88 \t Batch 420 \t Validation Loss: 37.52923308781215\n",
      "Epoch 88 \t Batch 440 \t Validation Loss: 37.28799024061723\n",
      "Epoch 88 \t Batch 460 \t Validation Loss: 37.593299218882684\n",
      "Epoch 88 \t Batch 480 \t Validation Loss: 38.14094607035319\n",
      "Epoch 88 \t Batch 500 \t Validation Loss: 37.91468815231323\n",
      "Epoch 88 \t Batch 520 \t Validation Loss: 37.72677719409649\n",
      "Epoch 88 \t Batch 540 \t Validation Loss: 37.52066245255647\n",
      "Epoch 88 \t Batch 560 \t Validation Loss: 37.363037831442696\n",
      "Epoch 88 \t Batch 580 \t Validation Loss: 37.16429083922814\n",
      "Epoch 88 \t Batch 600 \t Validation Loss: 37.41490391413371\n",
      "Epoch 88 Training Loss: 45.650407244100954 Validation Loss: 38.11154803672394\n",
      "Epoch 88 completed\n",
      "Epoch 89 \t Batch 20 \t Training Loss: 44.030601501464844\n",
      "Epoch 89 \t Batch 40 \t Training Loss: 45.000084400177\n",
      "Epoch 89 \t Batch 60 \t Training Loss: 45.35302848815918\n",
      "Epoch 89 \t Batch 80 \t Training Loss: 45.2057186126709\n",
      "Epoch 89 \t Batch 100 \t Training Loss: 44.876411514282225\n",
      "Epoch 89 \t Batch 120 \t Training Loss: 44.865628210703534\n",
      "Epoch 89 \t Batch 140 \t Training Loss: 44.98931743076869\n",
      "Epoch 89 \t Batch 160 \t Training Loss: 44.95183973312378\n",
      "Epoch 89 \t Batch 180 \t Training Loss: 44.97518130408393\n",
      "Epoch 89 \t Batch 200 \t Training Loss: 45.314303455352785\n",
      "Epoch 89 \t Batch 220 \t Training Loss: 45.25163295052268\n",
      "Epoch 89 \t Batch 240 \t Training Loss: 45.28126537005107\n",
      "Epoch 89 \t Batch 260 \t Training Loss: 45.3559564443735\n",
      "Epoch 89 \t Batch 280 \t Training Loss: 45.360990728650776\n",
      "Epoch 89 \t Batch 300 \t Training Loss: 45.37030632019043\n",
      "Epoch 89 \t Batch 320 \t Training Loss: 45.37708123922348\n",
      "Epoch 89 \t Batch 340 \t Training Loss: 45.4142193625955\n",
      "Epoch 89 \t Batch 360 \t Training Loss: 45.41271871990628\n",
      "Epoch 89 \t Batch 380 \t Training Loss: 45.453961422568874\n",
      "Epoch 89 \t Batch 400 \t Training Loss: 45.48697796821594\n",
      "Epoch 89 \t Batch 420 \t Training Loss: 45.47195059458415\n",
      "Epoch 89 \t Batch 440 \t Training Loss: 45.60137480822476\n",
      "Epoch 89 \t Batch 460 \t Training Loss: 45.60697767838188\n",
      "Epoch 89 \t Batch 480 \t Training Loss: 45.58839348157247\n",
      "Epoch 89 \t Batch 500 \t Training Loss: 45.54125454711914\n",
      "Epoch 89 \t Batch 520 \t Training Loss: 45.510582087590144\n",
      "Epoch 89 \t Batch 540 \t Training Loss: 45.50726926591661\n",
      "Epoch 89 \t Batch 560 \t Training Loss: 45.52115715571812\n",
      "Epoch 89 \t Batch 580 \t Training Loss: 45.554234458660254\n",
      "Epoch 89 \t Batch 600 \t Training Loss: 45.57617255528768\n",
      "Epoch 89 \t Batch 620 \t Training Loss: 45.60208710085961\n",
      "Epoch 89 \t Batch 640 \t Training Loss: 45.5658414542675\n",
      "Epoch 89 \t Batch 660 \t Training Loss: 45.59108220302697\n",
      "Epoch 89 \t Batch 680 \t Training Loss: 45.6282213715946\n",
      "Epoch 89 \t Batch 700 \t Training Loss: 45.6418586403983\n",
      "Epoch 89 \t Batch 720 \t Training Loss: 45.6820481883155\n",
      "Epoch 89 \t Batch 740 \t Training Loss: 45.68185805243415\n",
      "Epoch 89 \t Batch 760 \t Training Loss: 45.672235955690084\n",
      "Epoch 89 \t Batch 780 \t Training Loss: 45.62625418442946\n",
      "Epoch 89 \t Batch 800 \t Training Loss: 45.624906339645385\n",
      "Epoch 89 \t Batch 820 \t Training Loss: 45.5893130418731\n",
      "Epoch 89 \t Batch 840 \t Training Loss: 45.57897421518962\n",
      "Epoch 89 \t Batch 860 \t Training Loss: 45.60439462439958\n",
      "Epoch 89 \t Batch 880 \t Training Loss: 45.61284794373946\n",
      "Epoch 89 \t Batch 900 \t Training Loss: 45.59973729027642\n",
      "Epoch 89 \t Batch 20 \t Validation Loss: 25.864369297027586\n",
      "Epoch 89 \t Batch 40 \t Validation Loss: 26.055181860923767\n",
      "Epoch 89 \t Batch 60 \t Validation Loss: 26.029791084925332\n",
      "Epoch 89 \t Batch 80 \t Validation Loss: 26.382115375995635\n",
      "Epoch 89 \t Batch 100 \t Validation Loss: 27.127025098800658\n",
      "Epoch 89 \t Batch 120 \t Validation Loss: 27.83450187842051\n",
      "Epoch 89 \t Batch 140 \t Validation Loss: 28.15359170777457\n",
      "Epoch 89 \t Batch 160 \t Validation Loss: 30.122358936071397\n",
      "Epoch 89 \t Batch 180 \t Validation Loss: 33.99052753448486\n",
      "Epoch 89 \t Batch 200 \t Validation Loss: 35.56828867912292\n",
      "Epoch 89 \t Batch 220 \t Validation Loss: 36.7780728080056\n",
      "Epoch 89 \t Batch 240 \t Validation Loss: 37.240775406360626\n",
      "Epoch 89 \t Batch 260 \t Validation Loss: 39.333548776920026\n",
      "Epoch 89 \t Batch 280 \t Validation Loss: 40.400764897891456\n",
      "Epoch 89 \t Batch 300 \t Validation Loss: 41.657998870213824\n",
      "Epoch 89 \t Batch 320 \t Validation Loss: 42.16046216785908\n",
      "Epoch 89 \t Batch 340 \t Validation Loss: 42.0325238199795\n",
      "Epoch 89 \t Batch 360 \t Validation Loss: 41.86904645231035\n",
      "Epoch 89 \t Batch 380 \t Validation Loss: 42.05412826287119\n",
      "Epoch 89 \t Batch 400 \t Validation Loss: 41.57502574205399\n",
      "Epoch 89 \t Batch 420 \t Validation Loss: 41.507939486276534\n",
      "Epoch 89 \t Batch 440 \t Validation Loss: 41.14450789581645\n",
      "Epoch 89 \t Batch 460 \t Validation Loss: 41.41160556751749\n",
      "Epoch 89 \t Batch 480 \t Validation Loss: 41.819444662332536\n",
      "Epoch 89 \t Batch 500 \t Validation Loss: 41.519224859237674\n",
      "Epoch 89 \t Batch 520 \t Validation Loss: 41.3691628401096\n",
      "Epoch 89 \t Batch 540 \t Validation Loss: 41.12722424577784\n",
      "Epoch 89 \t Batch 560 \t Validation Loss: 40.968397894927435\n",
      "Epoch 89 \t Batch 580 \t Validation Loss: 40.78350054806676\n",
      "Epoch 89 \t Batch 600 \t Validation Loss: 40.96714859803517\n",
      "Epoch 89 Training Loss: 45.596363566823975 Validation Loss: 41.63192394337097\n",
      "Epoch 89 completed\n",
      "Epoch 90 \t Batch 20 \t Training Loss: 44.81874370574951\n",
      "Epoch 90 \t Batch 40 \t Training Loss: 44.99598350524902\n",
      "Epoch 90 \t Batch 60 \t Training Loss: 44.80134359995524\n",
      "Epoch 90 \t Batch 80 \t Training Loss: 45.02772846221924\n",
      "Epoch 90 \t Batch 100 \t Training Loss: 45.343843612670895\n",
      "Epoch 90 \t Batch 120 \t Training Loss: 45.386124006907146\n",
      "Epoch 90 \t Batch 140 \t Training Loss: 45.41341590881348\n",
      "Epoch 90 \t Batch 160 \t Training Loss: 45.40115411281586\n",
      "Epoch 90 \t Batch 180 \t Training Loss: 45.414154370625816\n",
      "Epoch 90 \t Batch 200 \t Training Loss: 45.537098770141604\n",
      "Epoch 90 \t Batch 220 \t Training Loss: 45.57237767306241\n",
      "Epoch 90 \t Batch 240 \t Training Loss: 45.54016772905985\n",
      "Epoch 90 \t Batch 260 \t Training Loss: 45.58434853186974\n",
      "Epoch 90 \t Batch 280 \t Training Loss: 45.65145695550101\n",
      "Epoch 90 \t Batch 300 \t Training Loss: 45.6763285446167\n",
      "Epoch 90 \t Batch 320 \t Training Loss: 45.683862280845645\n",
      "Epoch 90 \t Batch 340 \t Training Loss: 45.660128481247845\n",
      "Epoch 90 \t Batch 360 \t Training Loss: 45.6973849190606\n",
      "Epoch 90 \t Batch 380 \t Training Loss: 45.747184823688706\n",
      "Epoch 90 \t Batch 400 \t Training Loss: 45.793795022964474\n",
      "Epoch 90 \t Batch 420 \t Training Loss: 45.77615174793062\n",
      "Epoch 90 \t Batch 440 \t Training Loss: 45.73844718066129\n",
      "Epoch 90 \t Batch 460 \t Training Loss: 45.7568885139797\n",
      "Epoch 90 \t Batch 480 \t Training Loss: 45.773268127441405\n",
      "Epoch 90 \t Batch 500 \t Training Loss: 45.727643821716306\n",
      "Epoch 90 \t Batch 520 \t Training Loss: 45.78590373259324\n",
      "Epoch 90 \t Batch 540 \t Training Loss: 45.77589994359899\n",
      "Epoch 90 \t Batch 560 \t Training Loss: 45.728779138837545\n",
      "Epoch 90 \t Batch 580 \t Training Loss: 45.764436083826524\n",
      "Epoch 90 \t Batch 600 \t Training Loss: 45.74859958648682\n",
      "Epoch 90 \t Batch 620 \t Training Loss: 45.7326286808137\n",
      "Epoch 90 \t Batch 640 \t Training Loss: 45.720540231466295\n",
      "Epoch 90 \t Batch 660 \t Training Loss: 45.700045481595126\n",
      "Epoch 90 \t Batch 680 \t Training Loss: 45.687190712199495\n",
      "Epoch 90 \t Batch 700 \t Training Loss: 45.652502365112305\n",
      "Epoch 90 \t Batch 720 \t Training Loss: 45.64332969983419\n",
      "Epoch 90 \t Batch 740 \t Training Loss: 45.642634268064754\n",
      "Epoch 90 \t Batch 760 \t Training Loss: 45.619583024476704\n",
      "Epoch 90 \t Batch 780 \t Training Loss: 45.59973249190893\n",
      "Epoch 90 \t Batch 800 \t Training Loss: 45.56959406375885\n",
      "Epoch 90 \t Batch 820 \t Training Loss: 45.59751912093744\n",
      "Epoch 90 \t Batch 840 \t Training Loss: 45.578353795551116\n",
      "Epoch 90 \t Batch 860 \t Training Loss: 45.62196083512417\n",
      "Epoch 90 \t Batch 880 \t Training Loss: 45.64033445444974\n",
      "Epoch 90 \t Batch 900 \t Training Loss: 45.6119419309828\n",
      "Epoch 90 \t Batch 20 \t Validation Loss: 20.017207193374634\n",
      "Epoch 90 \t Batch 40 \t Validation Loss: 21.843656039237977\n",
      "Epoch 90 \t Batch 60 \t Validation Loss: 21.496853415171305\n",
      "Epoch 90 \t Batch 80 \t Validation Loss: 21.888790893554688\n",
      "Epoch 90 \t Batch 100 \t Validation Loss: 23.347168159484863\n",
      "Epoch 90 \t Batch 120 \t Validation Loss: 24.594283485412596\n",
      "Epoch 90 \t Batch 140 \t Validation Loss: 25.20080108642578\n",
      "Epoch 90 \t Batch 160 \t Validation Loss: 27.171089160442353\n",
      "Epoch 90 \t Batch 180 \t Validation Loss: 30.866326136059232\n",
      "Epoch 90 \t Batch 200 \t Validation Loss: 32.377581839561465\n",
      "Epoch 90 \t Batch 220 \t Validation Loss: 33.557178258895874\n",
      "Epoch 90 \t Batch 240 \t Validation Loss: 34.027252332369486\n",
      "Epoch 90 \t Batch 260 \t Validation Loss: 36.091144279333264\n",
      "Epoch 90 \t Batch 280 \t Validation Loss: 37.209135460853574\n",
      "Epoch 90 \t Batch 300 \t Validation Loss: 38.385069093704224\n",
      "Epoch 90 \t Batch 320 \t Validation Loss: 38.91216844320297\n",
      "Epoch 90 \t Batch 340 \t Validation Loss: 38.866797250859875\n",
      "Epoch 90 \t Batch 360 \t Validation Loss: 38.722995318306815\n",
      "Epoch 90 \t Batch 380 \t Validation Loss: 38.95220123592176\n",
      "Epoch 90 \t Batch 400 \t Validation Loss: 38.57662864208221\n",
      "Epoch 90 \t Batch 420 \t Validation Loss: 38.60542004903157\n",
      "Epoch 90 \t Batch 440 \t Validation Loss: 38.339673800901934\n",
      "Epoch 90 \t Batch 460 \t Validation Loss: 38.673791275853695\n",
      "Epoch 90 \t Batch 480 \t Validation Loss: 39.177261586983995\n",
      "Epoch 90 \t Batch 500 \t Validation Loss: 38.92050717163086\n",
      "Epoch 90 \t Batch 520 \t Validation Loss: 38.75711351541372\n",
      "Epoch 90 \t Batch 540 \t Validation Loss: 38.51808292600844\n",
      "Epoch 90 \t Batch 560 \t Validation Loss: 38.30803404194968\n",
      "Epoch 90 \t Batch 580 \t Validation Loss: 38.044680855191984\n",
      "Epoch 90 \t Batch 600 \t Validation Loss: 38.27071581204732\n",
      "Epoch 90 Training Loss: 45.589324231496164 Validation Loss: 38.946231938027715\n",
      "Epoch 90 completed\n",
      "Epoch 91 \t Batch 20 \t Training Loss: 45.42807178497314\n",
      "Epoch 91 \t Batch 40 \t Training Loss: 45.39374408721924\n",
      "Epoch 91 \t Batch 60 \t Training Loss: 45.19995606740316\n",
      "Epoch 91 \t Batch 80 \t Training Loss: 45.245230484008786\n",
      "Epoch 91 \t Batch 100 \t Training Loss: 45.68171100616455\n",
      "Epoch 91 \t Batch 120 \t Training Loss: 45.779224681854245\n",
      "Epoch 91 \t Batch 140 \t Training Loss: 45.8149859564645\n",
      "Epoch 91 \t Batch 160 \t Training Loss: 45.702534437179565\n",
      "Epoch 91 \t Batch 180 \t Training Loss: 45.73339680565728\n",
      "Epoch 91 \t Batch 200 \t Training Loss: 45.72650255203247\n",
      "Epoch 91 \t Batch 220 \t Training Loss: 45.70890643379905\n",
      "Epoch 91 \t Batch 240 \t Training Loss: 45.82902553876241\n",
      "Epoch 91 \t Batch 260 \t Training Loss: 45.93680468339186\n",
      "Epoch 91 \t Batch 280 \t Training Loss: 45.85068411145892\n",
      "Epoch 91 \t Batch 300 \t Training Loss: 45.79501319885254\n",
      "Epoch 91 \t Batch 320 \t Training Loss: 45.745267391204834\n",
      "Epoch 91 \t Batch 340 \t Training Loss: 45.74795446956859\n",
      "Epoch 91 \t Batch 360 \t Training Loss: 45.736461808946395\n",
      "Epoch 91 \t Batch 380 \t Training Loss: 45.670704811497735\n",
      "Epoch 91 \t Batch 400 \t Training Loss: 45.58790347099304\n",
      "Epoch 91 \t Batch 420 \t Training Loss: 45.528637949625654\n",
      "Epoch 91 \t Batch 440 \t Training Loss: 45.57059020996094\n",
      "Epoch 91 \t Batch 460 \t Training Loss: 45.54356079101562\n",
      "Epoch 91 \t Batch 480 \t Training Loss: 45.55473128954569\n",
      "Epoch 91 \t Batch 500 \t Training Loss: 45.55746939849853\n",
      "Epoch 91 \t Batch 520 \t Training Loss: 45.5349324959975\n",
      "Epoch 91 \t Batch 540 \t Training Loss: 45.543046682852285\n",
      "Epoch 91 \t Batch 560 \t Training Loss: 45.527376011439735\n",
      "Epoch 91 \t Batch 580 \t Training Loss: 45.54282955301219\n",
      "Epoch 91 \t Batch 600 \t Training Loss: 45.55657287597656\n",
      "Epoch 91 \t Batch 620 \t Training Loss: 45.566113127431564\n",
      "Epoch 91 \t Batch 640 \t Training Loss: 45.54298576116562\n",
      "Epoch 91 \t Batch 660 \t Training Loss: 45.534595501061645\n",
      "Epoch 91 \t Batch 680 \t Training Loss: 45.50509490966797\n",
      "Epoch 91 \t Batch 700 \t Training Loss: 45.461558069501606\n",
      "Epoch 91 \t Batch 720 \t Training Loss: 45.428544759750366\n",
      "Epoch 91 \t Batch 740 \t Training Loss: 45.45912998818063\n",
      "Epoch 91 \t Batch 760 \t Training Loss: 45.488236768622144\n",
      "Epoch 91 \t Batch 780 \t Training Loss: 45.49072778652876\n",
      "Epoch 91 \t Batch 800 \t Training Loss: 45.5092053604126\n",
      "Epoch 91 \t Batch 820 \t Training Loss: 45.538554149720724\n",
      "Epoch 91 \t Batch 840 \t Training Loss: 45.52843495777675\n",
      "Epoch 91 \t Batch 860 \t Training Loss: 45.54325511843659\n",
      "Epoch 91 \t Batch 880 \t Training Loss: 45.54015788598494\n",
      "Epoch 91 \t Batch 900 \t Training Loss: 45.52637842390272\n",
      "Epoch 91 \t Batch 20 \t Validation Loss: 19.55736036300659\n",
      "Epoch 91 \t Batch 40 \t Validation Loss: 22.149989128112793\n",
      "Epoch 91 \t Batch 60 \t Validation Loss: 21.887890783945718\n",
      "Epoch 91 \t Batch 80 \t Validation Loss: 22.600972032547\n",
      "Epoch 91 \t Batch 100 \t Validation Loss: 23.989184188842774\n",
      "Epoch 91 \t Batch 120 \t Validation Loss: 25.174297491709392\n",
      "Epoch 91 \t Batch 140 \t Validation Loss: 25.7476845741272\n",
      "Epoch 91 \t Batch 160 \t Validation Loss: 27.699048268795014\n",
      "Epoch 91 \t Batch 180 \t Validation Loss: 31.331709988911946\n",
      "Epoch 91 \t Batch 200 \t Validation Loss: 32.73900480270386\n",
      "Epoch 91 \t Batch 220 \t Validation Loss: 33.95976226980036\n",
      "Epoch 91 \t Batch 240 \t Validation Loss: 34.43927841981252\n",
      "Epoch 91 \t Batch 260 \t Validation Loss: 36.468480924459605\n",
      "Epoch 91 \t Batch 280 \t Validation Loss: 37.54895485128675\n",
      "Epoch 91 \t Batch 300 \t Validation Loss: 38.71630825996399\n",
      "Epoch 91 \t Batch 320 \t Validation Loss: 39.22157327234745\n",
      "Epoch 91 \t Batch 340 \t Validation Loss: 39.17425466144786\n",
      "Epoch 91 \t Batch 360 \t Validation Loss: 39.04261224799686\n",
      "Epoch 91 \t Batch 380 \t Validation Loss: 39.25242689283271\n",
      "Epoch 91 \t Batch 400 \t Validation Loss: 38.85995098352432\n",
      "Epoch 91 \t Batch 420 \t Validation Loss: 38.87625682694571\n",
      "Epoch 91 \t Batch 440 \t Validation Loss: 38.570750620148395\n",
      "Epoch 91 \t Batch 460 \t Validation Loss: 38.81671650720679\n",
      "Epoch 91 \t Batch 480 \t Validation Loss: 39.29526415467262\n",
      "Epoch 91 \t Batch 500 \t Validation Loss: 39.03118992805481\n",
      "Epoch 91 \t Batch 520 \t Validation Loss: 38.8022693835772\n",
      "Epoch 91 \t Batch 540 \t Validation Loss: 38.56652992566426\n",
      "Epoch 91 \t Batch 560 \t Validation Loss: 38.38927081823349\n",
      "Epoch 91 \t Batch 580 \t Validation Loss: 38.1672385955679\n",
      "Epoch 91 \t Batch 600 \t Validation Loss: 38.38467433452606\n",
      "Epoch 91 Training Loss: 45.5411518059484 Validation Loss: 39.05801583420146\n",
      "Epoch 91 completed\n",
      "Epoch 92 \t Batch 20 \t Training Loss: 44.773813819885255\n",
      "Epoch 92 \t Batch 40 \t Training Loss: 45.614386558532715\n",
      "Epoch 92 \t Batch 60 \t Training Loss: 45.921861521402995\n",
      "Epoch 92 \t Batch 80 \t Training Loss: 45.81808161735535\n",
      "Epoch 92 \t Batch 100 \t Training Loss: 46.044313201904295\n",
      "Epoch 92 \t Batch 120 \t Training Loss: 45.49509830474854\n",
      "Epoch 92 \t Batch 140 \t Training Loss: 45.45214143480573\n",
      "Epoch 92 \t Batch 160 \t Training Loss: 45.345831108093265\n",
      "Epoch 92 \t Batch 180 \t Training Loss: 45.30829283396403\n",
      "Epoch 92 \t Batch 200 \t Training Loss: 45.32786144256592\n",
      "Epoch 92 \t Batch 220 \t Training Loss: 45.44085429798473\n",
      "Epoch 92 \t Batch 240 \t Training Loss: 45.394592142105104\n",
      "Epoch 92 \t Batch 260 \t Training Loss: 45.42402527148907\n",
      "Epoch 92 \t Batch 280 \t Training Loss: 45.36426842553275\n",
      "Epoch 92 \t Batch 300 \t Training Loss: 45.43316232045492\n",
      "Epoch 92 \t Batch 320 \t Training Loss: 45.384948062896726\n",
      "Epoch 92 \t Batch 340 \t Training Loss: 45.471378326416016\n",
      "Epoch 92 \t Batch 360 \t Training Loss: 45.50766667260064\n",
      "Epoch 92 \t Batch 380 \t Training Loss: 45.48697141346179\n",
      "Epoch 92 \t Batch 400 \t Training Loss: 45.52224245071411\n",
      "Epoch 92 \t Batch 420 \t Training Loss: 45.506997889564154\n",
      "Epoch 92 \t Batch 440 \t Training Loss: 45.51960266286677\n",
      "Epoch 92 \t Batch 460 \t Training Loss: 45.517677273957624\n",
      "Epoch 92 \t Batch 480 \t Training Loss: 45.51922729810079\n",
      "Epoch 92 \t Batch 500 \t Training Loss: 45.55582145690918\n",
      "Epoch 92 \t Batch 520 \t Training Loss: 45.466449488126315\n",
      "Epoch 92 \t Batch 540 \t Training Loss: 45.46445364069056\n",
      "Epoch 92 \t Batch 560 \t Training Loss: 45.40907004901341\n",
      "Epoch 92 \t Batch 580 \t Training Loss: 45.444228869471054\n",
      "Epoch 92 \t Batch 600 \t Training Loss: 45.4492720413208\n",
      "Epoch 92 \t Batch 620 \t Training Loss: 45.470400194967944\n",
      "Epoch 92 \t Batch 640 \t Training Loss: 45.45697036981583\n",
      "Epoch 92 \t Batch 660 \t Training Loss: 45.43159948406797\n",
      "Epoch 92 \t Batch 680 \t Training Loss: 45.45073040793924\n",
      "Epoch 92 \t Batch 700 \t Training Loss: 45.44714048113142\n",
      "Epoch 92 \t Batch 720 \t Training Loss: 45.445361937416926\n",
      "Epoch 92 \t Batch 740 \t Training Loss: 45.47842072667302\n",
      "Epoch 92 \t Batch 760 \t Training Loss: 45.48118593316329\n",
      "Epoch 92 \t Batch 780 \t Training Loss: 45.49292017129751\n",
      "Epoch 92 \t Batch 800 \t Training Loss: 45.50186253547668\n",
      "Epoch 92 \t Batch 820 \t Training Loss: 45.52660021898223\n",
      "Epoch 92 \t Batch 840 \t Training Loss: 45.509156681242445\n",
      "Epoch 92 \t Batch 860 \t Training Loss: 45.53021220717319\n",
      "Epoch 92 \t Batch 880 \t Training Loss: 45.53321296951987\n",
      "Epoch 92 \t Batch 900 \t Training Loss: 45.54098345438639\n",
      "Epoch 92 \t Batch 20 \t Validation Loss: 19.450779533386232\n",
      "Epoch 92 \t Batch 40 \t Validation Loss: 21.132974004745485\n",
      "Epoch 92 \t Batch 60 \t Validation Loss: 20.785579125086468\n",
      "Epoch 92 \t Batch 80 \t Validation Loss: 21.24469678401947\n",
      "Epoch 92 \t Batch 100 \t Validation Loss: 22.93930118560791\n",
      "Epoch 92 \t Batch 120 \t Validation Loss: 24.27617745399475\n",
      "Epoch 92 \t Batch 140 \t Validation Loss: 24.994787079947336\n",
      "Epoch 92 \t Batch 160 \t Validation Loss: 27.037239289283754\n",
      "Epoch 92 \t Batch 180 \t Validation Loss: 30.92184082137214\n",
      "Epoch 92 \t Batch 200 \t Validation Loss: 32.48767131328583\n",
      "Epoch 92 \t Batch 220 \t Validation Loss: 33.73412107554349\n",
      "Epoch 92 \t Batch 240 \t Validation Loss: 34.26456983884176\n",
      "Epoch 92 \t Batch 260 \t Validation Loss: 36.37786414806659\n",
      "Epoch 92 \t Batch 280 \t Validation Loss: 37.538097170421054\n",
      "Epoch 92 \t Batch 300 \t Validation Loss: 38.79105148951213\n",
      "Epoch 92 \t Batch 320 \t Validation Loss: 39.32701670825482\n",
      "Epoch 92 \t Batch 340 \t Validation Loss: 39.277245378494264\n",
      "Epoch 92 \t Batch 360 \t Validation Loss: 39.15343013074663\n",
      "Epoch 92 \t Batch 380 \t Validation Loss: 39.38102028746354\n",
      "Epoch 92 \t Batch 400 \t Validation Loss: 38.998764855861666\n",
      "Epoch 92 \t Batch 420 \t Validation Loss: 38.98066226187206\n",
      "Epoch 92 \t Batch 440 \t Validation Loss: 38.67750989957289\n",
      "Epoch 92 \t Batch 460 \t Validation Loss: 38.94915282829948\n",
      "Epoch 92 \t Batch 480 \t Validation Loss: 39.42247211734454\n",
      "Epoch 92 \t Batch 500 \t Validation Loss: 39.129109869003294\n",
      "Epoch 92 \t Batch 520 \t Validation Loss: 38.94512744500087\n",
      "Epoch 92 \t Batch 540 \t Validation Loss: 38.741596887729784\n",
      "Epoch 92 \t Batch 560 \t Validation Loss: 38.56188054255077\n",
      "Epoch 92 \t Batch 580 \t Validation Loss: 38.37210287390084\n",
      "Epoch 92 \t Batch 600 \t Validation Loss: 38.600466701189674\n",
      "Epoch 92 Training Loss: 45.53619298238141 Validation Loss: 39.2913647208895\n",
      "Epoch 92 completed\n",
      "Epoch 93 \t Batch 20 \t Training Loss: 45.018510055541995\n",
      "Epoch 93 \t Batch 40 \t Training Loss: 45.35976457595825\n",
      "Epoch 93 \t Batch 60 \t Training Loss: 45.21757221221924\n",
      "Epoch 93 \t Batch 80 \t Training Loss: 45.21620502471924\n",
      "Epoch 93 \t Batch 100 \t Training Loss: 45.463347358703615\n",
      "Epoch 93 \t Batch 120 \t Training Loss: 45.5882851600647\n",
      "Epoch 93 \t Batch 140 \t Training Loss: 45.364033045087545\n",
      "Epoch 93 \t Batch 160 \t Training Loss: 45.2395254611969\n",
      "Epoch 93 \t Batch 180 \t Training Loss: 45.28494926028782\n",
      "Epoch 93 \t Batch 200 \t Training Loss: 45.14325511932373\n",
      "Epoch 93 \t Batch 220 \t Training Loss: 45.15956625504927\n",
      "Epoch 93 \t Batch 240 \t Training Loss: 45.172517935434975\n",
      "Epoch 93 \t Batch 260 \t Training Loss: 45.25329513549805\n",
      "Epoch 93 \t Batch 280 \t Training Loss: 45.27341387612479\n",
      "Epoch 93 \t Batch 300 \t Training Loss: 45.27354911804199\n",
      "Epoch 93 \t Batch 320 \t Training Loss: 45.29787976741791\n",
      "Epoch 93 \t Batch 340 \t Training Loss: 45.361458901798024\n",
      "Epoch 93 \t Batch 360 \t Training Loss: 45.44245055516561\n",
      "Epoch 93 \t Batch 380 \t Training Loss: 45.36397910871004\n",
      "Epoch 93 \t Batch 400 \t Training Loss: 45.397386121749875\n",
      "Epoch 93 \t Batch 420 \t Training Loss: 45.458725983755926\n",
      "Epoch 93 \t Batch 440 \t Training Loss: 45.44576705585826\n",
      "Epoch 93 \t Batch 460 \t Training Loss: 45.53808082912279\n",
      "Epoch 93 \t Batch 480 \t Training Loss: 45.52040471235911\n",
      "Epoch 93 \t Batch 500 \t Training Loss: 45.49951309204101\n",
      "Epoch 93 \t Batch 520 \t Training Loss: 45.44481823260968\n",
      "Epoch 93 \t Batch 540 \t Training Loss: 45.453194074277526\n",
      "Epoch 93 \t Batch 560 \t Training Loss: 45.4709491457258\n",
      "Epoch 93 \t Batch 580 \t Training Loss: 45.47956790266366\n",
      "Epoch 93 \t Batch 600 \t Training Loss: 45.40883421579997\n",
      "Epoch 93 \t Batch 620 \t Training Loss: 45.38229145542268\n",
      "Epoch 93 \t Batch 640 \t Training Loss: 45.349940770864485\n",
      "Epoch 93 \t Batch 660 \t Training Loss: 45.31627632487904\n",
      "Epoch 93 \t Batch 680 \t Training Loss: 45.31007679770975\n",
      "Epoch 93 \t Batch 700 \t Training Loss: 45.36787229810442\n",
      "Epoch 93 \t Batch 720 \t Training Loss: 45.413230917188855\n",
      "Epoch 93 \t Batch 740 \t Training Loss: 45.462269401550294\n",
      "Epoch 93 \t Batch 760 \t Training Loss: 45.46740188598633\n",
      "Epoch 93 \t Batch 780 \t Training Loss: 45.46052886278201\n",
      "Epoch 93 \t Batch 800 \t Training Loss: 45.46599604129791\n",
      "Epoch 93 \t Batch 820 \t Training Loss: 45.457932937435984\n",
      "Epoch 93 \t Batch 840 \t Training Loss: 45.46135733468192\n",
      "Epoch 93 \t Batch 860 \t Training Loss: 45.446500831426576\n",
      "Epoch 93 \t Batch 880 \t Training Loss: 45.45740532875061\n",
      "Epoch 93 \t Batch 900 \t Training Loss: 45.477395384046766\n",
      "Epoch 93 \t Batch 20 \t Validation Loss: 16.577826690673827\n",
      "Epoch 93 \t Batch 40 \t Validation Loss: 18.30140039920807\n",
      "Epoch 93 \t Batch 60 \t Validation Loss: 18.335973596572877\n",
      "Epoch 93 \t Batch 80 \t Validation Loss: 19.138326573371888\n",
      "Epoch 93 \t Batch 100 \t Validation Loss: 21.15421188354492\n",
      "Epoch 93 \t Batch 120 \t Validation Loss: 22.60129455725352\n",
      "Epoch 93 \t Batch 140 \t Validation Loss: 23.41893192699977\n",
      "Epoch 93 \t Batch 160 \t Validation Loss: 25.609045451879503\n",
      "Epoch 93 \t Batch 180 \t Validation Loss: 29.535808346006604\n",
      "Epoch 93 \t Batch 200 \t Validation Loss: 31.216110014915465\n",
      "Epoch 93 \t Batch 220 \t Validation Loss: 32.545579056306316\n",
      "Epoch 93 \t Batch 240 \t Validation Loss: 33.13326816956202\n",
      "Epoch 93 \t Batch 260 \t Validation Loss: 35.33126386128939\n",
      "Epoch 93 \t Batch 280 \t Validation Loss: 36.517748733929224\n",
      "Epoch 93 \t Batch 300 \t Validation Loss: 37.78722544670105\n",
      "Epoch 93 \t Batch 320 \t Validation Loss: 38.35022920370102\n",
      "Epoch 93 \t Batch 340 \t Validation Loss: 38.349927414164824\n",
      "Epoch 93 \t Batch 360 \t Validation Loss: 38.24584992196825\n",
      "Epoch 93 \t Batch 380 \t Validation Loss: 38.51703538894653\n",
      "Epoch 93 \t Batch 400 \t Validation Loss: 38.15003995418549\n",
      "Epoch 93 \t Batch 420 \t Validation Loss: 38.20011406853085\n",
      "Epoch 93 \t Batch 440 \t Validation Loss: 37.934185734662144\n",
      "Epoch 93 \t Batch 460 \t Validation Loss: 38.31133422436921\n",
      "Epoch 93 \t Batch 480 \t Validation Loss: 38.83567987283071\n",
      "Epoch 93 \t Batch 500 \t Validation Loss: 38.604064929962156\n",
      "Epoch 93 \t Batch 520 \t Validation Loss: 38.4733588548807\n",
      "Epoch 93 \t Batch 540 \t Validation Loss: 38.2503638126232\n",
      "Epoch 93 \t Batch 560 \t Validation Loss: 38.073913884162906\n",
      "Epoch 93 \t Batch 580 \t Validation Loss: 37.871516638788684\n",
      "Epoch 93 \t Batch 600 \t Validation Loss: 38.1107107702891\n",
      "Epoch 93 Training Loss: 45.49421305432834 Validation Loss: 38.79421827700231\n",
      "Epoch 93 completed\n",
      "Epoch 94 \t Batch 20 \t Training Loss: 46.132891464233396\n",
      "Epoch 94 \t Batch 40 \t Training Loss: 45.70885791778564\n",
      "Epoch 94 \t Batch 60 \t Training Loss: 45.66023298899333\n",
      "Epoch 94 \t Batch 80 \t Training Loss: 45.84678916931152\n",
      "Epoch 94 \t Batch 100 \t Training Loss: 45.73708599090576\n",
      "Epoch 94 \t Batch 120 \t Training Loss: 45.764967600504555\n",
      "Epoch 94 \t Batch 140 \t Training Loss: 45.747864205496654\n",
      "Epoch 94 \t Batch 160 \t Training Loss: 45.66119377613067\n",
      "Epoch 94 \t Batch 180 \t Training Loss: 45.775464905632866\n",
      "Epoch 94 \t Batch 200 \t Training Loss: 45.65703071594238\n",
      "Epoch 94 \t Batch 220 \t Training Loss: 45.65364322662354\n",
      "Epoch 94 \t Batch 240 \t Training Loss: 45.67160520553589\n",
      "Epoch 94 \t Batch 260 \t Training Loss: 45.62044908083402\n",
      "Epoch 94 \t Batch 280 \t Training Loss: 45.49544497898647\n",
      "Epoch 94 \t Batch 300 \t Training Loss: 45.56657543182373\n",
      "Epoch 94 \t Batch 320 \t Training Loss: 45.59124571084976\n",
      "Epoch 94 \t Batch 340 \t Training Loss: 45.525420200123506\n",
      "Epoch 94 \t Batch 360 \t Training Loss: 45.44126779768202\n",
      "Epoch 94 \t Batch 380 \t Training Loss: 45.44912815093994\n",
      "Epoch 94 \t Batch 400 \t Training Loss: 45.42920742034912\n",
      "Epoch 94 \t Batch 420 \t Training Loss: 45.50212057204474\n",
      "Epoch 94 \t Batch 440 \t Training Loss: 45.51543043309992\n",
      "Epoch 94 \t Batch 460 \t Training Loss: 45.53126569001571\n",
      "Epoch 94 \t Batch 480 \t Training Loss: 45.449526437123616\n",
      "Epoch 94 \t Batch 500 \t Training Loss: 45.41175093841553\n",
      "Epoch 94 \t Batch 520 \t Training Loss: 45.3869889992934\n",
      "Epoch 94 \t Batch 540 \t Training Loss: 45.41916980743408\n",
      "Epoch 94 \t Batch 560 \t Training Loss: 45.46576176370893\n",
      "Epoch 94 \t Batch 580 \t Training Loss: 45.443411636352536\n",
      "Epoch 94 \t Batch 600 \t Training Loss: 45.426130835215254\n",
      "Epoch 94 \t Batch 620 \t Training Loss: 45.42019986183413\n",
      "Epoch 94 \t Batch 640 \t Training Loss: 45.409251815080644\n",
      "Epoch 94 \t Batch 660 \t Training Loss: 45.446689461216785\n",
      "Epoch 94 \t Batch 680 \t Training Loss: 45.46212179520551\n",
      "Epoch 94 \t Batch 700 \t Training Loss: 45.43830241612026\n",
      "Epoch 94 \t Batch 720 \t Training Loss: 45.450551732381186\n",
      "Epoch 94 \t Batch 740 \t Training Loss: 45.44172788568445\n",
      "Epoch 94 \t Batch 760 \t Training Loss: 45.463417429673044\n",
      "Epoch 94 \t Batch 780 \t Training Loss: 45.46334111629388\n",
      "Epoch 94 \t Batch 800 \t Training Loss: 45.46565841674805\n",
      "Epoch 94 \t Batch 820 \t Training Loss: 45.468182210224434\n",
      "Epoch 94 \t Batch 840 \t Training Loss: 45.49978331157139\n",
      "Epoch 94 \t Batch 860 \t Training Loss: 45.49778298666311\n",
      "Epoch 94 \t Batch 880 \t Training Loss: 45.47674491188743\n",
      "Epoch 94 \t Batch 900 \t Training Loss: 45.51081630706787\n",
      "Epoch 94 \t Batch 20 \t Validation Loss: 18.502012634277342\n",
      "Epoch 94 \t Batch 40 \t Validation Loss: 20.383555316925047\n",
      "Epoch 94 \t Batch 60 \t Validation Loss: 20.03500065803528\n",
      "Epoch 94 \t Batch 80 \t Validation Loss: 21.065187907218935\n",
      "Epoch 94 \t Batch 100 \t Validation Loss: 23.03994342803955\n",
      "Epoch 94 \t Batch 120 \t Validation Loss: 24.41765371958415\n",
      "Epoch 94 \t Batch 140 \t Validation Loss: 25.130398178100585\n",
      "Epoch 94 \t Batch 160 \t Validation Loss: 27.27551885843277\n",
      "Epoch 94 \t Batch 180 \t Validation Loss: 31.07911574045817\n",
      "Epoch 94 \t Batch 200 \t Validation Loss: 32.619626684188844\n",
      "Epoch 94 \t Batch 220 \t Validation Loss: 33.9333009633151\n",
      "Epoch 94 \t Batch 240 \t Validation Loss: 34.4904500246048\n",
      "Epoch 94 \t Batch 260 \t Validation Loss: 36.602411754314716\n",
      "Epoch 94 \t Batch 280 \t Validation Loss: 37.729980383600505\n",
      "Epoch 94 \t Batch 300 \t Validation Loss: 38.966076644261676\n",
      "Epoch 94 \t Batch 320 \t Validation Loss: 39.49924187958241\n",
      "Epoch 94 \t Batch 340 \t Validation Loss: 39.47049450313344\n",
      "Epoch 94 \t Batch 360 \t Validation Loss: 39.381510869661966\n",
      "Epoch 94 \t Batch 380 \t Validation Loss: 39.644067796907926\n",
      "Epoch 94 \t Batch 400 \t Validation Loss: 39.297197296619416\n",
      "Epoch 94 \t Batch 420 \t Validation Loss: 39.30663134029933\n",
      "Epoch 94 \t Batch 440 \t Validation Loss: 39.0676763642918\n",
      "Epoch 94 \t Batch 460 \t Validation Loss: 39.3913585725038\n",
      "Epoch 94 \t Batch 480 \t Validation Loss: 39.87889185945193\n",
      "Epoch 94 \t Batch 500 \t Validation Loss: 39.61798481559753\n",
      "Epoch 94 \t Batch 520 \t Validation Loss: 39.5351888858355\n",
      "Epoch 94 \t Batch 540 \t Validation Loss: 39.37492296607406\n",
      "Epoch 94 \t Batch 560 \t Validation Loss: 39.28002536807742\n",
      "Epoch 94 \t Batch 580 \t Validation Loss: 39.15127961553376\n",
      "Epoch 94 \t Batch 600 \t Validation Loss: 39.4188173977534\n",
      "Epoch 94 Training Loss: 45.512146678329685 Validation Loss: 40.12632512581813\n",
      "Epoch 94 completed\n",
      "Epoch 95 \t Batch 20 \t Training Loss: 44.32570304870605\n",
      "Epoch 95 \t Batch 40 \t Training Loss: 44.88257884979248\n",
      "Epoch 95 \t Batch 60 \t Training Loss: 44.753885459899905\n",
      "Epoch 95 \t Batch 80 \t Training Loss: 44.971109580993655\n",
      "Epoch 95 \t Batch 100 \t Training Loss: 44.91152374267578\n",
      "Epoch 95 \t Batch 120 \t Training Loss: 44.82989495595296\n",
      "Epoch 95 \t Batch 140 \t Training Loss: 45.054328319004604\n",
      "Epoch 95 \t Batch 160 \t Training Loss: 45.030503678321836\n",
      "Epoch 95 \t Batch 180 \t Training Loss: 45.17298927307129\n",
      "Epoch 95 \t Batch 200 \t Training Loss: 45.14982088088989\n",
      "Epoch 95 \t Batch 220 \t Training Loss: 45.01674478704279\n",
      "Epoch 95 \t Batch 240 \t Training Loss: 45.068212588628136\n",
      "Epoch 95 \t Batch 260 \t Training Loss: 45.1059418458205\n",
      "Epoch 95 \t Batch 280 \t Training Loss: 45.160825851985386\n",
      "Epoch 95 \t Batch 300 \t Training Loss: 45.14570194244385\n",
      "Epoch 95 \t Batch 320 \t Training Loss: 45.15058917999268\n",
      "Epoch 95 \t Batch 340 \t Training Loss: 45.144897629232965\n",
      "Epoch 95 \t Batch 360 \t Training Loss: 45.174483638339574\n",
      "Epoch 95 \t Batch 380 \t Training Loss: 45.15430314917313\n",
      "Epoch 95 \t Batch 400 \t Training Loss: 45.24241150856018\n",
      "Epoch 95 \t Batch 420 \t Training Loss: 45.27582835242862\n",
      "Epoch 95 \t Batch 440 \t Training Loss: 45.20661718195135\n",
      "Epoch 95 \t Batch 460 \t Training Loss: 45.210803570954695\n",
      "Epoch 95 \t Batch 480 \t Training Loss: 45.27050297260284\n",
      "Epoch 95 \t Batch 500 \t Training Loss: 45.29483815765381\n",
      "Epoch 95 \t Batch 520 \t Training Loss: 45.33970061815702\n",
      "Epoch 95 \t Batch 540 \t Training Loss: 45.352868984363695\n",
      "Epoch 95 \t Batch 560 \t Training Loss: 45.33783460344587\n",
      "Epoch 95 \t Batch 580 \t Training Loss: 45.345422244894095\n",
      "Epoch 95 \t Batch 600 \t Training Loss: 45.32527523676554\n",
      "Epoch 95 \t Batch 620 \t Training Loss: 45.32423536854406\n",
      "Epoch 95 \t Batch 640 \t Training Loss: 45.33016214370728\n",
      "Epoch 95 \t Batch 660 \t Training Loss: 45.3967523690426\n",
      "Epoch 95 \t Batch 680 \t Training Loss: 45.352895416932945\n",
      "Epoch 95 \t Batch 700 \t Training Loss: 45.371465339660645\n",
      "Epoch 95 \t Batch 720 \t Training Loss: 45.34218307071262\n",
      "Epoch 95 \t Batch 740 \t Training Loss: 45.36739043158454\n",
      "Epoch 95 \t Batch 760 \t Training Loss: 45.394885374370375\n",
      "Epoch 95 \t Batch 780 \t Training Loss: 45.40818096063076\n",
      "Epoch 95 \t Batch 800 \t Training Loss: 45.39018371105194\n",
      "Epoch 95 \t Batch 820 \t Training Loss: 45.450119362807854\n",
      "Epoch 95 \t Batch 840 \t Training Loss: 45.4810228120713\n",
      "Epoch 95 \t Batch 860 \t Training Loss: 45.48974742445835\n",
      "Epoch 95 \t Batch 880 \t Training Loss: 45.48231254924428\n",
      "Epoch 95 \t Batch 900 \t Training Loss: 45.49483922746447\n",
      "Epoch 95 \t Batch 20 \t Validation Loss: 14.297022438049316\n",
      "Epoch 95 \t Batch 40 \t Validation Loss: 15.900587224960328\n",
      "Epoch 95 \t Batch 60 \t Validation Loss: 16.02248231569926\n",
      "Epoch 95 \t Batch 80 \t Validation Loss: 16.865116703510285\n",
      "Epoch 95 \t Batch 100 \t Validation Loss: 18.996319627761842\n",
      "Epoch 95 \t Batch 120 \t Validation Loss: 20.714546704292296\n",
      "Epoch 95 \t Batch 140 \t Validation Loss: 21.773823486055647\n",
      "Epoch 95 \t Batch 160 \t Validation Loss: 24.258455330133437\n",
      "Epoch 95 \t Batch 180 \t Validation Loss: 28.340892632802326\n",
      "Epoch 95 \t Batch 200 \t Validation Loss: 30.096688375473022\n",
      "Epoch 95 \t Batch 220 \t Validation Loss: 31.64304842515425\n",
      "Epoch 95 \t Batch 240 \t Validation Loss: 32.35463214317958\n",
      "Epoch 95 \t Batch 260 \t Validation Loss: 34.63398298483629\n",
      "Epoch 95 \t Batch 280 \t Validation Loss: 35.921232547078816\n",
      "Epoch 95 \t Batch 300 \t Validation Loss: 37.23921403566996\n",
      "Epoch 95 \t Batch 320 \t Validation Loss: 37.87965634167195\n",
      "Epoch 95 \t Batch 340 \t Validation Loss: 37.92707513640909\n",
      "Epoch 95 \t Batch 360 \t Validation Loss: 37.90258613957299\n",
      "Epoch 95 \t Batch 380 \t Validation Loss: 38.22063375523216\n",
      "Epoch 95 \t Batch 400 \t Validation Loss: 37.88428116559982\n",
      "Epoch 95 \t Batch 420 \t Validation Loss: 37.932514297394526\n",
      "Epoch 95 \t Batch 440 \t Validation Loss: 37.67421699220484\n",
      "Epoch 95 \t Batch 460 \t Validation Loss: 37.99990755371425\n",
      "Epoch 95 \t Batch 480 \t Validation Loss: 38.553493609031044\n",
      "Epoch 95 \t Batch 500 \t Validation Loss: 38.33748731803894\n",
      "Epoch 95 \t Batch 520 \t Validation Loss: 38.17640514923976\n",
      "Epoch 95 \t Batch 540 \t Validation Loss: 37.966502530486494\n",
      "Epoch 95 \t Batch 560 \t Validation Loss: 37.78990873779569\n",
      "Epoch 95 \t Batch 580 \t Validation Loss: 37.55879800237458\n",
      "Epoch 95 \t Batch 600 \t Validation Loss: 37.81290060202281\n",
      "Epoch 95 Training Loss: 45.46428022311844 Validation Loss: 38.503167166338336\n",
      "Epoch 95 completed\n",
      "Epoch 96 \t Batch 20 \t Training Loss: 44.93174819946289\n",
      "Epoch 96 \t Batch 40 \t Training Loss: 45.159930038452146\n",
      "Epoch 96 \t Batch 60 \t Training Loss: 45.528279050191244\n",
      "Epoch 96 \t Batch 80 \t Training Loss: 45.475528192520144\n",
      "Epoch 96 \t Batch 100 \t Training Loss: 45.48362075805664\n",
      "Epoch 96 \t Batch 120 \t Training Loss: 45.459729480743405\n",
      "Epoch 96 \t Batch 140 \t Training Loss: 45.412496430533274\n",
      "Epoch 96 \t Batch 160 \t Training Loss: 45.227100706100465\n",
      "Epoch 96 \t Batch 180 \t Training Loss: 45.27911656697591\n",
      "Epoch 96 \t Batch 200 \t Training Loss: 45.30584926605225\n",
      "Epoch 96 \t Batch 220 \t Training Loss: 45.34407891360196\n",
      "Epoch 96 \t Batch 240 \t Training Loss: 45.457589864730835\n",
      "Epoch 96 \t Batch 260 \t Training Loss: 45.465970934354345\n",
      "Epoch 96 \t Batch 280 \t Training Loss: 45.45547387259347\n",
      "Epoch 96 \t Batch 300 \t Training Loss: 45.47967399597168\n",
      "Epoch 96 \t Batch 320 \t Training Loss: 45.389621591567995\n",
      "Epoch 96 \t Batch 340 \t Training Loss: 45.42273194930133\n",
      "Epoch 96 \t Batch 360 \t Training Loss: 45.459143829345706\n",
      "Epoch 96 \t Batch 380 \t Training Loss: 45.4884760806435\n",
      "Epoch 96 \t Batch 400 \t Training Loss: 45.51267256736755\n",
      "Epoch 96 \t Batch 420 \t Training Loss: 45.461492729187015\n",
      "Epoch 96 \t Batch 440 \t Training Loss: 45.43716600591486\n",
      "Epoch 96 \t Batch 460 \t Training Loss: 45.42969520402991\n",
      "Epoch 96 \t Batch 480 \t Training Loss: 45.47760090827942\n",
      "Epoch 96 \t Batch 500 \t Training Loss: 45.466218437194826\n",
      "Epoch 96 \t Batch 520 \t Training Loss: 45.491499321277324\n",
      "Epoch 96 \t Batch 540 \t Training Loss: 45.524665959676106\n",
      "Epoch 96 \t Batch 560 \t Training Loss: 45.5283484186445\n",
      "Epoch 96 \t Batch 580 \t Training Loss: 45.501655473380254\n",
      "Epoch 96 \t Batch 600 \t Training Loss: 45.43034622192383\n",
      "Epoch 96 \t Batch 620 \t Training Loss: 45.43141494874031\n",
      "Epoch 96 \t Batch 640 \t Training Loss: 45.42524825930595\n",
      "Epoch 96 \t Batch 660 \t Training Loss: 45.415747399763625\n",
      "Epoch 96 \t Batch 680 \t Training Loss: 45.41542308470782\n",
      "Epoch 96 \t Batch 700 \t Training Loss: 45.43564507075718\n",
      "Epoch 96 \t Batch 720 \t Training Loss: 45.44378996425205\n",
      "Epoch 96 \t Batch 740 \t Training Loss: 45.45506218575142\n",
      "Epoch 96 \t Batch 760 \t Training Loss: 45.45109510923687\n",
      "Epoch 96 \t Batch 780 \t Training Loss: 45.46099151220077\n",
      "Epoch 96 \t Batch 800 \t Training Loss: 45.45315508365631\n",
      "Epoch 96 \t Batch 820 \t Training Loss: 45.44878371168927\n",
      "Epoch 96 \t Batch 840 \t Training Loss: 45.42664442062378\n",
      "Epoch 96 \t Batch 860 \t Training Loss: 45.445533002809036\n",
      "Epoch 96 \t Batch 880 \t Training Loss: 45.43398475646973\n",
      "Epoch 96 \t Batch 900 \t Training Loss: 45.423832444085015\n",
      "Epoch 96 \t Batch 20 \t Validation Loss: 22.584913539886475\n",
      "Epoch 96 \t Batch 40 \t Validation Loss: 24.2214848279953\n",
      "Epoch 96 \t Batch 60 \t Validation Loss: 23.90294426282247\n",
      "Epoch 96 \t Batch 80 \t Validation Loss: 24.41649661064148\n",
      "Epoch 96 \t Batch 100 \t Validation Loss: 25.536332092285157\n",
      "Epoch 96 \t Batch 120 \t Validation Loss: 26.554959948857626\n",
      "Epoch 96 \t Batch 140 \t Validation Loss: 26.96248162133353\n",
      "Epoch 96 \t Batch 160 \t Validation Loss: 28.79696943759918\n",
      "Epoch 96 \t Batch 180 \t Validation Loss: 32.27693929672241\n",
      "Epoch 96 \t Batch 200 \t Validation Loss: 33.62486825942993\n",
      "Epoch 96 \t Batch 220 \t Validation Loss: 34.73890020197088\n",
      "Epoch 96 \t Batch 240 \t Validation Loss: 35.13245258331299\n",
      "Epoch 96 \t Batch 260 \t Validation Loss: 37.13934075648968\n",
      "Epoch 96 \t Batch 280 \t Validation Loss: 38.220929397855485\n",
      "Epoch 96 \t Batch 300 \t Validation Loss: 39.311584803263344\n",
      "Epoch 96 \t Batch 320 \t Validation Loss: 39.780968913435935\n",
      "Epoch 96 \t Batch 340 \t Validation Loss: 39.700320813235116\n",
      "Epoch 96 \t Batch 360 \t Validation Loss: 39.550038615862526\n",
      "Epoch 96 \t Batch 380 \t Validation Loss: 39.76048126973604\n",
      "Epoch 96 \t Batch 400 \t Validation Loss: 39.361014378070834\n",
      "Epoch 96 \t Batch 420 \t Validation Loss: 39.33327303159805\n",
      "Epoch 96 \t Batch 440 \t Validation Loss: 39.037846220623365\n",
      "Epoch 96 \t Batch 460 \t Validation Loss: 39.340680203230484\n",
      "Epoch 96 \t Batch 480 \t Validation Loss: 39.824458529551826\n",
      "Epoch 96 \t Batch 500 \t Validation Loss: 39.55408678245544\n",
      "Epoch 96 \t Batch 520 \t Validation Loss: 39.39681576398703\n",
      "Epoch 96 \t Batch 540 \t Validation Loss: 39.16933799496403\n",
      "Epoch 96 \t Batch 560 \t Validation Loss: 38.99644117525646\n",
      "Epoch 96 \t Batch 580 \t Validation Loss: 38.780888384786145\n",
      "Epoch 96 \t Batch 600 \t Validation Loss: 38.99866685390472\n",
      "Epoch 96 Training Loss: 45.42446359033283 Validation Loss: 39.68615844807068\n",
      "Epoch 96 completed\n",
      "Epoch 97 \t Batch 20 \t Training Loss: 44.33877677917481\n",
      "Epoch 97 \t Batch 40 \t Training Loss: 45.12610311508179\n",
      "Epoch 97 \t Batch 60 \t Training Loss: 44.538926951090495\n",
      "Epoch 97 \t Batch 80 \t Training Loss: 45.23946657180786\n",
      "Epoch 97 \t Batch 100 \t Training Loss: 45.07101417541504\n",
      "Epoch 97 \t Batch 120 \t Training Loss: 45.23459952672322\n",
      "Epoch 97 \t Batch 140 \t Training Loss: 45.266232763017925\n",
      "Epoch 97 \t Batch 160 \t Training Loss: 45.280167722702025\n",
      "Epoch 97 \t Batch 180 \t Training Loss: 45.36321551005046\n",
      "Epoch 97 \t Batch 200 \t Training Loss: 45.275839500427246\n",
      "Epoch 97 \t Batch 220 \t Training Loss: 45.21367992054332\n",
      "Epoch 97 \t Batch 240 \t Training Loss: 45.15836653709412\n",
      "Epoch 97 \t Batch 260 \t Training Loss: 45.20396888439472\n",
      "Epoch 97 \t Batch 280 \t Training Loss: 45.2294324193682\n",
      "Epoch 97 \t Batch 300 \t Training Loss: 45.34178164164225\n",
      "Epoch 97 \t Batch 320 \t Training Loss: 45.31926646232605\n",
      "Epoch 97 \t Batch 340 \t Training Loss: 45.20244286481072\n",
      "Epoch 97 \t Batch 360 \t Training Loss: 45.211329778035484\n",
      "Epoch 97 \t Batch 380 \t Training Loss: 45.299185772946004\n",
      "Epoch 97 \t Batch 400 \t Training Loss: 45.26688020706177\n",
      "Epoch 97 \t Batch 420 \t Training Loss: 45.32120546613421\n",
      "Epoch 97 \t Batch 440 \t Training Loss: 45.357488198713824\n",
      "Epoch 97 \t Batch 460 \t Training Loss: 45.373387427951975\n",
      "Epoch 97 \t Batch 480 \t Training Loss: 45.34002078374227\n",
      "Epoch 97 \t Batch 500 \t Training Loss: 45.34104261016846\n",
      "Epoch 97 \t Batch 520 \t Training Loss: 45.37827072143555\n",
      "Epoch 97 \t Batch 540 \t Training Loss: 45.399063399985984\n",
      "Epoch 97 \t Batch 560 \t Training Loss: 45.424044779368806\n",
      "Epoch 97 \t Batch 580 \t Training Loss: 45.427660100213416\n",
      "Epoch 97 \t Batch 600 \t Training Loss: 45.40658582051595\n",
      "Epoch 97 \t Batch 620 \t Training Loss: 45.41154655333488\n",
      "Epoch 97 \t Batch 640 \t Training Loss: 45.41152885556221\n",
      "Epoch 97 \t Batch 660 \t Training Loss: 45.41849613767682\n",
      "Epoch 97 \t Batch 680 \t Training Loss: 45.398542544421026\n",
      "Epoch 97 \t Batch 700 \t Training Loss: 45.38701309749058\n",
      "Epoch 97 \t Batch 720 \t Training Loss: 45.364110204908584\n",
      "Epoch 97 \t Batch 740 \t Training Loss: 45.40374729053394\n",
      "Epoch 97 \t Batch 760 \t Training Loss: 45.437148059041874\n",
      "Epoch 97 \t Batch 780 \t Training Loss: 45.4757266215789\n",
      "Epoch 97 \t Batch 800 \t Training Loss: 45.45906830310822\n",
      "Epoch 97 \t Batch 820 \t Training Loss: 45.444501565142374\n",
      "Epoch 97 \t Batch 840 \t Training Loss: 45.45222503117153\n",
      "Epoch 97 \t Batch 860 \t Training Loss: 45.478304778697876\n",
      "Epoch 97 \t Batch 880 \t Training Loss: 45.48287120298906\n",
      "Epoch 97 \t Batch 900 \t Training Loss: 45.43755837334527\n",
      "Epoch 97 \t Batch 20 \t Validation Loss: 17.797243356704712\n",
      "Epoch 97 \t Batch 40 \t Validation Loss: 19.94359211921692\n",
      "Epoch 97 \t Batch 60 \t Validation Loss: 19.60562523206075\n",
      "Epoch 97 \t Batch 80 \t Validation Loss: 20.434200096130372\n",
      "Epoch 97 \t Batch 100 \t Validation Loss: 22.10488374710083\n",
      "Epoch 97 \t Batch 120 \t Validation Loss: 23.328765948613484\n",
      "Epoch 97 \t Batch 140 \t Validation Loss: 24.082487065451485\n",
      "Epoch 97 \t Batch 160 \t Validation Loss: 26.31617681980133\n",
      "Epoch 97 \t Batch 180 \t Validation Loss: 30.28668931855096\n",
      "Epoch 97 \t Batch 200 \t Validation Loss: 31.860527215003966\n",
      "Epoch 97 \t Batch 220 \t Validation Loss: 33.2230292970484\n",
      "Epoch 97 \t Batch 240 \t Validation Loss: 33.83143944342931\n",
      "Epoch 97 \t Batch 260 \t Validation Loss: 36.00869852946355\n",
      "Epoch 97 \t Batch 280 \t Validation Loss: 37.21113208362034\n",
      "Epoch 97 \t Batch 300 \t Validation Loss: 38.48753946304321\n",
      "Epoch 97 \t Batch 320 \t Validation Loss: 39.064585340023044\n",
      "Epoch 97 \t Batch 340 \t Validation Loss: 39.045684309566724\n",
      "Epoch 97 \t Batch 360 \t Validation Loss: 38.9876344203949\n",
      "Epoch 97 \t Batch 380 \t Validation Loss: 39.248898581454625\n",
      "Epoch 97 \t Batch 400 \t Validation Loss: 38.885245795249936\n",
      "Epoch 97 \t Batch 420 \t Validation Loss: 38.87739225115095\n",
      "Epoch 97 \t Batch 440 \t Validation Loss: 38.61292033628984\n",
      "Epoch 97 \t Batch 460 \t Validation Loss: 38.89325874577398\n",
      "Epoch 97 \t Batch 480 \t Validation Loss: 39.38646511634191\n",
      "Epoch 97 \t Batch 500 \t Validation Loss: 39.09829113006592\n",
      "Epoch 97 \t Batch 520 \t Validation Loss: 38.93679490823012\n",
      "Epoch 97 \t Batch 540 \t Validation Loss: 38.770037739365186\n",
      "Epoch 97 \t Batch 560 \t Validation Loss: 38.63104505879539\n",
      "Epoch 97 \t Batch 580 \t Validation Loss: 38.49959910162564\n",
      "Epoch 97 \t Batch 600 \t Validation Loss: 38.74351149559021\n",
      "Epoch 97 Training Loss: 45.415551941683404 Validation Loss: 39.44098085552067\n",
      "Epoch 97 completed\n",
      "Epoch 98 \t Batch 20 \t Training Loss: 45.96418418884277\n",
      "Epoch 98 \t Batch 40 \t Training Loss: 45.732877540588376\n",
      "Epoch 98 \t Batch 60 \t Training Loss: 45.449398803710935\n",
      "Epoch 98 \t Batch 80 \t Training Loss: 45.37627382278443\n",
      "Epoch 98 \t Batch 100 \t Training Loss: 45.430812492370606\n",
      "Epoch 98 \t Batch 120 \t Training Loss: 45.487403551737465\n",
      "Epoch 98 \t Batch 140 \t Training Loss: 45.283847781590055\n",
      "Epoch 98 \t Batch 160 \t Training Loss: 45.23357493877411\n",
      "Epoch 98 \t Batch 180 \t Training Loss: 45.35646673838298\n",
      "Epoch 98 \t Batch 200 \t Training Loss: 45.29731433868408\n",
      "Epoch 98 \t Batch 220 \t Training Loss: 45.383097700639205\n",
      "Epoch 98 \t Batch 240 \t Training Loss: 45.46067266464233\n",
      "Epoch 98 \t Batch 260 \t Training Loss: 45.451097752497745\n",
      "Epoch 98 \t Batch 280 \t Training Loss: 45.4652498790196\n",
      "Epoch 98 \t Batch 300 \t Training Loss: 45.49245454152425\n",
      "Epoch 98 \t Batch 320 \t Training Loss: 45.41381009817123\n",
      "Epoch 98 \t Batch 340 \t Training Loss: 45.464799555610206\n",
      "Epoch 98 \t Batch 360 \t Training Loss: 45.427597745259604\n",
      "Epoch 98 \t Batch 380 \t Training Loss: 45.49481936003033\n",
      "Epoch 98 \t Batch 400 \t Training Loss: 45.46155309677124\n",
      "Epoch 98 \t Batch 420 \t Training Loss: 45.47938045320057\n",
      "Epoch 98 \t Batch 440 \t Training Loss: 45.51129478107799\n",
      "Epoch 98 \t Batch 460 \t Training Loss: 45.482658377937646\n",
      "Epoch 98 \t Batch 480 \t Training Loss: 45.471096436182656\n",
      "Epoch 98 \t Batch 500 \t Training Loss: 45.442028419494626\n",
      "Epoch 98 \t Batch 520 \t Training Loss: 45.4116927587069\n",
      "Epoch 98 \t Batch 540 \t Training Loss: 45.462909401787655\n",
      "Epoch 98 \t Batch 560 \t Training Loss: 45.434362282071795\n",
      "Epoch 98 \t Batch 580 \t Training Loss: 45.40845993962781\n",
      "Epoch 98 \t Batch 600 \t Training Loss: 45.46940289815267\n",
      "Epoch 98 \t Batch 620 \t Training Loss: 45.47640533447266\n",
      "Epoch 98 \t Batch 640 \t Training Loss: 45.40329941511154\n",
      "Epoch 98 \t Batch 660 \t Training Loss: 45.43285013256651\n",
      "Epoch 98 \t Batch 680 \t Training Loss: 45.4026590571684\n",
      "Epoch 98 \t Batch 700 \t Training Loss: 45.37279537745884\n",
      "Epoch 98 \t Batch 720 \t Training Loss: 45.39323587417603\n",
      "Epoch 98 \t Batch 740 \t Training Loss: 45.40227959091599\n",
      "Epoch 98 \t Batch 760 \t Training Loss: 45.427041952233566\n",
      "Epoch 98 \t Batch 780 \t Training Loss: 45.42608150090927\n",
      "Epoch 98 \t Batch 800 \t Training Loss: 45.399156141281125\n",
      "Epoch 98 \t Batch 820 \t Training Loss: 45.38098493901695\n",
      "Epoch 98 \t Batch 840 \t Training Loss: 45.39102385384696\n",
      "Epoch 98 \t Batch 860 \t Training Loss: 45.38355790071709\n",
      "Epoch 98 \t Batch 880 \t Training Loss: 45.38019886450334\n",
      "Epoch 98 \t Batch 900 \t Training Loss: 45.389111760457354\n",
      "Epoch 98 \t Batch 20 \t Validation Loss: 18.685698318481446\n",
      "Epoch 98 \t Batch 40 \t Validation Loss: 19.744087862968446\n",
      "Epoch 98 \t Batch 60 \t Validation Loss: 19.70773809750875\n",
      "Epoch 98 \t Batch 80 \t Validation Loss: 20.27344833612442\n",
      "Epoch 98 \t Batch 100 \t Validation Loss: 21.93164059638977\n",
      "Epoch 98 \t Batch 120 \t Validation Loss: 23.382484102249144\n",
      "Epoch 98 \t Batch 140 \t Validation Loss: 24.053234563555037\n",
      "Epoch 98 \t Batch 160 \t Validation Loss: 26.169996857643127\n",
      "Epoch 98 \t Batch 180 \t Validation Loss: 30.069144105911256\n",
      "Epoch 98 \t Batch 200 \t Validation Loss: 31.686783099174498\n",
      "Epoch 98 \t Batch 220 \t Validation Loss: 33.021272732994774\n",
      "Epoch 98 \t Batch 240 \t Validation Loss: 33.58292632500331\n",
      "Epoch 98 \t Batch 260 \t Validation Loss: 35.76521147214449\n",
      "Epoch 98 \t Batch 280 \t Validation Loss: 36.96768414633615\n",
      "Epoch 98 \t Batch 300 \t Validation Loss: 38.21089095433553\n",
      "Epoch 98 \t Batch 320 \t Validation Loss: 38.76952206194401\n",
      "Epoch 98 \t Batch 340 \t Validation Loss: 38.75354128725388\n",
      "Epoch 98 \t Batch 360 \t Validation Loss: 38.62085656854841\n",
      "Epoch 98 \t Batch 380 \t Validation Loss: 38.858099001332334\n",
      "Epoch 98 \t Batch 400 \t Validation Loss: 38.49084922552109\n",
      "Epoch 98 \t Batch 420 \t Validation Loss: 38.563688216890604\n",
      "Epoch 98 \t Batch 440 \t Validation Loss: 38.317412638664244\n",
      "Epoch 98 \t Batch 460 \t Validation Loss: 38.61351321469183\n",
      "Epoch 98 \t Batch 480 \t Validation Loss: 39.12858367562294\n",
      "Epoch 98 \t Batch 500 \t Validation Loss: 38.88503618812561\n",
      "Epoch 98 \t Batch 520 \t Validation Loss: 38.666602008159344\n",
      "Epoch 98 \t Batch 540 \t Validation Loss: 38.366458110456115\n",
      "Epoch 98 \t Batch 560 \t Validation Loss: 38.12984128849847\n",
      "Epoch 98 \t Batch 580 \t Validation Loss: 37.79464310448745\n",
      "Epoch 98 \t Batch 600 \t Validation Loss: 38.00118233839671\n",
      "Epoch 98 Training Loss: 45.39516617453605 Validation Loss: 38.63543320476235\n",
      "Epoch 98 completed\n",
      "Epoch 99 \t Batch 20 \t Training Loss: 45.11396865844726\n",
      "Epoch 99 \t Batch 40 \t Training Loss: 45.176993465423585\n",
      "Epoch 99 \t Batch 60 \t Training Loss: 45.264308802286784\n",
      "Epoch 99 \t Batch 80 \t Training Loss: 45.44330105781555\n",
      "Epoch 99 \t Batch 100 \t Training Loss: 45.74763954162598\n",
      "Epoch 99 \t Batch 120 \t Training Loss: 45.6991348584493\n",
      "Epoch 99 \t Batch 140 \t Training Loss: 45.6866039276123\n",
      "Epoch 99 \t Batch 160 \t Training Loss: 45.714147782325746\n",
      "Epoch 99 \t Batch 180 \t Training Loss: 45.69516444736057\n",
      "Epoch 99 \t Batch 200 \t Training Loss: 45.71498107910156\n",
      "Epoch 99 \t Batch 220 \t Training Loss: 45.5552113793113\n",
      "Epoch 99 \t Batch 240 \t Training Loss: 45.57002158164978\n",
      "Epoch 99 \t Batch 260 \t Training Loss: 45.43930187225342\n",
      "Epoch 99 \t Batch 280 \t Training Loss: 45.45575299944196\n",
      "Epoch 99 \t Batch 300 \t Training Loss: 45.46855800628662\n",
      "Epoch 99 \t Batch 320 \t Training Loss: 45.4252214550972\n",
      "Epoch 99 \t Batch 340 \t Training Loss: 45.4638282214894\n",
      "Epoch 99 \t Batch 360 \t Training Loss: 45.45222680833604\n",
      "Epoch 99 \t Batch 380 \t Training Loss: 45.473967491953\n",
      "Epoch 99 \t Batch 400 \t Training Loss: 45.514893684387204\n",
      "Epoch 99 \t Batch 420 \t Training Loss: 45.42014272780646\n",
      "Epoch 99 \t Batch 440 \t Training Loss: 45.47580298510465\n",
      "Epoch 99 \t Batch 460 \t Training Loss: 45.4105849970942\n",
      "Epoch 99 \t Batch 480 \t Training Loss: 45.35112617015839\n",
      "Epoch 99 \t Batch 500 \t Training Loss: 45.35275131988526\n",
      "Epoch 99 \t Batch 520 \t Training Loss: 45.40094755612887\n",
      "Epoch 99 \t Batch 540 \t Training Loss: 45.40651671091715\n",
      "Epoch 99 \t Batch 560 \t Training Loss: 45.39983980996268\n",
      "Epoch 99 \t Batch 580 \t Training Loss: 45.40907297463253\n",
      "Epoch 99 \t Batch 600 \t Training Loss: 45.45670510609945\n",
      "Epoch 99 \t Batch 620 \t Training Loss: 45.416998038753384\n",
      "Epoch 99 \t Batch 640 \t Training Loss: 45.43659843206406\n",
      "Epoch 99 \t Batch 660 \t Training Loss: 45.452489968502164\n",
      "Epoch 99 \t Batch 680 \t Training Loss: 45.50641303903916\n",
      "Epoch 99 \t Batch 700 \t Training Loss: 45.434427860804966\n",
      "Epoch 99 \t Batch 720 \t Training Loss: 45.420914014180504\n",
      "Epoch 99 \t Batch 740 \t Training Loss: 45.41780776462039\n",
      "Epoch 99 \t Batch 760 \t Training Loss: 45.40848871532239\n",
      "Epoch 99 \t Batch 780 \t Training Loss: 45.359002533937115\n",
      "Epoch 99 \t Batch 800 \t Training Loss: 45.3679200220108\n",
      "Epoch 99 \t Batch 820 \t Training Loss: 45.305839026846535\n",
      "Epoch 99 \t Batch 840 \t Training Loss: 45.29389725185576\n",
      "Epoch 99 \t Batch 860 \t Training Loss: 45.28907222747803\n",
      "Epoch 99 \t Batch 880 \t Training Loss: 45.29506166631525\n",
      "Epoch 99 \t Batch 900 \t Training Loss: 45.31667402903239\n",
      "Epoch 99 \t Batch 20 \t Validation Loss: 14.838663339614868\n",
      "Epoch 99 \t Batch 40 \t Validation Loss: 16.84849610328674\n",
      "Epoch 99 \t Batch 60 \t Validation Loss: 16.731521209081013\n",
      "Epoch 99 \t Batch 80 \t Validation Loss: 17.727350091934206\n",
      "Epoch 99 \t Batch 100 \t Validation Loss: 19.884855651855467\n",
      "Epoch 99 \t Batch 120 \t Validation Loss: 21.7084100484848\n",
      "Epoch 99 \t Batch 140 \t Validation Loss: 22.79510942867824\n",
      "Epoch 99 \t Batch 160 \t Validation Loss: 25.33739053606987\n",
      "Epoch 99 \t Batch 180 \t Validation Loss: 29.61657264497545\n",
      "Epoch 99 \t Batch 200 \t Validation Loss: 31.48357731819153\n",
      "Epoch 99 \t Batch 220 \t Validation Loss: 32.99593854383989\n",
      "Epoch 99 \t Batch 240 \t Validation Loss: 33.76138490835826\n",
      "Epoch 99 \t Batch 260 \t Validation Loss: 36.05157122612\n",
      "Epoch 99 \t Batch 280 \t Validation Loss: 37.31705042294094\n",
      "Epoch 99 \t Batch 300 \t Validation Loss: 38.64695176442464\n",
      "Epoch 99 \t Batch 320 \t Validation Loss: 39.24206702113152\n",
      "Epoch 99 \t Batch 340 \t Validation Loss: 39.247477638020236\n",
      "Epoch 99 \t Batch 360 \t Validation Loss: 39.21809721522861\n",
      "Epoch 99 \t Batch 380 \t Validation Loss: 39.45315670214201\n",
      "Epoch 99 \t Batch 400 \t Validation Loss: 39.06680383682251\n",
      "Epoch 99 \t Batch 420 \t Validation Loss: 39.08802006131127\n",
      "Epoch 99 \t Batch 440 \t Validation Loss: 38.77128194028681\n",
      "Epoch 99 \t Batch 460 \t Validation Loss: 39.02498118773751\n",
      "Epoch 99 \t Batch 480 \t Validation Loss: 39.498647268613176\n",
      "Epoch 99 \t Batch 500 \t Validation Loss: 39.25066024017334\n",
      "Epoch 99 \t Batch 520 \t Validation Loss: 39.047392969865065\n",
      "Epoch 99 \t Batch 540 \t Validation Loss: 38.858897120864306\n",
      "Epoch 99 \t Batch 560 \t Validation Loss: 38.766557042939326\n",
      "Epoch 99 \t Batch 580 \t Validation Loss: 38.6128785560871\n",
      "Epoch 99 \t Batch 600 \t Validation Loss: 38.859541479746504\n",
      "Epoch 99 Training Loss: 45.35973621480728 Validation Loss: 39.551417601573\n",
      "Epoch 99 completed\n",
      "Epoch 100 \t Batch 20 \t Training Loss: 44.81452388763428\n",
      "Epoch 100 \t Batch 40 \t Training Loss: 45.43367176055908\n",
      "Epoch 100 \t Batch 60 \t Training Loss: 45.47930380503337\n",
      "Epoch 100 \t Batch 80 \t Training Loss: 45.28748841285706\n",
      "Epoch 100 \t Batch 100 \t Training Loss: 45.102560920715334\n",
      "Epoch 100 \t Batch 120 \t Training Loss: 45.20312576293945\n",
      "Epoch 100 \t Batch 140 \t Training Loss: 45.17543204171317\n",
      "Epoch 100 \t Batch 160 \t Training Loss: 45.24473886489868\n",
      "Epoch 100 \t Batch 180 \t Training Loss: 45.138334104749894\n",
      "Epoch 100 \t Batch 200 \t Training Loss: 45.100824203491214\n",
      "Epoch 100 \t Batch 220 \t Training Loss: 45.303048411282624\n",
      "Epoch 100 \t Batch 240 \t Training Loss: 45.27571598688761\n",
      "Epoch 100 \t Batch 260 \t Training Loss: 45.303281534635104\n",
      "Epoch 100 \t Batch 280 \t Training Loss: 45.36017144066947\n",
      "Epoch 100 \t Batch 300 \t Training Loss: 45.31464018503825\n",
      "Epoch 100 \t Batch 320 \t Training Loss: 45.3812029838562\n",
      "Epoch 100 \t Batch 340 \t Training Loss: 45.366002957961136\n",
      "Epoch 100 \t Batch 360 \t Training Loss: 45.406161562601724\n",
      "Epoch 100 \t Batch 380 \t Training Loss: 45.40650131827906\n",
      "Epoch 100 \t Batch 400 \t Training Loss: 45.40096429824829\n",
      "Epoch 100 \t Batch 420 \t Training Loss: 45.381333759852815\n",
      "Epoch 100 \t Batch 440 \t Training Loss: 45.395298671722415\n",
      "Epoch 100 \t Batch 460 \t Training Loss: 45.33345018469769\n",
      "Epoch 100 \t Batch 480 \t Training Loss: 45.37624614238739\n",
      "Epoch 100 \t Batch 500 \t Training Loss: 45.334620262146\n",
      "Epoch 100 \t Batch 520 \t Training Loss: 45.33167090782752\n",
      "Epoch 100 \t Batch 540 \t Training Loss: 45.34103507995606\n",
      "Epoch 100 \t Batch 560 \t Training Loss: 45.326522615977694\n",
      "Epoch 100 \t Batch 580 \t Training Loss: 45.304955949454474\n",
      "Epoch 100 \t Batch 600 \t Training Loss: 45.30785758972168\n",
      "Epoch 100 \t Batch 620 \t Training Loss: 45.318331373891525\n",
      "Epoch 100 \t Batch 640 \t Training Loss: 45.329917550086975\n",
      "Epoch 100 \t Batch 660 \t Training Loss: 45.31444259990345\n",
      "Epoch 100 \t Batch 680 \t Training Loss: 45.32748795677634\n",
      "Epoch 100 \t Batch 700 \t Training Loss: 45.30085076468332\n",
      "Epoch 100 \t Batch 720 \t Training Loss: 45.30599414507548\n",
      "Epoch 100 \t Batch 740 \t Training Loss: 45.324661321897764\n",
      "Epoch 100 \t Batch 760 \t Training Loss: 45.29697309795179\n",
      "Epoch 100 \t Batch 780 \t Training Loss: 45.27131050794552\n",
      "Epoch 100 \t Batch 800 \t Training Loss: 45.27288969516754\n",
      "Epoch 100 \t Batch 820 \t Training Loss: 45.25114800988174\n",
      "Epoch 100 \t Batch 840 \t Training Loss: 45.27358222234817\n",
      "Epoch 100 \t Batch 860 \t Training Loss: 45.314485070871754\n",
      "Epoch 100 \t Batch 880 \t Training Loss: 45.316604796322906\n",
      "Epoch 100 \t Batch 900 \t Training Loss: 45.31716094122993\n",
      "Epoch 100 \t Batch 20 \t Validation Loss: 12.765260648727416\n",
      "Epoch 100 \t Batch 40 \t Validation Loss: 15.971656823158265\n",
      "Epoch 100 \t Batch 60 \t Validation Loss: 15.691496006647746\n",
      "Epoch 100 \t Batch 80 \t Validation Loss: 16.704123699665068\n",
      "Epoch 100 \t Batch 100 \t Validation Loss: 19.08163010597229\n",
      "Epoch 100 \t Batch 120 \t Validation Loss: 20.86041378180186\n",
      "Epoch 100 \t Batch 140 \t Validation Loss: 22.025705657686505\n",
      "Epoch 100 \t Batch 160 \t Validation Loss: 24.635983604192734\n",
      "Epoch 100 \t Batch 180 \t Validation Loss: 28.849590031305947\n",
      "Epoch 100 \t Batch 200 \t Validation Loss: 30.70655424594879\n",
      "Epoch 100 \t Batch 220 \t Validation Loss: 32.216604540564795\n",
      "Epoch 100 \t Batch 240 \t Validation Loss: 32.943346639474235\n",
      "Epoch 100 \t Batch 260 \t Validation Loss: 35.26464644211989\n",
      "Epoch 100 \t Batch 280 \t Validation Loss: 36.544309210777286\n",
      "Epoch 100 \t Batch 300 \t Validation Loss: 37.92835594495138\n",
      "Epoch 100 \t Batch 320 \t Validation Loss: 38.578776648640634\n",
      "Epoch 100 \t Batch 340 \t Validation Loss: 38.609434259639066\n",
      "Epoch 100 \t Batch 360 \t Validation Loss: 38.58804609775543\n",
      "Epoch 100 \t Batch 380 \t Validation Loss: 38.90743462662948\n",
      "Epoch 100 \t Batch 400 \t Validation Loss: 38.573793127536774\n",
      "Epoch 100 \t Batch 420 \t Validation Loss: 38.60901573953174\n",
      "Epoch 100 \t Batch 440 \t Validation Loss: 38.356108385866335\n",
      "Epoch 100 \t Batch 460 \t Validation Loss: 38.7202880838643\n",
      "Epoch 100 \t Batch 480 \t Validation Loss: 39.24115874171257\n",
      "Epoch 100 \t Batch 500 \t Validation Loss: 38.997436361312865\n",
      "Epoch 100 \t Batch 520 \t Validation Loss: 38.90190118826353\n",
      "Epoch 100 \t Batch 540 \t Validation Loss: 38.7509228653378\n",
      "Epoch 100 \t Batch 560 \t Validation Loss: 38.60445252997535\n",
      "Epoch 100 \t Batch 580 \t Validation Loss: 38.49049097751749\n",
      "Epoch 100 \t Batch 600 \t Validation Loss: 38.74516950130462\n",
      "Epoch 100 Training Loss: 45.296406486952186 Validation Loss: 39.48174183554464\n",
      "Epoch 100 completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from dataloader import *\n",
    "\n",
    "model = SimpleFCN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "class RMSE(nn.Module):\n",
    "    \"\"\" \n",
    "        Weighted RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        self.mse = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    def __call__(self, prediction, target, weights = 1):\n",
    "        # prediction = prediction[:, 0]\n",
    "        return torch.sqrt(torch.mean(weights * self.mse(prediction,target)))\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.latlon = True\n",
    "        self.bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "        self.bm = True\n",
    "        self.patch_size = [15,15]\n",
    "        self.norm_strat = 'pct'\n",
    "        self.norm = False\n",
    "\n",
    "args = Args()\n",
    "fnames = ['data_nonan_0-5.h5', 'data_nonan_1-5.h5', 'data_nonan_2-5.h5', 'data_nonan_3-5.h5', 'data_nonan_4-5.h5']\n",
    "mode = 'train'\n",
    "ds_training = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "mode = 'val'\n",
    "ds_validation = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch in range(100):  # 10 epochs\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(\"inputs.shape: \", inputs.shape)\n",
    "        # print(\"targets.shape: \", targets.shape)\n",
    "        # # # print(outputs)\n",
    "        # print(\"outputs.shape: \", outputs.shape)\n",
    "        # loss1 = criterion(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    i=0\n",
    "    model.eval()\n",
    "    for inputs, targets in validloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        valid_loss += loss.item()\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    " \n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model2.pth')\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
