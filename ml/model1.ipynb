{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S2_bands': {'B01': {'mean': 0.13021514, 'std': 0.017152175, 'min': 1e-04, 'max': 1.1213, 'p1': 0.1273, 'p99': 0.1074}, 'B02': {'mean': 0.1363337, 'std': 0.018509913, 'min': 1e-04, 'max': 1.8768, 'p1': 0.1366, 'p99': 0.1128}, 'B03': {'mean': 0.16427371, 'std': 0.02087248, 'min': 0.0411, 'max': 1.7888, 'p1': 0.1692, 'p99': 0.1364}, 'B04': {'mean': 0.13865142, 'std': 0.025569845, 'min': 0.0121, 'max': 1.7232, 'p1': 0.1445, 'p99': 0.1184}, 'B05': {'mean': 0.20296873, 'std': 0.028621713, 'min': 0.0672, 'max': 1.6344, 'p1': 0.2157, 'p99': 0.1591}, 'B06': {'mean': 0.38582557, 'std': 0.070499, 'min': 0.0758, 'max': 1.6699, 'p1': 0.3286, 'p99': 0.2766}, 'B07': {'mean': 0.4361872, 'std': 0.086211845, 'min': 0.0573, 'max': 1.6645, 'p1': 0.3621, 'p99': 0.2278}, 'B08': {'mean': 0.4448093, 'std': 0.08623231, 'min': 0.0737, 'max': 1.6976, 'p1': 0.3588, 'p99': 0.2122}, 'B8A': {'mean': 0.4580875, 'std': 0.08798952, 'min': 0.0772, 'max': 1.6709, 'p1': 0.3775, 'p99': 0.26}, 'B09': {'mean': 0.45806482, 'std': 0.08441155, 'min': 0.0066, 'max': 1.7178, 'p1': 0.3818, 'p99': 0.2655}, 'B11': {'mean': 0.2941879, 'std': 0.04741236, 'min': 0.0994, 'max': 1.5738, 'p1': 0.3279, 'p99': 0.5299}, 'B12': {'mean': 0.19771282, 'std': 0.0438055, 'min': 0.0974, 'max': 1.6086, 'p1': 0.22, 'p99': 0.1688}}, 'BM': {'bm': {'mean': 50.714764, 'std': 35.78637, 'min': 0.0, 'max': 312.03958, 'p1': 0.0, 'p99': 127.57817}, 'std': {'mean': 7.036375, 'std': 4.400113, 'min': 0.0, 'max': 145.97275, 'p1': 0.0, 'p99': 2.180725}}, 'Sentinel_metadata': {'S2_vegetation_score': {'mean': 96.60825, 'std': 11.659279, 'min': 20.0, 'max': 100.0, 'p1': 100.0, 'p99': 48.0}, 'S2_date': {'mean': 470.45947, 'std': 169.251, 'min': 1.0, 'max': 620.0, 'p1': 212.0, 'p99': 122.0}}, 'GEDI': {'agbd': {'mean': 77.14966, 'std': 79.39693, 'min': 0.66613775, 'max': 499.96765, 'p1': 1.0619253, 'p99': 75.21779}, 'agbd_se': {'mean': 7.2501493, 'std': 2.1052322, 'min': 2.981795, 'max': 12.131867, 'p1': 3.004118, 'p99': 11.066575}, 'rh98': {'mean': 12.344467, 'std': 9.239699, 'min': 0.030004844, 'max': 96.50001, 'p1': 2.9900095, 'p99': 33.88001}, 'date': {'mean': 394.3948, 'std': 149.13177, 'min': 46.0, 'max': 531.0, 'p1': 158.0, 'p99': 503.0}}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the file in binary mode for reading\n",
    "with open('data/normalization_values.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Now you can analyze the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training tiles:  60\n",
      "validation tiles:  5\n",
      "testing tiles:  5\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "data = {'train': [], 'val': [], 'test': []} \n",
    "path_h5 = '/scratch2/biomass_estimation/code/ml/data'\n",
    "\n",
    "# Iterate over all the h5 files\n",
    "for fname in os.listdir(path_h5):\n",
    "    if fname.endswith('.h5'):\n",
    "        with h5py.File(os.path.join(path_h5, fname), 'r') as f:\n",
    "            # Get the list of all tiles in the file\n",
    "            all_tiles = list(f.keys())\n",
    "            \n",
    "            # Select one tile for validation, one for testing, and the rest for training\n",
    "            val_tile = all_tiles[0]\n",
    "            test_tile = all_tiles[1]\n",
    "            train_tiles = all_tiles[2:]\n",
    "            \n",
    "            # Add the selected tiles to the dictionary\n",
    "            data['val'].append(val_tile)\n",
    "            data['test'].append(test_tile)\n",
    "            data['train'].extend(train_tiles)\n",
    "\n",
    "print(\"training tiles: \", len(data['train']))\n",
    "print(\"validation tiles: \", len(data['val']))\n",
    "print(\"testing tiles: \", len(data['test']))\n",
    "# Pickle the DataFrame and save it to a file\n",
    "with open('/scratch2/biomass_estimation/code/ml/data/mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n"
     ]
    }
   ],
   "source": [
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=18,\n",
    "                 channel_dims = (16, 32, 64, 128),\n",
    "                 num_outputs=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=1):\n",
    "        \"\"\"\n",
    "        A simple fully convolutional neural network.\n",
    "        \"\"\"\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        layers = list()\n",
    "        for i in range(len(channel_dims)):\n",
    "            in_channels = in_features if i == 0 else channel_dims[i-1]\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=channel_dims[i], \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(num_features=channel_dims[i]))\n",
    "            layers.append(self.relu)\n",
    "        print(layers)\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(in_channels=channel_dims[-1], out_channels=num_outputs, kernel_size=1,\n",
    "                                     stride=1, padding=0, bias=True)\n",
    "        # self.fc = nn.Linear(15*15*num_outputs, 1)  # Fully connected layer to get a single output value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_output(x)\n",
    "        # x = x.flatten(start_dim=1)\n",
    "        # predictions = self.fc(x)\n",
    "        # return predictions.squeeze()  # Remove the extra dimension\n",
    "        return x\n",
    "    \n",
    "model = SimpleFCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n",
      "Epoch 1 \t Batch 20 \t Training Loss: 12331.168505859376\n",
      "Epoch 1 \t Batch 40 \t Training Loss: 8827.280963134766\n",
      "Epoch 1 \t Batch 60 \t Training Loss: 7065.375651041667\n",
      "Epoch 1 \t Batch 80 \t Training Loss: 6135.438473510742\n",
      "Epoch 1 \t Batch 100 \t Training Loss: 5545.693015136719\n",
      "Epoch 1 \t Batch 120 \t Training Loss: 5168.06693725586\n",
      "Epoch 1 \t Batch 140 \t Training Loss: 4896.802110072545\n",
      "Epoch 1 \t Batch 160 \t Training Loss: 4691.904225158692\n",
      "Epoch 1 \t Batch 180 \t Training Loss: 4547.167228190104\n",
      "Epoch 1 \t Batch 200 \t Training Loss: 4405.019379882812\n",
      "Epoch 1 \t Batch 220 \t Training Loss: 4287.649428488991\n",
      "Epoch 1 \t Batch 240 \t Training Loss: 4191.414419555664\n",
      "Epoch 1 \t Batch 260 \t Training Loss: 4108.676118351863\n",
      "Epoch 1 \t Batch 280 \t Training Loss: 4046.889047677176\n",
      "Epoch 1 \t Batch 300 \t Training Loss: 3970.0824389648437\n",
      "Epoch 1 \t Batch 320 \t Training Loss: 3910.6908737182616\n",
      "Epoch 1 \t Batch 340 \t Training Loss: 3862.9858857996323\n",
      "Epoch 1 \t Batch 360 \t Training Loss: 3827.90696750217\n",
      "Epoch 1 \t Batch 380 \t Training Loss: 3790.7090826737253\n",
      "Epoch 1 \t Batch 400 \t Training Loss: 3748.5975555419923\n",
      "Epoch 1 \t Batch 420 \t Training Loss: 3712.087893531436\n",
      "Epoch 1 \t Batch 440 \t Training Loss: 3686.784959272905\n",
      "Epoch 1 \t Batch 460 \t Training Loss: 3663.2632403829825\n",
      "Epoch 1 \t Batch 480 \t Training Loss: 3629.79263865153\n",
      "Epoch 1 \t Batch 500 \t Training Loss: 3598.265102050781\n",
      "Epoch 1 \t Batch 520 \t Training Loss: 3572.407932692308\n",
      "Epoch 1 \t Batch 540 \t Training Loss: 3553.7709757486978\n",
      "Epoch 1 \t Batch 560 \t Training Loss: 3535.0431448800223\n",
      "Epoch 1 \t Batch 580 \t Training Loss: 3517.0400415880927\n",
      "Epoch 1 \t Batch 600 \t Training Loss: 3495.198955078125\n",
      "Epoch 1 \t Batch 620 \t Training Loss: 3481.4541165259575\n",
      "Epoch 1 \t Batch 640 \t Training Loss: 3468.3815185546873\n",
      "Epoch 1 \t Batch 660 \t Training Loss: 3450.3001198508523\n",
      "Epoch 1 \t Batch 680 \t Training Loss: 3440.055856143727\n",
      "Epoch 1 \t Batch 700 \t Training Loss: 3423.567759137835\n",
      "Epoch 1 \t Batch 720 \t Training Loss: 3413.8175669352213\n",
      "Epoch 1 \t Batch 740 \t Training Loss: 3401.8791655669343\n",
      "Epoch 1 \t Batch 760 \t Training Loss: 3390.7431091308595\n",
      "Epoch 1 \t Batch 780 \t Training Loss: 3377.8225119566305\n",
      "Epoch 1 \t Batch 800 \t Training Loss: 3366.8868478393556\n",
      "Epoch 1 \t Batch 820 \t Training Loss: 3360.68972197742\n",
      "Epoch 1 \t Batch 840 \t Training Loss: 3353.1508120582216\n",
      "Epoch 1 \t Batch 860 \t Training Loss: 3342.7082604696584\n",
      "Epoch 1 \t Batch 880 \t Training Loss: 3331.103599687056\n",
      "Epoch 1 \t Batch 900 \t Training Loss: 3319.7695195855035\n",
      "Epoch 1 \t Batch 920 \t Training Loss: 3310.405427617612\n",
      "Epoch 1 \t Batch 940 \t Training Loss: 3301.0782546023106\n",
      "Epoch 1 \t Batch 960 \t Training Loss: 3294.43658803304\n",
      "Epoch 1 \t Batch 980 \t Training Loss: 3285.9811628069197\n",
      "Epoch 1 \t Batch 1000 \t Training Loss: 3279.5739482421877\n",
      "Epoch 1 \t Batch 1020 \t Training Loss: 3269.7431092505362\n",
      "Epoch 1 \t Batch 1040 \t Training Loss: 3264.9165635329027\n",
      "Epoch 1 \t Batch 1060 \t Training Loss: 3259.58293065485\n",
      "Epoch 1 \t Batch 1080 \t Training Loss: 3253.7309662995517\n",
      "Epoch 1 \t Batch 1100 \t Training Loss: 3249.745224165483\n",
      "Epoch 1 \t Batch 1120 \t Training Loss: 3245.1501839773996\n",
      "Epoch 1 \t Batch 1140 \t Training Loss: 3239.5065798040023\n",
      "Epoch 1 \t Batch 1160 \t Training Loss: 3233.241405408136\n",
      "Epoch 1 \t Batch 1180 \t Training Loss: 3226.214342847921\n",
      "Epoch 1 \t Batch 1200 \t Training Loss: 3219.244309285482\n",
      "Epoch 1 \t Batch 1220 \t Training Loss: 3213.9418895283684\n",
      "Epoch 1 \t Batch 1240 \t Training Loss: 3209.091687799269\n",
      "Epoch 1 \t Batch 1260 \t Training Loss: 3204.155411202567\n",
      "Epoch 1 \t Batch 1280 \t Training Loss: 3198.737105178833\n",
      "Epoch 1 \t Batch 1300 \t Training Loss: 3191.788045372596\n",
      "Epoch 1 \t Batch 1320 \t Training Loss: 3188.898414935488\n",
      "Epoch 1 \t Batch 1340 \t Training Loss: 3182.7542301918143\n",
      "Epoch 1 \t Batch 1360 \t Training Loss: 3180.543170884076\n",
      "Epoch 1 \t Batch 1380 \t Training Loss: 3175.230396746207\n",
      "Epoch 1 \t Batch 1400 \t Training Loss: 3169.428831438337\n",
      "Epoch 1 \t Batch 1420 \t Training Loss: 3165.976347587478\n",
      "Epoch 1 \t Batch 1440 \t Training Loss: 3161.622793918186\n",
      "Epoch 1 \t Batch 20 \t Validation Loss: 423.7471206665039\n",
      "Epoch 1 \t Batch 40 \t Validation Loss: 619.779825592041\n",
      "Epoch 1 \t Batch 60 \t Validation Loss: 647.4045344034831\n",
      "Epoch 1 \t Batch 80 \t Validation Loss: 657.1274681091309\n",
      "Epoch 1 \t Batch 100 \t Validation Loss: 784.0498468017578\n",
      "Epoch 1 \t Batch 120 \t Validation Loss: 1017.4684664408366\n",
      "Epoch 1 \t Batch 140 \t Validation Loss: 1089.0988082885742\n",
      "Epoch 1 \t Batch 160 \t Validation Loss: 1162.844323539734\n",
      "Epoch 1 \t Batch 180 \t Validation Loss: 1140.729462009006\n",
      "Epoch 1 \t Batch 200 \t Validation Loss: 1378.2876513671874\n",
      "Epoch 1 \t Batch 220 \t Validation Loss: 1445.4604017084296\n",
      "Epoch 1 \t Batch 240 \t Validation Loss: 1474.0999347050986\n",
      "Epoch 1 \t Batch 260 \t Validation Loss: 1514.3097706134504\n",
      "Epoch 1 \t Batch 280 \t Validation Loss: 1535.6437805720739\n",
      "Epoch 1 \t Batch 300 \t Validation Loss: 1510.2991830952963\n",
      "Epoch 1 \t Batch 320 \t Validation Loss: 1604.5888201713562\n",
      "Epoch 1 Training Loss: 3159.1693115234375 Validation Loss: 1604.5888201713562\n",
      "Validation Loss Decreased(inf--->513468.422454834) Saving The Model\n",
      "Epoch 1 completed\n",
      "Epoch 2 \t Batch 20 \t Training Loss: 2849.843151855469\n",
      "Epoch 2 \t Batch 40 \t Training Loss: 2926.514190673828\n",
      "Epoch 2 \t Batch 60 \t Training Loss: 2950.8673380533855\n",
      "Epoch 2 \t Batch 80 \t Training Loss: 2913.3115524291993\n",
      "Epoch 2 \t Batch 100 \t Training Loss: 2877.0734606933593\n",
      "Epoch 2 \t Batch 120 \t Training Loss: 2897.4665598551433\n",
      "Epoch 2 \t Batch 140 \t Training Loss: 2881.5365574428015\n",
      "Epoch 2 \t Batch 160 \t Training Loss: 2890.789365386963\n",
      "Epoch 2 \t Batch 180 \t Training Loss: 2861.75024210612\n",
      "Epoch 2 \t Batch 200 \t Training Loss: 2862.0514166259763\n",
      "Epoch 2 \t Batch 220 \t Training Loss: 2849.516347989169\n",
      "Epoch 2 \t Batch 240 \t Training Loss: 2850.9404139200847\n",
      "Epoch 2 \t Batch 260 \t Training Loss: 2842.9640629695014\n",
      "Epoch 2 \t Batch 280 \t Training Loss: 2844.3508845738\n",
      "Epoch 2 \t Batch 300 \t Training Loss: 2834.262353108724\n",
      "Epoch 2 \t Batch 320 \t Training Loss: 2839.591563796997\n",
      "Epoch 2 \t Batch 340 \t Training Loss: 2839.8190698960248\n",
      "Epoch 2 \t Batch 360 \t Training Loss: 2843.527622138129\n",
      "Epoch 2 \t Batch 380 \t Training Loss: 2844.204951878598\n",
      "Epoch 2 \t Batch 400 \t Training Loss: 2843.7543954467774\n",
      "Epoch 2 \t Batch 420 \t Training Loss: 2847.832661074684\n",
      "Epoch 2 \t Batch 440 \t Training Loss: 2841.4663976495917\n",
      "Epoch 2 \t Batch 460 \t Training Loss: 2840.371211839759\n",
      "Epoch 2 \t Batch 480 \t Training Loss: 2836.8157285054526\n",
      "Epoch 2 \t Batch 500 \t Training Loss: 2826.6284670410155\n",
      "Epoch 2 \t Batch 520 \t Training Loss: 2826.6156088022085\n",
      "Epoch 2 \t Batch 540 \t Training Loss: 2830.2419037995514\n",
      "Epoch 2 \t Batch 560 \t Training Loss: 2826.1686970302035\n",
      "Epoch 2 \t Batch 580 \t Training Loss: 2831.3563474457837\n",
      "Epoch 2 \t Batch 600 \t Training Loss: 2825.974951578776\n",
      "Epoch 2 \t Batch 620 \t Training Loss: 2823.305568965789\n",
      "Epoch 2 \t Batch 640 \t Training Loss: 2821.1792402267456\n",
      "Epoch 2 \t Batch 660 \t Training Loss: 2826.853428511186\n",
      "Epoch 2 \t Batch 680 \t Training Loss: 2823.435188742245\n",
      "Epoch 2 \t Batch 700 \t Training Loss: 2825.857705950056\n",
      "Epoch 2 \t Batch 720 \t Training Loss: 2824.0086563110353\n",
      "Epoch 2 \t Batch 740 \t Training Loss: 2821.817085224873\n",
      "Epoch 2 \t Batch 760 \t Training Loss: 2822.9478178325453\n",
      "Epoch 2 \t Batch 780 \t Training Loss: 2820.9049713917266\n",
      "Epoch 2 \t Batch 800 \t Training Loss: 2822.026887817383\n",
      "Epoch 2 \t Batch 820 \t Training Loss: 2822.280589927115\n",
      "Epoch 2 \t Batch 840 \t Training Loss: 2821.539679245722\n",
      "Epoch 2 \t Batch 860 \t Training Loss: 2819.3672885628635\n",
      "Epoch 2 \t Batch 880 \t Training Loss: 2819.712202869762\n",
      "Epoch 2 \t Batch 900 \t Training Loss: 2820.109178331163\n",
      "Epoch 2 \t Batch 920 \t Training Loss: 2817.4459090523096\n",
      "Epoch 2 \t Batch 940 \t Training Loss: 2817.6362987761804\n",
      "Epoch 2 \t Batch 960 \t Training Loss: 2814.524279022217\n",
      "Epoch 2 \t Batch 980 \t Training Loss: 2814.6158952985493\n",
      "Epoch 2 \t Batch 1000 \t Training Loss: 2814.9800455322265\n",
      "Epoch 2 \t Batch 1020 \t Training Loss: 2815.7911473891313\n",
      "Epoch 2 \t Batch 1040 \t Training Loss: 2815.219980445275\n",
      "Epoch 2 \t Batch 1060 \t Training Loss: 2814.1882323067143\n",
      "Epoch 2 \t Batch 1080 \t Training Loss: 2813.3512615062573\n",
      "Epoch 2 \t Batch 1100 \t Training Loss: 2811.240968905362\n",
      "Epoch 2 \t Batch 1120 \t Training Loss: 2809.208768245152\n",
      "Epoch 2 \t Batch 1140 \t Training Loss: 2809.2871011299\n",
      "Epoch 2 \t Batch 1160 \t Training Loss: 2811.758481466359\n",
      "Epoch 2 \t Batch 1180 \t Training Loss: 2810.599816791082\n",
      "Epoch 2 \t Batch 1200 \t Training Loss: 2810.618163960775\n",
      "Epoch 2 \t Batch 1220 \t Training Loss: 2809.2646560418802\n",
      "Epoch 2 \t Batch 1240 \t Training Loss: 2807.1760320848034\n",
      "Epoch 2 \t Batch 1260 \t Training Loss: 2808.952003503224\n",
      "Epoch 2 \t Batch 1280 \t Training Loss: 2808.188694763184\n",
      "Epoch 2 \t Batch 1300 \t Training Loss: 2807.552547701322\n",
      "Epoch 2 \t Batch 1320 \t Training Loss: 2805.5460375236744\n",
      "Epoch 2 \t Batch 1340 \t Training Loss: 2803.797446544135\n",
      "Epoch 2 \t Batch 1360 \t Training Loss: 2807.352836339614\n",
      "Epoch 2 \t Batch 1380 \t Training Loss: 2810.6932801177536\n",
      "Epoch 2 \t Batch 1400 \t Training Loss: 2809.147081124442\n",
      "Epoch 2 \t Batch 1420 \t Training Loss: 2809.658973371479\n",
      "Epoch 2 \t Batch 1440 \t Training Loss: 2812.1304841783312\n",
      "Epoch 2 \t Batch 20 \t Validation Loss: 252.49719924926757\n",
      "Epoch 2 \t Batch 40 \t Validation Loss: 306.1038539886475\n",
      "Epoch 2 \t Batch 60 \t Validation Loss: 342.18663965861003\n",
      "Epoch 2 \t Batch 80 \t Validation Loss: 334.4253282546997\n",
      "Epoch 2 \t Batch 100 \t Validation Loss: 578.2193104553222\n",
      "Epoch 2 \t Batch 120 \t Validation Loss: 1060.6795279184978\n",
      "Epoch 2 \t Batch 140 \t Validation Loss: 1226.0778480529784\n",
      "Epoch 2 \t Batch 160 \t Validation Loss: 1427.6848956108092\n",
      "Epoch 2 \t Batch 180 \t Validation Loss: 1430.5038149515788\n",
      "Epoch 2 \t Batch 200 \t Validation Loss: 1760.9362607574462\n",
      "Epoch 2 \t Batch 220 \t Validation Loss: 1897.3249340404163\n",
      "Epoch 2 \t Batch 240 \t Validation Loss: 1955.3506338119507\n",
      "Epoch 2 \t Batch 260 \t Validation Loss: 2033.1361934368426\n",
      "Epoch 2 \t Batch 280 \t Validation Loss: 2036.5347015925815\n",
      "Epoch 2 \t Batch 300 \t Validation Loss: 1996.9167115275065\n",
      "Epoch 2 \t Batch 320 \t Validation Loss: 2060.548722362518\n",
      "Epoch 2 Training Loss: 2811.1706892942907 Validation Loss: 2060.548722362518\n",
      "Epoch 2 completed\n",
      "Epoch 3 \t Batch 20 \t Training Loss: 2769.6683837890623\n",
      "Epoch 3 \t Batch 40 \t Training Loss: 2698.96396484375\n",
      "Epoch 3 \t Batch 60 \t Training Loss: 2719.789031982422\n",
      "Epoch 3 \t Batch 80 \t Training Loss: 2754.875227355957\n",
      "Epoch 3 \t Batch 100 \t Training Loss: 2775.9619152832033\n",
      "Epoch 3 \t Batch 120 \t Training Loss: 2774.4349416097007\n",
      "Epoch 3 \t Batch 140 \t Training Loss: 2810.9803353445873\n",
      "Epoch 3 \t Batch 160 \t Training Loss: 2785.849842071533\n",
      "Epoch 3 \t Batch 180 \t Training Loss: 2793.176976182726\n",
      "Epoch 3 \t Batch 200 \t Training Loss: 2778.0783874511717\n",
      "Epoch 3 \t Batch 220 \t Training Loss: 2801.926409357244\n",
      "Epoch 3 \t Batch 240 \t Training Loss: 2809.915360514323\n",
      "Epoch 3 \t Batch 260 \t Training Loss: 2808.461578838642\n",
      "Epoch 3 \t Batch 280 \t Training Loss: 2804.9557922363283\n",
      "Epoch 3 \t Batch 300 \t Training Loss: 2801.078640950521\n",
      "Epoch 3 \t Batch 320 \t Training Loss: 2795.9956230163575\n",
      "Epoch 3 \t Batch 340 \t Training Loss: 2796.8333129882812\n",
      "Epoch 3 \t Batch 360 \t Training Loss: 2798.322578938802\n",
      "Epoch 3 \t Batch 380 \t Training Loss: 2804.9702154862252\n",
      "Epoch 3 \t Batch 400 \t Training Loss: 2799.327232055664\n",
      "Epoch 3 \t Batch 420 \t Training Loss: 2792.291406831287\n",
      "Epoch 3 \t Batch 440 \t Training Loss: 2791.9532259854404\n",
      "Epoch 3 \t Batch 460 \t Training Loss: 2787.6419794497283\n",
      "Epoch 3 \t Batch 480 \t Training Loss: 2790.611028035482\n",
      "Epoch 3 \t Batch 500 \t Training Loss: 2788.049999511719\n",
      "Epoch 3 \t Batch 520 \t Training Loss: 2790.3605295034554\n",
      "Epoch 3 \t Batch 540 \t Training Loss: 2784.6028867368345\n",
      "Epoch 3 \t Batch 560 \t Training Loss: 2776.6788724626813\n",
      "Epoch 3 \t Batch 580 \t Training Loss: 2781.0806200750944\n",
      "Epoch 3 \t Batch 600 \t Training Loss: 2778.3546783447264\n",
      "Epoch 3 \t Batch 620 \t Training Loss: 2776.289915220199\n",
      "Epoch 3 \t Batch 640 \t Training Loss: 2777.6112092971803\n",
      "Epoch 3 \t Batch 660 \t Training Loss: 2777.1939380992544\n",
      "Epoch 3 \t Batch 680 \t Training Loss: 2775.2481330422793\n",
      "Epoch 3 \t Batch 700 \t Training Loss: 2773.0143610491073\n",
      "Epoch 3 \t Batch 720 \t Training Loss: 2771.3424964057076\n",
      "Epoch 3 \t Batch 740 \t Training Loss: 2770.1007456186653\n",
      "Epoch 3 \t Batch 760 \t Training Loss: 2771.9772245708264\n",
      "Epoch 3 \t Batch 780 \t Training Loss: 2771.0042975010015\n",
      "Epoch 3 \t Batch 800 \t Training Loss: 2769.8313305664065\n",
      "Epoch 3 \t Batch 820 \t Training Loss: 2769.9506547137003\n",
      "Epoch 3 \t Batch 840 \t Training Loss: 2769.4280752999443\n",
      "Epoch 3 \t Batch 860 \t Training Loss: 2763.837210153979\n",
      "Epoch 3 \t Batch 880 \t Training Loss: 2764.2762645374646\n",
      "Epoch 3 \t Batch 900 \t Training Loss: 2761.490986735026\n",
      "Epoch 3 \t Batch 920 \t Training Loss: 2761.298151032821\n",
      "Epoch 3 \t Batch 940 \t Training Loss: 2759.27856094685\n",
      "Epoch 3 \t Batch 960 \t Training Loss: 2758.2181138356527\n",
      "Epoch 3 \t Batch 980 \t Training Loss: 2756.0182329450336\n",
      "Epoch 3 \t Batch 1000 \t Training Loss: 2758.2179891357423\n",
      "Epoch 3 \t Batch 1020 \t Training Loss: 2757.86431537703\n",
      "Epoch 3 \t Batch 1040 \t Training Loss: 2758.7738935030425\n",
      "Epoch 3 \t Batch 1060 \t Training Loss: 2755.7376743532577\n",
      "Epoch 3 \t Batch 1080 \t Training Loss: 2755.2739673755786\n",
      "Epoch 3 \t Batch 1100 \t Training Loss: 2756.734677956321\n",
      "Epoch 3 \t Batch 1120 \t Training Loss: 2755.5951154436384\n",
      "Epoch 3 \t Batch 1140 \t Training Loss: 2753.115147426672\n",
      "Epoch 3 \t Batch 1160 \t Training Loss: 2752.576138621363\n",
      "Epoch 3 \t Batch 1180 \t Training Loss: 2754.021048852953\n",
      "Epoch 3 \t Batch 1200 \t Training Loss: 2751.9935490926105\n",
      "Epoch 3 \t Batch 1220 \t Training Loss: 2753.060523061283\n",
      "Epoch 3 \t Batch 1240 \t Training Loss: 2753.399216781124\n",
      "Epoch 3 \t Batch 1260 \t Training Loss: 2751.604604279049\n",
      "Epoch 3 \t Batch 1280 \t Training Loss: 2750.7913418769836\n",
      "Epoch 3 \t Batch 1300 \t Training Loss: 2750.3319282414363\n",
      "Epoch 3 \t Batch 1320 \t Training Loss: 2749.634390073834\n",
      "Epoch 3 \t Batch 1340 \t Training Loss: 2747.294628177472\n",
      "Epoch 3 \t Batch 1360 \t Training Loss: 2747.974470250747\n",
      "Epoch 3 \t Batch 1380 \t Training Loss: 2746.53516739555\n",
      "Epoch 3 \t Batch 1400 \t Training Loss: 2744.580383126395\n",
      "Epoch 3 \t Batch 1420 \t Training Loss: 2743.626901030205\n",
      "Epoch 3 \t Batch 1440 \t Training Loss: 2744.259423658583\n",
      "Epoch 3 \t Batch 20 \t Validation Loss: 180.7767303466797\n",
      "Epoch 3 \t Batch 40 \t Validation Loss: 250.02258777618408\n",
      "Epoch 3 \t Batch 60 \t Validation Loss: 269.42502072652184\n",
      "Epoch 3 \t Batch 80 \t Validation Loss: 273.0631050109863\n",
      "Epoch 3 \t Batch 100 \t Validation Loss: 480.0259353637695\n",
      "Epoch 3 \t Batch 120 \t Validation Loss: 898.9106620788574\n",
      "Epoch 3 \t Batch 140 \t Validation Loss: 1056.7580285208567\n",
      "Epoch 3 \t Batch 160 \t Validation Loss: 1314.2131593704223\n",
      "Epoch 3 \t Batch 180 \t Validation Loss: 1300.3889150831435\n",
      "Epoch 3 \t Batch 200 \t Validation Loss: 1627.525309829712\n",
      "Epoch 3 \t Batch 220 \t Validation Loss: 1765.5677214189009\n",
      "Epoch 3 \t Batch 240 \t Validation Loss: 1787.1999762852986\n",
      "Epoch 3 \t Batch 260 \t Validation Loss: 1835.0004109896147\n",
      "Epoch 3 \t Batch 280 \t Validation Loss: 1846.0961759839738\n",
      "Epoch 3 \t Batch 300 \t Validation Loss: 1799.4359847513836\n",
      "Epoch 3 \t Batch 320 \t Validation Loss: 1865.7511119008063\n",
      "Epoch 3 Training Loss: 2744.498628538777 Validation Loss: 1865.7511119008063\n",
      "Epoch 3 completed\n",
      "Epoch 4 \t Batch 20 \t Training Loss: 2711.223742675781\n",
      "Epoch 4 \t Batch 40 \t Training Loss: 2722.5719482421873\n",
      "Epoch 4 \t Batch 60 \t Training Loss: 2722.811905924479\n",
      "Epoch 4 \t Batch 80 \t Training Loss: 2727.859582519531\n",
      "Epoch 4 \t Batch 100 \t Training Loss: 2719.8227807617186\n",
      "Epoch 4 \t Batch 120 \t Training Loss: 2716.2284606933595\n",
      "Epoch 4 \t Batch 140 \t Training Loss: 2728.092769949777\n",
      "Epoch 4 \t Batch 160 \t Training Loss: 2714.656413269043\n",
      "Epoch 4 \t Batch 180 \t Training Loss: 2734.7090847439235\n",
      "Epoch 4 \t Batch 200 \t Training Loss: 2736.184366455078\n",
      "Epoch 4 \t Batch 220 \t Training Loss: 2741.008057750355\n",
      "Epoch 4 \t Batch 240 \t Training Loss: 2738.35498046875\n",
      "Epoch 4 \t Batch 260 \t Training Loss: 2737.4152052659256\n",
      "Epoch 4 \t Batch 280 \t Training Loss: 2726.2172476632254\n",
      "Epoch 4 \t Batch 300 \t Training Loss: 2723.9542220052085\n",
      "Epoch 4 \t Batch 320 \t Training Loss: 2723.3860725402833\n",
      "Epoch 4 \t Batch 340 \t Training Loss: 2720.3350919835707\n",
      "Epoch 4 \t Batch 360 \t Training Loss: 2727.6287706163193\n",
      "Epoch 4 \t Batch 380 \t Training Loss: 2730.848384174548\n",
      "Epoch 4 \t Batch 400 \t Training Loss: 2731.172169189453\n",
      "Epoch 4 \t Batch 420 \t Training Loss: 2724.8238444010417\n",
      "Epoch 4 \t Batch 440 \t Training Loss: 2730.1719829212534\n",
      "Epoch 4 \t Batch 460 \t Training Loss: 2731.275440779976\n",
      "Epoch 4 \t Batch 480 \t Training Loss: 2735.43393834432\n",
      "Epoch 4 \t Batch 500 \t Training Loss: 2729.6846857910155\n",
      "Epoch 4 \t Batch 520 \t Training Loss: 2727.7013115516074\n",
      "Epoch 4 \t Batch 540 \t Training Loss: 2729.8513656051073\n",
      "Epoch 4 \t Batch 560 \t Training Loss: 2730.5488963535854\n",
      "Epoch 4 \t Batch 580 \t Training Loss: 2725.4778452906116\n",
      "Epoch 4 \t Batch 600 \t Training Loss: 2729.655063273112\n",
      "Epoch 4 \t Batch 620 \t Training Loss: 2726.092655108052\n",
      "Epoch 4 \t Batch 640 \t Training Loss: 2729.6883405685426\n",
      "Epoch 4 \t Batch 660 \t Training Loss: 2727.1044474283854\n",
      "Epoch 4 \t Batch 680 \t Training Loss: 2722.732076487822\n",
      "Epoch 4 \t Batch 700 \t Training Loss: 2720.615489676339\n",
      "Epoch 4 \t Batch 720 \t Training Loss: 2720.5796254475913\n",
      "Epoch 4 \t Batch 740 \t Training Loss: 2721.213091216216\n",
      "Epoch 4 \t Batch 760 \t Training Loss: 2721.9075134277346\n",
      "Epoch 4 \t Batch 780 \t Training Loss: 2722.005480957031\n",
      "Epoch 4 \t Batch 800 \t Training Loss: 2720.136965637207\n",
      "Epoch 4 \t Batch 820 \t Training Loss: 2716.9179153070218\n",
      "Epoch 4 \t Batch 840 \t Training Loss: 2718.572213745117\n",
      "Epoch 4 \t Batch 860 \t Training Loss: 2715.8269494345022\n",
      "Epoch 4 \t Batch 880 \t Training Loss: 2713.100249966708\n",
      "Epoch 4 \t Batch 900 \t Training Loss: 2712.774386528863\n",
      "Epoch 4 \t Batch 920 \t Training Loss: 2709.9016448974608\n",
      "Epoch 4 \t Batch 940 \t Training Loss: 2713.351364720121\n",
      "Epoch 4 \t Batch 960 \t Training Loss: 2713.821655146281\n",
      "Epoch 4 \t Batch 980 \t Training Loss: 2714.591197609415\n",
      "Epoch 4 \t Batch 1000 \t Training Loss: 2713.6972747802733\n",
      "Epoch 4 \t Batch 1020 \t Training Loss: 2714.5562534706264\n",
      "Epoch 4 \t Batch 1040 \t Training Loss: 2714.6642459575946\n",
      "Epoch 4 \t Batch 1060 \t Training Loss: 2713.408966755417\n",
      "Epoch 4 \t Batch 1080 \t Training Loss: 2715.3218484384042\n",
      "Epoch 4 \t Batch 1100 \t Training Loss: 2711.9493438165837\n",
      "Epoch 4 \t Batch 1120 \t Training Loss: 2709.8785076686313\n",
      "Epoch 4 \t Batch 1140 \t Training Loss: 2709.32588715135\n",
      "Epoch 4 \t Batch 1160 \t Training Loss: 2710.321945400896\n",
      "Epoch 4 \t Batch 1180 \t Training Loss: 2711.9707704705706\n",
      "Epoch 4 \t Batch 1200 \t Training Loss: 2711.8250365193685\n",
      "Epoch 4 \t Batch 1220 \t Training Loss: 2712.0571890408874\n",
      "Epoch 4 \t Batch 1240 \t Training Loss: 2712.7376487485826\n",
      "Epoch 4 \t Batch 1260 \t Training Loss: 2712.8095715719555\n",
      "Epoch 4 \t Batch 1280 \t Training Loss: 2712.59048204422\n",
      "Epoch 4 \t Batch 1300 \t Training Loss: 2713.0495801720253\n",
      "Epoch 4 \t Batch 1320 \t Training Loss: 2711.9633269338897\n",
      "Epoch 4 \t Batch 1340 \t Training Loss: 2712.510490030317\n",
      "Epoch 4 \t Batch 1360 \t Training Loss: 2713.883775957893\n",
      "Epoch 4 \t Batch 1380 \t Training Loss: 2710.5761779785157\n",
      "Epoch 4 \t Batch 1400 \t Training Loss: 2710.7494809395926\n",
      "Epoch 4 \t Batch 1420 \t Training Loss: 2707.906503854671\n",
      "Epoch 4 \t Batch 1440 \t Training Loss: 2706.9370950486923\n",
      "Epoch 4 \t Batch 20 \t Validation Loss: 144.11990337371827\n",
      "Epoch 4 \t Batch 40 \t Validation Loss: 335.92021799087524\n",
      "Epoch 4 \t Batch 60 \t Validation Loss: 327.6750370661418\n",
      "Epoch 4 \t Batch 80 \t Validation Loss: 343.08167271614076\n",
      "Epoch 4 \t Batch 100 \t Validation Loss: 577.2200970840454\n",
      "Epoch 4 \t Batch 120 \t Validation Loss: 783.4472933769226\n",
      "Epoch 4 \t Batch 140 \t Validation Loss: 895.9136747360229\n",
      "Epoch 4 \t Batch 160 \t Validation Loss: 1004.3527513742447\n",
      "Epoch 4 \t Batch 180 \t Validation Loss: 999.6869696935017\n",
      "Epoch 4 \t Batch 200 \t Validation Loss: 1248.3638955879212\n",
      "Epoch 4 \t Batch 220 \t Validation Loss: 1302.4213440808383\n",
      "Epoch 4 \t Batch 240 \t Validation Loss: 1329.283940966924\n",
      "Epoch 4 \t Batch 260 \t Validation Loss: 1358.6938562980065\n",
      "Epoch 4 \t Batch 280 \t Validation Loss: 1406.0176729066031\n",
      "Epoch 4 \t Batch 300 \t Validation Loss: 1415.0071355819703\n",
      "Epoch 4 \t Batch 320 \t Validation Loss: 1676.4184715390206\n",
      "Epoch 4 Training Loss: 2706.262949969175 Validation Loss: 1676.4184715390206\n",
      "Epoch 4 completed\n",
      "Epoch 5 \t Batch 20 \t Training Loss: 2760.3909912109375\n",
      "Epoch 5 \t Batch 40 \t Training Loss: 2691.133135986328\n",
      "Epoch 5 \t Batch 60 \t Training Loss: 2638.9009358723956\n",
      "Epoch 5 \t Batch 80 \t Training Loss: 2650.0761993408205\n",
      "Epoch 5 \t Batch 100 \t Training Loss: 2667.582021484375\n",
      "Epoch 5 \t Batch 120 \t Training Loss: 2692.085479736328\n",
      "Epoch 5 \t Batch 140 \t Training Loss: 2684.2356733049664\n",
      "Epoch 5 \t Batch 160 \t Training Loss: 2677.1863578796388\n",
      "Epoch 5 \t Batch 180 \t Training Loss: 2674.0211791992188\n",
      "Epoch 5 \t Batch 200 \t Training Loss: 2685.26166015625\n",
      "Epoch 5 \t Batch 220 \t Training Loss: 2687.702526855469\n",
      "Epoch 5 \t Batch 240 \t Training Loss: 2698.0484212239585\n",
      "Epoch 5 \t Batch 260 \t Training Loss: 2687.1276658278243\n",
      "Epoch 5 \t Batch 280 \t Training Loss: 2680.585250854492\n",
      "Epoch 5 \t Batch 300 \t Training Loss: 2671.5452111816408\n",
      "Epoch 5 \t Batch 320 \t Training Loss: 2680.1933895111083\n",
      "Epoch 5 \t Batch 340 \t Training Loss: 2681.7840421788833\n",
      "Epoch 5 \t Batch 360 \t Training Loss: 2676.079829237196\n",
      "Epoch 5 \t Batch 380 \t Training Loss: 2682.812302117599\n",
      "Epoch 5 \t Batch 400 \t Training Loss: 2678.7750747680666\n",
      "Epoch 5 \t Batch 420 \t Training Loss: 2677.11066226051\n",
      "Epoch 5 \t Batch 440 \t Training Loss: 2671.6990742076528\n",
      "Epoch 5 \t Batch 460 \t Training Loss: 2678.522907024881\n",
      "Epoch 5 \t Batch 480 \t Training Loss: 2677.5141512552896\n",
      "Epoch 5 \t Batch 500 \t Training Loss: 2676.320802734375\n",
      "Epoch 5 \t Batch 520 \t Training Loss: 2678.9580059344953\n",
      "Epoch 5 \t Batch 540 \t Training Loss: 2679.9823942961516\n",
      "Epoch 5 \t Batch 560 \t Training Loss: 2674.901497759138\n",
      "Epoch 5 \t Batch 580 \t Training Loss: 2673.0869525777885\n",
      "Epoch 5 \t Batch 600 \t Training Loss: 2672.311703491211\n",
      "Epoch 5 \t Batch 620 \t Training Loss: 2670.173547953944\n",
      "Epoch 5 \t Batch 640 \t Training Loss: 2672.209722328186\n",
      "Epoch 5 \t Batch 660 \t Training Loss: 2668.4840302438447\n",
      "Epoch 5 \t Batch 680 \t Training Loss: 2669.708236694336\n",
      "Epoch 5 \t Batch 700 \t Training Loss: 2664.2970849609374\n",
      "Epoch 5 \t Batch 720 \t Training Loss: 2670.054040866428\n",
      "Epoch 5 \t Batch 740 \t Training Loss: 2674.499418351457\n",
      "Epoch 5 \t Batch 760 \t Training Loss: 2673.7495104337995\n",
      "Epoch 5 \t Batch 780 \t Training Loss: 2670.5351318359376\n",
      "Epoch 5 \t Batch 800 \t Training Loss: 2667.4277633666993\n",
      "Epoch 5 \t Batch 820 \t Training Loss: 2669.8472516315737\n",
      "Epoch 5 \t Batch 840 \t Training Loss: 2669.077870686849\n",
      "Epoch 5 \t Batch 860 \t Training Loss: 2667.4101236032884\n",
      "Epoch 5 \t Batch 880 \t Training Loss: 2664.9149491743606\n",
      "Epoch 5 \t Batch 900 \t Training Loss: 2665.426818576389\n",
      "Epoch 5 \t Batch 920 \t Training Loss: 2662.5952259893\n",
      "Epoch 5 \t Batch 940 \t Training Loss: 2660.7501706387134\n",
      "Epoch 5 \t Batch 960 \t Training Loss: 2661.7181821187337\n",
      "Epoch 5 \t Batch 980 \t Training Loss: 2664.7114819585063\n",
      "Epoch 5 \t Batch 1000 \t Training Loss: 2665.025382080078\n",
      "Epoch 5 \t Batch 1020 \t Training Loss: 2665.8833558325673\n",
      "Epoch 5 \t Batch 1040 \t Training Loss: 2666.051580810547\n",
      "Epoch 5 \t Batch 1060 \t Training Loss: 2665.8883063550265\n",
      "Epoch 5 \t Batch 1080 \t Training Loss: 2667.739597687898\n",
      "Epoch 5 \t Batch 1100 \t Training Loss: 2669.4521425559305\n",
      "Epoch 5 \t Batch 1120 \t Training Loss: 2669.8429069519043\n",
      "Epoch 5 \t Batch 1140 \t Training Loss: 2669.627639609889\n",
      "Epoch 5 \t Batch 1160 \t Training Loss: 2671.170823090652\n",
      "Epoch 5 \t Batch 1180 \t Training Loss: 2671.975605592889\n",
      "Epoch 5 \t Batch 1200 \t Training Loss: 2672.070174153646\n",
      "Epoch 5 \t Batch 1220 \t Training Loss: 2672.781801717789\n",
      "Epoch 5 \t Batch 1240 \t Training Loss: 2673.5874291204636\n",
      "Epoch 5 \t Batch 1260 \t Training Loss: 2671.9592176649307\n",
      "Epoch 5 \t Batch 1280 \t Training Loss: 2673.266441345215\n",
      "Epoch 5 \t Batch 1300 \t Training Loss: 2673.6294619516225\n",
      "Epoch 5 \t Batch 1320 \t Training Loss: 2672.4516606186376\n",
      "Epoch 5 \t Batch 1340 \t Training Loss: 2672.639981033553\n",
      "Epoch 5 \t Batch 1360 \t Training Loss: 2672.684181572409\n",
      "Epoch 5 \t Batch 1380 \t Training Loss: 2675.644238192793\n",
      "Epoch 5 \t Batch 1400 \t Training Loss: 2675.343533761161\n",
      "Epoch 5 \t Batch 1420 \t Training Loss: 2674.3015676579007\n",
      "Epoch 5 \t Batch 1440 \t Training Loss: 2674.032758585612\n",
      "Epoch 5 \t Batch 20 \t Validation Loss: 259.78717575073244\n",
      "Epoch 5 \t Batch 40 \t Validation Loss: 355.68396759033203\n",
      "Epoch 5 \t Batch 60 \t Validation Loss: 360.3290857950846\n",
      "Epoch 5 \t Batch 80 \t Validation Loss: 342.9057340621948\n",
      "Epoch 5 \t Batch 100 \t Validation Loss: 558.808295135498\n",
      "Epoch 5 \t Batch 120 \t Validation Loss: 1088.368220392863\n",
      "Epoch 5 \t Batch 140 \t Validation Loss: 1256.8400893075125\n",
      "Epoch 5 \t Batch 160 \t Validation Loss: 1565.3484250068664\n",
      "Epoch 5 \t Batch 180 \t Validation Loss: 1563.81388414171\n",
      "Epoch 5 \t Batch 200 \t Validation Loss: 1929.7848403930664\n",
      "Epoch 5 \t Batch 220 \t Validation Loss: 2106.918020075018\n",
      "Epoch 5 \t Batch 240 \t Validation Loss: 2147.9504526138307\n",
      "Epoch 5 \t Batch 260 \t Validation Loss: 2196.637547126183\n",
      "Epoch 5 \t Batch 280 \t Validation Loss: 2182.749199785505\n",
      "Epoch 5 \t Batch 300 \t Validation Loss: 2121.4075017293294\n",
      "Epoch 5 \t Batch 320 \t Validation Loss: 2168.893379986286\n",
      "Epoch 5 Training Loss: 2673.0680426315635 Validation Loss: 2168.893379986286\n",
      "Epoch 5 completed\n",
      "Epoch 6 \t Batch 20 \t Training Loss: 2581.885888671875\n",
      "Epoch 6 \t Batch 40 \t Training Loss: 2641.3011779785156\n",
      "Epoch 6 \t Batch 60 \t Training Loss: 2670.4071248372397\n",
      "Epoch 6 \t Batch 80 \t Training Loss: 2666.224349975586\n",
      "Epoch 6 \t Batch 100 \t Training Loss: 2672.470144042969\n",
      "Epoch 6 \t Batch 120 \t Training Loss: 2678.724401855469\n",
      "Epoch 6 \t Batch 140 \t Training Loss: 2670.241462925502\n",
      "Epoch 6 \t Batch 160 \t Training Loss: 2680.2989151000975\n",
      "Epoch 6 \t Batch 180 \t Training Loss: 2686.014797634549\n",
      "Epoch 6 \t Batch 200 \t Training Loss: 2672.6315270996092\n",
      "Epoch 6 \t Batch 220 \t Training Loss: 2680.549648215554\n",
      "Epoch 6 \t Batch 240 \t Training Loss: 2674.353338623047\n",
      "Epoch 6 \t Batch 260 \t Training Loss: 2669.2960444523737\n",
      "Epoch 6 \t Batch 280 \t Training Loss: 2667.459004865374\n",
      "Epoch 6 \t Batch 300 \t Training Loss: 2660.793996988932\n",
      "Epoch 6 \t Batch 320 \t Training Loss: 2660.4336963653564\n",
      "Epoch 6 \t Batch 340 \t Training Loss: 2665.651478845933\n",
      "Epoch 6 \t Batch 360 \t Training Loss: 2663.4744469536677\n",
      "Epoch 6 \t Batch 380 \t Training Loss: 2666.7817129034747\n",
      "Epoch 6 \t Batch 400 \t Training Loss: 2661.6987643432617\n",
      "Epoch 6 \t Batch 420 \t Training Loss: 2661.70785929362\n",
      "Epoch 6 \t Batch 440 \t Training Loss: 2659.112093561346\n",
      "Epoch 6 \t Batch 460 \t Training Loss: 2660.0827591605807\n",
      "Epoch 6 \t Batch 480 \t Training Loss: 2661.861631011963\n",
      "Epoch 6 \t Batch 500 \t Training Loss: 2664.8423056640627\n",
      "Epoch 6 \t Batch 520 \t Training Loss: 2661.631162907527\n",
      "Epoch 6 \t Batch 540 \t Training Loss: 2664.0013208459923\n",
      "Epoch 6 \t Batch 560 \t Training Loss: 2665.5112424577987\n",
      "Epoch 6 \t Batch 580 \t Training Loss: 2665.640519977438\n",
      "Epoch 6 \t Batch 600 \t Training Loss: 2663.697010701497\n",
      "Epoch 6 \t Batch 620 \t Training Loss: 2657.184031628024\n",
      "Epoch 6 \t Batch 640 \t Training Loss: 2655.3812202453614\n",
      "Epoch 6 \t Batch 660 \t Training Loss: 2652.6642437559185\n",
      "Epoch 6 \t Batch 680 \t Training Loss: 2652.50365708295\n",
      "Epoch 6 \t Batch 700 \t Training Loss: 2651.3541725376676\n",
      "Epoch 6 \t Batch 720 \t Training Loss: 2646.6677235921225\n",
      "Epoch 6 \t Batch 740 \t Training Loss: 2647.166841744088\n",
      "Epoch 6 \t Batch 760 \t Training Loss: 2646.7012933028373\n",
      "Epoch 6 \t Batch 780 \t Training Loss: 2647.4182099171176\n",
      "Epoch 6 \t Batch 800 \t Training Loss: 2647.408999786377\n",
      "Epoch 6 \t Batch 820 \t Training Loss: 2647.6872107529057\n",
      "Epoch 6 \t Batch 840 \t Training Loss: 2645.1069279261997\n",
      "Epoch 6 \t Batch 860 \t Training Loss: 2644.020886230469\n",
      "Epoch 6 \t Batch 880 \t Training Loss: 2641.3865597811614\n",
      "Epoch 6 \t Batch 900 \t Training Loss: 2639.3059677463107\n",
      "Epoch 6 \t Batch 920 \t Training Loss: 2640.9767059326173\n",
      "Epoch 6 \t Batch 940 \t Training Loss: 2644.53926547436\n",
      "Epoch 6 \t Batch 960 \t Training Loss: 2646.011382420858\n",
      "Epoch 6 \t Batch 980 \t Training Loss: 2643.7008894939813\n",
      "Epoch 6 \t Batch 1000 \t Training Loss: 2646.2338907470703\n",
      "Epoch 6 \t Batch 1020 \t Training Loss: 2645.7792960372626\n",
      "Epoch 6 \t Batch 1040 \t Training Loss: 2646.1549345750077\n",
      "Epoch 6 \t Batch 1060 \t Training Loss: 2646.147035533977\n",
      "Epoch 6 \t Batch 1080 \t Training Loss: 2647.6970058865018\n",
      "Epoch 6 \t Batch 1100 \t Training Loss: 2648.608877175071\n",
      "Epoch 6 \t Batch 1120 \t Training Loss: 2646.5911740984234\n",
      "Epoch 6 \t Batch 1140 \t Training Loss: 2645.8174933182568\n",
      "Epoch 6 \t Batch 1160 \t Training Loss: 2646.387050023572\n",
      "Epoch 6 \t Batch 1180 \t Training Loss: 2646.1062106892214\n",
      "Epoch 6 \t Batch 1200 \t Training Loss: 2644.6706663004557\n",
      "Epoch 6 \t Batch 1220 \t Training Loss: 2644.9894207063267\n",
      "Epoch 6 \t Batch 1240 \t Training Loss: 2645.170179797757\n",
      "Epoch 6 \t Batch 1260 \t Training Loss: 2648.2099378797743\n",
      "Epoch 6 \t Batch 1280 \t Training Loss: 2648.103422164917\n",
      "Epoch 6 \t Batch 1300 \t Training Loss: 2647.9989450307994\n",
      "Epoch 6 \t Batch 1320 \t Training Loss: 2648.3726470947267\n",
      "Epoch 6 \t Batch 1340 \t Training Loss: 2648.631086571537\n",
      "Epoch 6 \t Batch 1360 \t Training Loss: 2649.7295796113854\n",
      "Epoch 6 \t Batch 1380 \t Training Loss: 2650.7230042388474\n",
      "Epoch 6 \t Batch 1400 \t Training Loss: 2648.5497293526787\n",
      "Epoch 6 \t Batch 1420 \t Training Loss: 2649.7414891202684\n",
      "Epoch 6 \t Batch 1440 \t Training Loss: 2650.898755730523\n",
      "Epoch 6 \t Batch 20 \t Validation Loss: 170.44870414733887\n",
      "Epoch 6 \t Batch 40 \t Validation Loss: 343.1659002304077\n",
      "Epoch 6 \t Batch 60 \t Validation Loss: 324.59667218526204\n",
      "Epoch 6 \t Batch 80 \t Validation Loss: 330.7504282951355\n",
      "Epoch 6 \t Batch 100 \t Validation Loss: 590.6259566497803\n",
      "Epoch 6 \t Batch 120 \t Validation Loss: 975.867576789856\n",
      "Epoch 6 \t Batch 140 \t Validation Loss: 1112.396049445016\n",
      "Epoch 6 \t Batch 160 \t Validation Loss: 1278.1654527187347\n",
      "Epoch 6 \t Batch 180 \t Validation Loss: 1280.6993601057266\n",
      "Epoch 6 \t Batch 200 \t Validation Loss: 1557.2312557601929\n",
      "Epoch 6 \t Batch 220 \t Validation Loss: 1655.1951314405962\n",
      "Epoch 6 \t Batch 240 \t Validation Loss: 1711.6014715512595\n",
      "Epoch 6 \t Batch 260 \t Validation Loss: 1769.8510078723614\n",
      "Epoch 6 \t Batch 280 \t Validation Loss: 1790.0326044082642\n",
      "Epoch 6 \t Batch 300 \t Validation Loss: 1774.2305056508383\n",
      "Epoch 6 \t Batch 320 \t Validation Loss: 1860.2931859731675\n",
      "Epoch 6 Training Loss: 2650.266473182557 Validation Loss: 1860.2931859731675\n",
      "Epoch 6 completed\n",
      "Epoch 7 \t Batch 20 \t Training Loss: 2739.411029052734\n",
      "Epoch 7 \t Batch 40 \t Training Loss: 2643.730026245117\n",
      "Epoch 7 \t Batch 60 \t Training Loss: 2655.4915161132812\n",
      "Epoch 7 \t Batch 80 \t Training Loss: 2623.1688720703123\n",
      "Epoch 7 \t Batch 100 \t Training Loss: 2627.1186730957033\n",
      "Epoch 7 \t Batch 120 \t Training Loss: 2619.1441436767577\n",
      "Epoch 7 \t Batch 140 \t Training Loss: 2608.14259992327\n",
      "Epoch 7 \t Batch 160 \t Training Loss: 2619.477921295166\n",
      "Epoch 7 \t Batch 180 \t Training Loss: 2618.0423048231337\n",
      "Epoch 7 \t Batch 200 \t Training Loss: 2614.051760253906\n",
      "Epoch 7 \t Batch 220 \t Training Loss: 2618.0935696688566\n",
      "Epoch 7 \t Batch 240 \t Training Loss: 2618.6435256958007\n",
      "Epoch 7 \t Batch 260 \t Training Loss: 2642.273849252554\n",
      "Epoch 7 \t Batch 280 \t Training Loss: 2638.804752458845\n",
      "Epoch 7 \t Batch 300 \t Training Loss: 2648.4732515462238\n",
      "Epoch 7 \t Batch 320 \t Training Loss: 2645.1239459991457\n",
      "Epoch 7 \t Batch 340 \t Training Loss: 2648.6901722627526\n",
      "Epoch 7 \t Batch 360 \t Training Loss: 2648.1041670057507\n",
      "Epoch 7 \t Batch 380 \t Training Loss: 2647.784941984478\n",
      "Epoch 7 \t Batch 400 \t Training Loss: 2642.8954293823244\n",
      "Epoch 7 \t Batch 420 \t Training Loss: 2639.9784920828683\n",
      "Epoch 7 \t Batch 440 \t Training Loss: 2637.160047496449\n",
      "Epoch 7 \t Batch 460 \t Training Loss: 2630.085426396909\n",
      "Epoch 7 \t Batch 480 \t Training Loss: 2631.123306274414\n",
      "Epoch 7 \t Batch 500 \t Training Loss: 2630.860341796875\n",
      "Epoch 7 \t Batch 520 \t Training Loss: 2629.3040118877702\n",
      "Epoch 7 \t Batch 540 \t Training Loss: 2621.3212917751734\n",
      "Epoch 7 \t Batch 560 \t Training Loss: 2619.154629952567\n",
      "Epoch 7 \t Batch 580 \t Training Loss: 2621.1734250825025\n",
      "Epoch 7 \t Batch 600 \t Training Loss: 2622.685443725586\n",
      "Epoch 7 \t Batch 620 \t Training Loss: 2621.196985060169\n",
      "Epoch 7 \t Batch 640 \t Training Loss: 2621.5554895401\n",
      "Epoch 7 \t Batch 660 \t Training Loss: 2625.2023265491835\n",
      "Epoch 7 \t Batch 680 \t Training Loss: 2622.824379416073\n",
      "Epoch 7 \t Batch 700 \t Training Loss: 2620.4075782993864\n",
      "Epoch 7 \t Batch 720 \t Training Loss: 2618.1764458550347\n",
      "Epoch 7 \t Batch 740 \t Training Loss: 2619.9995443808066\n",
      "Epoch 7 \t Batch 760 \t Training Loss: 2616.3125819156044\n",
      "Epoch 7 \t Batch 780 \t Training Loss: 2617.3339900090145\n",
      "Epoch 7 \t Batch 800 \t Training Loss: 2615.4041888427732\n",
      "Epoch 7 \t Batch 820 \t Training Loss: 2615.600454042016\n",
      "Epoch 7 \t Batch 840 \t Training Loss: 2612.8326189313616\n",
      "Epoch 7 \t Batch 860 \t Training Loss: 2611.818198128634\n",
      "Epoch 7 \t Batch 880 \t Training Loss: 2613.66311118386\n",
      "Epoch 7 \t Batch 900 \t Training Loss: 2613.0411313205295\n",
      "Epoch 7 \t Batch 920 \t Training Loss: 2613.5954638937246\n",
      "Epoch 7 \t Batch 940 \t Training Loss: 2618.4268383269614\n",
      "Epoch 7 \t Batch 960 \t Training Loss: 2617.57533899943\n",
      "Epoch 7 \t Batch 980 \t Training Loss: 2620.981689204002\n",
      "Epoch 7 \t Batch 1000 \t Training Loss: 2621.477816040039\n",
      "Epoch 7 \t Batch 1020 \t Training Loss: 2621.952568143957\n",
      "Epoch 7 \t Batch 1040 \t Training Loss: 2622.799664424016\n",
      "Epoch 7 \t Batch 1060 \t Training Loss: 2623.5849407843825\n",
      "Epoch 7 \t Batch 1080 \t Training Loss: 2622.343898066768\n",
      "Epoch 7 \t Batch 1100 \t Training Loss: 2619.254305863814\n",
      "Epoch 7 \t Batch 1120 \t Training Loss: 2619.3043147495814\n",
      "Epoch 7 \t Batch 1140 \t Training Loss: 2623.8270998235334\n",
      "Epoch 7 \t Batch 1160 \t Training Loss: 2626.297114931304\n",
      "Epoch 7 \t Batch 1180 \t Training Loss: 2628.080483543267\n",
      "Epoch 7 \t Batch 1200 \t Training Loss: 2631.019814758301\n",
      "Epoch 7 \t Batch 1220 \t Training Loss: 2630.5241196929433\n",
      "Epoch 7 \t Batch 1240 \t Training Loss: 2630.508819580078\n",
      "Epoch 7 \t Batch 1260 \t Training Loss: 2633.013405064174\n",
      "Epoch 7 \t Batch 1280 \t Training Loss: 2629.108296394348\n",
      "Epoch 7 \t Batch 1300 \t Training Loss: 2631.0755951397236\n",
      "Epoch 7 \t Batch 1320 \t Training Loss: 2629.7871848366476\n",
      "Epoch 7 \t Batch 1340 \t Training Loss: 2631.396230760261\n",
      "Epoch 7 \t Batch 1360 \t Training Loss: 2632.6428447050207\n",
      "Epoch 7 \t Batch 1380 \t Training Loss: 2634.3769775390624\n",
      "Epoch 7 \t Batch 1400 \t Training Loss: 2634.5492283412386\n",
      "Epoch 7 \t Batch 1420 \t Training Loss: 2633.3164919571136\n",
      "Epoch 7 \t Batch 1440 \t Training Loss: 2631.107324133979\n",
      "Epoch 7 \t Batch 20 \t Validation Loss: 271.7666442871094\n",
      "Epoch 7 \t Batch 40 \t Validation Loss: 418.5557323455811\n",
      "Epoch 7 \t Batch 60 \t Validation Loss: 400.96679229736327\n",
      "Epoch 7 \t Batch 80 \t Validation Loss: 395.93075771331786\n",
      "Epoch 7 \t Batch 100 \t Validation Loss: 661.5822099304199\n",
      "Epoch 7 \t Batch 120 \t Validation Loss: 1295.7000268300374\n",
      "Epoch 7 \t Batch 140 \t Validation Loss: 1515.6761961800712\n",
      "Epoch 7 \t Batch 160 \t Validation Loss: 1960.9501257896422\n",
      "Epoch 7 \t Batch 180 \t Validation Loss: 1965.5793242560492\n",
      "Epoch 7 \t Batch 200 \t Validation Loss: 2355.24196647644\n",
      "Epoch 7 \t Batch 220 \t Validation Loss: 2573.615341949463\n",
      "Epoch 7 \t Batch 240 \t Validation Loss: 2626.4364782333373\n",
      "Epoch 7 \t Batch 260 \t Validation Loss: 2670.652808028001\n",
      "Epoch 7 \t Batch 280 \t Validation Loss: 2642.380393927438\n",
      "Epoch 7 \t Batch 300 \t Validation Loss: 2576.5375411478676\n",
      "Epoch 7 \t Batch 320 \t Validation Loss: 2625.4721261501313\n",
      "Epoch 7 Training Loss: 2630.810654643484 Validation Loss: 2625.4721261501313\n",
      "Epoch 7 completed\n",
      "Epoch 8 \t Batch 20 \t Training Loss: 2783.366906738281\n",
      "Epoch 8 \t Batch 40 \t Training Loss: 2826.407360839844\n",
      "Epoch 8 \t Batch 60 \t Training Loss: 2763.229121907552\n",
      "Epoch 8 \t Batch 80 \t Training Loss: 2741.629612731934\n",
      "Epoch 8 \t Batch 100 \t Training Loss: 2701.1981896972657\n",
      "Epoch 8 \t Batch 120 \t Training Loss: 2688.865326944987\n",
      "Epoch 8 \t Batch 140 \t Training Loss: 2675.9535443987165\n",
      "Epoch 8 \t Batch 160 \t Training Loss: 2669.6094566345214\n",
      "Epoch 8 \t Batch 180 \t Training Loss: 2650.037424045139\n",
      "Epoch 8 \t Batch 200 \t Training Loss: 2643.516441650391\n",
      "Epoch 8 \t Batch 220 \t Training Loss: 2645.16328346946\n",
      "Epoch 8 \t Batch 240 \t Training Loss: 2644.2293528238934\n",
      "Epoch 8 \t Batch 260 \t Training Loss: 2636.330569223257\n",
      "Epoch 8 \t Batch 280 \t Training Loss: 2631.285464477539\n",
      "Epoch 8 \t Batch 300 \t Training Loss: 2639.1609338378908\n",
      "Epoch 8 \t Batch 320 \t Training Loss: 2636.4154102325438\n",
      "Epoch 8 \t Batch 340 \t Training Loss: 2638.3977241067323\n",
      "Epoch 8 \t Batch 360 \t Training Loss: 2652.2971981472438\n",
      "Epoch 8 \t Batch 380 \t Training Loss: 2646.832636140522\n",
      "Epoch 8 \t Batch 400 \t Training Loss: 2645.9861923217773\n",
      "Epoch 8 \t Batch 420 \t Training Loss: 2649.272671072824\n",
      "Epoch 8 \t Batch 440 \t Training Loss: 2648.46081459739\n",
      "Epoch 8 \t Batch 460 \t Training Loss: 2642.9607079547386\n",
      "Epoch 8 \t Batch 480 \t Training Loss: 2638.74873936971\n",
      "Epoch 8 \t Batch 500 \t Training Loss: 2635.4317536621093\n",
      "Epoch 8 \t Batch 520 \t Training Loss: 2625.477815598708\n",
      "Epoch 8 \t Batch 540 \t Training Loss: 2620.3620092321326\n",
      "Epoch 8 \t Batch 560 \t Training Loss: 2624.5291377476283\n",
      "Epoch 8 \t Batch 580 \t Training Loss: 2626.9898349104255\n",
      "Epoch 8 \t Batch 600 \t Training Loss: 2624.5205151367186\n",
      "Epoch 8 \t Batch 620 \t Training Loss: 2624.3896224483365\n",
      "Epoch 8 \t Batch 640 \t Training Loss: 2624.502769470215\n",
      "Epoch 8 \t Batch 660 \t Training Loss: 2625.4701512192237\n",
      "Epoch 8 \t Batch 680 \t Training Loss: 2626.6148060518153\n",
      "Epoch 8 \t Batch 700 \t Training Loss: 2625.939354596819\n",
      "Epoch 8 \t Batch 720 \t Training Loss: 2622.363653055827\n",
      "Epoch 8 \t Batch 740 \t Training Loss: 2623.659876148121\n",
      "Epoch 8 \t Batch 760 \t Training Loss: 2622.7628793816816\n",
      "Epoch 8 \t Batch 780 \t Training Loss: 2626.1953635191308\n",
      "Epoch 8 \t Batch 800 \t Training Loss: 2624.8132989501955\n",
      "Epoch 8 \t Batch 820 \t Training Loss: 2622.787720768626\n",
      "Epoch 8 \t Batch 840 \t Training Loss: 2623.8704588390533\n",
      "Epoch 8 \t Batch 860 \t Training Loss: 2624.4857653240824\n",
      "Epoch 8 \t Batch 880 \t Training Loss: 2619.494864030318\n",
      "Epoch 8 \t Batch 900 \t Training Loss: 2620.053969590929\n",
      "Epoch 8 \t Batch 920 \t Training Loss: 2617.2084903882896\n",
      "Epoch 8 \t Batch 940 \t Training Loss: 2619.6285396494764\n",
      "Epoch 8 \t Batch 960 \t Training Loss: 2618.2474302927653\n",
      "Epoch 8 \t Batch 980 \t Training Loss: 2617.370338035116\n",
      "Epoch 8 \t Batch 1000 \t Training Loss: 2617.167559692383\n",
      "Epoch 8 \t Batch 1020 \t Training Loss: 2614.098960726869\n",
      "Epoch 8 \t Batch 1040 \t Training Loss: 2615.5982658973107\n",
      "Epoch 8 \t Batch 1060 \t Training Loss: 2616.556732292895\n",
      "Epoch 8 \t Batch 1080 \t Training Loss: 2614.6736907958984\n",
      "Epoch 8 \t Batch 1100 \t Training Loss: 2610.3428645463423\n",
      "Epoch 8 \t Batch 1120 \t Training Loss: 2612.4973267691475\n",
      "Epoch 8 \t Batch 1140 \t Training Loss: 2611.9127042000755\n",
      "Epoch 8 \t Batch 1160 \t Training Loss: 2612.434954307819\n",
      "Epoch 8 \t Batch 1180 \t Training Loss: 2613.5582557807534\n",
      "Epoch 8 \t Batch 1200 \t Training Loss: 2613.6667515055337\n",
      "Epoch 8 \t Batch 1220 \t Training Loss: 2612.4699480901\n",
      "Epoch 8 \t Batch 1240 \t Training Loss: 2612.0817972490863\n",
      "Epoch 8 \t Batch 1260 \t Training Loss: 2611.687040686229\n",
      "Epoch 8 \t Batch 1280 \t Training Loss: 2609.548544216156\n",
      "Epoch 8 \t Batch 1300 \t Training Loss: 2610.755814396785\n",
      "Epoch 8 \t Batch 1320 \t Training Loss: 2610.412968676018\n",
      "Epoch 8 \t Batch 1340 \t Training Loss: 2611.6147165782418\n",
      "Epoch 8 \t Batch 1360 \t Training Loss: 2610.083196034151\n",
      "Epoch 8 \t Batch 1380 \t Training Loss: 2609.4421237226848\n",
      "Epoch 8 \t Batch 1400 \t Training Loss: 2612.0501160539898\n",
      "Epoch 8 \t Batch 1420 \t Training Loss: 2611.25151685258\n",
      "Epoch 8 \t Batch 1440 \t Training Loss: 2611.2702735053167\n",
      "Epoch 8 \t Batch 20 \t Validation Loss: 387.344556427002\n",
      "Epoch 8 \t Batch 40 \t Validation Loss: 552.6562732696533\n",
      "Epoch 8 \t Batch 60 \t Validation Loss: 517.6282315572103\n",
      "Epoch 8 \t Batch 80 \t Validation Loss: 491.9379587173462\n",
      "Epoch 8 \t Batch 100 \t Validation Loss: 654.1916096496582\n",
      "Epoch 8 \t Batch 120 \t Validation Loss: 961.1248668670654\n",
      "Epoch 8 \t Batch 140 \t Validation Loss: 1074.318238721575\n",
      "Epoch 8 \t Batch 160 \t Validation Loss: 1252.1022763252258\n",
      "Epoch 8 \t Batch 180 \t Validation Loss: 1220.6716636657716\n",
      "Epoch 8 \t Batch 200 \t Validation Loss: 1516.8545140838623\n",
      "Epoch 8 \t Batch 220 \t Validation Loss: 1645.8949269381437\n",
      "Epoch 8 \t Batch 240 \t Validation Loss: 1659.638069343567\n",
      "Epoch 8 \t Batch 260 \t Validation Loss: 1679.2292024465708\n",
      "Epoch 8 \t Batch 280 \t Validation Loss: 1693.3679877689906\n",
      "Epoch 8 \t Batch 300 \t Validation Loss: 1662.30244817098\n",
      "Epoch 8 \t Batch 320 \t Validation Loss: 1738.0978882789611\n",
      "Epoch 8 Training Loss: 2611.933117919754 Validation Loss: 1738.0978882789611\n",
      "Epoch 8 completed\n",
      "Epoch 9 \t Batch 20 \t Training Loss: 2602.3893127441406\n",
      "Epoch 9 \t Batch 40 \t Training Loss: 2664.7972198486327\n",
      "Epoch 9 \t Batch 60 \t Training Loss: 2637.9913045247395\n",
      "Epoch 9 \t Batch 80 \t Training Loss: 2628.2576797485353\n",
      "Epoch 9 \t Batch 100 \t Training Loss: 2612.5806591796877\n",
      "Epoch 9 \t Batch 120 \t Training Loss: 2612.118524169922\n",
      "Epoch 9 \t Batch 140 \t Training Loss: 2604.840949358259\n",
      "Epoch 9 \t Batch 160 \t Training Loss: 2604.4284439086914\n",
      "Epoch 9 \t Batch 180 \t Training Loss: 2617.049247233073\n",
      "Epoch 9 \t Batch 200 \t Training Loss: 2632.647572631836\n",
      "Epoch 9 \t Batch 220 \t Training Loss: 2617.061485706676\n",
      "Epoch 9 \t Batch 240 \t Training Loss: 2610.5666514078775\n",
      "Epoch 9 \t Batch 260 \t Training Loss: 2593.419456599309\n",
      "Epoch 9 \t Batch 280 \t Training Loss: 2598.618582589286\n",
      "Epoch 9 \t Batch 300 \t Training Loss: 2601.016590169271\n",
      "Epoch 9 \t Batch 320 \t Training Loss: 2609.2246253967287\n",
      "Epoch 9 \t Batch 340 \t Training Loss: 2611.6949118221505\n",
      "Epoch 9 \t Batch 360 \t Training Loss: 2609.388477918837\n",
      "Epoch 9 \t Batch 380 \t Training Loss: 2612.8387502569904\n",
      "Epoch 9 \t Batch 400 \t Training Loss: 2607.0701986694335\n",
      "Epoch 9 \t Batch 420 \t Training Loss: 2617.6016552153087\n",
      "Epoch 9 \t Batch 440 \t Training Loss: 2611.6091394597834\n",
      "Epoch 9 \t Batch 460 \t Training Loss: 2615.9108533776325\n",
      "Epoch 9 \t Batch 480 \t Training Loss: 2622.8992813110353\n",
      "Epoch 9 \t Batch 500 \t Training Loss: 2623.0035434570314\n",
      "Epoch 9 \t Batch 520 \t Training Loss: 2623.6562884990985\n",
      "Epoch 9 \t Batch 540 \t Training Loss: 2627.306502730758\n",
      "Epoch 9 \t Batch 560 \t Training Loss: 2626.482811628069\n",
      "Epoch 9 \t Batch 580 \t Training Loss: 2623.6324648100754\n",
      "Epoch 9 \t Batch 600 \t Training Loss: 2624.1654677327474\n",
      "Epoch 9 \t Batch 620 \t Training Loss: 2619.6963825841103\n",
      "Epoch 9 \t Batch 640 \t Training Loss: 2617.770224189758\n",
      "Epoch 9 \t Batch 660 \t Training Loss: 2616.9987258448746\n",
      "Epoch 9 \t Batch 680 \t Training Loss: 2616.44211730957\n",
      "Epoch 9 \t Batch 700 \t Training Loss: 2614.1776879882814\n",
      "Epoch 9 \t Batch 720 \t Training Loss: 2608.9811760796442\n",
      "Epoch 9 \t Batch 740 \t Training Loss: 2608.9876900337836\n",
      "Epoch 9 \t Batch 760 \t Training Loss: 2607.5868461207338\n",
      "Epoch 9 \t Batch 780 \t Training Loss: 2606.2808549929887\n",
      "Epoch 9 \t Batch 800 \t Training Loss: 2605.484355773926\n",
      "Epoch 9 \t Batch 820 \t Training Loss: 2606.5085800543065\n",
      "Epoch 9 \t Batch 840 \t Training Loss: 2604.620478457496\n",
      "Epoch 9 \t Batch 860 \t Training Loss: 2606.5814819335938\n",
      "Epoch 9 \t Batch 880 \t Training Loss: 2606.4060014204547\n",
      "Epoch 9 \t Batch 900 \t Training Loss: 2603.419546576606\n",
      "Epoch 9 \t Batch 920 \t Training Loss: 2601.6037070896314\n",
      "Epoch 9 \t Batch 940 \t Training Loss: 2600.3573672030834\n",
      "Epoch 9 \t Batch 960 \t Training Loss: 2599.530148188273\n",
      "Epoch 9 \t Batch 980 \t Training Loss: 2600.9463285485094\n",
      "Epoch 9 \t Batch 1000 \t Training Loss: 2600.3462401123047\n",
      "Epoch 9 \t Batch 1020 \t Training Loss: 2598.3902985217524\n",
      "Epoch 9 \t Batch 1040 \t Training Loss: 2599.38530085637\n",
      "Epoch 9 \t Batch 1060 \t Training Loss: 2600.9650155697227\n",
      "Epoch 9 \t Batch 1080 \t Training Loss: 2600.3754559552226\n",
      "Epoch 9 \t Batch 1100 \t Training Loss: 2601.4786954012784\n",
      "Epoch 9 \t Batch 1120 \t Training Loss: 2602.026592581613\n",
      "Epoch 9 \t Batch 1140 \t Training Loss: 2601.4347604851973\n",
      "Epoch 9 \t Batch 1160 \t Training Loss: 2602.8470422152814\n",
      "Epoch 9 \t Batch 1180 \t Training Loss: 2604.144237970902\n",
      "Epoch 9 \t Batch 1200 \t Training Loss: 2601.4888029988606\n",
      "Epoch 9 \t Batch 1220 \t Training Loss: 2599.3012610263513\n",
      "Epoch 9 \t Batch 1240 \t Training Loss: 2598.3337272397935\n",
      "Epoch 9 \t Batch 1260 \t Training Loss: 2597.810860188802\n",
      "Epoch 9 \t Batch 1280 \t Training Loss: 2597.0159302711486\n",
      "Epoch 9 \t Batch 1300 \t Training Loss: 2598.311932748648\n",
      "Epoch 9 \t Batch 1320 \t Training Loss: 2596.9728331594756\n",
      "Epoch 9 \t Batch 1340 \t Training Loss: 2598.0633045708955\n",
      "Epoch 9 \t Batch 1360 \t Training Loss: 2599.872080185834\n",
      "Epoch 9 \t Batch 1380 \t Training Loss: 2599.7520969556726\n",
      "Epoch 9 \t Batch 1400 \t Training Loss: 2597.944868687221\n",
      "Epoch 9 \t Batch 1420 \t Training Loss: 2596.8274549027565\n",
      "Epoch 9 \t Batch 1440 \t Training Loss: 2597.5728422376847\n",
      "Epoch 9 \t Batch 20 \t Validation Loss: 214.25248680114746\n",
      "Epoch 9 \t Batch 40 \t Validation Loss: 365.42179718017576\n",
      "Epoch 9 \t Batch 60 \t Validation Loss: 342.8765213012695\n",
      "Epoch 9 \t Batch 80 \t Validation Loss: 333.29885215759276\n",
      "Epoch 9 \t Batch 100 \t Validation Loss: 492.28565216064453\n",
      "Epoch 9 \t Batch 120 \t Validation Loss: 753.883571879069\n",
      "Epoch 9 \t Batch 140 \t Validation Loss: 866.6167615618025\n",
      "Epoch 9 \t Batch 160 \t Validation Loss: 1000.5662052154541\n",
      "Epoch 9 \t Batch 180 \t Validation Loss: 988.2427052815755\n",
      "Epoch 9 \t Batch 200 \t Validation Loss: 1256.8342907714843\n",
      "Epoch 9 \t Batch 220 \t Validation Loss: 1351.9219010786576\n",
      "Epoch 9 \t Batch 240 \t Validation Loss: 1372.9013380686442\n",
      "Epoch 9 \t Batch 260 \t Validation Loss: 1394.942117661696\n",
      "Epoch 9 \t Batch 280 \t Validation Loss: 1421.7540701184953\n",
      "Epoch 9 \t Batch 300 \t Validation Loss: 1398.4302119445802\n",
      "Epoch 9 \t Batch 320 \t Validation Loss: 1522.3122790932655\n",
      "Epoch 9 Training Loss: 2596.6821559114674 Validation Loss: 1522.3122790932655\n",
      "Validation Loss Decreased(513468.422454834--->487139.92930984497) Saving The Model\n",
      "Epoch 9 completed\n",
      "Epoch 10 \t Batch 20 \t Training Loss: 2653.208447265625\n",
      "Epoch 10 \t Batch 40 \t Training Loss: 2684.5066009521483\n",
      "Epoch 10 \t Batch 60 \t Training Loss: 2725.99936319987\n",
      "Epoch 10 \t Batch 80 \t Training Loss: 2682.22861328125\n",
      "Epoch 10 \t Batch 100 \t Training Loss: 2656.6042687988283\n",
      "Epoch 10 \t Batch 120 \t Training Loss: 2615.755661010742\n",
      "Epoch 10 \t Batch 140 \t Training Loss: 2607.113025774275\n",
      "Epoch 10 \t Batch 160 \t Training Loss: 2596.0681755065916\n",
      "Epoch 10 \t Batch 180 \t Training Loss: 2596.6496812608507\n",
      "Epoch 10 \t Batch 200 \t Training Loss: 2589.225770263672\n",
      "Epoch 10 \t Batch 220 \t Training Loss: 2601.6377385919745\n",
      "Epoch 10 \t Batch 240 \t Training Loss: 2589.7261032104493\n",
      "Epoch 10 \t Batch 260 \t Training Loss: 2576.636293851412\n",
      "Epoch 10 \t Batch 280 \t Training Loss: 2580.370771135603\n",
      "Epoch 10 \t Batch 300 \t Training Loss: 2574.195457356771\n",
      "Epoch 10 \t Batch 320 \t Training Loss: 2565.440891265869\n",
      "Epoch 10 \t Batch 340 \t Training Loss: 2563.2893884995406\n",
      "Epoch 10 \t Batch 360 \t Training Loss: 2567.862146674262\n",
      "Epoch 10 \t Batch 380 \t Training Loss: 2569.1763042249177\n",
      "Epoch 10 \t Batch 400 \t Training Loss: 2574.010427246094\n",
      "Epoch 10 \t Batch 420 \t Training Loss: 2574.166688174293\n",
      "Epoch 10 \t Batch 440 \t Training Loss: 2573.5874525590375\n",
      "Epoch 10 \t Batch 460 \t Training Loss: 2579.1751395847486\n",
      "Epoch 10 \t Batch 480 \t Training Loss: 2579.40415242513\n",
      "Epoch 10 \t Batch 500 \t Training Loss: 2574.1933205566406\n",
      "Epoch 10 \t Batch 520 \t Training Loss: 2577.1386138916014\n",
      "Epoch 10 \t Batch 540 \t Training Loss: 2581.8237347638164\n",
      "Epoch 10 \t Batch 560 \t Training Loss: 2582.830667768206\n",
      "Epoch 10 \t Batch 580 \t Training Loss: 2581.622036848397\n",
      "Epoch 10 \t Batch 600 \t Training Loss: 2583.3618145751952\n",
      "Epoch 10 \t Batch 620 \t Training Loss: 2586.0310840237526\n",
      "Epoch 10 \t Batch 640 \t Training Loss: 2583.928015899658\n",
      "Epoch 10 \t Batch 660 \t Training Loss: 2585.2623982747396\n",
      "Epoch 10 \t Batch 680 \t Training Loss: 2585.7753674675437\n",
      "Epoch 10 \t Batch 700 \t Training Loss: 2586.563225969587\n",
      "Epoch 10 \t Batch 720 \t Training Loss: 2587.5670554267035\n",
      "Epoch 10 \t Batch 740 \t Training Loss: 2590.9262810784417\n",
      "Epoch 10 \t Batch 760 \t Training Loss: 2590.7506354081\n",
      "Epoch 10 \t Batch 780 \t Training Loss: 2591.125648850661\n",
      "Epoch 10 \t Batch 800 \t Training Loss: 2591.021499328613\n",
      "Epoch 10 \t Batch 820 \t Training Loss: 2590.4836016399104\n",
      "Epoch 10 \t Batch 840 \t Training Loss: 2587.7322501046315\n",
      "Epoch 10 \t Batch 860 \t Training Loss: 2588.5719456872275\n",
      "Epoch 10 \t Batch 880 \t Training Loss: 2586.155986993963\n",
      "Epoch 10 \t Batch 900 \t Training Loss: 2585.189831000434\n",
      "Epoch 10 \t Batch 920 \t Training Loss: 2589.290304963485\n",
      "Epoch 10 \t Batch 940 \t Training Loss: 2586.4481837495846\n",
      "Epoch 10 \t Batch 960 \t Training Loss: 2585.470972442627\n",
      "Epoch 10 \t Batch 980 \t Training Loss: 2585.664105598294\n",
      "Epoch 10 \t Batch 1000 \t Training Loss: 2586.1204771728517\n",
      "Epoch 10 \t Batch 1020 \t Training Loss: 2585.70709288354\n",
      "Epoch 10 \t Batch 1040 \t Training Loss: 2584.730708195613\n",
      "Epoch 10 \t Batch 1060 \t Training Loss: 2585.849434445939\n",
      "Epoch 10 \t Batch 1080 \t Training Loss: 2585.0826846652562\n",
      "Epoch 10 \t Batch 1100 \t Training Loss: 2584.614500066584\n",
      "Epoch 10 \t Batch 1120 \t Training Loss: 2586.0075128827775\n",
      "Epoch 10 \t Batch 1140 \t Training Loss: 2584.0975620202853\n",
      "Epoch 10 \t Batch 1160 \t Training Loss: 2582.175703798491\n",
      "Epoch 10 \t Batch 1180 \t Training Loss: 2582.077464992717\n",
      "Epoch 10 \t Batch 1200 \t Training Loss: 2582.722911376953\n",
      "Epoch 10 \t Batch 1220 \t Training Loss: 2581.5353757764474\n",
      "Epoch 10 \t Batch 1240 \t Training Loss: 2581.1444426505795\n",
      "Epoch 10 \t Batch 1260 \t Training Loss: 2580.6275681268603\n",
      "Epoch 10 \t Batch 1280 \t Training Loss: 2582.0226157188417\n",
      "Epoch 10 \t Batch 1300 \t Training Loss: 2580.660807729868\n",
      "Epoch 10 \t Batch 1320 \t Training Loss: 2580.1343722256747\n",
      "Epoch 10 \t Batch 1340 \t Training Loss: 2580.7451588189424\n",
      "Epoch 10 \t Batch 1360 \t Training Loss: 2580.43143714456\n",
      "Epoch 10 \t Batch 1380 \t Training Loss: 2581.116825623443\n",
      "Epoch 10 \t Batch 1400 \t Training Loss: 2580.6974699183875\n",
      "Epoch 10 \t Batch 1420 \t Training Loss: 2581.000413147832\n",
      "Epoch 10 \t Batch 1440 \t Training Loss: 2581.388029225667\n",
      "Epoch 10 \t Batch 20 \t Validation Loss: 295.39055786132815\n",
      "Epoch 10 \t Batch 40 \t Validation Loss: 366.73431701660155\n",
      "Epoch 10 \t Batch 60 \t Validation Loss: 368.9825008392334\n",
      "Epoch 10 \t Batch 80 \t Validation Loss: 352.6966519355774\n",
      "Epoch 10 \t Batch 100 \t Validation Loss: 547.2249649810791\n",
      "Epoch 10 \t Batch 120 \t Validation Loss: 915.0567406336467\n",
      "Epoch 10 \t Batch 140 \t Validation Loss: 1055.0245568956648\n",
      "Epoch 10 \t Batch 160 \t Validation Loss: 1191.0344385623932\n",
      "Epoch 10 \t Batch 180 \t Validation Loss: 1185.6841208987767\n",
      "Epoch 10 \t Batch 200 \t Validation Loss: 1498.0926768875122\n",
      "Epoch 10 \t Batch 220 \t Validation Loss: 1628.1450728329746\n",
      "Epoch 10 \t Batch 240 \t Validation Loss: 1665.060968430837\n",
      "Epoch 10 \t Batch 260 \t Validation Loss: 1711.2904206789458\n",
      "Epoch 10 \t Batch 280 \t Validation Loss: 1728.7402173995972\n",
      "Epoch 10 \t Batch 300 \t Validation Loss: 1692.0403367360434\n",
      "Epoch 10 \t Batch 320 \t Validation Loss: 1775.3013079881669\n",
      "Epoch 10 Training Loss: 2579.659942563857 Validation Loss: 1775.3013079881669\n",
      "Epoch 10 completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from dataloader import *\n",
    "\n",
    "model = SimpleFCN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.latlon = True\n",
    "        self.bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "        self.bm = True\n",
    "        self.patch_size = [15,15]\n",
    "        self.norm_strat = 'pct'\n",
    "        self.norm = False\n",
    "\n",
    "args = Args()\n",
    "fnames = ['data_nonan_0-5.h5', 'data_nonan_1-5.h5', 'data_nonan_2-5.h5', 'data_nonan_3-5.h5', 'data_nonan_4-5.h5']\n",
    "mode = 'train'\n",
    "ds_training = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "mode = 'val'\n",
    "ds_validation = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch in range(10):  # 10 epochs\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(\"inputs.shape: \", inputs.shape)\n",
    "        # print(\"targets.shape: \", targets.shape)\n",
    "        # # # print(outputs)\n",
    "        # print(\"outputs.shape: \", outputs.shape)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    i=0\n",
    "    model.eval()\n",
    "    for inputs, targets in validloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "        valid_loss += loss.item()\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    " \n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
