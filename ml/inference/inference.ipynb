{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions from the dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def encode_lat_lon(lat, lon) :\n",
    "    \"\"\"\n",
    "    Encode the latitude and longitude into sin/cosine values. We use a simple WRAP positional encoding, as \n",
    "    Mac Aodha et al. (2019).\n",
    "\n",
    "    Args:\n",
    "    - lat (float): the latitude\n",
    "    - lon (float): the longitude\n",
    "\n",
    "    Returns:\n",
    "    - (lat_cos, lat_sin, lon_cos, lon_sin) (tuple): the sin/cosine values for the latitude and longitude\n",
    "    \"\"\"\n",
    "\n",
    "    # The latitude goes from -90 to 90\n",
    "    lat_cos, lat_sin = np.cos(np.pi * lat / 90), np.sin(np.pi * lat / 90)\n",
    "    # The longitude goes from -180 to 180\n",
    "    lon_cos, lon_sin = np.cos(np.pi * lon / 180), np.sin(np.pi * lon / 180)\n",
    "\n",
    "    # Now we put everything in the [0,1] range\n",
    "    lat_cos, lat_sin = (lat_cos + 1) / 2, (lat_sin + 1) / 2\n",
    "    lon_cos, lon_sin = (lon_cos + 1) / 2, (lon_sin + 1) / 2\n",
    "\n",
    "    return lat_cos, lat_sin, lon_cos, lon_sin\n",
    "\n",
    "\n",
    "def encode_coords(central_lat, central_lon, patch_size, resolution = 10) :\n",
    "    \"\"\" \n",
    "    This function computes the latitude and longitude of a patch, from the latitude and longitude of its central pixel.\n",
    "    It then encodes these values into sin/cosine values, and scales the results to [0,1].\n",
    "\n",
    "    Args:\n",
    "    - central_lat (float): the latitude of the central pixel\n",
    "    - central_lon (float): the longitude of the central pixel\n",
    "    - patch_size (tuple): the size of the patch\n",
    "    - resolution (int): the resolution of the patch\n",
    "\n",
    "    Returns:\n",
    "    - (lat_cos, lat_sin, lon_cos, lon_sin) (tuple): the sin/cosine values for the latitude and longitude\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize arrays to store latitude and longitude coordinates\n",
    "\n",
    "    i_indices, j_indices = np.indices(patch_size)\n",
    "\n",
    "    # Calculate the distance offset in meters for each pixel\n",
    "    offset_lat = (i_indices - patch_size[0] // 2) * resolution\n",
    "    offset_lon = (j_indices - patch_size[1] // 2) * resolution\n",
    "\n",
    "    # Calculate the latitude and longitude for each pixel\n",
    "    latitudes = central_lat + (offset_lat / 6371000) * (180 / np.pi)\n",
    "    longitudes = central_lon + (offset_lon / 6371000) * (180 / np.pi) / np.cos(central_lat * np.pi / 180)\n",
    "\n",
    "    lat_cos, lat_sin, lon_cos, lon_sin = encode_lat_lon(latitudes, longitudes)\n",
    "\n",
    "    return lat_cos, lat_sin, lon_cos, lon_sin\n",
    "\n",
    "\n",
    "def normalize_data(data, norm_values, norm_strat, nodata_value = None) :\n",
    "    \"\"\"\n",
    "    Normalize the data, according to various strategies:\n",
    "    - mean_std: subtract the mean and divide by the standard deviation\n",
    "    - pct: subtract the 1st percentile and divide by the 99th percentile\n",
    "    - min_max: subtract the minimum and divide by the maximum\n",
    "\n",
    "    Args:\n",
    "    - data (np.array): the data to normalize\n",
    "    - norm_values (dict): the normalization values\n",
    "    - norm_strat (str): the normalization strategy\n",
    "\n",
    "    Returns:\n",
    "    - normalized_data (np.array): the normalized data\n",
    "    \"\"\"\n",
    "\n",
    "    if norm_strat == 'mean_std' :\n",
    "        mean, std = norm_values['mean'], norm_values['std']\n",
    "        if nodata_value is not None :\n",
    "            data = np.where(data == nodata_value, 0, (data - mean) / std)\n",
    "        else : data = (data - mean) / std\n",
    "\n",
    "    elif norm_strat == 'pct' :\n",
    "        p1, p99 = norm_values['p1'], norm_values['p99']\n",
    "        if nodata_value is not None :\n",
    "            data = np.where(data == nodata_value, 0, (data - p1) / (p99 - p1))\n",
    "        else :\n",
    "            data = (data - p1) / (p99 - p1)\n",
    "        data = np.clip(data, 0, 1)\n",
    "\n",
    "    elif norm_strat == 'min_max' :\n",
    "        min_val, max_val = norm_values['min'], norm_values['max']\n",
    "        if nodata_value is not None :\n",
    "            data = np.where(data == nodata_value, 0, (data - min_val) / (max_val - min_val))\n",
    "        else:\n",
    "            data = (data - min_val) / (max_val - min_val)\n",
    "    \n",
    "    else: \n",
    "        raise ValueError(f'Normalization strategy `{norm_strat}` is not valid.')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_bands(bands_data, norm_values, order, norm_strat, nodata_value = None) :\n",
    "    \"\"\"\n",
    "    This function normalizes the bands data using the normalization values and strategy.\n",
    "\n",
    "    Args:\n",
    "    - bands_data (np.array): the bands data to normalize\n",
    "    - norm_values (dict): the normalization values\n",
    "    - order (list): the order of the bands\n",
    "    - norm_strat (str): the normalization strategy\n",
    "    - nodata_value (int/float): the nodata value\n",
    "\n",
    "    Returns:\n",
    "    - bands_data (np.array): the normalized bands data\n",
    "    \"\"\"\n",
    "    normalized = {}\n",
    "    for i, band in enumerate(order) :\n",
    "        if band != 'SCL' and band != 'transform':\n",
    "            print('normalizing ', band)\n",
    "            band_norm = norm_values[band]\n",
    "            # print(band_norm)\n",
    "            # print(bands_data[band].shape)\n",
    "            normalized[band] = normalize_data(bands_data[band], band_norm, norm_strat, nodata_value)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the s2 zip files and the icesat cropped mosaic (with code from create_patches.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Extracting patches for product 52VEL.\n",
      ">> Found /scratch3/Siberia/S2A_MSIL2A_20200804T024551_N9999_R132_T52VEL_20240217T142025.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import traceback\n",
    "import geopandas as gpd\n",
    "from zipfile import ZipFile\n",
    "from shutil import rmtree\n",
    "sys.path.insert(1, '/scratch2/biomass_estimation/code/patches')\n",
    "from helper_patches import *\n",
    "from create_patches import *\n",
    "path_icesat = '/scratch2/biomass_estimation/code/notebook/cropped_mosaic_no_nan'\n",
    "tilenames = 'tile_names_inference.txt'\n",
    "path_shp = os.path.join('/scratch2', 'biomass_estimation', 'code', 'notebook', 'S2_tiles_Siberia_polybox', 'S2_tiles_Siberia_all.geojson')\n",
    "path_s2 = '/scratch3/Siberia'\n",
    "\n",
    "# Read the Sentinel-2 grid shapefile\n",
    "grid_df = gpd.read_file(path_shp, engine = 'pyogrio')\n",
    "\n",
    "# List all S2 tiles and their geometries\n",
    "tile_names, tile_geoms = list_s2_tiles(tilenames, grid_df, path_s2)\n",
    "\n",
    "for s2_prod in tile_names:\n",
    "    print(f'>> Extracting patches for product {s2_prod}.')\n",
    "    # Get the filename that includes s2_prod in the path_s2 folder\n",
    "    total_s2_path = glob.glob(f\"{path_s2}/*{s2_prod}*\")[0]\n",
    "\n",
    "    # Extract the folder and the filename separately\n",
    "    s2_folder_path, s2_file_name = os.path.split(total_s2_path)\n",
    "\n",
    "    print(f'>> Found {total_s2_path}.')\n",
    "\n",
    "    # Unzip the S2 L2A product if it hasn't been done\n",
    "    total_unzipped_path = total_s2_path[:-4] + '.SAFE'\n",
    "    if not os.path.exists(total_unzipped_path):\n",
    "        try:\n",
    "            with ZipFile(total_s2_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(path_s2)\n",
    "        except Exception as e:\n",
    "            print(f'>> Could not unzip {s2_prod}.')\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    # Reproject and upsample the S2 bands            \n",
    "    try: \n",
    "        transform, upsampling_shape, processed_bands, crs_2, bounds = process_S2_tile(product=s2_file_name[:-4], path_s2 = s2_folder_path)\n",
    "    except IndexError:\n",
    "        s2_folder_path = os.path.join(path_s2, 'scratch2', 'gsialelli', 'S2_L2A', 'Siberia')\n",
    "        total_unzipped_path = os.path.join(s2_folder_path, s2_file_name[:-4] + '.SAFE')\n",
    "        try: \n",
    "            transform, upsampling_shape, processed_bands, crs_2, bounds = process_S2_tile(product=s2_file_name[:-4], path_s2 = s2_folder_path)\n",
    "        except Exception as e:\n",
    "            print(f'>> Could not process product {s2_prod}.')\n",
    "            print(traceback.format_exc())\n",
    "            continue\n",
    "    icesat_raw = load_BM_data(path_bm=path_icesat, tile_name=s2_prod)\n",
    "\n",
    "\n",
    "    # Remove the unzipped S2 product\n",
    "    rmtree(total_unzipped_path)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize both icesat and s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_bands keys  dict_keys(['B02', 'B03', 'B04', 'B08', 'B05', 'B06', 'B07', 'B8A', 'B11', 'B12', 'SCL', 'B01', 'B09'])\n",
      "norm_values[S2_bands] keys dict_keys(['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12'])\n",
      "dict_keys(['S2_bands', 'BM', 'Sentinel_metadata', 'GEDI'])\n",
      "normalizing  bm\n",
      "normalizing  std\n",
      "normalizing  B01\n",
      "normalizing  B02\n",
      "normalizing  B03\n",
      "normalizing  B04\n",
      "normalizing  B05\n",
      "normalizing  B06\n",
      "normalizing  B07\n",
      "normalizing  B08\n",
      "normalizing  B09\n",
      "normalizing  B11\n",
      "normalizing  B12\n",
      "normalizing  B8A\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "norm_path = '/scratch2/biomass_estimation/code/ml/data'\n",
    "with open(join(norm_path, 'normalization_values.pkl'), mode = 'rb') as f:\n",
    "            norm_values = pickle.load(f)\n",
    "print(\"processed_bands keys \", processed_bands.keys())\n",
    "print(\"norm_values[S2_bands] keys\", norm_values['S2_bands'].keys()) #we don't have SCL in the normalization values so not in the model...?\n",
    "print(norm_values.keys())\n",
    "\n",
    "norm_strat = \"mean_std\"\n",
    "icesat_order = sorted(list(icesat_raw.keys()))\n",
    "icesat_norm = normalize_bands(icesat_raw, norm_values['BM'], icesat_order, norm_strat, nodata_value = -9999.0)\n",
    "\n",
    "s2_order = sorted(list(processed_bands.keys()))\n",
    "s2_bands_dict = normalize_bands(processed_bands, norm_values['S2_bands'], s2_order, norm_strat, nodata_value = 0)\n",
    "s2_indices = [s2_order.index(band) for band in s2_bands_dict]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a np array from a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "(10980, 10980, 12)\n"
     ]
    }
   ],
   "source": [
    "s2_bands = np.stack([s2_bands_dict[key] for key in s2_bands_dict.keys()], axis=-1)\n",
    "s2_bands = s2_bands[:, :, s2_indices]\n",
    "print(s2_bands.dtype)\n",
    "print(s2_bands.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upsample icesat to s2 resolution (code from create_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icesat_raw keys  dict_keys(['bm', 'std', 'transform'])\n",
      "processed_bands keys  dict_keys(['B02', 'B03', 'B04', 'B08', 'B05', 'B06', 'B07', 'B8A', 'B11', 'B12', 'SCL', 'B01', 'B09'])\n",
      "s2 bands shape  (10980, 10980)\n",
      "icesat_raw bands shape  (3612, 3612)\n",
      "icesat_raw transform: \n",
      " | 30.41, 0.00, 499955.10|\n",
      "| 0.00,-30.41, 6600021.21|\n",
      "| 0.00, 0.00, 1.00|\n",
      "icesat bands shape after upsampling (10980, 10980)\n"
     ]
    }
   ],
   "source": [
    "print(\"icesat_raw keys \" , icesat_raw.keys())\n",
    "print(\"processed_bands keys \", processed_bands.keys())\n",
    "print(\"s2 bands shape \",s2_bands_dict['B01'].shape)\n",
    "print(\"icesat_raw bands shape \", icesat_norm['bm'].shape)\n",
    "print(\"icesat_raw transform: \\n\", icesat_raw['transform']) #what should we do with this??\n",
    "icesat = {}\n",
    "icesat['bm'] = upsampling_with_nans(icesat_norm['bm'], s2_bands_dict['B01'].shape, -9999, 3)\n",
    "icesat['std'] = upsampling_with_nans(icesat_norm['std'], s2_bands_dict['B01'].shape, -9999, 3)\n",
    "\n",
    "print(\"icesat bands shape after upsampling\", icesat['bm'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=18,\n",
    "                 channel_dims = (16, 32, 64, 128),\n",
    "                 num_outputs=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=1):\n",
    "        \"\"\"\n",
    "        A simple fully convolutional neural network.\n",
    "        \"\"\"\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        layers = list()\n",
    "        for i in range(len(channel_dims)):\n",
    "            in_channels = in_features if i == 0 else channel_dims[i-1]\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=channel_dims[i], \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(num_features=channel_dims[i]))\n",
    "            layers.append(self.relu)\n",
    "        print(layers)\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(in_channels=channel_dims[-1], out_channels=num_outputs, kernel_size=1,\n",
    "                                     stride=1, padding=0, bias=True)\n",
    "        # self.fc = nn.Linear(15*15*num_outputs, 1)  # Fully connected layer to get a single output value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_output(x)\n",
    "        # x = x.flatten(start_dim=1)\n",
    "        # predictions = self.fc(x)\n",
    "        # return predictions.squeeze()  # Remove the extra dimension\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2 transform\n",
      "| 10.00, 0.00, 499980.00|\n",
      "| 0.00,-10.00, 6600000.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "S2  EPSG:32652\n",
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n",
      "15\n",
      "(15, 15, 12)\n",
      "(15, 15, 1)\n",
      "(15, 15, 1)\n",
      "(15, 15, 1)\n",
      "(15, 15, 1)\n",
      "(15, 15, 1)\n",
      "(15, 15, 1)\n",
      "7\n",
      "tensor([[[[5.5139e+35, 8.6832e+35, 1.1441e+36, 1.1593e+36, 1.1866e+36,\n",
      "           1.2026e+36, 1.2205e+36, 1.2377e+36, 1.2523e+36, 1.2622e+36,\n",
      "           1.2672e+36, 1.2004e+36, 9.7543e+35, 8.9112e+35, 6.1301e+35],\n",
      "          [7.4248e+35, 1.0860e+36, 1.4280e+36, 1.3758e+36, 1.3786e+36,\n",
      "           1.3974e+36, 1.4160e+36, 1.4314e+36, 1.4429e+36, 1.4495e+36,\n",
      "           1.4513e+36, 1.1703e+36, 7.4722e+35, 7.7343e+35, 8.8041e+35],\n",
      "          [7.9910e+35, 1.2412e+36, 1.4772e+36, 1.3548e+36, 1.3877e+36,\n",
      "           1.4072e+36, 1.4245e+36, 1.4377e+36, 1.4468e+36, 1.4499e+36,\n",
      "           1.4501e+36, 1.1980e+36, 7.7064e+35, 1.0461e+36, 1.3190e+36],\n",
      "          [8.7688e+35, 1.4012e+36, 1.4885e+36, 1.3743e+36, 1.3908e+36,\n",
      "           1.4097e+36, 1.4287e+36, 1.4457e+36, 1.4587e+36, 1.4665e+36,\n",
      "           1.4690e+36, 1.2100e+36, 7.7544e+35, 9.0505e+35, 1.2524e+36],\n",
      "          [9.4255e+35, 1.5504e+36, 1.5985e+36, 1.4146e+36, 1.4335e+36,\n",
      "           1.4525e+36, 1.4725e+36, 1.4918e+36, 1.5079e+36, 1.5178e+36,\n",
      "           1.5213e+36, 1.2993e+36, 7.0820e+35, 8.7495e+35, 1.2545e+36],\n",
      "          [9.5645e+35, 1.5763e+36, 1.6242e+36, 1.4465e+36, 1.4684e+36,\n",
      "           1.4888e+36, 1.5105e+36, 1.5312e+36, 1.5483e+36, 1.5583e+36,\n",
      "           1.5601e+36, 1.3321e+36, 7.2520e+35, 8.9016e+35, 1.2701e+36],\n",
      "          [9.7620e+35, 1.6083e+36, 1.6547e+36, 1.4778e+36, 1.5018e+36,\n",
      "           1.5247e+36, 1.5487e+36, 1.5706e+36, 1.5870e+36, 1.5961e+36,\n",
      "           1.5972e+36, 1.3608e+36, 7.4461e+35, 9.0757e+35, 1.2901e+36],\n",
      "          [9.9802e+35, 1.6406e+36, 1.6843e+36, 1.5009e+36, 1.5288e+36,\n",
      "           1.5539e+36, 1.5788e+36, 1.6000e+36, 1.6145e+36, 1.6215e+36,\n",
      "           1.6212e+36, 1.3796e+36, 7.6247e+35, 9.2555e+35, 1.3160e+36],\n",
      "          [1.0174e+36, 1.6673e+36, 1.7079e+36, 1.5138e+36, 1.5445e+36,\n",
      "           1.5709e+36, 1.5958e+36, 1.6156e+36, 1.6281e+36, 1.6339e+36,\n",
      "           1.6332e+36, 1.3871e+36, 7.7528e+35, 9.4247e+35, 1.3434e+36],\n",
      "          [1.0314e+36, 1.6849e+36, 1.7234e+36, 1.5165e+36, 1.5475e+36,\n",
      "           1.5738e+36, 1.5982e+36, 1.6175e+36, 1.6293e+36, 1.6345e+36,\n",
      "           1.6323e+36, 1.3854e+36, 7.7894e+35, 9.5433e+35, 1.3678e+36],\n",
      "          [1.0391e+36, 1.6929e+36, 1.7300e+36, 1.5094e+36, 1.5411e+36,\n",
      "           1.5667e+36, 1.5903e+36, 1.6086e+36, 1.6192e+36, 1.6233e+36,\n",
      "           1.6204e+36, 1.3776e+36, 7.7470e+35, 9.6164e+35, 1.3881e+36],\n",
      "          [1.1649e+36, 1.5752e+36, 1.5383e+36, 1.3048e+36, 1.3383e+36,\n",
      "           1.3575e+36, 1.3737e+36, 1.3848e+36, 1.3908e+36, 1.3934e+36,\n",
      "           1.3931e+36, 1.4892e+36, 6.9515e+35, 9.4607e+35, 1.5749e+36],\n",
      "          [8.4844e+35, 1.1276e+36, 1.1817e+36, 1.0703e+36, 1.0618e+36,\n",
      "           1.0750e+36, 1.0906e+36, 1.1066e+36, 1.1224e+36, 1.1338e+36,\n",
      "           1.1405e+36, 1.2057e+36, 6.4059e+35, 8.8063e+35, 1.7230e+36],\n",
      "          [3.3515e+35, 8.7108e+35, 9.5462e+35, 8.9506e+35, 9.6746e+35,\n",
      "           9.8366e+35, 1.0080e+36, 1.0350e+36, 1.0610e+36, 1.0809e+36,\n",
      "           1.0976e+36, 1.2476e+36, 1.3396e+36, 1.6577e+36, 2.7613e+36],\n",
      "          [7.7735e+35, 1.2892e+36, 1.5001e+36, 1.4559e+36, 1.4597e+36,\n",
      "           1.4883e+36, 1.5248e+36, 1.5624e+36, 1.5951e+36, 1.6175e+36,\n",
      "           1.6274e+36, 1.8457e+36, 1.9001e+36, 1.4171e+36, 2.4896e+36]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from affine import Affine\n",
    "from pyproj import Transformer\n",
    "import torch\n",
    "print(\"S2 transform\")\n",
    "print(transform)\n",
    "print(\"S2 \", crs_2)\n",
    "fwd = Affine.from_gdal(transform[2], transform[0], transform[1], transform[5], transform[3], transform[4])\n",
    "\n",
    "coordinate_transformer = Transformer.from_crs(crs_2, 'epsg:4326')\n",
    "\n",
    "model = SimpleFCN()\n",
    "model.load_state_dict(torch.load('/scratch2/biomass_estimation/code/ml/saved_model2.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for i in range(7, 10980, 15):\n",
    "    for j in range(7, 10980, 15):\n",
    "                \n",
    "        data = []\n",
    "        s2_temp = s2_bands[i-7:i+8, j-7:j+8,:]\n",
    "        data.extend([s2_temp])\n",
    "        print(len(s2_temp))\n",
    "        \n",
    "\n",
    "        lat1, lon1 = fwd * (i, j)\n",
    "        #print(lat1, lon1)\n",
    "        lat2, lon2 = coordinate_transformer.transform(lat1, lon1)\n",
    "        #print(lat2, lon2)\n",
    "        lat_cos, lat_sin, lon_cos, lon_sin = encode_coords(lat2, lon2, (15, 15))\n",
    "        data.extend([lat_cos[..., np.newaxis], lat_sin[..., np.newaxis], lon_cos[..., np.newaxis], lon_sin[..., np.newaxis]])\n",
    "\n",
    "        icesat_temp_bm = icesat['bm'][i-7:i+8, j-7:j+8, np.newaxis]\n",
    "        icesat_temp_std = icesat['std'][i-7:i+8, j-7:j+8, np.newaxis]\n",
    "        data.extend([icesat_temp_bm, icesat_temp_std])\n",
    "        for i in range(len(data)):\n",
    "            print(data[i].shape)\n",
    "        print(len(data))\n",
    "        # Concatenate the data together\n",
    "        data = torch.from_numpy(np.concatenate(data, axis = -1).swapaxes(-1, 0)).to(torch.float)\n",
    "        outputs = model(data.unsqueeze(0))\n",
    "        print(outputs)\n",
    "        break\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
